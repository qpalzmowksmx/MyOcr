IT Certification Guaranteed, The Easy Way!
Exam : SAA-C03-KR
Title : Amazon AWS Certified
Solutions Architect -
Associate (SAA-C03
Korean Version)
Vendor : Amazon
Version : V21.65
1

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 1
한 회사가 AWS에서 전자상거래 웹사이트의 프로토타입을 만들고 있습니다. 이 웹사이트는
Application Load Balancer, 웹 서버용 Amazon EC2 인스턴스의 Auto Scaling 그룹, Single-AZ
구성으로 실행되는 MySQL DB 인스턴스용 Amazon RDS로 구성되어 있습니다.
제품 카탈로그 검색 시 웹사이트의 응답이 느립니다. 제품 카탈로그는 회사에서 자주
사용하지 않는 MySQL 데이터베이스의 테이블 그룹입니다. 솔루션 아키텍트는 제품 카탈로그
검색 시 DB 인스턴스의 CPU 사용률이 높다는 것을 확인했습니다.
솔루션 아키텍트는 제품 카탈로그 검색 중 웹사이트 성능을 개선하기 위해 무엇을 권장해야
할까요?
A. 제품 카탈로그를 Amazon Redshift 데이터베이스로 마이그레이션합니다. COPY 명령을
사용하여 제품 카탈로그 테이블을 로드합니다.
B. 제품 카탈로그를 캐시하기 위해 Amazon ElastiCache for Redis 클러스터를 구현합니다.
지연 로딩을 사용하여 캐시를 채웁니다.
C. 데이터베이스 응답이 느릴 때 추가 EC2 인스턴스를 시작하기 위해 자동 크기 조정 그룹에
추가적인 크기 조정 정책을 추가합니다.
D. DB 인스턴스에 대한 Multi-AZ 구성을 켭니다. EC2 인스턴스를 구성하여 데이터베이스로
전송되는 제품 카탈로그 쿼리를 제한합니다.
Answer: B
Explanation:
* Requirement Analysis: The product catalog search is causing high CPU utilization on the
MySQL DB instance, slowing down the website.
* ElastiCache Overview: Amazon ElastiCache for Redis can be used to cache frequently
accessed data, reducing load on the database.
* Lazy Loading: This caching strategy loads data into the cache only when it is requested,
improving response times for repeated queries.
* Implementation:
* Set up an ElastiCache for Redis cluster.
* Modify the application to check the cache before querying the database.
* Use lazy loading to populate the cache on cache misses.
* Conclusion: This approach reduces database load and improves website performance
during product catalog searches.
References
* Amazon ElastiCache: ElastiCache Documentation
* Caching Strategies: ElastiCache Caching Strategies
QUESTION NO: 2
전자상거래 애플리케이션은 Amazon EC2 인스턴스에서 실행되는 PostgreSQL
데이터베이스를 사용합니다. 월별 판매 이벤트 중에 데이터베이스 사용량이 증가하고
애플리케이션에 대한 데이터베이스 연결 문제가 발생합니다. 후속 월별 판매 이벤트에 대한
트래픽은 예측할 수 없으며 이는 판매 예측에 영향을 미칩니다. 회사는 예측할 수 없는 트래픽
증가가 있을 때 성능을 유지해야 합니다.
가장 비용 효과적인 방법으로 이 문제를 해결하는 솔루션은 무엇입니까?
A. PostgreSQL 데이터베이스를 Amazon Aurora Serverless v2로 마이그레이션합니다.
B. 증가된 사용량을 수용하기 위해 EC2 인스턴스에서 PostgreSQL 데이터베이스에 대한 자동
크기 조정을 활성화합니다.
2

IT Certification Guaranteed, The Easy Way!
C. 더 큰 인스턴스 유형을 사용하여 PostgreSQL 데이터베이스를 PostgreSQL용 Amazon
RDS로 마이그레이션합니다.
D. 증가된 사용량을 수용하기 위해 PostgreSQL 데이터베이스를 Amazon Redshift로
마이그레이션합니다.
Answer: A
Explanation:
Amazon Aurora Serverless v2 is a cost-effective solution that can automatically scale the
database capacity up and down based on the application's needs. It can handle
unpredictable traffic spikes without requiring any provisioning or management of database
instances. It is compatible with PostgreSQL and offers high performance, availability, and
durability1. References: 1: AWS Ramp-Up Guide: Architect2, page 312: AWS Certified
Solutions Architect - Associate exam guide3, page 9.
QUESTION NO: 3
한 회사에서 온프레미스 데이터 센터에서 AWS 클라우드의 새로운 VPC로 새로운
애플리케이션을 마이그레이션하고 있습니다.
이 회사는 여러 서브넷과 애플리케이션을 공유하는 여러 AWS 계정과 VPC를 보유하고
있습니다. 이 회사는 새로운 애플리케이션에 대한 세분화된 액세스 제어를 원합니다. 이
회사는 새로운 애플리케이션에 액세스할 수 있는 권한이 부여된 계정과 VPC의 모든 네트워크
리소스가 애플리케이션에 액세스할 수 있도록 보장하고자 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 새 애플리케이션 VPC에 액세스해야 하는 각 VPC에 대해 VPC 피어링 연결을 설정합니다.
각 VPC에서 경로 테이블을 업데이트하여 연결을 활성화합니다.
B. 새 애플리케이션을 호스팅하는 계정에 전송 게이트웨이를 배포합니다. 애플리케이션에
연결해야 하는 각 계정과 전송 게이트웨이를 공유합니다. 새 애플리케이션을 호스팅하는
VPC와 전송 게이트웨이에서 경로 테이블을 업데이트하여 연결을 활성화합니다.
C. AWS PrivateLink 엔드포인트 서비스를 사용하여 다른 VPC에서 새 애플리케이션에
액세스할 수 있도록 합니다.
엔드포인트 정책을 사용하여 애플리케이션에 대한 액세스를 제어합니다.
D. 애플리케이션 로드 밸런서(ALB)를 사용하여 새 애플리케이션을 인터넷에 노출합니다.
인증 및 권한 부여 프로세스를 구성하여 지정된 VPC만 애플리케이션에 액세스할 수 있도록
합니다.
Answer: B
Explanation:
* A. VPC peering: Creates a fully meshed architecture, which is complex to manage for
multiple VPCs.
* B. Transit gateway: Simplifies network management by connecting multiple VPCs and on-
premises networks via a central hub.
* C. PrivateLink: Restricts communication to the application endpoint but may not allow full
VPC connectivity.
* D. ALB with internet exposure: Not secure or specific to private network communication.
References: AWS Transit Gateway
QUESTION NO: 4
회사에는 회사 데이터 센터에서 us-east-1 지역의 VPC까지 AWS Direct Connect 연결이
3

IT Certification Guaranteed, The Easy Way!
있습니다. 이 회사는 최근 온프레미스 데이터 센터와 eu-west-2 지역 간에 여러 개의 VPC와
Direct Connect 연결을 갖춘 회사를 인수했습니다. 회사와 회사의 VPC에 대한 CIDR 블록은
중복되지 않습니다. 회사에서는 두 지역과 데이터 센터 간의 연결이 필요합니다. 회사에는
운영 오버헤드를 줄이면서 확장 가능한 솔루션이 필요합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. us-east-1의 VPC와 eu-west-2의 VPC 간에 리전 간 VPC 피어링을 설정합니다.
B. us-east-1의 Direct Connect 연결에서 eu-west-2의 VPC로 프라이빗 가상 인터페이스를
생성합니다.
C. Amazon EC2에서 호스팅하는 완전 메시형 VPN 네트워크에 VPN 어플라이언스를
설정합니다. AWS VPN CloudHub를 사용하여 데이터 센터와 각 VPC 간에 데이터를 보내고
받습니다.
D. 기존 Direct Connect 연결을 Direct Connect 게이트웨이에 연결합니다. 각 리전에 있는
VPC의 가상 프라이빗 게이트웨이에서 Direct Connect 게이트웨이로 트래픽을 라우팅합니다.
Answer: D
Explanation:
This solution meets the requirements because it allows the company to use a single Direct
Connect connection to connect to multiple VPCs in different Regions using a Direct Connect
gateway. A Direct Connect gateway is a globally available resource that enables you to
connect your on-premises network to VPCs in any AWS Region, except the AWS China
Regions. You can associate a Direct Connect gateway with a transit gateway or a virtual
private gateway in each Region. By routing traffic from the virtual private gateways of the
VPCs to the Direct Connect gateway, you can enable inter-Region and on-premises
connectivity for your VPCs.
This solution is scalable because you can add more VPCs in different Regions to the Direct
Connect gateway without creating additional connections. This solution also reduces
operational overhead because you do not need to manage multiple VPN appliances, VPN
connections, or VPC peering connections.
References:
* Direct Connect gateways
* Inter-Region VPC peering
QUESTION NO: 5
회사의 마케팅 데이터는 여러 소스에서 Amazon S3 버킷으로 업로드됩니다. 일련의 데이터
준비 작업은 보고를 위해 데이터를 집계합니다. 데이터 준비 작업은 정기적인 간격으로
병렬로 실행되어야 합니다. 몇 가지 작업은 나중에 특정 순서로 실행해야 합니다. 작업 오류
처리 재시도 논리 및 상태 관리의 운영 오버헤드를 제거하려고 합니다. 이러한 요구 사항을
충족하는 솔루션은 무엇입니까?
A. AWS Lambda 함수를 사용하여 데이터가 S3 버킷에 업로드되는 즉시 데이터를
처리합니다. 정기적으로 예약된 간격으로 다른 Lambda 함수를 호출합니다.
B. Amazon Athena를 사용하여 데이터 처리 Amazon EventBndge Scheduler를 사용하여 일반
내부 프로세스에서 Athena를 호출합니다.
C. AWS Glue DataBrew를 사용하여 데이터 처리 AWS Step Functions 상태 머신을 사용하여
DataBrew 데이터 준비 작업 실행
D. AWS Data Pipeline을 사용하여 데이터를 처리합니다. 자정에 한 번 데이터를 처리하도록
데이터 파이프라인을 예약합니다.
4

IT Certification Guaranteed, The Easy Way!
Answer: C
Explanation:
AWS Glue DataBrew is a visual data preparation tool that allows you to easily clean,
normalize, and transform your data without writing any code. You can create and run data
preparation jobs on your data stored in Amazon S3, Amazon Redshift, or other data sources.
AWS Step Functions is a service that lets you coordinate multiple AWS services into
serverless workflows. You can use Step Functions to orchestrate your DataBrew jobs, define
the order and parallelism of execution, handle errors and retries, and monitor the state of
your workflow. By using AWS Glue DataBrew and AWS Step Functions, you can meet the
requirements of the company with minimal operational overhead, as you do not need to write
any code, manage any servers, or deal with complex dependencies.
References:
* AWS Glue DataBrew
* AWS Step Functions
* Orchestrate AWS Glue DataBrew jobs using AWS Step Functions
QUESTION NO: 6
한 회사는 CIFS 및 NFS 파일 공유를 위해 기본 AWS 지역에서 NetApp ONTAP용 Amazon
FSx를 사용합니다.
Amazon EC2 인스턴스에서 실행되는 애플리케이션은 파일 공유에 액세스합니다. 회사는
보조 지역에 스토리지 재해 복구(OR) 솔루션이 필요합니다. 보조 리전에 복제된 데이터는
기본 리전과 동일한 프로토콜을 사용하여 액세스해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Lambda 함수를 생성하여 데이터를 Amazon S3 버킷에 복사합니다. S3 버킷(또는
보조 리전)을 복제합니다.
B. AWS Backup을 사용하여 FSx for ONTAP 볼륨의 백업을 생성합니다. 볼륨을 보조 리전에
복사합니다. 백업에서 ONTAP 인스턴스용 새 FSx를 생성합니다.
C. 보조 지역에 FSx for ONTAP 인스턴스를 생성합니다. NetApp SnapMirror를 사용하여 기본
지역에서 보조 지역으로 데이터를 복제합니다.
D. Amazon Elastic File System(Amazon EFS) 볼륨을 생성합니다. 현재 데이터를 볼륨으로
마이그레이션합니다. 볼륨을 보조 리전에 복제합니다.
Answer: C
Explanation:
* Understanding the Requirement: The company needs a disaster recovery solution for FSx
for NetApp ONTAP in a secondary region, accessible using the same protocols (CIFS and
NFS).
* Analysis of Options:
* Lambda Function and S3: Involves copying data to S3, which changes the access protocols
and increases operational overhead.
* AWS Backup: Suitable for backup and restore but not for real-time or near-real-time
replication for disaster recovery.
* FSx for ONTAP with SnapMirror: SnapMirror provides efficient replication between ONTAP
instances, maintaining access protocols and requiring minimal operational overhead.
* Amazon EFS: Does not support CIFS and requires migrating data, increasing complexity
and changing access protocols.
5

IT Certification Guaranteed, The Easy Way!
* Best Solution:
* FSx for ONTAP with SnapMirror: This solution ensures seamless disaster recovery with the
same access protocols and minimal operational overhead.
References:
* Amazon FSx for NetApp ONTAP
* NetApp SnapMirror
QUESTION NO: 7
회사는 Amazon S3 버킷에 PDF 형식으로 데이터를 저장합니다. 회사는 모든 신규 데이터와
기존 데이터를 Amazon S3에 7년 동안 보관해야 하는 법적 요구 사항을 따라야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. S3 버킷에 대한 S3 Versionmg 기능을 활성화합니다. S3 수명 주기를 구성하여 7년 후
데이터를 삭제합니다. 모든 S3 객체에 대해 다단계 인증(MFA) 삭제를 구성합니다.
B. S3 버킷에 대해 거버넌스 보존 모드로 S3 객체 잠금을 설정합니다. 보존 기간이 7년 후에
만료되도록 설정합니다. 모든 기존 개체를 다시 복사하여 기존 데이터를 규정에 맞게
가져옵니다.
C. S3 버킷에 대해 규정 준수 보존 모드로 S3 객체 잠금을 켭니다. 7년 후 만료되도록 보존
기간을 설정합니다. 모든 기존 개체를 다시 복사하여 기존 데이터를 규정에 맞게 가져옵니다.
D. S3 버킷에 대해 규정 준수 보존 모드로 S3 객체 잠금을 켭니다. 7년 후 만료되도록 보존
기간을 설정합니다. S3 배치 작업을 사용하여 기존 데이터를 규정 준수
Answer: C
Explanation:
S3 Object Lock enables a write-once-read-many (WORM) model for objects stored in
Amazon S3. It can help prevent objects from being deleted or overwritten for a fixed amount
of time or indefinitely1. S3 Object Lock has two retention modes: governance mode and
compliance mode. Compliance mode provides the highest level of protection and prevents
any user, including the root user, from deleting or modifying an object version until the
retention period expires. To use S3 Object Lock, a new bucket with Object Lock enabled
must be created, and a default retention period can be optionally configured for objects
placed in the bucket2.
To bring existing objects into compliance, they must be recopied into the bucket with a
retention period specified.
Option A is incorrect because S3 Versioning and S3 Lifecycle do not provide WORM
protection for objects.
Moreover, MFA delete only applies to deleting object versions, not modifying them.
Option B is incorrect because governance mode allows users with special permissions to
override or remove the retention settings or delete the object if necessary. This does not
meet the legal requirement of retaining all data for 7 years.
Option D is incorrect because S3 Batch Operations cannot be used to apply compliance
mode retention periods to existing objects. S3 Batch Operations can only apply governance
mode retention periods or legal holds. Reference URL: 2:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-console.html
3: https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-
dynamic-data-access
4: https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html 1:
6

IT Certification Guaranteed, The Easy Way!
https://docs.aws.
amazon.com/AmazonS3/latest/userguide/object-lock.html :
https://docs.aws.amazon.com/AmazonS3/latest
/userguide/object-lock-overview.html :
https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock- managing.html :
https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-
and- s3-access-points/
QUESTION NO: 8
회사는 이벤트 기반 주문 처리 시스템을 설계하고 있습니다. 각 주문에는 주문이 생성된 후
여러 검증 단계가 필요합니다. 독립적인 AWS Lambda 함수가 각 검증 단계를 수행합니다. 각
검증 단계는 다른 검증 단계와 독립적입니다. 개별 검증 단계에는 주문 이벤트 정보의 하위
집합만 필요합니다.
회사는 각 검증 단계 Lambda 함수가 함수에 필요한 주문 이벤트의 정보에만 액세스할 수
있기를 원합니다. 주문 처리 시스템의 구성 요소는 향후 비즈니스 변경을 수용할 수 있도록
느슨하게 결합되어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 각 검증 단계에 대해 Amazon Simple Queue Service(Amazon SQS> 대기열을 생성합니다.
주문 데이터를 각 검증 단계에 필요한 형식으로 변환하고 메시지를 적절한 SQS 대기열에
게시하는 새로운 Lambda 함수를 생성합니다. 각 검증 단계 Lambda 함수를 해당 SQS
대기열에 구독합니다.
B. Amazon Simple 알림 서비스 {Amazon SNS) 주제를 생성합니다. SNS 주제에 대한 검증
단계 Lambda 함수를 구독합니다. 메시지 본문 필터링을 사용하여 구독한 각 Lambda 함수에
필요한 데이터만 보냅니다.
C. Amazon EventBridge 이벤트 버스를 생성합니다. 각 검증 단계에 대한 이벤트 규칙 생성 각
대상 검증 단계 Lambda 함수에 필요한 데이터만 전송하도록 입력 변환기를 구성합니다.
D. Amazon Simple Queue Service {Amazon SQS) 대기열을 생성합니다. SQS 대기열을
구독하고 주문 데이터를 각 검증 단계에 필요한 형식으로 변환하는 새로운 Lambda 함수를
생성합니다. 새로운 Lambda 함수를 사용하여 별도의 스레드에서 병렬로 검증 단계 Lambda
함수의 동기식 호출을 수행합니다.
Answer: C
Explanation:
* Understanding the Requirement: The order processing system requires multiple
independent validation steps, each handled by separate Lambda functions, with each
function accessing only the subset of order information it needs. The system should be
loosely coupled to accommodate future changes.
* Analysis of Options:
* Amazon SQS with a new Lambda function for transformation: This involves additional
complexity in creating and managing multiple SQS queues and an extra Lambda function for
data transformation.
* Amazon SNS with message filtering: While SNS supports message filtering, it is more
suited for pub/sub messaging patterns rather than event-driven processing requiring fine-
grained control over the data sent to each function.
* Amazon EventBridge with input transformers: EventBridge is designed for event-driven
architectures, allowing for fine-grained control with input transformers that can modify and
7

IT Certification Guaranteed, The Easy Way!
filter the event data sent to each target Lambda function, ensuring each function receives
only the necessary information.
* SQS with synchronous Lambda invocations: This approach adds unnecessary complexity
with synchronous invocations and is not ideal for an event-driven, loosely coupled
architecture.
* Best Solution:
* Amazon EventBridge with input transformers: This option provides the most flexible,
scalable, and loosely coupled architecture, enabling each Lambda function to receive only
the required subset of data.
References:
* Amazon EventBridge
* EventBridge Input Transformer
QUESTION NO: 9
한 회사에 동일한 AWS 리전의 Amazon S3 버킷에서 대량의 데이터를 읽고 쓰는 서비스가
있습니다. 서비스는 VPC의 프라이빗 서브넷 내의 Amazon EC2 인스턴스에 배포됩니다. 이
서비스는 퍼블릭 서브넷의 NAT 게이트웨이를 통해 Amazon S3와 통신합니다. 그러나 회사는
데이터 출력 비용을 줄일 수 있는 솔루션을 원합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 퍼블릭 서브넷에 전용 EC2 NAT 인스턴스를 프로비저닝합니다. 이 인스턴스의 탄력적
네트워크 인터페이스를 모든 S3 트래픽의 대상으로 사용하도록 프라이빗 서브넷의 라우팅
테이블을 구성합니다.
B. 프라이빗 서브넷에 전용 EC2 NAT 인스턴스를 프로비저닝합니다. 이 인스턴스의 탄력적
네트워크 인터페이스를 모든 S3 트래픽의 대상으로 사용하도록 퍼블릭 서브넷의 라우팅
테이블을 구성합니다.
C. VPC 게이트웨이 엔드포인트를 프로비저닝합니다. 게이트웨이 엔드포인트를 모든 S3
트래픽의 경로로 사용하도록 프라이빗 서브넷의 라우팅 테이블을 구성합니다.
D. 두 번째 NAT 게이트웨이를 프로비저닝합니다. 이 NAT 게이트웨이를 모든 S3 트래픽의
대상으로 사용하도록 프라이빗 서브넷의 라우팅 테이블을 구성합니다.
Answer: C
Explanation:
it allows the company to reduce the data output costs for accessing Amazon S3 from
Amazon EC2 instances in a VPC. By provisioning a VPC gateway endpoint, the company
can enable private connectivity between the VPC and S3. By configuring the route table for
the private subnet to use the gateway endpoint as the route for all S3 traffic, the company
can avoid using a NAT gateway, which charges for data processing and data transfer.
References:
* VPC Endpoints for Amazon S3
* VPC Endpoints Pricing
QUESTION NO: 10
한 회사가 수요 급증에 맞춰 확장하고 주문된 프로세스를 처리해야 하는 소셜 미디어
애플리케이션을 개발하고 있습니다.
어떤 AWS 서비스가 이러한 요구 사항을 충족합니까?
A. 분리를 위한 Fargate, RDS, SQS를 갖춘 ECS.
8

IT Certification Guaranteed, The Easy Way!
B. 분리를 위한 Fargate, RDS, SNS를 갖춘 ECS.
C. DynamoDB, Lambda, DynamoDB Streams 및 Step Functions.
D. 분리를 위한 Elastic Beanstalk, RDS 및 SNS.
Answer: A
* Option A combines ECS with Fargate for scalability, RDS for relational data, and SQS for
decoupling with message ordering (FIFO queues).
* Option B uses SNS, which does not maintain message order.
* Option C is suitable for serverless workflows but not relational data.
* Option D relies on Elastic Beanstalk, which offers less flexibility for scaling.
QUESTION NO: 11
회사에서 Amazon S3 버킷으로 데이터를 마이그레이션할 계획입니다. 데이터는 S3 버킷
내에서 암호화되어야 합니다. 암호화 키는 매년 자동으로 교체되어야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 데이터를 S3 버킷으로 마이그레이션합니다. Amazon S3 관리형 키(SSE-S3)로 서버 측
암호화를 사용합니다.
SSE-S3의 내장 키 순환 동작을 사용하세요.
암호화 키.
B. AWS Key Management Service(AWS KMS) 고객 관리형 키 생성 자동 키 교체 활성화 고객
관리형 KMS 키를 사용하도록 S3 버킷의 기본 암호화 동작을 설정합니다. 데이터를 S3
버킷으로 마이그레이션합니다.
C. AWS Key Management Service(AWS KMS) 고객 관리형 키 생성 고객 관리형 KMS 키를
사용하도록 S3 버킷의 기본 암호화 동작을 설정합니다. 데이터를 S3 버킷으로
마이그레이션합니다.
매년 KMS 키를 수동으로 순환합니다.
D. 고객 키 자료를 사용하여 데이터를 암호화합니다. 데이터를 S3 버킷으로
마이그레이션합니다. 키 자료 없이 AWS Key Management Service(AWS KMS) 키 생성 고객
키 자료를 KMS 키로 가져옵니다. 자동 키 순환을 활성화합니다.
Answer: B
Explanation:
* Understanding the Requirement: The data must be encrypted at rest with automatic key
rotation every year, with minimal operational overhead.
* Analysis of Options:
* SSE-S3: This option provides encryption with S3 managed keys and automatic key rotation
but offers less control and flexibility compared to KMS keys.
* AWS KMS with Customer Managed Key (automatic rotation): This option offers full control
over encryption keys, with AWS KMS handling automatic key rotation, minimizing operational
overhead.
* AWS KMS with Customer Managed Key (manual rotation): This requires manual
intervention for key rotation, increasing operational overhead.
* Customer Key Material: This involves more complex management, including importing key
material and setting up automatic rotation, which increases operational overhead.
* Best Option for Minimal Operational Overhead:
* AWS KMS with a customer managed key and automatic rotation provides the needed
security and key rotation with minimal operational effort. Setting the S3 bucket's default
9

IT Certification Guaranteed, The Easy Way!
encryption to use this key ensures all data is encrypted as required.
References:
* AWS Key Management Service (KMS)
* Amazon S3 default encryption
QUESTION NO: 12
회사는 보험 견적을 처리하는 웹 애플리케이션을 설계하기 위해 AWS를 사용하고 있습니다.
사용자는 애플리케이션에서 견적을 요청할 것입니다. 견적은 견적 유형별로 구분되어야 하고,
24시간 이내에 응답해야 하며, 분실되어서는 안 됩니다. 솔루션은 운영 효율성을 극대화하고
유지보수를 최소화해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 견적 유형을 기반으로 여러 Amazon Kinesis 데이터 스트림 생성 적절한 데이터 스트림으로
메시지를 보내도록 웹 애플리케이션 구성 KCL(Kinesis 클라이언트 라이브러리)을 사용하여
자체 데이터에서 메시지를 풀링하도록 애플리케이션 서버의 각 백엔드 그룹 구성 개울
B. 각 견적 유형에 대해 AWS Lambda 함수 및 Amazon Simple 알림 서비스(Amazon SNS)
주제를 생성합니다. 연결된 SNS 주제에 대한 Lambda 함수를 구독합니다. 적절한 SNS
주제에 대한 견적 요청을 게시하도록 애플리케이션을 구성합니다.
C. 단일 Amazon Simple Notification Service(Amazon SNS) 주제 생성 SNS 주제에 대한
Amazon Simple Queue Service(Amazon SQS) 대기열 구독 견적 유형에 따라 적절한 SQS
대기열에 메시지를 게시하도록 SNS 메시지 필터링 구성 각 백엔드 구성 자체 SQS 대기열을
사용하는 애플리케이션 서버
D. 견적 유형을 기반으로 여러 Amazon Kinesis Data Firehose 전송 스트림을 생성하여
Amazon Elasucsearch Service(Amazon ES) 클러스터에 데이터 스트림을 전달합니다. 적절한
전송 스트림으로 메시지를 보내도록 애플리케이션을 구성합니다. 애플리케이션 서버의 각
백엔드 그룹을 다음과 같이 구성합니다. Amazon ES에서 메시지를 검색하고 그에 따라
처리합니다.
Answer: C
Explanation:
https://aws.amazon.com/getting-started/hands-on/filter-messages-published-to-topics/
QUESTION NO: 13
회사는 Amazon EC2 인스턴스의 비용을 최적화해야 합니다. 또한 회사는 2~3개월마다 EC2
인스턴스의 유형과 제품군을 변경해야 합니다.
이러한 요구 사항을 충족하려면 회사는 무엇을 해야 합니까?
A. 3년 기간 동안 부분 선결제 예약 인스턴스를 구매합니다.
B. 1년 기간 동안 선불 컴퓨팅 비용 절감 플랜을 구매하지 마세요.
C. 1년 기간 동안 모든 선결제 예약 인스턴스를 구매합니다.
D. 1년 기간 동안 전체 선결제 EC2 인스턴스 절감 플랜을 구매하세요.
Answer: B
Explanation:
* Understanding the Requirements: The company needs to optimize costs and has the
flexibility to change EC2 instance types and families frequently (every 2-3 months).
* Savings Plans Overview: Savings Plans offer significant savings over On-Demand pricing,
with the flexibility to use any instance type and family within a region.
* No Upfront Compute Savings Plan: This plan allows for cost optimization without any
upfront payment, offering flexibility to change instance types and families.
10

IT Certification Guaranteed, The Easy Way!
* Term Selection: A 1-year term is appropriate for balancing cost savings and flexibility given
the frequent changes in instance types.
* Conclusion: A No Upfront Compute Savings Plan for a 1-year term provides the needed
flexibility and cost savings without the commitment and inflexibility of Reserved Instances.
References
* AWS Savings Plans: AWS Savings Plans
* AWS Cost Management Documentation: AWS Cost Management
QUESTION NO: 14
한 회사가 VPC에 2계층 웹 애플리케이션을 배포하고 있습니다. 웹 계층은 여러 가용 영역에
걸쳐 있는 퍼블릭 서브넷이 있는 Amazon EC2 Auto Scaling 그룹을 사용하고 있습니다.
데이터베이스 계층은 별도의 프라이빗 서브넷에 있는 MySQL용 Amazon RDS DB 인스턴스로
구성됩니다. 웹 계층에서는 제품 정보를 검색하기 위해 데이터베이스에 액세스해야 합니다.
웹 애플리케이션이 의도한 대로 작동하지 않습니다. 웹 애플리케이션이 데이터베이스에
연결할 수 없다고 보고합니다. 데이터베이스가 작동되어 실행 중인 것으로 확인되었습니다.
네트워크 ACL에 대한 모든 구성입니다. 보안 그룹 및 라우팅 테이블은 여전히 ​​기본
상태입니다.
솔루션 설계자는 애플리케이션을 수정하기 위해 무엇을 권장해야 합니까?
A. 웹 계층의 EC2 인스턴스로부터의 트래픽을 허용하도록 프라이빗 서브넷의 네트워크
ACL에 명시적인 규칙을 추가합니다.
B. 웹 계층의 EC2 인스턴스와 Ihe 데이터베이스 계층 간의 트래픽을 허용하도록 VPC 라우팅
테이블에 경로를 추가합니다.
C. 웹 계층의 EC2 인스턴스와 데이터베이스 계층의 RDS 인스턴스를 두 개의 개별 VPC에
배포합니다. VPC 피어링을 구성합니다.
D. 데이터베이스 계층 RDS 인스턴스의 보안 그룹에 인바운드 규칙을 추가하여 웹 계층 보안
그룹의 트래픽을 허용합니다.
Answer: D
Explanation:
This answer is correct because it allows the web tier to access the database tier by using
security groups as a source, which is a recommended best practice for VPC connectivity.
Security groups are stateful and can reference other security groups in the same VPC, which
simplifies the configuration and maintenance of the firewall rules. By adding an inbound rule
to the database tier's security group, the web tier's EC2 instances can connect to the RDS
instance on port 3306, regardless of their IP addresses or subnets.
References:
* Security groups - Amazon Virtual Private Cloud
* Best practices and reference architectures for VPC design
QUESTION NO: 15
솔루션 아키텍트는 사용자가 등록 양식을 작성하고 제출하는 데 도움이 되는 애플리케이션을
설계하고 있습니다. 솔루션 아키텍트는 웹 애플리케이션 서버 계층과 작업자 계층을 포함하는
2계층 아키텍처를 사용할 계획입니다.
애플리케이션은 제출된 양식을 빠르게 처리해야 합니다. 애플리케이션은 각 양식을 정확히 한
번만 처리해야 합니다. 솔루션은 데이터가 손실되지 않도록 해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
11

IT Certification Guaranteed, The Easy Way!
A. 웹 애플리케이션 서버 계층과 작업자 계층 사이에 Amazon Simple Queue Service
{Amazon SQS) FIFO 대기열을 사용하여 양식 데이터를 저장하고 전달합니다.
B. 웹 애플리케이션 서버 계층과 작업자 계층 간에 Amazon API Gateway HTTP API를
사용하여 양식 데이터를 저장하고 전달합니다.
C. 웹 애플리케이션 서버 계층과 작업자 계층 사이에 Amazon Simple Queue Service(Amazon
SQS) 표준 대기열을 사용하여 양식 데이터를 저장하고 전달합니다.
D. AWS Step Functions 워크플로를 사용합니다. 웹 애플리케이션 서버 계층과 작업자 계층
간에 양식 데이터를 저장하고 전달하는 동기 워크플로를 만듭니다.
Answer: A
Explanation:
To process each form exactly once and ensure no data is lost, using an Amazon SQS FIFO
(First-In-First-Out) queue is the most appropriate solution. SQS FIFO queues guarantee that
messages are processed in the exact order they are sent and ensure that each message is
processed exactly once. This ensures data consistency and reliability, both of which are
crucial for processing user-submitted forms without data loss.
SQS acts as a buffer between the web application server and the worker tier, ensuring that
submitted forms are stored reliably and forwarded to the worker tier for processing. This also
decouples the application, improving its scalability and resilience.
* Option B (API Gateway): API Gateway is better suited for API management rather than
acting as a message queue for form processing.
* Option C (SQS Standard Queue): While SQS Standard queues offer high throughput, they
do not guarantee exactly-once processing or the strict ordering needed for this use case.
* Option D (Step Functions): Step Functions are useful for orchestrating workflows but add
unnecessary complexity for simple message queuing and form processing.
AWS References:
* Amazon SQS FIFO Queues
* Decoupling Application Tiers Using Amazon SQS
QUESTION NO: 16
디지털 이미지 처리 회사가 온프레미스 모놀리식 애플리케이션을 AWS 클라우드로
마이그레이션하려고 합니다. 이 회사는 수천 개의 이미지를 처리하고 처리 워크플로의 일부로
대용량 파일을 생성합니다.
이 회사는 점점 늘어나는 이미지 처리 작업을 관리할 솔루션이 필요합니다. 이 솔루션은 또한
이미지 처리 워크플로에서 수동 작업을 줄여야 합니다. 이 회사는 솔루션의 기본 인프라를
관리하고 싶어하지 않습니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. Amazon EC2 Spot Instances와 함께 Amazon Elastic Container Service(Amazon ECS)를
사용하여 이미지를 처리합니다. Amazon Simple Queue Service(Amazon SQS)를 구성하여
워크플로를 조정합니다. 처리된 파일을 Amazon Elastic File System(Amazon EFS)에
저장합니다.
B. AWS Batch 작업을 사용하여 이미지를 처리합니다. AWS Step Functions를 사용하여
워크플로를 조정합니다.
처리된 파일을 Amazon S3 버킷에 저장합니다.
C. AWS Lambda 함수와 Amazon EC2 Spot Instances를 사용하여 이미지를 처리합니다.
처리된 파일을 Amazon FSx에 저장합니다.
12

IT Certification Guaranteed, The Easy Way!
D. 이미지를 처리하기 위해 Amazon EC2 인스턴스 그룹을 배포합니다. AWS Step
Functions를 사용하여 워크플로를 조정합니다. 처리된 파일을 Amazon Elastic Block
Store(Amazon EBS) 볼륨에 저장합니다.
Answer: B
Explanation:
For processing thousands of images and generating large files while minimizing manual
tasks and operational overhead, using AWS Batch is the best solution. AWS Batch allows
you to run large-scale, parallel, and managed batch computing jobs without needing to
manage the underlying infrastructure.
* AWS Batch: Automates the image processing jobs, dynamically allocating the necessary
resources based on the job requirements, which reduces operational overhead.
* AWS Step Functions: Orchestrates the entire image processing workflow, ensuring that
tasks are executed in the correct sequence, improving manageability.
* Amazon S3: Stores the processed files, providing scalable and cost-effective storage.
* Option A (ECS with EC2 Spot Instances): While cost-effective, managing ECS and Spot
Instances involves more operational effort.
* Option C (Lambda with EC2 Spot): Lambda functions have size and duration limitations,
making them less suited for large image processing tasks.
* Option D (EC2 with Step Functions): Managing EC2 instances involves more overhead than
using AWS Batch.
AWS References:
* AWS Batch
* AWS Step Functions
QUESTION NO: 17
한 회사가 AWS 클라우드에 웹 애플리케이션을 보유하고 있으며 거래 데이터를 실시간으로
수집하려고 합니다. 회사에서는 데이터 중복을 방지하고 인프라 관리를 원하지 않습니다.
회사는 데이터가 수집된 후 해당 데이터에 대해 추가 처리를 수행하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon Simple Queue Service(Amazon SOS) FIFO 대기열을 구성합니다. 데이터를
처리하기 위해 FIFO 대기열에 대한 이벤트 소스 매핑으로 AWS Lambda 함수를 구성합니다.
B. Amazon Simple Queue Service(Amazon SQS) FIFO 대기열 구성 AWS Batch 작업을
사용하여 대기열에서 중복 데이터 제거 데이터를 처리하도록 AWS Lambda 함수를
구성합니다.
C. Amazon Kinesis Data Streams를 사용하여 중복 데이터를 제거하는 AWS Batch 작업으로
수신 트랜잭션 데이터를 보냅니다. 사용자 지정 스크립트를 실행하여 데이터를 처리하는
Amazon EC2 인스턴스를 시작합니다.
D. 중복 데이터를 제거하기 위해 수신 트랜잭션 데이터를 AWS Lambda 함수로 보내도록
AWS Step Functions 상태 머신을 설정합니다. 데이터를 처리하기 위해 사용자 지정
스크립트를 실행하는 Amazon EC2 인스턴스를 시작합니다.
Answer: A
Explanation:
* Understanding the Requirement: The company needs to collect transaction data in real
time, avoid data duplication, and perform additional processing without managing
infrastructure.
13

IT Certification Guaranteed, The Easy Way!
* Analysis of Options:
* SQS FIFO Queue with Lambda: Ensures data is processed in order and prevents
duplication.
Lambda handles processing without the need to manage servers.
* SQS FIFO Queue with AWS Batch: While this ensures no duplicates, it introduces
additional complexity and management overhead with AWS Batch.
* Kinesis Data Streams with AWS Batch and EC2: Involves more components and
infrastructure management, which is against the requirement of not wanting to manage
infrastructure.
* Step Functions with Lambda and EC2: Involves setting up multiple services and still
requires managing EC2 instances, increasing complexity.
* Best Solution:
* SQS FIFO Queue with Lambda: This combination ensures real-time data processing,
prevents duplication, and minimizes infrastructure management, meeting all requirements
efficiently.
References:
* Amazon SQS FIFO Queues
* AWS Lambda and SQS Integration
QUESTION NO: 18
개발팀이 다른 회사와 협력하여 통합 제품을 만들고 있습니다. 다른 회사는 개발 팀의 계정에
포함된 Amazon Simple Queue Service(Amazon SQS) 대기열에 액세스해야 합니다. 다른
회사는 자신의 계정 권한을 포기하지 않고 대기열을 폴링하려고 합니다.
솔루션 설계자는 SQS 대기열에 대한 액세스를 어떻게 제공해야 합니까?
A. SQS 대기열에 대한 다른 회사 액세스를 제공하는 인스턴스 프로파일을 생성합니다.
B. SQS 대기열에 대한 다른 회사 액세스를 제공하는 오전 1시 정책을 만듭니다.
C. SQS 대기열에 대한 다른 회사 액세스를 제공하는 SQS 액세스 정책을 만듭니다.
D. 다른 회사에 SQS 대기열에 대한 액세스를 제공하는 Amazon Simple Notification
Service(Amazon SNS) 액세스 정책을 생성합니다.
Answer: C
Explanation:
To provide access to the SQS queue to the other company without giving up its own account
permissions, a solutions architect should create an SQS access policy that provides the other
company access to the SQS queue. An SQS access policy is a resource-based policy that
defines who can access the queue and what actions they can perform. The policy can specify
the AWS account ID of the other company as a principal, and grant permissions for actions
such as sqs:ReceiveMessage, sqs:DeleteMessage, and sqs:
GetQueueAttributes. This way, the other company can poll the queue using its own
credentials, without needing to assume a role or use cross-account access keys. References
:
* Using identity-based policies (IAM policies) for Amazon SQS
* Using custom policies with the Amazon SQS access policy language
QUESTION NO: 19
회사는 Amazon EC2 인스턴스에서 애플리케이션을 실행합니다. 회사는 AWS 비용에 대해
14

IT Certification Guaranteed, The Easy Way!
정기적인 재무 평가를 수행합니다. 회사는 최근 비정상적인 지출을 확인했습니다.
회사는 비정상적인 지출을 방지하기 위한 솔루션이 필요합니다. 솔루션은 비용을
모니터링하고 비정상적인 지출이 발생할 경우 책임 있는 이해관계자에게 알려야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Budgets 템플릿을 사용하여 지출 없는 예산 생성
B. AWS Billing and Cost Management 콘솔에서 AWS 비용 이상 탐지 모니터를 생성합니다.
C. 현재 실행 중인 워크로드 가격 세부 정보에 대한 CreateAWS 가격 계산기 추정치_
D. Amazon CloudWatch를 사용하여 비용을 모니터링하고 비정상적인 지출을 식별합니다.
Answer: B
Explanation:
it allows the company to monitor costs and notify responsible stakeholders in the event of
unusual spending.
By creating an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost
Management console, the company can use a machine learning service that automatically
detects and alerts on anomalous spend. By configuring alert thresholds, notification
preferences, and root cause analysis, the company can prevent unusual spending and
identify its source. References:
* AWS Cost Anomaly Detection
* Creating a Cost Anomaly Monitor
QUESTION NO: 20
한 회사가 AWS Lake Formation을 사용하여 AWS에 데이터 분석 플랫폼을 구축하고
있습니다. 플랫폼은 Amazon S3 및 Amazon RDS와 같은 다양한 소스에서 데이터를
수집합니다. 회사에는 민감한 정보가 포함된 데이터 부분에 대한 액세스를 방지하기 위한
보안 솔루션이 필요합니다.
A. Lake Formation 테이블에 액세스할 수 있는 권한이 포함된 IAM 역할을 생성합니다.
B. 행 수준 보안과 셀 수준 보안을 구현하기 위한 데이터 필터를 만듭니다.
C. Lake Formation이 데이터를 다시 수집하기 전에 민감한 정보를 제거하는 AWS Lambda
함수를 생성합니다.
D. Lake Formation 테이블에서 민감한 정보를 정기적으로 쿼리하고 제거하는 AWS Lambda
함수를 생성합니다.
Answer: B
Explanation:
This option is the most efficient because it uses data filters, which are specifications that
restrict access to certain data in query results and engines integrated with Lake Formation1.
Data filters can be used to implement row-level security and cell-level security, which are
techniques to prevent access to portions of the data that contain sensitive information2. Data
filters can be applied when granting Lake Formation permissions on a Data Catalog table,
and can use PartiQL expressions to filter data based on conditions3. This solution meets the
requirement of providing a secure solution to prevent access to portions of the data that
contain sensitive information. Option A is less efficient because it uses an IAM role that
includes permissions to access Lake Formation tables, which is a way to grant access to
data in Lake Formation using IAM policies
4. However, this does not provide a way to prevent access to portions of the data that contain
15

IT Certification Guaranteed, The Easy Way!
sensitive information. Option C is less efficient because it uses an AWS Lambda function that
removes sensitive information before Lake Formation ingests the data, which is a way to
perform data cleansing or transformation using serverless functions. However, this could
involve significant changes to the application code and logic, and could also result in data
loss or inconsistency. Option D is less efficient because it uses an AWS Lambda function that
periodically queries and removes sensitive information from Lake Formation tables, which is
a way to perform data cleansing or transformation using serverless functions. However, this
could involve significant changes to the application code and logic, and could also result in
data loss or inconsistency.
QUESTION NO: 21
회사에는 다음과 같이 구성된 데이터 수집 워크플로가 있습니다.
새로운 데이터 전송에 대한 알림을 위한 Amazon Simple Notification Service(Amazon SNS)
주제 데이터를 처리하고 메타데이터를 기록하는 AWS Lambda 함수 회사에서는 네트워크
연결 문제로 인해 수집 워크플로가 가끔 실패하는 것을 관찰했습니다. 이러한 오류가
발생하면 회사에서 수동으로 작업을 다시 실행하지 않는 한 Lambda 함수는 해당 데이터를
수집하지 않습니다.
Lambda 함수가 향후 모든 데이터를 수집하도록 솔루션 아키텍트는 어떤 조치 조합을 취해야
합니까? (2개를 선택하세요.)
A. 여러 가용 영역에서 Lambda 함수를 구성합니다.
B. Amazon Simple Queue Service(Amazon SQS) 대기열을 생성하고 SNS 주제를
구독합니다.
C. Lambda 함수에 할당되는 CPU와 메모리를 늘립니다.
D. Lambda 함수에 대한 프로비저닝된 처리량을 늘립니다.
E. Amazon Simple Queue Service(Amazon SQS) 대기열에서 읽도록 Lambda 함수를
수정합니다.
Answer: B E
Explanation:
To ensure that the Lambda function ingests all data in the future despite occasional network
connectivity issues, the following actions should be taken:
* Create an Amazon Simple Queue Service (SQS) queue and subscribe it to the SNS topic.
This allows for decoupling of the notification and processing, so that even if the processing
Lambda function fails, the message remains in the queue for further processing later.
* Modify the Lambda function to read from the SQS queue instead of directly from SNS. This
decoupling allows for retries and fault tolerance and ensures that all messages are
processed by the Lambda function.
Reference:
AWS SNS documentation: https://aws.amazon.com/sns/
AWS SQS documentation: https://aws.amazon.com/sqs/
AWS Lambda documentation: https://aws.amazon.com/lambda/
QUESTION NO: 22
한 회사가 데이터 관리 애플리케이션을 AWS로 이전하고 있습니다. 회사는 이벤트 중심
아키텍처로 전환하려고 합니다. 아키텍처는 워크플로의 다양한 측면을 수행하면서 더욱
분산되고 서버리스 개념을 사용해야 합니다. 또한 회사는 운영 오버헤드를 최소화하려고
16

IT Certification Guaranteed, The Easy Way!
합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Glue에서 워크플로 구축 AWS Glue를 사용하여 AWS Lambda 함수를 호출하여
워크플로 슬랩을 처리합니다.
B. AWS Step Functions에서 워크플로 구축 Amazon EC2 인스턴스에 애플리케이션 배포
Step Functions를 사용하여 EC2 인스턴스에서 워크플로 단계 호출
C. Amazon EventBridge에서 워크플로를 구축합니다. EventBridge를 사용하여 일정에 따라
AWS Lambda 함수를 호출하여 워크플로 단계를 처리합니다.
D. AWS Step Functions에서 워크플로 구축 Step Functions를 사용하여 오래된 머신 생성
오래된 머신을 사용하여 AWS Lambda 함수를 호출하여 워크플로 단계 처리
Answer: D
Explanation:
This answer is correct because it meets the requirements of transitioning to an event-driven
architecture, using serverless concepts, and minimizing operational overhead. AWS Step
Functions is a serverless service that lets you coordinate multiple AWS services into
workflows using state machines. State machines are composed of tasks and transitions that
define the logic and order of execution of the workflow steps. AWS Lambda is a serverless
function-as-a-service platform that lets you run code without provisioning or managing
servers. Lambda functions can be invoked by Step Functions as tasks in a state machine,
and can perform different aspects of the data management workflow, such as data ingestion,
transformation, validation, and analysis. By using Step Functions and Lambda, the company
can benefit from the following advantages:
* Event-driven: Step Functions can trigger Lambda functions based on events, such as
timers, API calls, or other AWS service events. Lambda functions can also emit events to
other services or state machines, creating an event-driven architecture.
* Serverless: Step Functions and Lambda are fully managed by AWS, so the company does
not need to provision or manage any servers or infrastructure. The company only pays for the
resources consumed by the workflows and functions, and can scale up or down automatically
based on demand.
* Operational overhead: Step Functions and Lambda simplify the development and
deployment of workflows and functions, as they provide built-in features such as monitoring,
logging, tracing, error handling, retry logic, and security. The company can focus on the
business logic and data processing rather than the operational details.
References:
* What is AWS Step Functions?
* What is AWS Lambda?
QUESTION NO: 23
한 회사에서 PostgreSQL용 Amazon RDS를 사용하는 애플리케이션을 실행하고 있습니다.
애플리케이션은 평일 업무 시간에만 트래픽을 수신합니다. 회사는 이러한 사용량을 기반으로
비용을 최적화하고 운영 오버헤드를 줄이고 싶어합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS의 인스턴스 스케줄러를 사용하여 시작 및 중지 일정을 구성합니다.
B. 자동 백업을 끕니다. 데이터베이스의 주간 수동 스냅샷을 생성합니다.
C. 최소 CPU 사용률을 기준으로 데이터베이스를 시작 및 중지하는 사용자 지정 AWS Lambda
17

IT Certification Guaranteed, The Easy Way!
함수를 생성합니다.
D. 모든 Upfront 예약 DB 인스턴스를 구매합니다.
Answer: A
Explanation:
* Understanding the Requirement: The company wants to optimize costs and reduce
operational overhead for an RDS for PostgreSQL database that only needs to be active
during business hours on weekdays.
* Analysis of Options:
* Instance Scheduler on AWS: Allows for automated start and stop schedules based on
specified times, ideal for resources only needed during certain hours. This directly optimizes
costs by running the database only when needed.
* Turn off automatic backups and create weekly snapshots: Does not address the
requirement of reducing operational overhead and optimizing runtime costs.
* Custom Lambda function: This could work but adds unnecessary complexity compared to
using the Instance Scheduler.
* All Upfront Reserved DB Instances: While this reduces costs, it does not optimize for usage
patterns that require the database only during specific hours.
* Best Solution:
* Instance Scheduler on AWS: This option effectively manages the database runtime based
on the specified schedule, reducing costs and operational overhead.
References:
* Instance Scheduler on AWS
QUESTION NO: 24
회사는 AWS Secrets Manager에 저장된 RDS 자격 증명에 액세스해야 하는 EC2
인스턴스에서 애플리케이션을 실행합니다.
어떤 솔루션이 이 요구 사항을 충족합니까?
A. IAM 역할을 만들고 각 EC2 인스턴스 프로필에 역할을 연결합니다. ID 기반 정책을
사용하여 역할에 비밀에 대한 액세스 권한을 부여합니다.
B. IAM 사용자를 생성하고 사용자를 각 EC2 인스턴스 프로필에 연결합니다. 리소스 기반
정책을 사용하여 사용자에게 비밀에 대한 액세스 권한을 부여합니다.
C. 비밀에 대한 리소스 기반 정책을 만듭니다. EC2 Instance Connect를 사용하여 비밀에
액세스합니다.
D. 비밀에 대한 ID 기반 정책을 만듭니다. EC2 인스턴스에 직접 액세스 권한을 부여합니다.
Answer: A
* Option A uses an IAM role attached to the EC2 instance profile, enabling secure and
automated access to Secrets Manager. This is the recommended approach.
* Option B uses IAM users, which is less secure and harder to manage.
* Option C is not practical for accessing secrets programmatically.
* Option D violates best practices by granting direct access to the EC2 instance.
QUESTION NO: 25
회사에 AWS에서 호스팅되는 웹 사이트가 있습니다. 웹 사이트는 HTTP와 HTTPS를 별도로
처리하도록 구성된 ALB(Application Load Balancer) 뒤에 있습니다. 회사는 요청이 HTTPS를
사용하도록 모든 요청을 웹사이트로 전달하려고 합니다.
18

IT Certification Guaranteed, The Easy Way!
솔루션 설계자는 이 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. HTTPS 트래픽만 허용하도록 ALB의 네트워크 ACL 업데이트
B. URL의 HTTP를 HTTPS로 바꾸는 규칙을 만듭니다.
C. ALB에서 리스너 규칙을 생성하여 HTTP 트래픽을 HTTPS로 리디렉션합니다.
D. ALB를 SNI(서버 이름 표시)를 사용하도록 구성된 Network Load Balancer로 교체합니다.
Answer: C
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-https-using-
alb/ How can I redirect HTTP requests to HTTPS using an Application Load Balancer? Last
updated: 2020-10-30 I want to redirect HTTP requests to HTTPS using Application Load
Balancer listener rules. How can I do this? Resolution Reference:
https://aws.amazon.com/premiumsupport/knowledge-center/elb-redirect-http-to-
https-using-alb/
QUESTION NO: 26
한 회사가 데이터베이스를 Amazon RDS for PostgreSQL로 마이그레이션하고 있습니다. 이
회사는 애플리케이션을 Amazon EC2 인스턴스로 마이그레이션하고 있습니다. 이 회사는
장기 실행 워크로드에 대한 비용을 최적화하고자 합니다.
이 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. PostgreSQL 워크로드를 위한 Amazon RDS에 온디맨드 인스턴스를 사용합니다. EC2
인스턴스에 대해 업프런트 옵션 없이 1년 Compute Savings Plan을 구매합니다.
B. PostgreSQL 워크로드를 위한 Amazon RDS에 대해 1년 기간의 예약 인스턴스를 선불 없이
구매하세요. EC2 인스턴스에 대해 선불 없이 1년 EC2 인스턴스 절약 플랜을 구매하세요.
C. Amazon RDS for PostgreSQL 워크로드에 대한 부분 선불 옵션으로 1년 기간의 예약
인스턴스를 구매하세요. EC2 인스턴스에 대한 부분 선불 옵션으로 1년 EC2 인스턴스 절약
플랜을 구매하세요.
D. Amazon RDS for PostgreSQL 워크로드에 대한 All Upfront 옵션으로 3년 기간의 Reserved
Instances를 구매하세요. EC2 인스턴스에 대한 All Upfront 옵션으로 3년 EC2 Instance
Savings Plan을 구매하세요.
Answer: D
QUESTION NO: 27
한 회사가 디렉토리 서비스와 DNS를 포함한 핵심 네트워크 서비스를 온프레미스 데이터
센터에서 호스팅합니다. 데이터 센터는 AWS Direct Connect(DX)를 사용하여 AWS
클라우드에 연결됩니다. 이러한 네트워크 서비스에 대한 빠르고 비용 효율적이며 일관된
액세스가 필요한 추가 AWS 계정이 계획되어 있습니다.
이러한 요구 사항을 최소한의 운영 비용으로 충족하기 위해 솔루션 아키텍트는 무엇을
구현해야 할까요?
A. 각 새 계정에서 DX 연결을 만듭니다. 네트워크 트래픽을 온프레미스 서버로 라우팅합니다.
B. DX VPC에서 모든 필수 서비스에 대한 VPC 엔드포인트를 구성합니다. 네트워크 트래픽을
온프레미스 서버로 라우팅합니다.
C. 각 새 계정과 DX VPC 사이에 VPN 연결을 만듭니다. 네트워크 트래픽을 온프레미스 서버로
라우팅합니다.
D. 계정 간에 AWS Transit Gateway를 구성합니다. DX를 Transit Gateway에 할당하고
19

IT Certification Guaranteed, The Easy Way!
네트워크 트래픽을 온프레미스 서버로 라우팅합니다.
Answer: D
Explanation:
* Requirement Analysis: Need quick, cost-effective, and consistent access to on-premises
network services from multiple AWS accounts.
* AWS Transit Gateway: Centralizes and simplifies network management by connecting
VPCs and on- premises networks.
* Direct Connect Integration: Assigning DX to the transit gateway ensures consistent and
high- performance connectivity.
* Operational Overhead: Minimal because Transit Gateway simplifies routing and
management.
* Implementation:
* Set up AWS Transit Gateway.
* Connect new AWS accounts to the Transit Gateway.
* Route traffic through Transit Gateway to on-premises servers via Direct Connect.
* Conclusion: This solution provides a scalable, cost-effective, and low-overhead method to
meet connectivity requirements.
References
* AWS Transit Gateway: AWS Transit Gateway Documentation
QUESTION NO: 28
한 회사는 Amazon EC2 인스턴스에서 실행되는 사용자 지정 애플리케이션에서 회계 기록을
유지 관리합니다. 이 회사는 애플리케이션 데이터의 개발 및 유지 관리를 위해 데이터를 AWS
관리 서비스로 마이그레이션해야 합니다. 솔루션은 최소한의 운영 지원이 필요하고 데이터
변경에 대한 변경 불가능하고 암호화된 검증이 가능한 로그를 제공해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. 애플리케이션의 레코드를 Amazon Redshift 클러스터로 복사합니다.
B. 애플리케이션의 레코드를 Amazon Neptune 클러스터로 복사합니다.
C. 애플리케이션의 레코드를 Amazon Timestream 데이터베이스로 복사합니다.
D. 애플리케이션의 레코드를 Amazon Quantum Ledger Database(Amazon QLDB) 원장으로
복사합니다.
Answer: D
Explanation:
Amazon QLDB is the most cost-effective and suitable service for maintaining immutable,
cryptographically verifiable logs of data changes. QLDB provides a fully managed ledger
database with a built-in cryptographic hash chain, making it ideal for recording changes to
accounting records, ensuring data integrity and security.
QLDB reduces operational overhead by offering fully managed services, so there's no need
for server management, and it's built specifically to ensure immutability and verifiability,
making it the best fit for the given requirements.
* Option A (Redshift): Redshift is designed for analytics and not for immutable,
cryptographically verifiable logs.
* Option B (Neptune): Neptune is a graph database, which is not suitable for this use case.
* Option C (Timestream): Timestream is a time series database optimized for time-stamped
data, but it does not provide immutable or cryptographically verifiable logs.
20

IT Certification Guaranteed, The Easy Way!
AWS References:
* Amazon QLDB
* How QLDB Works
QUESTION NO: 29
회사에 새로운 모바일 앱이 있습니다. 세계 어디에서나 사용자는 자신이 선택한 주제에 대한
지역 뉴스를 볼 수 있습니다. 사용자는 앱 내부에서 사진과 비디오를 게시할 수도 있습니다.
사용자는 콘텐츠가 게시된 후 처음 몇 분 안에 콘텐츠에 액세스하는 경우가 많습니다. 새로운
콘텐츠가 이전 콘텐츠를 빠르게 대체한 다음 이전 콘텐츠는 사라집니다. 뉴스의 지역적
특성은 사용자가 뉴스가 업로드되는 AWS 지역 내에서 콘텐츠의 90%를 소비한다는 것을
의미합니다.
콘텐츠 업로드에 가장 짧은 지연 시간을 제공하여 사용자 경험을 최적화하는 솔루션은
무엇입니까?
A. Amazon S3에 콘텐츠를 업로드하고 저장합니다. 업로드에는 Amazon CloudFront를
사용하십시오.
B. Amazon S3에 콘텐츠를 업로드하고 저장합니다. 업로드에는 S3 Transfer Acceleration을
사용하십시오.
C. 사용자에게 가장 가까운 지역의 Amazon EC2 인스턴스에 콘텐츠를 업로드합니다.
데이터를 Amazon S3에 복사합니다.
D. 사용자에게 가장 가까운 지역의 Amazon S3에 콘텐츠를 업로드하고 저장합니다. Amazon
CloudFront의 여러 배포판을 사용하십시오.
Answer: B
Explanation:
The most suitable solution for optimizing the user experience by providing the lowest latency
for content uploads is to upload and store content in Amazon S3 and use S3 Transfer
Acceleration for the uploads. This solution will enable the company to leverage the AWS
global network and edge locations to speed up the data transfer between the users and the
S3 buckets.
Amazon S3 is a storage service that provides scalable, durable, and highly available object
storage for any type of data. Amazon S3 allows users to store and retrieve data from
anywhere on the web, and offers various features such as encryption, versioning, lifecycle
management, and replication1.
S3 Transfer Acceleration is a feature of Amazon S3 that helps users transfer data to and
from S3 buckets more quickly. S3 Transfer Acceleration works by using optimized network
paths and Amazon's backbone network to accelerate data transfer speeds. Users can enable
S3 Transfer Acceleration for their buckets and use a distinct URL to access them, such as
<bucket>.s3-accelerate.amazonaws.com2.
The other options are not correct because they either do not provide the lowest latency or are
not suitable for the use case. Uploading and storing content in Amazon S3 and using
Amazon CloudFront for the uploads is not correct because this solution is not designed for
optimizing uploads, but rather for optimizing downloads.
Amazon CloudFront is a content delivery network (CDN) that helps users distribute their
content globally with low latency and high transfer speeds. CloudFront works by caching the
content at edge locations around the world, so that users can access it quickly and easily
from anywhere3. Uploading content to Amazon EC2 instances in the Region that is closest to
21

IT Certification Guaranteed, The Easy Way!
the user and copying the data to Amazon S3 is not correct because this solution adds
unnecessary complexity and cost to the process. Amazon EC2 is a computing service that
provides scalable and secure virtual servers in the cloud. Users can launch, stop, or
terminate EC2 instances as needed, and choose from various instance types, operating
systems, and configurations4. Uploading and storing content in Amazon S3 in the Region
that is closest to the user and using multiple distributions of Amazon CloudFront is not correct
because this solution is not cost-effective or efficient for the use case. As mentioned above,
Amazon CloudFront is a CDN that helps users distribute their content globally with low
latency and high transfer speeds. However, creating multiple CloudFront distributions for
each Region would incur additional charges and management overhead, and would not be
necessary since 90% of the content is consumed within the same Region where it is
uploaded3.
References:
* What Is Amazon Simple Storage Service? - Amazon Simple Storage Service
* Amazon S3 Transfer Acceleration - Amazon Simple Storage Service
* What Is Amazon CloudFront? - Amazon CloudFront
* What Is Amazon EC2? - Amazon Elastic Compute Cloud
QUESTION NO: 30
회사는 Amazon EC2 인스턴스에서 Java 기반 작업을 실행합니다. 작업은 1시간마다
실행되며 실행하는 데 10초가 걸립니다. 작업은 예약된 간격으로 실행되며 1GB의 메모리를
사용합니다. 작업이 사용 가능한 최대 CPU를 사용하는 짧은 급증을 제외하고 인스턴스의
CPU 사용률은 낮습니다. 회사는 작업을 실행하는 데 드는 비용을 최적화하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS App2Container(A2C)를 사용하여 작업을 컨테이너화합니다. 0.5 vCPU(가상 CPU) 및
1GB 메모리를 사용하여 AWS Fargate에서 Amazon Elastic Container Service(Amazon ECS)
작업으로 작업을 실행합니다.
B. 메모리가 1GB인 AWS Lambda 함수에 코드를 복사합니다. 매시간 코드를 실행하는
Amazon EventBridge 예약 규칙을 생성합니다.
C. AWS App2Container(A2C)를 사용하여 작업을 컨테이너화합니다. 기존 Amazon 머신
이미지(AMI)에 컨테이너를 설치합니다. 작업이 완료되면 일정이 컨테이너를 중지하는지
확인하세요.
D. 작업 완료 시 EC2 인스턴스를 중지하고 다음 작업 시작 시 EC2 인스턴스를 다시
시작하도록 기존 일정을 구성합니다.
Answer: B
Explanation:
AWS Lambda is a serverless compute service that allows you to run code without
provisioning or managing servers. You can create Lambda functions using various
languages, including Java, and specify the amount of memory and CPU allocated to your
function. Lambda charges you only for the compute time you consume, which is calculated
based on the number of requests and the duration of your code execution. You can use
Amazon EventBridge to trigger your Lambda function on a schedule, such as every hour,
using cron or rate expressions. This solution will optimize the costs to run the job, as you will
not pay for any idle time or unused resources, unlike running the job on an EC2 instance.
References: 1: AWS Lambda - FAQs2, General Information section2: Tutorial: Schedule AWS
22

IT Certification Guaranteed, The Easy Way!
Lambda functions using EventBridge3, Introduction section3:
Schedule expressions using rate or cron - AWS Lambda4, Introduction section.
QUESTION NO: 31
한 회사에 이메일 수집을 자동화하는 솔루션이 필요합니다. 회사는 이메일 메시지를 자동으로
구문 분석하고, 이메일 첨부 파일을 찾고, 모든 첨부 파일을 거의 실시간으로 Amazon S3
버킷에 저장해야 합니다.
이메일의 양은 날마다 상당히 다릅니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon Simple Email Service {Amazon SES}에서 이메일 수신을 설정합니다. 규칙 세트와
수신 규칙을 만듭니다. Amazon SES가 이메일 본문과 첨부 파일을 처리하기 위해 호출할 수
있는 AWS Lambda 함수를 만듭니다.
B. Amazon Simple Email Service(Amazon SES)에서 이메일 콘텐츠 필터링을 설정합니다.
발신자, 수신자, 메시지 본문 및 첨부 파일을 기반으로 콘텐츠 필터링 규칙을 만듭니다.
C. Amazon Simple Email Service(Amazon SES)에서 이메일 수신을 설정합니다. Amazon
SES 및 S3 이벤트 알림을 구성하여 이메일 본문과 첨부 파일을 처리합니다.
D. 이메일 본문과 첨부 파일을 처리하는 AWS Lambda 함수를 만듭니다. Amazon
EventBridge를 사용하여 Lambda 함수를 호출합니다. 들어오는 이메일을 수신하도록
EventBridge 규칙을 구성합니다.
Answer: A
Explanation:
Amazon SES (Simple Email Service) allows for the automatic ingestion of incoming emails.
By setting up email receiving in SES and creating a rule set with a receipt rule, you can
configure SES to invoke an AWS Lambda function whenever an email is received. The
Lambda function can then process the email body and attachments, saving any attachments
to an Amazon S3 bucket. This solution is highly scalable, cost-effective, and provides near
real-time processing of emails with minimal operational overhead.
* Option B (Content filtering): This only filters emails based on content and does not provide
the functionality to save attachments to S3.
* Option C (S3 Event Notifications): While SES can store emails in S3, SES with Lambda
offers more flexibility for processing attachments in real-time.
* Option D (EventBridge rule): EventBridge cannot directly listen for incoming emails, making
this solution incorrect.
AWS References:
* Receiving Email with Amazon SES
* Invoking Lambda from SES
QUESTION NO: 32
한 회사가 최근 모놀리식 애플리케이션을 Amazon EC2 인스턴스와 Amazon RDS로
마이그레이션했습니다. 애플리케이션에는 밀접하게 결합된 모듈이 있습니다. 애플리케이션의
기존 설계는 애플리케이션이 단일 EC2 인스턴스에서만 실행될 수 있는 기능을 제공합니다.
이 회사는 피크 사용 시간 동안 EC2 인스턴스에서 높은 CPU 사용률을 발견했습니다. 높은
CPU 사용률은 Amazon RDS에서 읽기 요청의 성능이 저하된 것과 일치합니다. 이 회사는
높은 CPU 사용률을 줄이고 읽기 요청 성능을 개선하고자 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
23

IT Certification Guaranteed, The Easy Way!
A. EC2 인스턴스의 크기를 CPU 용량이 더 많은 EC2 인스턴스 유형으로 조정합니다. 최소 및
최대 크기가 1인 자동 확장 그룹을 구성합니다. 읽기 요청에 대한 RDS 읽기 복제본을
구성합니다.
B. EC2 인스턴스의 크기를 CPU 용량이 더 많은 EC2 인스턴스 유형으로 조정합니다. 최소 및
최대 크기가 1인 자동 확장 그룹을 구성합니다. RDS 읽기 복제본을 추가하고 모든 읽기/쓰기
트래픽을 복제본으로 리디렉션합니다.
C. 최소 크기 1, 최대 크기 2로 자동 크기 조정 그룹을 구성합니다. CPU 용량이 더 많은
인스턴스 유형으로 RDS DB 인스턴스 크기를 조정합니다.
D. CPU 용량이 더 많은 EC2 인스턴스 유형으로 EC2 인스턴스 크기를 조정합니다. 최소 및
최대 크기가 1인 자동 크기 조정 그룹을 구성합니다. CPU 용량이 더 많은 인스턴스 유형으로
RDS DB 인스턴스 크기를 조정합니다.
Answer: A
Explanation:
To address the high CPU utilization on the EC2 instance and the degraded performance of
Amazon RDS for read requests, the solution involves two key actions: resizing the EC2
instance and leveraging Amazon RDS read replicas.
* Resizing the EC2 Instance: The first step is to resize the EC2 instance to a type with more
CPU capacity to handle the higher computational demands during peak usage times. This
helps to alleviate the immediate pressure on the CPU.
* Auto Scaling Group with a Size of 1: Although the application can only run on a single EC2
instance due to its monolithic nature, creating an Auto Scaling group with a minimum and
maximum size of 1 ensures that the instance is automatically restarted or replaced in case of
failure, maintaining high availability.
* RDS Read Replica: Configuring an RDS read replica allows the application to offload read
requests to a separate instance, thus reducing the load on the primary RDS instance. This
improves the performance of read operations, which were previously bottlenecked due to the
high CPU usage on the EC2 instance.
* Why Not Other Options?:
* Option B: Redirecting all traffic to the RDS read replica is not recommended because
replicas are meant for read traffic only, not for write operations. This could lead to data
consistency issues.
* Option C: Increasing the RDS instance type capacity helps, but it doesn't address the high
CPU usage on the EC2 instance, nor does it provide a solution for scaling reads.
* Option D: While resizing both the EC2 and RDS instances increases their capacities, it
doesn't address the specific need to offload read traffic from the primary RDS instance.
AWS References:
* Amazon RDS Read Replicas - Explains how to create and use read replicas to offload read
traffic from the primary database instance.
* Resizing Your EC2 Instance - Guidance on resizing EC2 instances to meet workload
demands.
QUESTION NO: 33
한 회사가 AWS에서 전자상거래 웹 애플리케이션을 구축하고 있습니다. 애플리케이션은
새로운 주문에 대한 정보를 Amazon API Gateway REST API로 보내 처리합니다. 회사는
주문이 접수된 순서대로 처리되기를 원합니다.
24

IT Certification Guaranteed, The Easy Way!
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 애플리케이션이 주문을 수신하면 API Gateway 통합을 사용하여 Amazon SNS(Amazon
SNS) 주제에 메시지를 게시합니다. 처리를 수행하려면 주제에 대한 AWS Lambda 함수를
구독하세요.
B. 애플리케이션이 주문을 받으면 API Gateway 통합을 사용하여 Amazon Simple Queue
Service(Amazon SQS) FIFO 대기열에 메시지를 보냅니다. 처리를 위해 AWS Lambda 함수를
호출하도록 SQS FIFO 대기열을 구성합니다.
C. API 게이트웨이 권한 부여자를 사용하여 애플리케이션이 주문을 처리하는 동안 모든
요청을 차단합니다.
D. 애플리케이션이 주문을 받으면 API Gateway 통합을 사용하여 Amazon Simple Queue
Service(Amazon SQS) 표준 대기열에 메시지를 보냅니다. 처리를 위해 AWS Lambda 함수를
호출하도록 SQS 표준 대기열을 구성합니다.
Answer: B
Explanation:
To ensure that orders are processed in the order that they are received, the best solution is
to use an Amazon SQS FIFO (First-In-First-Out) queue. This type of queue maintains the
exact order in which messages are sent and received. In this case, the application can send
information about new orders to an Amazon API Gateway REST API, which can then use an
API Gateway integration to send a message to an Amazon SQS FIFO queue for processing.
The queue can then be configured to invoke an AWS Lambda function to perform the
necessary processing on each order. This ensures that orders are processed in the exact
order in which they are received.
QUESTION NO: 34
회사가 AWS Business Support 플랜에 가입되어 있습니다. 규정 준수 규칙에 따라 회사는
배포를 진행하기 전에 AWS 인프라 상태를 확인해야 합니다. 회사에는 새로운 배포를 시작할
때 인프라 상태를 확인하기 위한 프로그래밍 방식의 자동화된 방법이 필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 각 배포 시작 시 AWS Trusted Advisor API를 사용합니다. API가 문제를 반환하는 경우
모든 새 배포를 일시 중지합니다.
B. 각 배포 시작 시 AWS Health API를 사용합니다. API가 문제를 반환하는 경우 모든 새
배포를 일시 중지합니다.
C. 각 배포 시작 시 AWS Support API를 쿼리합니다. API가 미해결 문제를 반환하는 경우 모든
새 배포를 일시 중지합니다.
D. 배포에 앞서 각 워크로드에 API 호출을 보냅니다. API 호출이 실패하면 배포를 일시
중지합니다.
Answer: B
Explanation:
The AWS Health API provides programmatic access to the AWS Health information that is
presented in the AWS Personal Health Dashboard. You can use the API operations to get
information about AWS Health events that affect your AWS services and resources. You can
also use the API to enable or disable health- based insights for your organization. You can
use the AWS Health API at the start of each deployment to check on AWS infrastructure
health and pause all new deployments if the API returns any issues. References:
25

IT Certification Guaranteed, The Easy Way!
https://docs.aws.amazon.com/health/latest/APIReference/Welcome.html
QUESTION NO: 35
한 온라인 소매 회사는 5천만 명 이상의 활성 고객을 보유하고 있으며 매일 25,000건 이상의
주문을 받습니다. 회사는 고객의 구매 데이터를 수집하고 이 데이터를 Amazon S3에
저장합니다. 추가 고객 데이터는 Amazon RDS에 저장됩니다.
회사는 다양한 팀이 분석을 수행할 수 있도록 모든 데이터를 다양한 팀에서 사용할 수 있도록
하려고 합니다. 솔루션은 데이터에 대한 세분화된 권한을 관리하는 기능을 제공해야 하며
운영 오버헤드를 최소화해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 구매 데이터를 마이그레이션하여 Amazon RDS에 직접 씁니다. RDS 액세스 제어를
사용하여 액세스를 제한합니다.
B. Amazon RDS에서 Amazon S3로 데이터를 주기적으로 복사하도록 AWS Lambda 함수를
예약합니다. AWS Glue 크롤러를 생성합니다. Amazon Athena를 사용하여 데이터를
쿼리합니다. S3 정책을 사용하여 액세스를 제한합니다.
C. AWS Lake Formation을 사용하여 데이터 레이크를 생성합니다. Amazon RDS에 대한 AWS
Glue JDBC 연결을 생성합니다. Lake Formation에 S3 버킷을 등록합니다. Lake Formation
액세스 제어를 사용하여 액세스를 제한하세요.
D. Amazon Redshift 클러스터를 생성합니다. Amazon S3 및 Amazon RDS에서 Amazon
Redshift로 데이터를 주기적으로 복사하도록 AWS Lambda 함수를 예약합니다. Amazon
Redshift 액세스 제어를 사용하여 액세스를 제한합니다.
Answer: C
Explanation:
https://aws.amazon.com/blogs/big-data/manage-fine-grained-access-control-using-aws-lake-
formation/
QUESTION NO: 36
솔루션 아키텍트는 정적 콘텐츠로 구성된 회사 웹사이트의 아키텍처를 설계하고 있습니다.
회사의 타겟 고객은 미국과 유럽에 있습니다.
솔루션 아키텍트는 비용을 최소화하기 위해 어떤 아키텍처를 권장해야 할까요?
A. us-east-2 지역의 Amazon S3에 웹사이트 파일을 저장합니다. 사용 중인 엣지 위치를
제한하도록 구성된 가격 클래스가 있는 Amazon CloudFront 배포를 사용합니다.
B. us-east-2 지역의 Amazon S3에 웹사이트 파일을 저장합니다. 엣지 위치 사용을
극대화하도록 가격 클래스가 구성된 Amazon CloudFront 배포를 사용합니다.
C. us-east-2 Region과 eu-west-1 Region의 Amazon S3에 웹사이트 파일을 저장합니다.
Amazon CloudFront 지리적 위치 라우팅 정책을 사용하여 요청을 사용자에게 가장 가까운
Region으로 라우팅합니다.
D. us-east-2 지역과 eu-west-1 지역의 Amazon S3에 웹사이트 파일을 저장합니다. Amazon
Route 53 대기 시간 라우팅 정책이 있는 Amazon CloudFront 배포를 사용하여 요청을
사용자에게 가장 가까운 지역으로 라우팅합니다.
Answer: A
* The question focuses on minimizing costs while serving static content to users in the US
and Europe.
* Option A uses a single S3 bucket and configures CloudFront to limit edge locations,
26

IT Certification Guaranteed, The Easy Way!
reducing costs by using fewer edge locations while still improving performance.
* Option B maximizes edge locations, which increases costs unnecessarily.
* Options C and D involve storing data in multiple regions, which increases storage and
operational costs.Thus, Option A is the most cost-effective solution.
QUESTION NO: 37
회사는 1주일 동안 진행될 예정된 이벤트를 위해 특정 AWS 리전의 3개의 특정 가용 영역에서
보장된 Amazon EC2 용량이 필요합니다.
EC2 용량을 보장하기 위해 회사는 무엇을 해야 합니까?
A. 필요한 리전을 지정하는 예약 인스턴스 구매
B. 필요한 지역을 지정하는 온디맨드 용량 예약 생성
C. 필요한 리전과 3개의 가용 영역을 지정하는 예약 인스턴스 구매
D. 필요한 지역과 3개의 가용 영역을 지정하는 온디맨드 용량 예약 생성
Answer: D
Explanation:
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html
Reserve instances: You will have to pay for the whole term (1 year or 3years) which is not
cost effective
QUESTION NO: 38
AWS를 사용하는 회사에는 매달 제조 프로세스에 필요한 리소스를 예측하는 솔루션이
필요합니다. 솔루션은 현재 Amazon S3 버킷에 저장된 기록 값을 사용해야 합니다. 회사는
기계 학습(ML) 경험이 없으며 교육 및 예측을 위해 관리형 서비스를 사용하려고 합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.)
A. Amazon SageMaker 모델을 배포합니다. 추론을 위해 SageMaker 엔드포인트를
생성합니다.
B. Amazon SageMaker를 사용하여 S3 버킷의 기록 데이터를 사용하여 모델을 교육합니다.
C. Amazon SageMaker 엔드포인트를 사용하여 입력을 기반으로 예측을 생성하는 함수
URL로 AWS Lambda 함수를 구성합니다.
D. Amazon Forecast 예측기를 사용하여 입력을 기반으로 예측을 생성하는 함수 URL로 AWS
Lambda 함수를 구성합니다.
E. S3 버킷의 기록 데이터를 사용하여 Amazon Forecast 예측기를 교육합니다.
Answer: B E
Explanation:
To predict the resources needed for manufacturing processes each month using historical
values that are currently stored in an Amazon S3 bucket, a solutions architect should use
Amazon SageMaker to train a model by using the historical data in the S3 bucket, and deploy
an Amazon SageMaker model and create a SageMaker endpoint for inference. Amazon
SageMaker is a fully managed service that provides an easy way to build, train, and deploy
machine learning (ML) models. The solutions architect can use the built-in algorithms or
frameworks provided by SageMaker, or bring their own custom code, to train a model using
the historical data in the S3 bucket as input. The trained model can then be deployed to a
SageMaker endpoint, which is a scalable and secure web service that can handle requests
for predictions from the application. The solutions architect does not need to have any ML
27

IT Certification Guaranteed, The Easy Way!
experience or manage any infrastructure to use SageMaker.
QUESTION NO: 39
전자상거래 회사는 여러 AWS 계정에서 여러 내부 애플리케이션을 실행합니다. 이 회사는
AWS Organizations를 사용하여 AWS 계정을 관리합니다.
회사의 네트워킹 계정의 보안 어플라이언스는 AWS 계정 전반의 애플리케이션 간 상호
작용을 검사해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 네트워킹 계정에 네트워크 로드 밸런서(NLB)를 배포하여 보안 어플라이언스로 트래픽을
전송합니다. 애플리케이션 계정에서 인터페이스 VPC 엔드포인트를 사용하여 NLB로
트래픽을 전송하도록 애플리케이션 계정을 구성합니다.
B. 애플리케이션 계정에 애플리케이션 부하 분산 장치(ALB)를 배포하여 트래픽을 보안
어플라이언스로 직접 전송합니다.
C. 네트워킹 계정에 게이트웨이 로드 밸런서(GWLB)를 배포하여 보안 어플라이언스로
트래픽을 전송합니다. 애플리케이션 계정에서 인터페이스 GWLB 엔드포인트를 사용하여
GWLB로 트래픽을 전송하도록 애플리케이션 계정을 구성합니다.
D. 애플리케이션 계정에 인터페이스 VPC 엔드포인트를 배포하여 트래픽을 보안
어플라이언스로 직접 전송합니다.
Answer: C
Explanation:
The Gateway Load Balancer (GWLB) is specifically designed to route traffic through a
security appliance in a hub-and-spoke model, making it the ideal solution for inspecting traffic
between multiple AWS accounts.
GWLB enables you to simplify, scale, and deploy third-party virtual appliances transparently,
and it can work across multiple VPCs or accounts using interface endpoints (Gateway Load
Balancer Endpoints).
Key AWS features:
* Traffic Inspection: The GWLB allows the centralized security appliance to inspect traffic
between different VPCs, making it suitable for inspecting inter-account interactions.
* Interface VPC Endpoints: By using interface endpoints in the application accounts, traffic
can securely and efficiently be routed to the security appliance in the networking account.
* AWS Documentation: The use of GWLB aligns with AWS's best practices for centralized
network security, simplifying architecture and reducing operational complexity.
QUESTION NO: 40
애플리케이션은 VPC의 Amazon EC2 인스턴스에서 실행됩니다. 애플리케이션은 Amazon S3
버킷에 저장된 로그를 처리합니다. EC2 인스턴스는 인터넷에 연결하지 않고 S3 버킷에
액세스해야 합니다.
Amazon S3에 프라이빗 네트워크 연결을 제공하는 솔루션은 무엇입니까?
A. S3 버킷에 대한 게이트웨이 VPC 엔드포인트를 생성합니다.
B. 로그를 Amazon CloudWatch Logs로 스트리밍합니다. 로그를 S3 버킷으로 내보냅니다.
C. S3 액세스를 허용하려면 Amazon EC2에 인스턴스 프로필을 생성합니다.
D. S3 엔드포인트에 액세스하기 위한 프라이빗 링크가 포함된 Amazon API Gateway API를
생성합니다.
Answer: A
28

IT Certification Guaranteed, The Easy Way!
Explanation:
VPC endpoint allows you to connect to AWS services using a private network instead of
using the public Internet
QUESTION NO: 41
한 회사가 Amazon EC2 인스턴스에서 고객을 위해 데모 환경을 운영하고 있습니다. 각 환경은
자체 VPC에 격리되어 있습니다. 환경에 대한 RDP 또는 SSH 액세스가 설정되면 회사 운영
팀에 알려야 합니다.
A. RDP 또는 SSH 액세스가 감지되면 AWS Systems Manager OpsItems를 생성하도록
Amazon CloudWatch Application Insights를 구성합니다.
B. AmazonSSMManagedInstanceCore 정책이 연결된 IAM 역할이 있는 IAM 인스턴스
프로파일로 EC2 인스턴스를 구성합니다.
C. Amazon CloudWatch Logs에 VPC 흐름 로그를 게시합니다. 필수 지표 필터를 생성합니다.
경보가 ALARM 상태일 때 알림 작업이 포함된 Amazon CloudWatch 지표 경보를 생성합니다.
D. EC2 인스턴스 상태 변경 알림 유형의 이벤트를 수신하도록 Amazon EventBridge 규칙을
구성합니다. Amazon Simple 알림 서비스(Amazon SNS) 주제를 대상으로 구성합니다.
운영팀의 주제를 구독하세요.
Answer: C
Explanation:
https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-
attempts-to-amazon- ec2-linux-instances/
QUESTION NO: 42
한 회사가 Amazon Aurora PostgreSQL Serverless v2 클러스터에 애플리케이션을 배포할
계획입니다. 애플리케이션은 많은 양의 트래픽을 수신하게 됩니다. 회사는 애플리케이션의
로드가 증가함에 따라 클러스터의 스토리지 성능을 최적화하려고 합니다. 어떤 솔루션이
이러한 요구 사항을 가장 비용 효율적으로 충족합니까?
A. Aurora Standard 스토리지 구성을 사용하도록 클러스터를 구성합니다.
B. 클러스터 스토리지 유형을 프로비저닝된 IOPS로 구성합니다.
C. 클러스터 스토리지 유형을 범용으로 구성합니다.
D. Aurora l/O-Optimized 스토리지 구성을 사용하도록 클러스터를 구성합니다.
Answer: D
Explanation:
* Aurora I/O-Optimized: This storage configuration is designed to provide consistent high
performance for Aurora databases. It automatically scales IOPS as the workload increases,
without needing to provision IOPS separately.
* Cost-Effectiveness: With Aurora I/O-Optimized, you only pay for the storage and I/O you
use, making it a cost-effective solution for applications with varying and unpredictable I/O
demands.
* Implementation:
* During the creation of the Aurora PostgreSQL Serverless v2 cluster, select the I/O-
Optimized storage configuration.
* The storage system will automatically handle scaling and performance optimization based
on the application load.
* Operational Efficiency: This configuration reduces the need for manual tuning and ensures
29

IT Certification Guaranteed, The Easy Way!
optimal performance without additional administrative overhead.
References:
* Amazon Aurora I/O-Optimized
QUESTION NO: 43
한 회사가 회사에서 관리하는 온프레미스 Microsoft Active Directory에서 AWS로
애플리케이션을 마이그레이션하고 있습니다. 이 회사는 여러 AWS 계정에 애플리케이션을
배포합니다. 이 회사는 AWS Organizations를 사용하여 계정을 중앙에서 관리합니다.
회사의 보안팀은 회사의 모든 AWS 계정에서 단일 로그인 솔루션이 필요합니다. 회사는
온프레미스 Active Directory에 있는 사용자와 그룹을 계속 관리해야 합니다. 어떤 솔루션이
이러한 요구 사항을 충족할까요?
A. Microsoft Active Directory용 AWS Directory Service에서 Enterprise Edition Active
Directory를 만듭니다. Active Directory를 AWS 1AM Identity Center의 ID 소스로 구성합니다.
B. AWS 1AM Identity Center를 활성화합니다. Microsoft Active Directory용 AWS Directory
Service를 사용하여 회사의 자체 관리 Active Directory를 1AM Identity Center와 연결하기
위한 양방향 포리스트 신뢰 관계를 구성합니다.
C. AWS Directory Service를 사용하여 회사가 자체 관리하는 Active Directory와 양방향 신뢰
관계를 구축합니다.
D. Amazon EC2에 ID 공급자(IdP)를 배포합니다. AWS 내에서 IdP를 ID 소스로 연결합니다.
오전 1시 신원 센터.
Answer: B
Explanation:
The company is looking for a solution that provides single sign-on (SSO) across multiple
AWS accounts while continuing to manage users and groups in their on-premises Active
Directory (AD). AWS IAM Identity Center (formerly AWS SSO) is the recommended solution
for this type of requirement.
Explanation:
* AWS IAM Identity Center provides a centralized identity management solution, enabling
single sign- on across multiple AWS accounts and other cloud applications. It can integrate
with on-premises Active Directory to leverage existing users and groups.
* By configuring a two-way forest trust relationship between AWS Directory Service for
Microsoft Active Directory and the company's on-premises Active Directory, users can be
authenticated by their on-premises AD and still access AWS resources through IAM Identity
Center. This solution allows centralized management of AWS accounts within AWS
Organizations.
* The two-way trust allows mutual access between the on-premises AD and the AWS
Directory Service.
This means that users and groups in the on-premises AD can be used for authentication in
AWS IAM Identity Center while maintaining the existing identity management system.
AWS References:
* AWS IAM Identity Center Documentation
* AWS Directory Service for Microsoft Active Directory Trust Relationships
* AWS Directory Service Integration with IAM Identity Center
Why the other options are incorrect:
* A. Create an Enterprise Edition Active Directory in AWS Directory Service: This would
30

IT Certification Guaranteed, The Easy Way!
require setting up a new directory and managing it in AWS, which adds unnecessary
overhead. The requirement is to continue using the existing on-premises AD, making this
option unsuitable.
* C. Use AWS Directory Service and create a two-way trust relationship: While this approach
establishes a trust between on-premises AD and AWS Directory Service, it does not address
the single sign-on (SSO) requirements across multiple AWS accounts through IAM Identity
Center.
* D. Deploy an identity provider (IdP) on Amazon EC2: This is more complex than necessary
and introduces more management overhead. AWS IAM Identity Center natively supports
integration with on-premises Active Directory without requiring a custom IdP.
QUESTION NO: 44
회사는 데이터를 온프레미스에 저장합니다. 데이터의 양은 회사가 사용할 수 있는 용량을
초과하여 증가하고 있습니다.
회사는 온프레미스 위치에서 Amazon S3 버킷으로 데이터를 마이그레이션하려고 합니다.
회사에는 전송 후 데이터의 무결성을 자동으로 검증하는 솔루션이 필요합니다. 어떤 솔루션이
이러한 요구 사항을 충족합니까?
A. AWS Snowball Edge 디바이스 주문 S3 버킷으로의 온라인 데이터 전송을 수행하도록
Snowball Edge 디바이스를 구성합니다.
B. AWS DataSync 에이전트를 온프레미스에 배포합니다. S3 버킷으로의 온라인 데이터
전송을 수행하도록 DataSync 에이전트를 구성합니다.
C. 온프레미스에서 Amazon S3 파일 게이트웨이를 생성합니다. S3 버킷으로의 온라인 데이터
전송을 수행하도록 S3 파일 게이트웨이 구성
D. Amazon S3 Transfer Acceleration 온프레미스에서 액셀러레이터를 구성합니다. S3
버킷으로의 온라인 데이터 전송을 수행하도록 액셀러레이터를 구성합니다.
Answer: B
Explanation:
it allows the company to migrate its data from the on-premises location to an Amazon S3
bucket and automatically validate the integrity of the data after the transfer. By deploying an
AWS DataSync agent on premises, the company can use a fully managed data transfer
service that makes it easy to move large amounts of data to and from AWS. By configuring
the DataSync agent to perform the online data transfer to an S3 bucket, the company can
take advantage of DataSync's features, such as encryption, compression, bandwidth
throttling, and data validation. DataSync automatically verifies data integrity at both source
and destination after each transfer task. References:
* AWS DataSync
* Deploying an Agent for AWS DataSync
* How AWS DataSync Works
QUESTION NO: 45
회사는 Amazon Elastic Block Store(Amazon EBS)를 연결된 스토리지로 사용하는 Amazon
EC2 인스턴스에 애플리케이션을 다시 호스팅할 계획입니다. 솔루션 설계자는 새로 생성된
모든 Amazon EBS 볼륨이 기본적으로 암호화되도록 솔루션을 설계해야 합니다. 솔루션은
암호화되지 않은 EBS 볼륨의 생성도 방지해야 합니다. 어떤 솔루션이 이러한 요구 사항을
충족합니까?
31

IT Certification Guaranteed, The Easy Way!
A. 항상 새 EBS 볼륨을 암호화하도록 EC2 계정 속성을 구성합니다.
B. AWS Config를 사용합니다. 암호화된 볼륨 식별자를 구성합니다. 기본 AWS Key
Management Service(AWS KMS) 키를 적용합니다.
C. EBS 볼륨의 암호화된 복사본을 생성하도록 AWS Systems Manager를 구성합니다.
암호화된 볼륨을 사용하도록 EC2 인스턴스를 재구성합니다.
D. AWS Key Management Service(AWS KMS)에서 고객 관리형 키를 생성합니다. 회사가
워크로드를 마이그레이션할 때 키를 사용하도록 AWS Migration Hub를 구성합니다.
Answer: A
Explanation:
* EC2 Account Attributes: Amazon EC2 allows you to set account attributes to automatically
encrypt new EBS volumes. This ensures that all new volumes created in your account are
encrypted by default.
* Configuration Steps:
* Go to the EC2 Dashboard.
* Select "Account Attributes" and then "EBS encryption".
* Enable default EBS encryption and select the default AWS KMS key or a customer-
managed key.
* Prevention of Unencrypted Volumes: By setting this account attribute, you ensure that it is
not possible to create unencrypted EBS volumes, thereby enforcing compliance with security
requirements.
* Operational Efficiency: This solution requires minimal configuration changes and provides
automatic enforcement of encryption policies, reducing operational overhead.
References:
* Amazon EC2 Default EBS Encryption
QUESTION NO: 46
회사가 여러 Amazon EC2 인스턴스에서 애플리케이션을 호스팅합니다. 애플리케이션이
Amazon SQS 대기열의 메시지를 처리하고 Amazon RDS 테이블에 작성하고 대기열에서
메시지를 삭제합니다. RDS 테이블에서 가끔 중복 레코드가 발견됩니다. SQS 대기열에는
중복 메시지가 없습니다.
메시지가 한 번만 처리되도록 솔루션 설계자는 무엇을 해야 합니까?
A. CreateQueue API 호출을 사용하여 새 대기열 생성
B. 권한 추가 API 호출을 사용하여 적절한 권한 추가
C. ReceiveMessage API 호출을 사용하여 적절한 울음 시간 설정
D. ChangeMessageVisibility API 호출을 사용하여 가시성 시간 초과를 늘립니다.
Answer: D
Explanation:
The visibility timeout begins when Amazon SQS returns a message. During this time, the
consumer processes and deletes the message. However, if the consumer fails before
deleting the message and your system doesn't call the DeleteMessage action for that
message before the visibility timeout expires, the message becomes visible to other
consumers and the message is received again. If a message must be received only once,
your consumer should delete it within the duration of the visibility timeout.
https://docs.aws.amazon.com
32

IT Certification Guaranteed, The Easy Way!
/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html Keyword:
SQS queue writes to an Amazon RDS From this, Option D best suite &amp; other Options
ruled out [Option A - You can't intruduce one more Queue in the existing one; Option B - onl
y Permission &amp; Option C - Only Retrieves Messages] FIF O queues are designed to
never introduce duplicate messages.
However, your message producer might introduce duplicates in certain scenarios: for
example, if the producer sends a message, does not receive a response, and then resends
the same message. Amazon SQS APIs provide deduplication functionality that prevents your
message producer from sending duplicates. Any duplicates introduced by the message
producer are removed within a 5-minute deduplication interval. For standard queues, you
might occasionally receive a duplicate copy of a message (at-least- once delivery). If you use
a standard queue, you must design your applications to be idempotent (that is, they must not
be affected adversely when processing the same message more than once).
QUESTION NO: 47
한 회사가 AWS에서 다중 계층 애플리케이션을 호스팅합니다. 규정 준수, 거버넌스, 감사 및
보안을 위해 회사는 AWS 리소스의 구성 변경 사항을 추적하고 이러한 리소스에 대한 API
호출 기록을 기록해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. AWS CloudTrail을 사용하여 구성 변경 사항을 추적하고 AWS Config를 사용하여 API
호출을 기록합니다.
B. AWS Config를 사용하여 구성 변경 사항을 추적하고 AWS CloudTrail을 사용하여 API
호출을 기록합니다.
C. AWS Config를 사용하여 구성 변경 사항을 추적하고 Amazon CloudWatch를 사용 하여 API
호출을 기록합니다.
D. AWS CloudTrail을 사용하여 구성 변경 사항을 추적하고 Amazon CloudWatch를 사용하여
API 호출을 기록합니다.
Answer: B
Explanation:
AWS Config is a fully managed service that allows the company to assess, audit, and
evaluate the configurations of its AWS resources. It provides a detailed inventory of the
resources in use and tracks changes to resource configurations. AWS Config can detect
configuration changes and alert the company when changes occur. It also provides a
historical view of changes, which is essential for compliance and governance purposes. AWS
CloudTrail is a fully managed service that provides a detailed history of API calls made to the
company's AWS resources. It records all API activity in the AWS account, including who
made the API call, when the call was made, and what resources were affected by the call.
This information is critical for security and auditing purposes, as it allows the company to
investigate any suspicious activity that might occur on its AWS resources.
QUESTION NO: 48
한 회사는 보안 위험을 초래하는 리소스에 대한 정책을 식별하고 시행하기 위해 IT 인프라
맵을 구축하려고 합니다. 회사의 보안팀은 IT 인프라 맵의 데이터를 쿼리하고 보안 위험을
신속하게 식별할 수 있어야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
33

IT Certification Guaranteed, The Easy Way!
A. Amazon RDS를 사용하여 데이터를 저장합니다. SQL을 사용하여 데이터를 쿼리하여 보안
위험을 식별합니다.
B. Amazon Neptune을 사용하여 데이터를 저장합니다. SPARQL을 사용하여 데이터를
쿼리하여 보안 위험을 식별합니다.
C. Amazon Redshift를 사용하여 데이터를 저장합니다. SQL을 사용하여 데이터를 쿼리하여
보안 위험을 식별합니다.
D. Amazon DynamoDB를 사용하여 데이터를 저장합니다. PartiQL을 사용하여 데이터를
쿼리하여 보안 위험을 식별합니다.
Answer: B
Explanation:
* Understanding the Requirement: The company needs to map its IT infrastructure to identify
and enforce security policies, with the ability to quickly query and identify security risks.
* Analysis of Options:
* Amazon RDS: While suitable for relational data, it is not optimized for handling complex
relationships and querying those relationships, which is essential for an IT infrastructure map.
* Amazon Neptune: A graph database service designed for handling highly connected data. It
uses SPARQL to query graph data efficiently, making it ideal for mapping IT infrastructure
and identifying relationships that pose security risks.
* Amazon Redshift: A data warehouse solution optimized for complex queries on large
datasets but not specifically for graph data.
* Amazon DynamoDB: A NoSQL database that uses PartiQL for querying, but it is not
optimized for complex relationships in graph data.
* Best Option for Mapping and Querying IT Infrastructure:
* Amazon Neptune provides the most suitable solution with the least operational overhead. It
is purpose-built for graph data and enables efficient querying of complex relationships to
identify security risks.
References:
* Amazon Neptune
* Querying with SPARQL
QUESTION NO: 49
한 회사에는 다양한 런타임으로 AWS Lambda 함수를 분당 최대 800회 호출하는 이벤트 기반
애플리케이션이 있습니다. Lambda 함수는 Amazon Aurora MySQL OB 클러스터에 저장된
데이터에 액세스합니다. 사용자 활동이 증가함에 따라 회사에서 연결 시간 초과를 확인하고
있습니다. 데이터베이스에 과부하가 발생한 징후는 없습니다. CPU. 메모리 및 디스크 액세스
지표가 모두 낮습니다.
최소한의 운영 오버헤드로 이 문제를 해결할 수 있는 솔루션은 무엇입니까?
A. 더 많은 연결을 처리하도록 Aurora MySQL 노드의 크기를 조정합니다. 데이터베이스 연결
시도에 대해 Lambda 함수에서 재시도 논리를 구성합니다.
B. 데이터베이스에서 일반적으로 읽은 항목을 캐시하도록 Amazon ElastiCache 또는 Redls를
설정합니다. 읽기를 위해 ElastiCache에 연결하도록 Lambda 함수를 구성합니다.
C. Aurora 복제본을 리더 노드로 추가합니다. 작성자 엔드포인트가 아닌 OB 클러스터의 리더
엔드포인트에 연결하도록 Lambda 함수를 구성합니다.
D. Amazon ROS 프록시를 사용하여 프록시를 생성합니다. DB 클러스터를 대상
데이터베이스로 설정합니다. Lambda 함수를 구성하여 DB 클러스터가 아닌 프록시에
34

IT Certification Guaranteed, The Easy Way!
연결합니다.
Answer: D
Explanation:
1. database shows no signs of being overloaded. CPU, memory, and disk access metrics are
all low==>A and C out. We cannot only add nodes instance or add read replica, because
database workload is totally fine, very low. 2. "least operational overhead"==>B out, because
b need to configure lambda. 3. ROS proxy: Shares infrequently used connections; High
availability with failover; Drives increased efficiency==>proxy can leverage failover to redirect
traffic from timeout rds instance to healthy rds instance. So D is right.
QUESTION NO: 50
한 회사가 Amazon S3에서 정적 웹 사이트를 호스팅하고 있으며 DNS에 Amazon Route 53을
사용하고 있습니다. 이 웹사이트는 전 세계적으로 수요가 증가하고 있습니다. 회사는
웹사이트에 접속하는 사용자의 지연 시간을 줄여야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 웹 사이트가 포함된 S3 버킷을 모든 AWS 리전에 복제합니다. Route 53 지리적 위치
라우팅 항목을 추가합니다.
B. AWS Global Accelerator에서 액셀러레이터를 프로비저닝합니다. 제공된 IP 주소를 S3
버킷과 연결합니다. 액셀러레이터의 IP 주소를 가리키도록 Route 53 항목을 편집합니다.
C. S3 버킷 앞에 Amazon CloudFront 배포를 추가합니다. CloudFront 배포를 가리키도록
Route 53 항목을 편집합니다.
D. 버킷에서 S3 Transfer Acceleration을 활성화합니다. 새 엔드포인트를 가리키도록 Route 53
항목을 편집합니다.
Answer: C
Explanation:
Amazon CloudFront is a content delivery network (CDN) that caches content at edge
locations around the world, providing low latency and high transfer speeds to users
accessing the content. Adding a CloudFront distribution in front of the S3 bucket will cache
the static website's content at edge locations around the world, decreasing latency for users
accessing the website. This solution is also cost-effective as it only charges for the data
transfer and requests made by users accessing the content from the CloudFront edge
locations.
Additionally, this solution provides scalability and reliability benefits as CloudFront can
automatically scale to handle increased demand and provide high availability for the website.
QUESTION NO: 51
한 회사에는 매일 1TB의 상태 경고를 집합적으로 생성하는 수천 개의 엣지 장치가 있습니다.
각 경고의 크기는 약 2KB입니다. 솔루션 설계자는 향후 분석을 위해 경고를 수집하고
저장하는 솔루션을 구현해야 합니다.
회사는 가용성이 높은 솔루션을 원합니다. 그러나 회사는 비용을 최소화해야 하며 추가
인프라를 관리하고 싶지 않습니다. 또한 회사는 즉각적인 분석을 위해 14일 동안의 데이터를
보관하고 14일보다 오래된 모든 데이터를 보관하기를 원합니다.
이러한 요구 사항을 충족하는 가장 운영 효율적인 솔루션은 무엇입니까?
A. Amazon Kinesis Data Firehose 전송 스트림을 생성하여 알림을 수집합니다. Amazon S3
버킷에 알림을 전송하도록 Kinesis Data Firehose 스트림을 구성합니다. 14일 후에 데이터를
35

IT Certification Guaranteed, The Easy Way!
Amazon S3 Glacier로 전환하도록 S3 수명 주기 구성을 설정합니다.
B. 두 개의 가용 영역에 걸쳐 Amazon EC2 인스턴스를 시작하고 이를 Elastic Load Balancer
뒤에 배치하여 알림을 수집합니다. Amazon S3 버킷에 3개의 알림을 저장할 EC2 인스턴스에
스크립트를 생성합니다. 데이터를 전환하기 위한 S3 수명 주기 구성을 설정합니다. 14일 후
Amazon S3 Glacier로
C. Amazon Kinesis Data Firehose 전송 스트림을 생성하여 알림을 수집합니다. Amazon
Elasticsearch Service(Amazon ES) Duster에 알림을 전송하도록 Kinesis Data Firehose
스트림을 구성합니다. 매일 수동 스냅샷을 생성하고 삭제하도록 Amazon ES 클러스터를
설정합니다. 14일이 지난 먼지떨이의 데이터
D. Amazon Simple Queue Service(Amazon SQS i 표준 대기열을 생성하여 알림을 수집하고
메시지 보존 기간을 14일로 설정합니다. SQS 대기열을 폴링하도록 소비자를 구성합니다.
메시지 수명을 확인하고 필요에 따라 메시지 데이터를 분석합니다. 메시지가 14일이 지난
경우 소비자는 메시지를 Amazon S3 버킷에 복사하고 SQS 대기열에서 메시지를 삭제해야
합니다.
Answer: A
Explanation:
https://aws.amazon.com/kinesis/data-
firehose/features/?nc=sn&loc=2#:~:text=into%20Amazon%20S3%2C%
20Amazon%20Redshift%2C%20Amazon%20OpenSearch%20Service%2C%20Kinesis,Deliv
ery%20streams
QUESTION NO: 52
회사의 동적 웹 사이트는 미국의 온프레미스 서버를 사용하여 호스팅됩니다. 이 회사는
유럽에서 제품을 출시하고 있으며 새로운 유럽 사용자를 위해 사이트 로딩 시간을
최적화하려고 합니다. 사이트의 백엔드는 미국에 있어야 합니다. 제품이 며칠 안에 출시되며
즉각적인 솔루션이 필요합니다.
솔루션 설계자는 무엇을 권장해야 합니까?
A. us-east-1에서 Amazon EC2 인스턴스를 시작하고 사이트를 마이그레이션합니다.
B. 웹사이트를 Amazon S3로 이동합니다. 리전 간 교차 리전 복제를 사용합니다.
C. 온프레미스 서버를 가리키는 사용자 지정 오리진과 함께 Amazon CloudFront를
사용합니다.
D. 온프레미스 서버를 가리키는 Amazon Route 53 지리적 근접 라우팅 정책을 사용합니다.
Answer: C
Explanation:
https://aws.amazon.com/pt/blogs/aws/amazon-cloudfront-support-for-custom-origins/ You
can now create a CloudFront distribution using a custom origin. Each distribution will can
point to an S3 or to a custom origin. This could be another storage service, or it could be
something more interesting and more dynamic, such as an EC2 instance or even an Elastic
Load Balancer
QUESTION NO: 53
한 회사가 하이브리드 애플리케이션의 가용성과 성능을 개선하고자 합니다. 이
애플리케이션은 다양한 AWS 지역의 Amazon EC2 인스턴스에서 호스팅되는 상태 저장 TCP
기반 워크로드와 온프레미스에서 호스팅되는 상태 없는 UDP 기반 워크로드로 구성됩니다.
솔루션 아키텍트는 가용성과 성능을 개선하기 위해 어떤 작업 조합을 취해야 할까요?
36

IT Certification Guaranteed, The Easy Way!
(두 개를 선택하세요.)
A. AWS Global Accelerator를 사용하여 가속기를 만듭니다. 로드 밸런서를 엔드포인트로
추가합니다.
B. Amazon Route 53 지연 기반 라우팅을 사용하여 요청을 로드 밸런서로 라우팅하는 원본이
있는 Amazon CloudFront 배포를 생성합니다.
C. 각 지역에 두 개의 애플리케이션 로드 밸런서를 구성합니다. 첫 번째는 EC2 엔드포인트로
라우팅됩니다.
두 번째는 온프레미스 엔드포인트로 라우팅됩니다.
D. EC2 엔드포인트를 처리하기 위해 각 지역에 네트워크 로드 밸런서를 구성합니다.
온프레미스 엔드포인트로 라우팅하는 각 지역에 네트워크 로드 밸런서를 구성합니다.
E. 각 지역에 EC2 엔드포인트를 처리하기 위한 네트워크 로드 밸런서를 구성합니다.
온프레미스 엔드포인트로 라우팅하는 각 지역에 애플리케이션 로드 밸런서를 구성합니다.
Answer: A D
Explanation:
For improving availability and performance of the hybrid application, the following solutions
are optimal:
* AWS Global Accelerator (Option A): Global Accelerator provides high availability and
improves performance by using the AWS global network to route user traffic to the nearest
healthy endpoint (across AWS Regions). By adding the Network Load Balancers as
endpoints, Global Accelerator ensures that traffic is routed efficiently to the closest endpoint,
improving both availability and performance.
* Network Load Balancer (Option D): The stateful TCP-based workload hosted on Amazon
EC2 instances and the stateless UDP-based workload hosted on-premises are best served
by Network Load Balancers (NLBs). NLBs are designed to handle TCP and UDP traffic with
ultra-low latency and can route traffic to both EC2 and on-premises endpoints.
* Option B (CloudFront and Route 53): CloudFront is better suited for HTTP/HTTPS
workloads, not for TCP/UDP-based applications.
* Option C (ALB): Application Load Balancers do not support the stateless UDP-based
workload, making NLBs the better choice for both TCP and UDP.
AWS References:
* AWS Global Accelerator
* Network Load Balancer
QUESTION NO: 54
한 회사가 AWS에 최신 제품을 배포했습니다. 제품은 Network Load Balancer 뒤의 Auto
Scaling 그룹에서 실행됩니다. 회사는 제품의 객체를 Amazon S3 버킷에 저장합니다.
이 회사는 최근 자사 시스템에 대한 악의적인 공격을 경험했습니다. 회사에는 AWS 계정의
악의적인 활동, 워크로드 및 S3 버킷에 대한 액세스 패턴을 지속적으로 모니터링하는
솔루션이 필요합니다. 또한 솔루션은 의심스러운 활동을 보고하고 대시보드에 정보를
표시해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 결과를 모니터링하고 AWS Config에 보고하도록 Amazon Made를 구성합니다.
B. 결과를 모니터링하고 AWS CloudTrail에 보고하도록 Amazon Inspector를 구성합니다.
C. 결과를 모니터링하고 AWS Security Hub에 보고하도록 Amazon GuardDuty를 구성합니다.
37

IT Certification Guaranteed, The Easy Way!
D. 결과를 모니터링하고 Amazon EventBridge에 보고하도록 AWS Config를 구성합니다.
Answer: C
Explanation:
Amazon GuardDuty is a threat detection service that continuously monitors for malicious
activity and unauthorized behavior across the AWS account and workloads. GuardDuty
analyzes data sources such as AWS CloudTrail event logs, Amazon VPC Flow Logs, and
DNS logs to identify potential threats such as compromised instances, reconnaissance, port
scanning, and data exfiltration. GuardDuty can report its findings to AWS Security Hub, which
is a service that provides a comprehensive view of the security posture of the AWS account
and workloads. Security Hub aggregates, organizes, and prioritizes security alerts from
multiple AWS services and partner solutions, and displays them on a dashboard. This
solution will meet the requirements, as it enables continuous monitoring, reporting, and
visualization of malicious activity in the AWS account, workloads, and access patterns to the
S3 bucket.
References:
* 1 provides an overview of Amazon GuardDuty and its benefits.
* 2 explains how GuardDuty generates and reports findings based on threat detection.
* 3 provides an overview of AWS Security Hub and its benefits.
* 4 describes how Security Hub collects and displays findings from multiple sources on a
dashboard
QUESTION NO: 55
한 회사가 단일 AWS 리전에서 모바일 게임 앱을 개발하고 있습니다. 앱은 Auto Scaling
그룹의 여러 Amazon EC2 인스턴스에서 실행됩니다. 회사는 앱 데이터를 Amazon
DynamoDB에 저장합니다. 앱은 사용자와 서버 간의 TCP 트래픽과 UDP 트래픽을 사용하여
통신합니다. 응용 프로그램은 전 세계적으로 사용됩니다. 회사는 모든 사용자에 대해 가능한
가장 낮은 대기 시간을 보장하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Global Accelerator를 사용하여 액셀러레이터를 생성합니다. Global Accelerator
통합을 사용하고 TCP 및 UDP 포트에서 수신 대기하는 가속기 엔드포인트 뒤에
ALB(Application Load Balancer)를 생성합니다. ALB에 인스턴스를 등록하려면 Auto Scaling
그룹을 업데이트하세요.
B. AWS Global Accelerator를 사용하여 액셀러레이터를 생성합니다. Global Accelerator
통합을 사용하고 TCP 및 UDP 포트에서 수신 대기하는 가속기 엔드포인트 뒤에 NLB(Network
Load Balancer)를 생성합니다. NLB에 인스턴스를 등록하도록 Auto Scaling 그룹을
업데이트합니다.
C. Amazon CloudFront 콘텐츠 전송 네트워크(CDN) 엔드포인트를 생성합니다. 엔드포인트
뒤에 NLB(Network Load Balancer)를 생성하고 TCP 및 UDP 포트를 수신합니다. NLB에
인스턴스를 등록하려면 Auto Scaling 그룹을 업데이트하세요. NLB를 오리진으로 사용하도록
CloudFront를 업데이트합니다.
D. Amazon Cloudfront 콘텐츠 전송 네트워크(CDN) 엔드포인트를 생성합니다. 엔드포인트
뒤에 ALB(Application Load Balancer)를 생성하고 TCP 및 UDP 포트를 수신합니다. ALB에
인스턴스를 등록하려면 Auto Scaling 그룹을 업데이트하세요. ALB를 오리진으로 사용하도록
CloudFront 업데이트
Answer: B
38

IT Certification Guaranteed, The Easy Way!
Explanation:
AWS Global Accelerator is a networking service that improves the performance and
availability of applications for global users. It uses the AWS global network to route user
traffic to the optimal endpoint based on performance and health. It also provides static IP
addresses that act as a fixed entry point to the applications and support both TCP and UDP
protocols1. By using AWS Global Accelerator, the solution can ensure the lowest possible
latency for all users.
A: Use AWS Global Accelerator to create an accelerator. Create an Application Load
Balancer (ALB) behind an accelerator endpoint that uses Global Accelerator integration and
listening on the TCP and UDP ports.
Update the Auto Scaling group to register instances on the ALB. This solution will not work,
as ALB does not support UDP protocol2.
C: Create an Amazon CloudFront content delivery network (CDN) endpoint. Create a
Network Load Balancer (NLB) behind the endpoint and listening on the TCP and UDP ports.
Update the Auto Scaling group to register instances on the NLB. Update CloudFront to use
the NLB as the origin. This solution will not work, as CloudFront does not support UDP
protocol3.
D: Create an Amazon Cloudfront content delivery network (CDN) endpoint. Create an
Application Load Balancer (ALB) behind the endpoint and listening on the TCP and UDP
ports. Update the Auto Scaling group to register instances on the ALB. Update CloudFront to
use the ALB as the origin. This solution will not work, as CloudFront and ALB do not support
UDP protocol23.
Reference URL: https://aws.amazon.com/global-accelerator/
QUESTION NO: 56
분석 회사는 Amazon VPC를 사용하여 다중 계층 서비스를 실행합니다. 회사는 RESTful
API를 사용하여 수백만 명의 사용자에게 웹 분석 서비스를 제공하려고 합니다. API에
액세스하려면 인증 서비스를 사용하여 사용자를 확인해야 합니다.
가장 효율적인 운영 효율성으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 사용자 인증을 위해 Amazon Cognito 사용자 풀을 구성합니다. Cognito 권한 부여자를
사용하여 Amazon API Gateway REST API를 구현합니다.
B. 사용자 인증을 위해 Amazon Cognito 자격 증명 풀을 구성합니다. Cognito 권한 부여자를
사용하여 Amazon API Gateway HTTP API를 구현합니다.
C. 사용자 인증을 처리하도록 AWS Lambda 함수를 구성합니다. Lambda 권한 부여자를
사용하여 Amazon API Gateway REST API를 구현합니다.
D. 사용자 인증을 처리하도록 1AM 사용자를 구성합니다. 오전 1시 권한 부여자를 사용하여
Amazon API Gateway HTTP API를 구현합니다.
Answer: A
Explanation:
This solution will meet the requirements with the most operational efficiency because:
* Amazon Cognito user pools provide a secure and scalable user directory that can store and
manage user profiles, and handle user sign-up, sign-in, and access control. User pools can
also integrate with social identity providers and enterprise identity providers via SAML or
OIDC. User pools can issue JSON Web Tokens (JWTs) that can be used to authenticate
users and authorize API requests.
39

IT Certification Guaranteed, The Easy Way!
* Amazon API Gateway REST APIs enable you to create and deploy APIs that expose your
backend services to your clients. REST APIs support multiple authorization mechanisms,
including Cognito user pools, IAM, Lambda, and custom authorizers. A Cognito authorizer is
a type of Lambda authorizer that uses a Cognito user pool as the identity source. When a
client makes a request to a REST API method that is configured with a Cognito authorizer,
API Gateway verifies the JWTs that are issued by the user pool and grants access based on
the token's claims and the authorizer's configuration.
* By using Cognito user pools and API Gateway REST APIs with a Cognito authorizer, you
can achieve a high level of security, scalability, and performance for your web analytics
service. You can also leverage the built-in features of Cognito and API Gateway, such as
user management, token validation, caching, throttling, and monitoring, without having to
implement them yourself. This reduces the operational overhead and complexity of your
solution.
References:
* Amazon Cognito User Pools
* Amazon API Gateway REST APIs
* Use API Gateway Lambda authorizers
QUESTION NO: 57
전자상거래 회사에서 계절별 온라인 세일을 진행하고 있습니다. 이 회사는 여러 가용 영역에
걸쳐 있는 Amazon EC2 인스턴스에서 웹 사이트를 호스팅합니다. 회사는 자사 웹사이트에서
세일 기간 동안 급격한 트래픽 증가를 관리할 수 있기를 원합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 최대 트래픽 로드를 처리할 수 있을 만큼 큰 Auto Scaling 그룹을 생성합니다. Amazon EC2
인스턴스의 절반을 중지합니다. 트래픽이 증가하면 중지된 인스턴스를 사용하여 확장하도록
Auto Scaling 그룹을 구성합니다.
B. 웹 사이트에 대한 Auto Scaling 그룹을 생성합니다. 확장할 필요 없이 높은 트래픽 볼륨을
처리할 수 있도록 Auto Scaling 그룹의 최소 크기를 설정합니다.
C. Amazon CIoudFront 및 Amazon ElastiCache를 사용하여 Auto Scaling 그룹이 원본으로
설정된 동적 콘텐츠를 캐시합니다. CIoudFront 및 ElastiCache를 채우는 데 필요한
인스턴스로 Auto Scaling 그룹을 구성합니다. 캐시가 완전히 채워진 후 규모를 축소합니다.
D. 트래픽 증가에 따라 확장되도록 Auto Scaling 그룹을 구성합니다. 사전 구성된 Amazon
머신 이미지(AMI)에서 새 인스턴스를 시작하기 위한 시작 템플릿을 생성합니다.
Answer: D
Explanation:
The solution that meets the requirements of high availability, resiliency, and minimal
operational effort is to use AWS Transfer for SFTP and an Amazon S3 bucket for storage.
This solution allows the company to securely transfer files over SFTP to Amazon S3, which is
a durable and scalable object storage service. The company can then modify the application
to pull the batch files from Amazon S3 to an Amazon EC2 instance for processing. The EC2
instance can be part of an Auto Scaling group with a scheduled scaling policy to run the
batch operation only at night. This way, the company can save costs by scaling down the
EC2 instances when they are not needed. The other solutions do not meet all the
requirements because they either use Amazon EFS or Amazon EBS for storage, which are
more expensive and less scalable than Amazon S3, or they do not use a scheduled scaling
40

IT Certification Guaranteed, The Easy Way!
policy to optimize the EC2 instances usage. References :=
* AWS Transfer for SFTP
* Amazon S3
* Amazon EC2 Auto Scaling
QUESTION NO: 58
회사는 AWS 클라우드에서 웹 애플리케이션을 호스팅합니다. 회사는 AWS Certificate
Manager(ACM)로 가져온 인증서를 사용하도록 Elastic Load Balancer를 구성합니다. 각
인증서가 만료되기 30일 전에 회사 보안팀에 알려야 합니다.
솔루션 설계자는 요구 사항을 충족하기 위해 무엇을 권장해야 합니까?
A. 인증서가 만료되기 30일 전부터 매일 Amazon Simple Notification Service(Amazon SNS)
주제에 사용자 지정 메시지를 게시하는 규칙 m ACM을 추가합니다.
B. 30일 이내에 만료되는 인증서를 확인하는 AWS Config 규칙을 생성합니다. AWS Config가
규정 미준수 리소스를 보고할 때 Amazon Simple Notification Service(Amazon SNS)를 통해
사용자 지정 알림을 호출하도록 Amazon EventBridge(Amazon CloudWatch Events) 구성
C. AWS Trusted Advisor를 사용하여 며칠 이내에 만료되는 인증서를 확인합니다. 상태 변경
확인에 대한 Trusted Advisor 지표를 기반으로 하는 Amazon CloudWatch 경보 생성 Amazon
Simple rectification Service(Amazon SNS)를 통해 사용자 지정 알림을 보내도록 경보 구성
D. 30일 이내에 만료되는 모든 인증서를 감지하는 Amazon EventBridge(Amazon CloudWatch
Events) 규칙을 생성합니다. AWS Lambda 함수를 호출하도록 규칙을 구성합니다. Amazon
Simple Notification Service(Amazon SNS)를 통해 사용자 지정 알림을 보내도록 Lambda
함수를 구성합니다.
Answer: B
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/acm-certificate-expiration/
QUESTION NO: 59
회사는 다양한 제품 라인에 대해 AWS에서 여러 애플리케이션을 호스팅합니다.
애플리케이션은 Amazon EC2 인스턴스 및 Application Load Balancer를 포함한 다양한
컴퓨팅 리소스를 사용합니다. 애플리케이션은 여러 AWS 지역에 걸쳐 AWS Organizations의
동일한 조직에 속한 다양한 AWS 계정에서 실행됩니다. 각 제품 라인의 팀은 개별 계정의 각
컴퓨팅 리소스에 태그를 지정했습니다.
회사는 조직의 통합 청구 기능에서 각 제품 라인의 비용에 대한 자세한 내용을 원합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.)
A. AWS 결제 콘솔에서 특정 AWS 생성 태그를 선택합니다.
B. AWS 결제 콘솔에서 특정 사용자 정의 태그를 선택합니다.
C. AWS 리소스 그룹 콘솔에서 특정 사용자 정의 태그를 선택합니다.
D. 각 AWS 계정에서 선택한 태그를 활성화합니다.
E. 조직 마스터 계정에서 선택한 태그를 활성화합니다.
Answer: B E
Explanation:
User-defined tags are key-value pairs that can be applied to AWS resources to categorize
and track them.
User-defined tags can also be used to allocate costs and create detailed billing reports in the
41

IT Certification Guaranteed, The Easy Way!
AWS Billing console. To use user-defined tags for cost allocation, the tags must be activated
from the Organizations management account, which is the root account that has full control
over all the member accounts in the organization. Once activated, the user-defined tags will
appear as columns in the cost allocation report, and can be used to filter and group costs by
product line. This solution will meet the requirements with the least operational overhead, as
it leverages the existing tagging strategy and does not require any code development or
manual intervention.
References:
* 1 explains how to use user-defined tags for cost allocation.
* 2 describes how to access and manage member accounts from the Organizations
management account.
* 3 discusses how to create and view cost allocation reports in the AWS Billing console.
QUESTION NO: 60
한 회사의 보안 팀은 네트워크 트래픽이 VPC 흐름 로그에 캡처되도록 요청합니다. 로그는
90일 동안 자주 액세스된 후 간헐적으로 액세스됩니다.
로그를 구성할 때 이러한 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. Amazon CloudWatch를 대상으로 사용합니다. 90일 만료로 CloudWatch 로그 그룹 설정
B. Amazon Kinesis를 대상으로 사용합니다. 항상 90일 동안 로그를 보관하도록 Kinesis
스트림을 구성합니다.
C. AWS CloudTrail을 대상으로 사용합니다. Amazon S3 버킷에 저장하도록 CloudTrail을
구성하고 S3 Intelligent-Tiering을 활성화합니다.
D. Amazon S3를 대상으로 사용합니다. S3 수명 주기 정책을 활성화하여 90일 후에 로그를
S3 Standard-Infrequent Access(S3 Standard-IA)로 전환합니다.
Answer: D
Explanation:
There's a table here that specifies that VPC Flow logs can go directly to S3. Does not need to
go via CloudTrail and then to S3. Nor via CW.
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AWS- logs-and-resource
-policy.html#AWS-logs-infrastructure-S3
QUESTION NO: 61
한 회사가 Amazon EC2 인스턴스에 새로운 게임 애플리케이션을 배포하고 있습니다. 게임
애플리케이션은 공유 스토리지에 액세스할 수 있어야 합니다.
이 회사는 애플리케이션이 기존 사용자 지정 프로토콜을 사용하여 공유 스토리지에 액세스할
수 있는 기능을 제공하기 위해 고성능 솔루션이 필요합니다. 솔루션은 낮은 대기 시간을
보장해야 하며 운영상 효율적이어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon FSx 파일 게이트웨이를 만듭니다. 기존 사용자 지정 프로토콜을 사용하는 파일
공유를 만듭니다.
애플리케이션을 호스팅하는 EC2 인스턴스를 파일 공유에 연결합니다.
B. Amazon EC2 Windows 인스턴스를 만듭니다. 인스턴스에 Windows 파일 공유 역할을
설치하고 구성합니다. 애플리케이션을 호스팅하는 EC2 인스턴스를 파일 공유에 연결합니다.
C. Amazon Elastic File System(Amazon EFS) 파일 시스템을 만듭니다. Lustre를 지원하도록
파일 시스템을 구성합니다. 애플리케이션을 호스팅하는 EC2 인스턴스를 파일 시스템에
42

IT Certification Guaranteed, The Easy Way!
연결합니다.
D. Amazon FSx for Lustre 파일 시스템을 만듭니다. 애플리케이션을 호스팅하는 EC2
인스턴스를 파일 시스템에 연결합니다.
Answer: D
Explanation:
Amazon FSx for Lustre is a high-performance, fully managed file system that is ideal for
applications requiring low-latency access to shared storage, especially in use cases like
gaming where high throughput and low latency are essential. It integrates easily with EC2
instances, providing fast and scalable shared storage, and supports custom protocols for
specific application needs.
* Option A (FSx File Gateway): FSx File Gateway is designed for hybrid cloud storage and is
not suited for high-performance gaming workloads.
* Option B (EC2 Windows instance): Setting up a file share on a Windows instance would
introduce additional administrative overhead and would not provide the necessary
performance.
* Option C (EFS with Lustre): While Lustre is integrated with FSx, EFS does not natively
support Lustre.
AWS References:
* Amazon FSx for Lustre
QUESTION NO: 62
회사에서 AWS 환경에서 NAT 게이트웨이를 사용하려고 합니다. 프라이빗 서브넷에 있는
회사의 Amazon EC2 인스턴스는 NAT 게이트웨이를 통해 퍼블릭 인터넷에 연결할 수 있어야
합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EC2 인스턴스와 동일한 프라이빗 서브넷에 퍼블릭 NAT 게이트웨이를 생성합니다.
B. EC2 인스턴스와 동일한 프라이빗 서브넷에 프라이빗 NAT 게이트웨이를 생성합니다.
C. EC2 인스턴스와 동일한 VPC의 퍼블릭 서브넷에 퍼블릭 NAT 게이트웨이를 생성합니다.
D. EC2 인스턴스와 동일한 VPC의 퍼블릭 서브넷에 프라이빗 NAT 게이트웨이를 생성합니다.
Answer: C
Explanation:
A public NAT gateway enables instances in a private subnet to send outbound traffic to the
internet, while preventing the internet from initiating connections with the instances. A public
NAT gateway requires an elastic IP address and a route to the internet gateway for the VPC.
A private NAT gateway enables instances in a private subnet to connect to other VPCs or on-
premises networks through a transit gateway or a virtual private gateway. A private NAT
gateway does not require an elastic IP address or an internet gateway. Both private and
public NAT gateways map the source private IPv4 address of the instances to the private
IPv4 address of the NAT gateway, but in the case of a public NAT gateway, the internet
gateway then maps the private IPv4 address of the public NAT gateway to the elastic IP
address associated with the NAT gateway.
When sending response traffic to the instances, whether it's a public or private NAT gateway,
the NAT gateway translates the address back to the original source IP address.
Creating public NAT gateways in the same private subnets as the EC2 instances (option A) is
not a valid solution, as the NAT gateways would not have a route to the internet gateway.
43

IT Certification Guaranteed, The Easy Way!
Creating private NAT gateways in the same private subnets as the EC2 instances (option B)
is also not a valid solution, as the instances would not be able to access the internet through
the private NAT gateways. Creating private NAT gateways in public subnets in the same
VPCs as the EC2 instances (option D) is not a valid solution either, as the internet gateway
would drop the traffic from the private NAT gateways.
Therefore, the only valid solution is to create public NAT gateways in public subnets in the
same VPCs as the EC2 instances (option C), as this would allow the instances to access the
internet through the public NAT gateways and the internet gateway. References:
* NAT gateways - Amazon Virtual Private Cloud
* NAT gateway use cases - Amazon Virtual Private Cloud
* Amazon Web Services - Introduction to NAT Gateways
* What is AWS NAT Gateway? - KnowledgeHut
QUESTION NO: 63
Amazon EC2 관리자는 여러 사용자가 포함된 IAM 그룹과 연결된 다음 정책을 생성했습니다.
44

IT Certification Guaranteed, The Easy Way!
이 정책의 효과는 무엇입니까?
A. 사용자는 us-east-1을 제외한 모든 AWS 리전에서 EC2 인스턴스를 종료할 수 있습니다.
B. 사용자는 us-east-1 리전에서 IP 주소가 10 100 100 1인 EC2 인스턴스를 종료할 수
있습니다.
C. 사용자는 사용자의 소스 IP가 다음과 같을 때 us-east-1 리전에서 EC2 인스턴스를 종료할
수 있습니다.
10.100.100.254.
D. 사용자의 소스 IP가 10.100 100인 경우 사용자는 us-east-1 리전에서 EC2 인스턴스를
종료할 수 없습니다.
254
Answer: C
Explanation:
as the policy prevents anyone from doing any EC2 action on any region except us-east-1 and
45

IT Certification Guaranteed, The Easy Way!
allows only users with source ip 10.100.100.0/24 to terminate instances. So user with source
ip 10.100.100.254 can terminate instances in us-east-1 region.
QUESTION NO: 64
날씨 예보 회사는 다양한 센서에서 지속적으로 온도 판독값을 수집합니다. 기존 데이터 수집
프로세스는 판독값을 수집하여 더 큰 Apache Parquet 파일에 집계합니다. 그런 다음
프로세스는 KMS 관리 키(CSE-KMS)를 사용하여 클라이언트 측 암호화를 사용하여 파일을
암호화합니다. 마지막으로 프로세스는 각 달력 날짜에 대한 별도의 접두사가 있는 Amazon S3
버킷에 파일을 씁니다.
회사에서는 특정 날짜에 대한 샘플 이동 평균을 구하기 위해 가끔씩 데이터에 SQL 쿼리를
실행하려고 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. 암호화된 파일을 읽도록 Amazon Athena를 구성합니다. Amazon S3에서 직접 데이터에
대한 SQL 쿼리를 실행합니다.
B. Amazon S3 Select를 사용하여 Amazon S3에 있는 데이터에 대해 직접 SQL 쿼리를
실행합니다.
C. 암호화된 파일을 읽도록 Amazon Redshift를 구성합니다. Redshift Spectrum과 Redshift
쿼리 편집기 v2를 사용하여 Amazon S3에 있는 데이터에 대해 직접 SQL 쿼리를 실행합니다.
D. 암호화된 파일을 읽도록 Amazon EMR Serverless를 구성합니다. Apache SparkSQL을
사용하여 Amazon S3에서 직접 데이터에 대한 SQL 쿼리를 실행합니다.
Answer: A
Explanation:
Amazon Athena is a serverless query service that allows you to run SQL queries directly on
data stored in Amazon S3 without the need for a data warehouse. It is cost-effective because
you only pay for the queries you run, and it can handle Apache Parquet files efficiently.
Additionally, Athena integrates with KMS, making it suitable for querying encrypted data.
Key AWS features:
* Cost-Effective: Athena charges only for the data scanned by the queries, making it a more
cost- effective solution compared to Redshift or EMR for occasional queries.
* Direct S3 Querying: Athena supports querying data directly in S3, including Parquet files,
without needing to move the data.
* AWS Documentation: Athena's compatibility with encrypted Parquet files in S3 makes it the
ideal choice for this scenario, reducing both cost and complexity.
QUESTION NO: 65
회사가 AWS 클라우드에서 결제 처리 시스템을 운영합니다. 자금이 부족하거나 기술적
문제로 인해 결제가 실패하면 사용자가 결제를 다시 제출하려고 시도합니다. 결제 재제출이
동일한 결제 ID에 대해 여러 결제 메시지를 호출하는 경우가 있습니다.
솔루션 아키텍트는 결제 처리 시스템이 메시지가 생성된 시점에 따라 동일한 결제 ID를 가진
결제 메시지를 순차적으로 수신하도록 해야 합니다. 처리 시스템은 메시지를 수신한 순서대로
메시지를 처리해야 합니다. 솔루션은 분석을 위해 모든 결제 메시지를 10일 동안 보관해야
합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬 수 있을까요? (2개를 선택하세요.)
A. 결제 ID를 파티션 키로 사용하는 Amazon DynamoDB 테이블에 결제 메시지를 작성합니다.
B. 결제 ID를 파티션 키로 사용하는 Amazon Kinesis 데이터 스트림에 결제 메시지를
46

IT Certification Guaranteed, The Easy Way!
작성합니다.
C. 결제 ID를 키로 사용하는 Memcached 클러스터용 Amazon ElastiCache에 결제 메시지를
씁니다.
D. Amazon Simple Queue Service(Amazon SQS) 대기열에 결제 메시지를 씁니다. 결제 ID를
사용하도록 메시지 속성을 설정합니다.
E. Amazon Simple Queue Service(Amazon SQS) FIFO 대기열에 결제 메시지를 씁니다.
메시지 그룹이 결제 ID를 사용하도록 설정합니다.
Answer: B E
Explanation:
Both Amazon Kinesis and SQS FIFO queues ensure the sequential processing of messages.
By using the payment ID as the partition key in Kinesis or as the message group in the SQS
FIFO queue, messages are processed in order. Both solutions also allow for long-term
retention (up to 10 days) of messages, making them suitable for this payment processing use
case.
* Option A (DynamoDB): DynamoDB does not guarantee message ordering for real-time
processing.
* Option C (ElastiCache): ElastiCache is for caching, not suitable for sequential message
processing.
* Option D (Standard SQS queue): A standard SQS queue does not guarantee ordering of
messages.
AWS References:
* Amazon Kinesis
* Amazon SQS FIFO Queues
QUESTION NO: 66
전 세계에 기자를 보유하고 있는 한 언론사는 AWS에서 방송 시스템을 호스팅하고 있습니다.
기자는 방송 시스템에 생방송을 보냅니다. 기자들은 전화기의 소프트웨어를 사용하여
RTMP(Real Time Messaging Protocol)를 통해 라이브 스트림을 보냅니다.
솔루션 설계자는 보고자에게 최고 품질의 스트림을 전송할 수 있는 기능을 제공하는 솔루션을
설계해야 합니다. 솔루션은 방송 시스템에 다시 가속화된 TCP 연결을 제공해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 사용해야 합니까?
A. Amazon CloudFront
B. AWS 글로벌 액셀러레이터
C. AWS 클라이언트 VPN
D. Amazon EC2 인스턴스 및 AWS 탄력적 IP 주소
Answer: B
Explanation:
* AWS Global Accelerator: This service provides a global fixed entry point to your
applications and optimizes the path to your application through the AWS global network,
reducing latency and improving performance.
* Accelerated TCP Connections:
* Global Accelerator uses the AWS global network to route traffic to the nearest edge
location, improving the performance and reliability of your live streams.
* It provides static IP addresses that act as a fixed entry point to your application, simplifying
DNS management.
47

IT Certification Guaranteed, The Easy Way!
* High-Quality Streams:
* By leveraging Global Accelerator, reporters can send live streams with the highest quality
and low latency.
* This service automatically reroutes traffic to the nearest available AWS Region, ensuring
consistent performance even during traffic spikes or failures.
* Operational Efficiency: Using Global Accelerator simplifies the network setup and provides
an optimized path for live streams without the need for complex configurations, making it an
efficient solution for real-time streaming applications.
References:
* AWS Global Accelerator
* How Global Accelerator Works
QUESTION NO: 67
솔루션 설계자는 대용량 SaaS(Software as a Service) 플랫폼에 대한 재해 복구(DR) 계획을
수립해야 합니다. 플랫폼의 모든 데이터는 Amazon Aurora MySQL DB 클러스터에
저장됩니다.
DR 계획은 데이터를 보조 AWS 리전에 복제해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
Aurora 클러스터에 MySQL 바이너리 로그 복제 사용
A. 보조 지역의 Aurora 클러스터에 MySQL 바이너리 로그 복제를 사용합니다. 보조 지역의
Aurora 클러스터에 DB 인스턴스 하나를 프로비저닝합니다.
B. DB 클러스터에 대한 Aurora 글로벌 데이터베이스를 설정합니다. 설정이 완료되면 보조
리전에서 DB 인스턴스를 제거합니다.
C. AWS QMS(AWS Database Migration Service)를 사용하여 보조 리전의 Aurora 클러스터에
데이터를 지속적으로 복제합니다. 보조 리전에서 DB 인스턴스를 제거합니다.
D. DB 클러스터에 대한 Aurora 글로벌 데이터베이스 설정 보조 리전에 최소 하나의 DB
인스턴스를 지정합니다.
Answer: D
Explanation:
"Replication from the primary DB cluster to all secondaries is handled by the Aurora storage
layer rather than by the database engine, so lag time for replicating changes is minimal-
typically, less than 1 second. Keeping the database engine out of the replication process
means that the database engine is dedicated to processing workloads. It also means that you
don't need to configure or manage the Aurora MySQL binlog (binary logging) replication."
QUESTION NO: 68
컨설팅 회사는 전 세계 고객에게 전문 서비스를 제공합니다. 이 회사는 고객이 AWS에서
데이터를 수집하고 분석하는 것을 가속화하기 위한 솔루션과 도구를 제공합니다. 이 회사는
고객이 셀프 서비스 목적으로 사용할 수 있는 공통 솔루션과 도구 세트를 중앙에서 관리하고
배포해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 고객을 위한 AWS Cloud Formation 템플릿을 만듭니다.
B. 고객을 위한 AWS 서비스 카탈로그 제품을 생성합니다.
C. 고객을 위한 AWS Systems Manager 템플릿을 만듭니다.
D. 고객을 위한 AWS Config 항목을 생성합니다.
48

IT Certification Guaranteed, The Easy Way!
Answer: B
Explanation:
AWS Service Catalog allows organizations to centrally manage commonly deployed IT
services and offers self-service deployment capabilities to customers. By creating Service
Catalog products, the consulting company can package their solutions and tools for easy
reuse by customers while maintaining central control over configuration and access. This
provides a standardized and automated solution with the least operational overhead for
managing and deploying solutions across different customers.
* Option A (CloudFormation): CloudFormation templates are useful but don't provide the
same level of management and user-friendly self-service capabilities as Service Catalog.
* Option C (Systems Manager): Systems Manager is more focused on managing
infrastructure and doesn't offer the same self-service capabilities.
* Option D (AWS Config): AWS Config is used for tracking resource configurations, not for
deploying solutions.
AWS References:
* AWS Service Catalog
QUESTION NO: 69
솔루션 아키텍트는 회사의 AWS 계정에 대한 AWS Identity and Access Management(1AM)
인증 모델을 설계하고 있습니다. 회사는 AWS 계정의 AWS 서비스 및 리소스에 대한 전체
액세스 권한을 갖도록 5명의 특정 직원을 지정했습니다.
솔루션 설계자는 지정된 직원 5명 각각에 대해 오전 1시 사용자를 생성하고 오전 1시 사용자
그룹을 생성했습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AdministratorAccess 리소스 기반 정책을 1AM 사용자 그룹에 연결합니다. 지정된 직원 IAM
사용자 5명을 각각 1AM 사용자 그룹에 배치합니다.
B. SystemAdministrator 자격 증명 기반 정책을 IAM 사용자 그룹에 연결합니다. 지정된 직원
IAM 사용자 5명을 각각 IAM 사용자 그룹에 배치합니다.
C. AdministratorAccess 자격 증명 기반 정책을 IAM 사용자 그룹에 연결합니다. 지정된 직원
IAM 사용자 5명을 각각 IAM 사용자 그룹에 배치합니다.
D. SystemAdministrator 리소스 기반 정책을 IAM 사용자 그룹에 연결합니다. 지정된 직원 IAM
사용자 5명을 각각 IAM 사용자 그룹에 배치합니다.
Answer: C
Explanation:
This solution meets the requirements because it uses the following components and features
:
* AdministratorAccess identity-based policy: This is an AWS managed policy that provides
full access to AWS services and resources1. By attaching this policy to the IAM user group,
the solutions architect can grant the permissions needed for the designated employees to
perform any task in the AWS account.
* IAM user group: This is a collection of IAM users that share common permissions2. By
creating a user group and adding the five designated employees as members, the solutions
architect can simplify the management of permissions and reduce the risk of human errors or
inconsistencies.
* IAM users: These are identities that represent the designated employees in AWS2. By
49

IT Certification Guaranteed, The Easy Way!
creating an IAM user for each employee and requiring them to sign in with their own
credentials, the solutions architect can enhance the security and accountability of the AWS
account.
QUESTION NO: 70
회사 시설에는 건물 전체의 모든 입구에 배지 판독기가 있습니다. 배지가 스캔되면 독자는
HTTPS를 통해 메시지를 보내 특정 입구에 누가 접근하려고 했는지 알려줍니다.
솔루션 설계자는 센서에서 보내는 이러한 메시지를 처리하기 위한 시스템을 설계해야 합니다.
솔루션은 가용성이 높아야 하며 회사의 보안 팀이 분석할 수 있도록 결과를 제공해야 합니다.
솔루션 아키텍트는 어떤 시스템 아키텍처를 권장해야 합니까?
A. Amazon EC2 인스턴스를 시작하여 HTTPS 엔드포인트 역할을 하고 메시지를 처리합니다.
결과를 Amazon S3 버킷에 저장하도록 EC2 인스턴스를 구성합니다.
B. Amazon API Gateway에서 HTTPS 엔드포인트를 생성합니다. AWS Lambda 함수를
호출하여 메시지를 처리하고 결과를 Amazon DynamoDB 테이블에 저장하도록 API
게이트웨이 엔드포인트를 구성합니다.
C. Amazon Route 53을 사용하여 수신 센서 메시지를 AWS Lambda 함수로 보냅니다.
메시지를 처리하고 결과를 Amazon DynamoDB 테이블에 저장하도록 Lambda 함수를
구성합니다.
D. Amazon S3용 게이트웨이 VPC 엔드포인트를 생성합니다. 센서 데이터가 VPC
엔드포인트를 통해 S3 버킷에 직접 기록될 수 있도록 시설 네트워크에서 VPC로의 Site-to-Site
VPN 연결을 구성합니다.
Answer: B
Explanation:
Deploy Amazon API Gateway as an HTTPS endpoint and AWS Lambda to process and save
the messages to an Amazon DynamoDB table. This option provides a highly available and
scalable solution that can easily handle large amounts of data. It also integrates with other
AWS services, making it easier to analyze and visualize the data for the security team.
QUESTION NO: 71
회사에는 Amazon Route 53을 통해 트래픽이 전달되는 10개 이상의 Amazon EC2 인스턴스를
호스팅하는 웹 애플리케이션이 있습니다. 회사에서 애플리케이션을 탐색하려고 할 때 때때로
시간 초과 오류가 발생합니다. 네트워킹 팀은 일부 DNS 쿼리가 비정상 인스턴스의 IP 주소를
반환하여 시간 초과 오류가 발생한다는 사실을 발견했습니다.
이러한 시간 초과 오류를 극복하기 위해 솔루션 설계자는 무엇을 구현해야 합니까?
A. 각 EC2 인스턴스에 대한 Route 53 단순 라우팅 정책 레코드를 생성합니다. 각 레코드에
상태 확인을 연결합니다.
B. 각 EC2 인스턴스에 대해 Route 53 장애 조치 라우팅 정책 레코드를 생성합니다. 각
레코드에 상태 확인을 연결합니다.
C. EC2 인스턴스를 오리진으로 사용하여 Amazon CloudFront 배포를 생성합니다. 상태
확인을 EC2 인스턴스와 연결합니다.
D. EC2 인스턴스 앞에 상태 확인을 통해 Application Load Balancer(ALB)를 생성합니다.
Route 53에서 ALB로 이동합니다.
Answer: D
Explanation:
An Application Load Balancer (ALB) allows you to distribute incoming traffic across multiple
50

IT Certification Guaranteed, The Easy Way!
backend instances, and can automatically route traffic to healthy instances while removing
traffic from unhealthy instances. By using an ALB in front of the EC2 instances and routing
traffic to it from Route 53, the load balancer can perform health checks on the instances and
only route traffic to healthy instances, which should help to reduce or eliminate timeout errors
caused by unhealthy instances.
QUESTION NO: 72
한 회사가 API를 기반으로 하는 클라우드 커뮤니케이션 플랫폼을 설계하고 있습니다.
애플리케이션은 NLB(Network Load Balancer) 뒤의 Amazon EC2 인스턴스에서
호스팅됩니다. 회사는 Amazon API Gateway를 사용하여 외부 사용자에게 API를 통해
애플리케이션에 대한 액세스를 제공합니다. 회사는 SQL 주입과 같은 웹 공격으로부터
플랫폼을 보호하고 대규모의 정교한 DDoS 공격을 탐지하고 완화하기를 원합니다.
가장 강력한 보호 기능을 제공하는 솔루션 조합은 무엇입니까? (2개를 선택하세요.)
A. AWS WAF를 사용하여 NLB를 보호합니다.
B. NLB와 함께 AWS Shield Advanced를 사용합니다.
C. AWS WAF를 사용하여 Amazon API Gateway를 보호합니다.
D. AWS Shield Standard와 함께 Amazon GuardDuty를 사용합니다.
E. Amazon API Gateway와 함께 AWS Shield Standard를 사용합니다.
Answer: B C
Explanation:
AWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2
instances, Elastic Load Balancing load balancers, CloudFront distributions, Route 53 hosted
zones, and AWS Global Accelerator standard accelerators.
AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests
that are forwarded to your protected web application resources. You can protect the following
resource types:
Amazon CloudFront distribution
Amazon API Gateway REST API
Application Load Balancer
AWS AppSync GraphQL API
Amazon Cognito user pool
https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html
QUESTION NO: 73
한 회사가 5개의 Amazon EC2 인스턴스에 애플리케이션을 배포합니다. ALB(Applicatin Load
Balancer)는 대상 그룹을 사용하여 인스턴스에 트래픽을 분산합니다. 각 인스턴스의 평균
CPU 사용량은 대부분 10% 미만입니다. 가끔 65%까지 급등합니다.
솔루션 설계자는 애플리케이션의 확장성을 자동화하는 솔루션을 구현해야 합니다. 솔루션은
아키텍처 비용을 최적화해야 하며 급증이 발생할 때 애플리케이션에 충분한 CPU 리소스가
있는지 확인해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. CPUUtilization 지표가 20% 미만일 때 ALARM 상태로 전환되는 Amazon CloudWatch
경보를 생성합니다. ALB 대상 그룹의 EC2 인스턴스 중 하나를 종료하기 위해 CloudWatch
경보가 호출하는 AWS Lambda 함수를 생성합니다.
B. EC2 Auto Scaling을 생성합니다. 기존 ALB를 로드 밸런서로 선택하고 기존 대상 그룹을
51

IT Certification Guaranteed, The Easy Way!
대상 그룹으로 선택합니다. ASGAverageCPUUtilization 지표를 기반으로 대상 추적 조정
정책을 설정합니다. 최소 인스턴스를 2로, 원하는 용량을 3으로, 원하는 용량을 3으로, 최대
인스턴스를 6으로, 목표 값을 50%로 설정합니다. 그리고 EC2 인스턴스를 Auto Scaling
그룹에 추가합니다.
C. EC2 Auto Scaling을 생성합니다. 기존 ALB를 로드 밸런서로 선택하고 기존 대상 그룹을
선택합니다. 최소 인스턴스를 2로, 원하는 용량을 3으로, 최대 인스턴스를 6으로 설정합니다.
EC2 인스턴스를 Scaling 그룹에 추가합니다.
D. 두 개의 Amazon CloudWatch 경보를 생성합니다. 평균 CPUTUilization 지표가 20% 미만일
때 ALARM satet을 입력하도록 첫 번째 CloudWatch 경보를 구성합니다. 평균 CPUUtilization
지표가 50%를 초과하면 ALARM 상태로 들어가도록 두 번째 CloudWatch 경보를 구성합니다.
이메일 메시지를 보내기 위해 Amazon Simple 알림 서비스(Amazon SNS) 주제에 게시하도록
경보를 구성합니다. 메시지를 받은 후 로그인하여 실행 중인 EC2 인스턴스 수를 줄이거나
늘리세요.
Answer: B
Explanation:
* An Auto Scaling group will automatically scale the EC2 instances to match changes in
demand. This optimizes cost by only running as many instances as needed.
* A target tracking scaling policy monitors the ASGAverageCPUUtilization metric and scales
to keep the average CPU around the 50% target value. This ensures there are enough
resources during CPU surges.
* The ALB and target group are reused, so the application architecture does not change. The
Auto Scaling group is associated to the existing load balancer setup.
* A minimum of 2 and maximum of 6 instances provides the ability to scale between 3 and 6
instances as needed based on demand.
* Costs are optimized by starting with only 3 instances (the desired capacity) and scaling up
as needed. When CPU usage drops, instances are terminated to match the desired capacity.
QUESTION NO: 74
한 회사가 AWS에서 실시간 데이터 수집 솔루션을 실행하고 있습니다. 이 솔루션은 최신
버전의 Amazon Managed Streaming for Apache Kafka(Amazon MSK)로 구성됩니다. 이
솔루션은 3개의 가용 영역에 걸쳐 프라이빗 서브넷의 VPC에 배포됩니다.
솔루션 설계자는 인터넷을 통해 공개적으로 사용할 수 있도록 데이터 수집 솔루션을
재설계해야 합니다. 전송 중인 데이터도 암호화되어야 합니다.
가장 효율적인 운영 효율성으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 기존 VPC에서 퍼블릭 서브넷을 구성합니다. 퍼블릭 서브넷에 MSK 클러스터를
배포합니다. 상호 TLS 인증을 활성화하려면 MSK 클러스터 보안 설정을 업데이트하세요.
B. 퍼블릭 서브넷이 있는 새 VPC를 생성합니다. 퍼블릭 서브넷에 MSK 클러스터를
배포합니다. 상호 TLS 인증을 활성화하려면 MSK 클러스터 보안 설정을 업데이트하세요.
C. 프라이빗 서브넷을 사용하는 ALB(Application Load Balancer)를 배포합니다. HTTPS
프로토콜에 대한 VPC CIDR 블록의 인바운드 트래픽을 허용하도록 ALB 보안 그룹 인바운드
규칙을 구성합니다.
D. 프라이빗 서브넷을 사용하는 NLB(Network Load Balancer)를 배포합니다. 인터넷을 통한
HTTPS 통신을 위해 NLB 수신기를 구성합니다.
Answer: A
Explanation:
52

IT Certification Guaranteed, The Easy Way!
The solution that meets the requirements with the most operational efficiency is to configure
public subnets in the existing VPC and deploy an MSK cluster in the public subnets. This
solution allows the data ingestion solution to be publicly available over the internet without
creating a new VPC or deploying a load balancer.
The solution also ensures that the data in transit is encrypted by enabling mutual TLS
authentication, which requires both the client and the server to present certificates for
verification. This solution leverages the public access feature of Amazon MSK, which is
available for clusters running Apache Kafka 2.6.0 or later versions1.
The other solutions are not as efficient as the first one because they either create
unnecessary resources or do not encrypt the data in transit. Creating a new VPC with public
subnets would incur additional costs and complexity for managing network resources and
routing. Deploying an ALB or an NLB would also add more costs and latency for the data
ingestion solution. Moreover, an ALB or an NLB would not encrypt the data in transit by itself,
unless they are configured with HTTPS listeners and certificates, which would require
additional steps and maintenance. Therefore, these solutions are not optimal for the given
requirements.
References:
* Public access - Amazon Managed Streaming for Apache Kafka
QUESTION NO: 75
한 회사가 Application Load Balancer를 사용하여 세 개의 AWS 지역에 애플리케이션을
배포하고 있습니다. Amazon Route 53은 이들 지역 간에 트래픽을 분산하는 데 사용됩니다.
솔루션 아키텍트는 MOST 고성능 경험을 제공하기 위해 어떤 Route 53 구성을 사용해야
합니까?
A. 대기 시간 정책이 포함된 A 레코드를 생성합니다.
B. 지리적 위치 정책을 사용하여 A 레코드를 만듭니다.
C. 장애 조치 정책을 사용하여 CNAME 레코드를 생성합니다.
D. 지리 근접 정책을 사용하여 CNAME 레코드를 생성합니다.
Answer: A
Explanation:
To provide the most high-performing experience for the users of the application, a solutions
architect should use a latency routing policy for the Route 53 A record. This policy allows
Route 53 to route traffic to the AWS Region that provides the lowest possible latency for the
users1. A latency routing policy can also improve the availability of the application, as Route
53 can automatically route traffic to another Region if the primary Region becomes
unavailable2.
References:
* 1: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-
policy.html#routing-policy- latency
* 2: https://aws.amazon.com/route53/faqs/#Latency_Based_Routing
QUESTION NO: 76
한 회사에서 온라인 멀티플레이어 게임용 네트워크를 설계하고 있습니다. 이 게임은 UDP
네트워킹 프로토콜을 사용하며 8개 AWS 리전에 배포됩니다. 네트워크 아키텍처는 최종
사용자에게 고품질 게임 경험을 제공하기 위해 대기 시간과 패킷 손실을 최소화해야 합니다.
53

IT Certification Guaranteed, The Easy Way!
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 각 리전에 Transit Gateway를 설정합니다. 각 전송 게이트웨이 간에 리전 간 피어링 연결을
생성합니다.
B. 각 리전의 UDP 리스너 및 엔드포인트 그룹을 사용하여 AWS Global Accelerator를
설정합니다.
C. UDP를 활성화하여 Amazon CloudFront를 설정합니다. 각 리전에서 오리진을 구성합니다.
D. 각 리전 간에 VPC 피어링 메시를 설정합니다. 각 VPC에 대해 UDP를 활성화합니다.
Answer: B
Explanation:
The best solution for this situation is option B, setting up AWS Global Accelerator with UDP
listeners and endpoint groups in each Region. AWS Global Accelerator is a networking
service that improves the availability and performance of internet applications by routing user
requests to the nearest AWS Region [1].
It also improves the performance of UDP applications by providing faster, more reliable data
transfers with lower latency and fewer packet losses. By setting up UDP listeners and
endpoint groups in each Region, Global Accelerator will route traffic to the nearest Region for
faster response times and a better user experience.
QUESTION NO: 77
한 회사에는 동적 트래픽 부하를 처리하는 웹사이트가 있습니다. 웹사이트 아키텍처는 예약된
스케일링을 사용하도록 구성된 Auto Scaling 그룹의 Amazon EC2 인스턴스를 기반으로
합니다. 각 EC2 인스턴스는 Amazon Elastic File System(Amazon EFS) 볼륨에서 코드를
실행하고 공유 데이터를 동일한 볼륨에 다시 저장합니다.
회사는 웹사이트 비용을 최적화하고자 합니다.
어떤 솔루션이 이 요구 사항을 충족시킬까요?
A. 원하는 인스턴스 수를 설정하기 위해 자동 스케일링 그룹을 재구성합니다. 예약된
스케일링을 끕니다.
B. 더 큰 EC2 인스턴스를 사용하는 자동 크기 조정 그룹에 대한 새로운 시작 템플릿 버전을
만듭니다.
C. 대상 추적 확장 정책을 사용하도록 자동 확장 그룹을 재구성합니다.
D. EFS 볼륨을 인스턴스 스토어 볼륨으로 바꿉니다.
Answer: C
Explanation:
* A. Fixed desired instances: Does not adapt to traffic load fluctuations, leading to
inefficiencies.
* B. Larger EC2 instances: Increases costs unnecessarily.
* C. Target tracking scaling policy: Adjusts capacity based on actual demand, optimizing
costs.
* D. Instance store volumes: Not persistent and unsuitable for shared data across instances.
References: Auto Scaling
QUESTION NO: 78
회사에는 AWS에 배포된 3티어 웹 애플리케이션이 있습니다. 웹 서버는 VPC의 퍼블릭
서브넷에 배포됩니다. 애플리케이션 서버와 데이터베이스 서버는 동일한 VPC의 프라이빗
서브넷에 배포됩니다. 회사는 AWS Marketplace의 타사 가상 방화벽 어플라이언스를 검사
54

IT Certification Guaranteed, The Easy Way!
VPC에 배포했습니다. 어플라이언스는 IP 패킷을 수락할 수 있는 IP 인터페이스로 구성됩니다
.
솔루션 설계자는 웹 애플리케이션을 어플라이언스와 통합하여 트래픽이 웹 서버를 학습하기
전에 애플리케이션에 대한 모든 트래픽을 검사해야 합니다. 최소한의 운영 오버헤드로 이러한
요구 사항을 해결하는 솔루션은 무엇입니까?
A. 애플리케이션 VPC의 퍼블릭 서브넷에 Network Load Balancer를 생성하여 패킷 검사를
위해 어플라이언스로 트래픽을 라우팅합니다.
B. 애플리케이션 VPC의 퍼블릭 서브넷에 Application Load Balancer를 생성하여 패킷 검사를
위해 트래픽을 어플라이언스로 라우팅합니다.
C. 검사 VPC에 전송 게이트웨이를 배포합니다. 전송 게이트웨이를 통해 수신 포켓을
라우팅하도록 라우팅 테이블을 구성합니다.
D. 검사 VPC에 게이트웨이 로드 밸런서를 배포합니다. 게이트웨이 로드 밸런서 엔드포인트를
생성하여 수신 패킷을 수신하고 패킷을 어플라이언스에 전달합니다.
Answer: D
Explanation:
https://aws.amazon.com/blogs/networking-and-content-delivery/scaling-network-traffic-
inspection-using-aws- gateway-load-balancer/
QUESTION NO: 79
회사는 VPC의 프라이빗 서브넷에 있는 Amazon EC2 인스턴스에 배포되는 웹
애플리케이션을 실행합니다. 퍼블릭 서브넷 전체에 걸쳐 확장되는 ALB(Application Load
Balancer)는 웹 트래픽을 EC2 인스턴스로 전달합니다. 회사는 ALB에서 EC2 인스턴스로의
인바운드 트래픽을 제한하는 동시에 EC2 인스턴스의 프라이빗 서브넷 내부 또는 외부의 다른
소스로부터의 액세스를 방지하는 새로운 보안 조치를 구현하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 인터넷에서 EC2 인스턴스의 프라이빗 IP 주소로 트래픽을 전달하도록 라우팅 테이블에
경로를 구성합니다.
B. ALB에 대한 보안 그룹에서 들어오는 트래픽만 허용하도록 EC2 인스턴스에 대한 보안
그룹을 구성합니다.
C. EC2 인스턴스를 퍼블릭 서브넷으로 이동합니다. EC2 인스턴스에 탄력적 IP 주소 세트를
제공합니다.
D. 모든 포트에서 모든 TCP 트래픽을 허용하도록 ALB에 대한 보안 그룹을 구성합니다.
Answer: B
Explanation:
To restrict inbound traffic from the ALB to the EC2 instances, the security group for the EC2
instances should only allow traffic that comes from the security group for the ALB. This way,
the EC2 instances can only receive requests from the ALB and not from any other source
inside or outside the private subnet.
References:
* Security Groups for Your Application Load Balancers
* Security Groups for Your VPC
QUESTION NO: 80
한 회사에서 고객 요구를 지원하기 위해 애플리케이션을 개발하고 있습니다. 회사는 동일한
가용 영역 내의 여러 Amazon EC2 Nitro 기반 인스턴스에 애플리케이션을 배포하려고 합니다.
55

IT Certification Guaranteed, The Easy Way!
또한 회사는 더 높은 애플리케이션 가용성을 달성하기 위해 여러 EC2 Nitro 기반 인스턴스의
여러 블록 스토리지 볼륨에 동시에 쓸 수 있는 기능을 애플리케이션에 제공하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon Elastic Block Store(Amazon EBS) 다중 연결과 함께 범용 SSD(gp3) EBS 볼륨을
사용합니다.
B. Amazon Elastic Block Store(Amazon EBS) 다중 연결과 함께 처리량 최적화 HDD(st1) EBS
볼륨을 사용합니다.
C. Amazon Elastic Block Store(Amazon EBS) 다중 연결과 함께 프로비저닝된 IOPS SSD(io2)
EBS 볼륨을 사용합니다.
D. Amazon Elastic Block Store(Amazon E8S) 다중 연결과 함께 범용 SSD(gp2) EBS 볼륨을
사용합니다.
Answer: C
Explanation:
* Understanding the Requirement: The application needs to write to multiple block storage
volumes in multiple EC2 Nitro-based instances simultaneously to achieve higher availability.
* Analysis of Options:
* General Purpose SSD (gp3) with Multi-Attach: Supports Multi-Attach but does not provide
the highest performance required for critical applications.
* Throughput Optimized HDD (st1) with Multi-Attach: Not suitable for applications requiring
high performance and low latency.
* Provisioned IOPS SSD (io2) with Multi-Attach: Provides high performance and durability,
suitable for applications requiring simultaneous writes and high availability.
* General Purpose SSD (gp2) with Multi-Attach: Similar to gp3 but with less flexibility and
performance.
* Best Solution:
* Provisioned IOPS SSD (io2) with Multi-Attach: This solution ensures the highest
performance and availability for the application by allowing multiple EC2 instances to attach
to and write to the same EBS volume simultaneously.
References:
* Amazon EBS Multi-Attach
* Provisioned IOPS SSD (io2)
QUESTION NO: 81
솔루션 아키텍트가 두 개의 IAM 정책을 만들었습니다: Policy1과 Policy2. 두 정책 모두 IAM
그룹에 연결됩니다.
56

IT Certification Guaranteed, The Easy Way!
클라우드 엔지니어가 IAM 그룹에 IAM 사용자로 추가되었습니다. 클라우드 엔지니어는 어떤
작업을 수행할 수 있을까요?
A. IAM 사용자 삭제
57

IT Certification Guaranteed, The Easy Way!
B. 디렉토리 삭제
C. Amazon EC2 인스턴스 삭제
D. Amazon CloudWatch Logs에서 로그 삭제
Answer: C
Explanation:
https://awscli.amazonaws.com/v2/documentation/api/latest/reference/ds/index.html
QUESTION NO: 82
한 회사는 Amazon DynamoDB 테이블을 사용하여 회사에서 기기에서 수신하는 데이터를
저장합니다. DynamoDB 테이블은 고객 대면 웹사이트에서 고객 기기의 최근 활동을 표시하는
것을 지원합니다. 이 회사는 쓰기 및 읽기에 대한 프로비저닝된 처리량으로 테이블을
구성했습니다. 이 회사는 매일 고객 기기 데이터에 대한 성능 지표를 계산하려고 합니다. 이
솔루션은 테이블의 프로비저닝된 읽기 및 쓰기 용량에 최소한의 영향을 미쳐야 합니다. 어떤
솔루션이 이러한 요구 사항을 충족할까요?
A. Amazon Athena DynamoDB 커넥터와 함께 Amazon Athena SQL 쿼리를 사용하여 반복
일정에 따라 성능 지표를 계산합니다.
B. AWS Glue DynamoDB 내보내기 커넥터와 함께 AWS Glue 작업을 사용하여 반복 일정에
따라 성능 지표를 계산합니다.
C. Amazon Redshift COPY 명령을 사용하여 반복 일정에 대한 성능 지표를 계산합니다.
D. Apache Hive 외부 테이블과 함께 Amazon EMR 작업을 사용하여 반복 일정에 따라 성능
지표를 계산합니다.
Answer: A
Explanation:
Amazon Athena provides a cost-effective, serverless way to query data without affecting the
performance of DynamoDB. By using the Athena DynamoDB connector, the company can
perform the necessary SQL queries without consuming read capacity on the DynamoDB
table, which is essential for minimizing impact on provisioned throughput.
Key benefits:
* Minimal Impact on Provisioned Capacity: Athena queries do not directly impact
DynamoDB's read capacity, making it ideal for running analytics without affecting the
customer-facing workloads.
* Cost-Effective: Athena is a serverless solution, meaning you pay only for the queries you
run, making it highly cost-effective compared to running a dedicated cluster like Amazon
EMR or Redshift.
* AWS Documentation: The use of Athena to query DynamoDB through its connector aligns
with AWS's best practices for performance efficiency and cost optimization.
QUESTION NO: 83
한 회사에서 PostgreSQL용 Amazon RDS를 사용하는 애플리케이션을 실행하고 있습니다.
애플리케이션은 평일 업무 시간에만 트래픽을 수신합니다. 회사는 이러한 사용량을 기반으로
비용을 최적화하고 운영 오버헤드를 줄이고 싶어합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS의 인스턴스 스케줄러를 사용하여 시작 및 중지 일정을 구성합니다.
B. 자동 백업을 끕니다. 데이터베이스의 주간 수동 스냅샷을 생성합니다.
58

IT Certification Guaranteed, The Easy Way!
C. 최소 CPU 사용률을 기준으로 데이터베이스를 시작 및 중지하는 사용자 지정 AWS Lambda
함수를 생성합니다.
D. 모든 Upfront 예약 DB 인스턴스를 구매합니다.
Answer: A
Explanation:
https://aws.amazon.com/solutions/implementations/instance-scheduler-on-aws/?nc1=h_ls
The Instance Scheduler on AWS solution automates the starting and stopping of Amazon
Elastic Compute Cloud (Amazon EC2) and Amazon Relational Database Service (Amazon
RDS) instances. This solution helps reduce operational costs by stopping resources that are
not in use and starting them when they are needed1. The solution allows you to define
custom schedules and periods using a command line interface (CLI) or an SSM maintenance
window1. You can also choose between different payment options for the reserved DB
instances, such as No Upfront, Partial Up front, or All Upfront2.
QUESTION NO: 84
회사에서 온프레미스 PostgreSQL 데이터베이스를 Amazon RDS for PostgreSQL DB
인스턴스로 옮겼습니다. 회사는 신제품을 성공적으로 출시했습니다. 데이터베이스의
워크로드가 증가했습니다.
회사는 인프라를 추가하지 않고 더 큰 워크로드를 수용하려고 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 전체 워크로드에 대해 예약된 DB 인스턴스를 구매합니다. PostgreSQL DB 인스턴스용
Amazon RDS를 더 크게 만듭니다.
B. Amazon RDS for PostgreSQL DB 인스턴스를 다중 AZ DB 인스턴스로 만듭니다.
C. 총 워크로드에 대해 예약된 DB 인스턴스를 구매합니다. PostgreSQL DB 인스턴스용 다른
Amazon RDS를 추가합니다.
D. Amazon RDS for PostgreSQL DB 인스턴스를 온디맨드 DB 인스턴스로 만듭니다.
Answer: A
Explanation:
This answer is correct because it meets the requirements of accommodating the larger
workload without adding infrastructure and minimizing the cost. Reserved DB instances are a
billing discount applied to the use of certain on-demand DB instances in your account.
Reserved DB instances provide you with a significant discount compared to on-demand DB
instance pricing. You can buy reserved DB instances for the total workload and choose
between three payment options: No Upfront, Partial Upfront, or All Upfront. You can make the
Amazon RDS for PostgreSQL DB instance larger by modifying its instance type to a higher
performance class. This way, you can increase the CPU, memory, and network capacity of
your DB instance and handle the increased workload.
References:
* https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide
/USER_WorkingWithReservedDBInstances.html
*
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.ht
ml
QUESTION NO: 85
59

IT Certification Guaranteed, The Easy Way!
회사는 1Gbps AWS Direct Connect 연결 비용을 최소화해야 합니다. 회사의 평균 연결
사용률은 10% 미만입니다. 솔루션 설계자는 보안을 손상시키지 않으면서 비용을 절감할
솔루션을 추천해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 새로운 1Gbps 직접 연결 연결을 설정합니다. 다른 AWS 계정과 연결을 공유합니다.
B. AWS Management Console에서 새로운 200Mbps Direct Connect 연결을 설정합니다.
C. 1Gbps 연결을 주문하려면 AWS Direct Connect 파트너에게 문의하십시오. 다른 AWS
계정과 연결을 공유합니다.
D. 기존 AWS 계정에 대한 200Mbps 호스팅 연결을 주문하려면 AWS Direct Connect
파트너에게 문의하십시오.
Answer: D
Explanation:
company need to setup a cheaper connection (200 M) but B is incorrect because you can
only order port speeds of 1, 10, or 100 Gbps for more flexibility you can go with hosted
connection, You can order port speeds between 50 Mbps and 10 Gbps.
https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity- options/aws-direct
-connect.html
QUESTION NO: 86
한 회사가 관계형 데이터베이스를 사용하여 사용자 데이터와 애플리케이션 구성을 저장하는
새로운 애플리케이션을 개발하고 있습니다. 이 회사는 애플리케이션이 꾸준한 사용자 증가를
보일 것으로 예상합니다. 이 회사는 데이터베이스 사용이 가변적이고 읽기 중심적이며 가끔씩
쓰기가 있을 것으로 예상합니다.
이 회사는 데이터베이스 솔루션의 비용을 최적화하고자 합니다. 이 회사는 필요한 성능을
제공하는 AWS 관리형 데이터베이스 솔루션을 사용하고자 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. Amazon RDS에 데이터베이스를 배포합니다. Provisioned IOPS SSD 스토리지를 사용하여
읽기 및 쓰기 작업에 대한 일관된 성능을 보장합니다.
B. Amazon Aurora Serveriess에 데이터베이스를 배포하여 실제 사용량에 따라 데이터베이스
용량을 자동으로 확장하여 작업 부하를 수용합니다.
C. Amazon DynamoDB에 데이터베이스를 배포합니다. 온디맨드 용량 모드를 사용하여 작업
부하를 수용하기 위해 처리량을 자동으로 확장합니다.
D. Amazon RDS에 데이터베이스 배포 자기 스토리지를 사용하고 읽기 복제본을 사용하여
작업 부하를 수용합니다.
Answer: B
Explanation:
Amazon Aurora Serverless is a cost-effective, on-demand, autoscaling configuration for
Amazon Aurora. It automatically adjusts the database's capacity based on the current
demand, which is ideal for workloads with variable and unpredictable usage patterns. Since
the application is expected to be read-heavy with occasional writes and steady growth,
Aurora Serverless can provide the necessary performance without requiring the management
of database instances.
* Cost-Optimization: Aurora Serverless only charges for the database capacity you use,
making it a more cost-effective solution compared to always running provisioned database
60

IT Certification Guaranteed, The Easy Way!
instances, especially for workloads with fluctuating demand.
* Scalability: It automatically scales database capacity up or down based on actual usage,
ensuring that you always have the right amount of resources available.
* Performance: Aurora Serverless is built on the same underlying storage as Amazon Aurora,
providing high performance and availability.
* Why Not Other Options?:
* Option A (RDS with Provisioned IOPS SSD): While Provisioned IOPS SSD ensures
consistent performance, it is generally more expensive and less flexible compared to the
autoscaling nature of Aurora Serverless.
* Option C (DynamoDB with On-Demand Capacity): DynamoDB is a NoSQL database and
may not be the best fit for applications requiring relational database features.
* Option D (RDS with Magnetic Storage and Read Replicas): Magnetic storage is outdated
and generally slower. While read replicas help with read-heavy workloads, the overall
performance might not be optimal, and magnetic storage doesn't provide the necessary
performance.
AWS References:
* Amazon Aurora Serverless - Information on how Aurora Serverless works and its use cases
.
* Amazon Aurora Pricing - Details on the cost-effectiveness of Aurora Serverless.
QUESTION NO: 87
회사의 웹 애플리케이션이 Application Load Balancer 뒤의 Amazon EC2 인스턴스에서
실행되고 있습니다. 회사는 최근 정책을 변경하여 이제 특정 국가에서만 애플리케이션에
액세스하도록 요구하고 있습니다.
이 요구 사항을 충족하는 구성은 무엇입니까?
A. EC2 인스턴스에 대한 보안 그룹을 구성합니다.
B. Application Load Balancer에 보안 그룹을 구성합니다.
C. VPC의 Application Load Balancer에서 AWS WAF를 구성합니다.
D. EC2 인스턴스가 포함된 서브넷에 대한 네트워크 ACL을 구성합니다.
Answer: C
Explanation:
https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-
match/
QUESTION NO: 88
한 회사가 Amazon Elastic Block Store(Amazon EBS) 볼륨을 사용하여 Amazon EC2
인스턴스에서 프로덕션 워크로드를 실행합니다. 솔루션 아키텍트는 현재 EBS 볼륨 비용을
분석하고 최적화를 권장해야 합니다. 권장 사항에는 추정 월별 절감 기회가 포함되어야
합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon Inspector 보고를 사용하여 최적화를 위한 EBS 볼륨 권장 사항을 생성합니다.
B. AWS Systems Manager 보고를 사용하여 최적화를 위한 EBS 볼륨 권장 사항을
결정합니다.
C. Amazon CloudWatch 메트릭 보고를 사용하여 최적화를 위한 EBS 볼륨 권장 사항을
결정합니다.
61

IT Certification Guaranteed, The Easy Way!
D. AWS Compute Optimizer를 사용하여 최적화를 위한 EBS 볼륨 권장 사항을 생성합니다.
Answer: D
Explanation:
AWS Compute Optimizer provides detailed recommendations for optimizing Amazon EBS
volumes, including insights into underutilized volumes, inefficient configurations, and potential
cost savings. It analyzes usage patterns and provides recommendations based on historical
data, making it the ideal tool for this use case.
* Option A (Amazon Inspector): Amazon Inspector is for security assessments, not for cost
optimization.
* Option B (Systems Manager): Systems Manager does not specifically provide EBS
optimization recommendations.
* Option C (CloudWatch): CloudWatch metrics help monitor usage but do not offer
optimization recommendations like Compute Optimizer.
AWS References:
* AWS Compute Optimizer
QUESTION NO: 89
한 회사가 여러 가용성 영역에 배포된 Amazon RDS 인스턴스에서 실행되는 데이터베이스를
호스팅합니다. 주기적 스크립트는 데이터베이스를 쿼리하여 중요한 애플리케이션에 부정적인
영향을 미칩니다. 최소한의 비용으로 애플리케이션 성능을 개선할 수 있는 방법은
무엇입니까?
A. 스크립트에 활성 연결이 가장 적은 인스턴스를 식별하고 해당 인스턴스를 쿼리하는 기능을
추가합니다.
B. 데이터베이스의 읽기 복제본을 만듭니다. 읽기 복제본만 쿼리하도록 스크립트를
구성합니다.
C. 개발팀에 하루가 끝나면 수동으로 새 항목을 내보내도록 지시합니다.
D. Amazon ElastiCache를 사용하여 스크립트가 실행하는 일반적인 쿼리를 캐시합니다.
Answer: B
* Option A introduces complexity and does not scale well.
* Option B creates a read replica, offloading read traffic from the primary RDS instance
without impacting the critical application.
* Option C is manual and inefficient.
* Option D might help for caching frequently queried data but is not ideal for ad-hoc reporting.
Therefore, Option B is the best choice.
QUESTION NO: 90
한 회사는 여러 소스에서 실시간 스트리밍 데이터를 수집할 새로운 데이터 플랫폼을 준비하고
있습니다. 회사는 Amazon S3에 데이터를 쓰기 전에 데이터를 변환해야 합니다. 회사에서는
SQL을 사용하여 변환된 데이터를 쿼리할 수 있는 능력이 필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까? (2개를 선택하세요.)
A. Amazon Kinesis Data Streams를 사용하여 데이터를 스트리밍합니다. Amazon Kinesis
Data Analytics를 사용하여 데이터를 변환합니다. Amazon Kinesis Data Firehose를 사용하여
Amazon S3에 데이터를 씁니다. Amazon Athena를 사용하여 Amazon S3에서 변환된
데이터를 쿼리합니다.
B. Amazon Managed Streaming for Apache Kafka(Amazon MSK)를 사용하여 데이터를
62

IT Certification Guaranteed, The Easy Way!
스트리밍합니다. AWS Glue를 사용하여 데이터를 변환하고 Amazon S3에 데이터를 씁니다.
Amazon Athena를 사용하여 Amazon S3에서 변환된 데이터를 쿼리합니다.
C. AWS Database Migration Service(AWS DMS)를 사용하여 데이터를 수집합니다. Amazon
EMR을 사용하여 데이터를 변환하고 Amazon S3에 데이터를 씁니다. Amazon Athena를
사용하여 Amazon S3에서 변환된 데이터를 쿼리합니다.
D. Amazon Managed Streaming for Apache Kafka(Amazon MSK)를 사용하여 데이터를
스트리밍합니다. Amazon Kinesis Data Analytics를 사용하여 데이터를 변환하고 Amazon
S3에 데이터를 씁니다. Amazon RDS 쿼리 편집기를 사용하여 Amazon S3에서 변환된
데이터를 쿼리합니다.
E. Amazon Kinesis Data Streams를 사용하여 데이터를 스트리밍합니다. AWS Glue를
사용하여 데이터를 변환합니다. Amazon Kinesis Data Firehose를 사용하여 Amazon S3에
데이터를 씁니다. Amazon RDS 쿼리 편집기를 사용하여 Amazon S3에서 변환된 데이터를
쿼리합니다.
Answer: A B
Explanation:
To ingest, transform, and query real-time streaming data from multiple sources, Amazon
Kinesis and Amazon MSK are suitable solutions. Amazon Kinesis Data Streams can stream
the data from various sources and integrate with other AWS services. Amazon Kinesis Data
Analytics can transform the data using SQL or Apache Flink. Amazon Kinesis Data Firehose
can write the data to Amazon S3 or other destinations. Amazon Athena can query the
transformed data from Amazon S3 using standard SQL. Amazon MSK can stream the data
using Apache Kafka, which is a popular open-source platform for streaming data. AWS Glue
can transform the data using Apache Spark or Python scripts and write the data to Amazon
S3 or other destinations. Amazon Athena can also query the transformed data from Amazon
S3 using standard SQL.
References:
* What Is Amazon Kinesis Data Streams?
* What Is Amazon Kinesis Data Analytics?
* What Is Amazon Kinesis Data Firehose?
* What Is Amazon Athena?
* What Is Amazon MSK?
* What Is AWS Glue?
QUESTION NO: 91
게임 회사는 Amazon DynamoDB를 사용하여 지리적 위치, 플레이어 데이터 및 순위표와 같은
사용자 정보를 저장합니다. 회사는 최소한의 코딩으로 Amazon S3 버킷에 대한 지속적인
백업을 구성해야 합니다. 백업은 애플리케이션의 가용성에 영향을 주어서는 안 되며 테이블에
대해 정의된 RCU(읽기 용량 단위)에 영향을 주어서는 안 됩니다.
A. Amazon EMR 클러스터를 사용합니다. Apache Hive 작업을 생성하여 Amazon S3에
데이터를 백업합니다.
B. 연속 백업을 통해 DynamoDB에서 Amazon S3로 데이터를 직접 내보냅니다. 테이블에 대해
지정 시간 복구를 설정합니다.
C. Amazon DynamoDB 스트림을 구성합니다. 스트림을 사용하고 데이터를 Amazon S3
버킷으로 내보내는 AWS Lambda 함수를 생성합니다.
D. 정기적으로 데이터베이스 테이블에서 Amazon S3로 데이터를 내보내는 AWS Lambda
63

IT Certification Guaranteed, The Easy Way!
함수를 생성합니다. 테이블에 대해 지정 시간 복구를 설정합니다.
Answer: B
Explanation:
https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/
https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/
QUESTION NO: 92
회사는 온프레미스에서 다중 계층 웹 애플리케이션을 실행하고 있습니다. 웹 애플리케이션은
컨테이너화되어 사용자 레코드가 포함된 PostgreSQL 데이터베이스에 연결된 여러 Linux
호스트에서 실행됩니다. 인프라 유지 관리 및 용량 계획에 따른 운영 오버헤드가 회사의
성장을 제한하고 있습니다. 솔루션 설계자는 애플리케이션의 인프라를 개선해야 합니다.
이를 달성하기 위해 솔루션 아키텍트는 어떤 조치 조합을 취해야 합니까? (2개를 선택하세요.)
A. PostgreSQL 데이터베이스를 Amazon Aurora로 마이그레이션
B. Amazon EC2 인스턴스에서 호스팅할 웹 애플리케이션을 마이그레이션합니다.
C. 웹 애플리케이션 콘텐츠에 대한 Amazon CloudFront 배포를 설정합니다.
D. 웹 애플리케이션과 PostgreSQL 데이터베이스 간에 Amazon ElastiCache를 설정합니다.
E. Amazon Elastic Container Service(Amazon ECS)를 사용하여 AWS Fargate에서 호스팅할
웹 애플리케이션을 마이그레이션합니다.
Answer: A E
Explanation:
Amazon Aurora is a fully managed, scalable, and highly available relational database service
that is compatible with PostgreSQL. Migrating the database to Amazon Aurora would reduce
the operational overhead of maintaining the database infrastructure and allow the company
to focus on building and scaling the application. AWS Fargate is a fully managed container
orchestration service that enables users to run containers without the need to manage the
underlying EC2 instances. By using AWS Fargate with Amazon Elastic Container Service
(Amazon ECS), the solutions architect can improve the scalability and efficiency of the web
application and reduce the operational overhead of maintaining the underlying infrastructure.
QUESTION NO: 93
한 회사에서 이벤트 데이터를 제작하는 서비스를 운영하고 있습니다.
ㅏ. 회사는 AWS를 사용하여 수신된 이벤트 데이터를 처리하려고 합니다. 데이터는 처리
전반에 걸쳐 유지되어야 하는 특정 순서로 기록됩니다. 회사는 운영 오버헤드를 최소화하는
솔루션을 구현하려고 합니다.
솔루션 설계자는 이를 어떻게 달성해야 합니까?
A. Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 생성하여 메시지를
보관합니다. AWS Lambda 함수를 설정하여 대기열의 메시지를 처리합니다.
B. 처리할 페이로드가 포함된 알림을 전달하기 위한 Amazon Simple 알림 서비스(Amazon
SNS) 주제를 생성합니다. AWS Lambda 함수를 구독자로 구성합니다.
C. 메시지를 보관할 Amazon Simple Queue Service(Amazon SQS) 표준 대기열을
생성합니다. 대기열의 메시지를 독립적으로 처리하도록 AWS Lambda 함수 설정
D. 처리할 페이로드가 포함된 알림을 전달하기 위한 Amazon Simple 알림 서비스(Amazon
SNS) 주제를 생성합니다. Amazon Simple Queue Service(Amazon SQS) 대기열을 구독자로
구성합니다.
64

IT Certification Guaranteed, The Easy Way!
Answer: A
Explanation:
The details are revealed in below url:
https://docs.aws.amazon.com/AWSSimpleQueueService/latest
/SQSDeveloperGuide/FIFO-queues.html
FIFO (First-In-First-Out) queues are designed to enhance messaging between applications
when the order of operations and events is critical, or where duplicates can't be tolerated.
Examples of situations where you might use FIFO queues include the following: To make
sure that user-entered commands are run in the right order. To display the correct product
price by sending price modifications in the right order. To prevent a student from enrolling in
a course before registering for an account.
QUESTION NO: 94
회사에서 새로운 비즈니스 애플리케이션을 구현하고 있습니다. 애플리케이션은 두 개의
Amazon EC2 인스턴스에서 실행되며 문서 저장을 위해 Amazon S3 버킷을 사용합니다.
솔루션 아키텍트는 EC2 인스턴스가 S3 버킷에 액세스할 수 있는지 확인해야 합니다.
이 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. S3 버킷에 대한 액세스 권한을 부여하는 IAM 역할을 생성합니다. EC2 인스턴스에 역할을
연결합니다.
B. S3 버킷에 대한 액세스 권한을 부여하는 IAM 정책을 생성합니다. EC2 인스턴스에 정책을
연결합니다.
C. S3 버킷에 대한 액세스 권한을 부여하는 IAM 그룹을 생성합니다. 그룹을 EC2 인스턴스에
연결합니다.
D. S3 버킷에 대한 액세스 권한을 부여하는 IAM 사용자를 생성합니다. 사용자 계정을 EC2
인스턴스에 연결합니다.
Answer: A
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-access-s3-bucket/
QUESTION NO: 95
한 회사는 현재 온프레미스 블록 스토리지 시스템에 5TB의 데이터를 저장하고 있습니다. 이
회사의 현재 스토리지 솔루션은 추가 데이터를 위한 제한된 공간을 제공합니다. 이 회사는
낮은 대기 시간으로 자주 액세스하는 데이터를 검색할 수 있어야 하는 온프레미스
애플리케이션을 실행합니다. 이 회사에는 클라우드 기반 스토리지 솔루션이 필요합니다.
이러한 요구 사항을 충족하면서 가장 운영 효율적인 솔루션은 무엇일까요?
A. Amazon S3 파일 게이트웨이 사용 S3 파일 게이트웨이를 온프레미스 애플리케이션과
통합하여 SMB 파일 시스템을 사용하여 파일을 저장하고 직접 검색합니다.
B. 캐시된 볼륨을 iSCSt 대상으로 하는 AWS Storage Gateway 볼륨 게이트웨이를
사용합니다.
C. 볼륨을 iSCSI 대상으로 저장한 AWS Storage Gateway 볼륨 게이트웨이를 사용합니다.
D. AWS Storage Gateway Tape Gateway를 사용합니다. Tape Gateway를 온프레미스
애플리케이션과 통합하여 Amazon S3에 가상 테이프를 저장합니다.
Answer: B
Explanation:
65

IT Certification Guaranteed, The Easy Way!
The company needs a cloud-based storage solution for frequently accessed data with low
latency, while retaining their current on-premises infrastructure for some data storage. AWS
Storage Gateway's Volume Gateway with cached volumes is the most appropriate solution
for this scenario.
* AWS Storage Gateway - Volume Gateway (Cached Volumes):
* Volume Gateway with cached volumes allows you to store frequently accessed data in the
AWS Cloud while keeping the most recently accessed data cached locally on-premises. This
ensures low-latency access to active data while providing scalability for the rest of the data in
the cloud.
* The cached volume option stores the primary data in Amazon S3 but caches frequently
accessed data locally, ensuring fast access. This configuration is well-suited for applications
that require fast access to frequently used data but can tolerate cloud-based storage for the
rest.
* Since the company is facing limited on-premises storage, cached volumes provide an ideal
solution, as they reduce the need for additional on-premises storage infrastructure.
* Why Not the Other Options?:
* Option A (S3 File Gateway): S3 File Gateway provides a file-based interface (SMB/NFS) for
storing data directly in S3. While it is great for file storage, the company's need for block-level
storage with iSCSI targets makes Volume Gateway a better fit.
* Option C (Volume Gateway - Stored Volumes): Stored volumes keep all the data on
-premises and asynchronously back up to AWS. This would not address the company's
storage limitations since they would still need substantial on-premises storage.
* Option D (Tape Gateway): Tape Gateway is designed for archiving and backup, not for
frequently accessed low-latency data.
AWS References:
* AWS Storage Gateway - Volume Gateway
QUESTION NO: 96
한 회사가 다양한 브랜드를 위해 AWS에서 여러 웹사이트를 운영합니다. 각 웹사이트는 매일
수십 기가바이트의 웹 트래픽 로그를 생성합니다. 솔루션 아키텍트는 회사 개발자가 모든
회사 웹사이트의 트래픽 패턴을 분석할 수 있는 확장 가능한 솔루션을 설계해야 합니다.
개발자의 이러한 분석은 몇 달 동안 일주일에 한 번씩 필요에 따라 수행됩니다. 솔루션은 표준
SQL을 사용한 쿼리를 지원해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. Amazon S3에 로그를 저장합니다. 분석에는 Amazon Athena를 사용합니다.
B. Amazon RDS에 로그를 저장합니다. 분석을 위해 데이터베이스 클라이언트를 사용합니다.
C. Amazon OpenSearch Service에 로그를 저장합니다. OpenSearch Service를 사용하여
분석합니다.
D. Amazon EMR 클러스터에 로그를 저장합니다. SQL 기반 분석을 위해 지원되는 오픈 소스
프레임워크를 사용합니다.
Answer: A
Explanation:
This solution is the most cost-effective and scalable for analyzing large amounts of web traffic
logs.
* Amazon S3: Storing the logs in Amazon S3 is highly scalable, durable, and cost-effective.
66

IT Certification Guaranteed, The Easy Way!
S3 is designed to handle large-scale data storage, which is ideal for storing tens of gigabytes
of log data generated daily by multiple websites.
* Amazon Athena: Athena is a serverless, interactive query service that allows you to analyze
data in S3 using standard SQL. It works directly with the data stored in S3, so there's no
need to load the data into a database, which saves on costs and reduces complexity. Athena
charges based on the amount of data scanned by queries, making it a cost-effective solution
for on-demand analysis that only occurs once a week.
* Why Not Other Options?:
* Option B (Amazon RDS): Storing logs in a relational database like Amazon RDS would be
more expensive due to the storage and I/O costs associated with RDS. Additionally, it would
require more management overhead.
* Option C (Amazon OpenSearch Service): OpenSearch is a good option for full-text search
and analytics on log data, but it might be more costly and complex to manage compared to
the simplicity and cost-effectiveness of Athena for periodic SQL-based queries.
* Option D (Amazon EMR): While EMR can handle large-scale data processing, it involves
more operational overhead and might be overkill for the type of ad-hoc, SQL-based analysis
required here. Additionally, EMR costs can be higher due to the need to maintain a cluster.
AWS References:
* Amazon S3 - Information on how to store and manage data in Amazon S3.
* Amazon Athena - Documentation on using Amazon Athena for querying data stored in S3
using SQL.
QUESTION NO: 97
한 회사가 3개의 가용 영역에서 작동하는 AWS 클라우드에서 3계층 웹 애플리케이션을
실행합니다. 애플리케이션 아키텍처에는 Application Load Balancer, 사용자 세션 상태를
호스팅하는 Amazon EC2 웹 서버, EC2 인스턴스에서 실행되는 MySQL 데이터베이스가
있습니다. 회사는 애플리케이션 트래픽이 갑자기 증가할 것으로 예상합니다. 회사는 미래의
애플리케이션 용량 요구 사항을 충족하고 3개의 가용 영역 모두에서 고가용성을 보장하기
위해 확장할 수 있기를 원합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 다중 AZ DB 클러스터 배포를 통해 MySQL 데이터베이스를 MySQL용 Amazon RDS로
마이그레이션합니다. 고가용성 Redis용 Amazon ElastiCache를 사용하여 세션 데이터를
저장하고 읽기를 캐시하십시오. 세 개의 가용 영역에 있는 Auto Scaling 그룹으로 웹 서버를
마이그레이션합니다.
B. 다중 AZ DB 클러스터 배포를 통해 MySQL 데이터베이스를 MySQL용 Amazon RDS로
마이그레이션합니다. 고가용성 Memcached용 Amazon ElastiCache를 사용하여 세션
데이터를 저장하고 읽기를 캐시하십시오. 세 개의 가용 영역에 있는 Auto Scaling 그룹으로 웹
서버를 마이그레이션합니다.
C. MySQL 데이터베이스를 Amazon DynamoDB로 마이그레이션합니다. DynamoDB
Accelerator(DAX)를 사용하여 읽기를 캐시합니다. DynamoDB에 세션 데이터를 저장합니다.
세 개의 가용 영역에 있는 Auto Scaling 그룹으로 웹 서버를 마이그레이션합니다.
D. 단일 가용 영역에서 MySQL 데이터베이스를 MySQL용 Amazon RDS로
마이그레이션합니다. 고가용성 Redis용 Amazon ElastiCache를 사용하여 세션 데이터를
저장하고 읽기를 캐시하십시오. 세 개의 가용 영역에 있는 Auto Scaling 그룹으로 웹 서버를
마이그레이션합니다.
67

IT Certification Guaranteed, The Easy Way!
Answer: A
Explanation:
This answer is correct because it meets the requirements of scaling to meet future
application capacity demands and ensuring high availability across all three Availability
Zones. By migrating the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB
cluster deployment, the company can benefit from automatic failover, backup, and patching
of the database across multiple Availability Zones. By using Amazon ElastiCache for Redis
with high availability, the company can store session data and cache reads in a fast, in-
memory data store that can also fail over across Availability Zones. By migrating the web
server to an Auto Scaling group that is in three Availability Zones, the company can
automatically scale the web server capacity based on the demand and traffic patterns.
References:
* https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html
* https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html
* https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-
scaling.html
QUESTION NO: 98
회사는 애플리케이션을 생성 중입니다. 회사는 여러 온프레미스 위치에 있는 애플리케이션
테스트의 데이터를 저장합니다. 회사는 온프레미스 위치를 AWS 클라우드의 AWS 지역에
있는 VPC에 연결해야 합니다. 내년 네트워크 아키텍처는 새로운 연결 관리를 단순화하고
확장 기능을 제공해야 합니다.
최소한의 관리 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. VPC 간에 피어링 연결을 생성합니다. VPC와 온프레미스 위치 간에 VPN 연결을
생성합니다.
B. Amazon EC2 인스턴스를 시작합니다. 인스턴스에 VPN 연결을 사용하여 모든 VPC와
온프레미스 위치를 연결하는 VPN 소프트웨어를 포함합니다.
C. 전송 게이트웨이 생성 VPC 연결을 위한 VPC 연결 생성 온프레미스 연결을 위한 VPN
연결을 생성합니다.
D. 온프레미스 위치와 중앙 VPC 간에 AWS Direct Connect 연결을 생성합니다. 피어링 연결을
사용하여 중앙 VPC를 다른 VPC에 연결합니다.
Answer: C
Explanation:
* A. VPC peering + VPN: Not scalable for multiple VPCs and accounts.
* B. EC2 with VPN software: Requires manual setup and high administrative overhead.
* C. Transit gateway: Simplifies connection management across VPCs and on-premises
locations.
* D. Direct Connect with central VPC: Limits scalability and requires additional peering
connections.
References: AWS Transit Gateway
QUESTION NO: 99
회사에는 여러 위치에 데이터 수집 센서가 있습니다. 데이터 수집 센서는 대량의 데이터를
회사에 스트리밍합니다. 회사는 대용량 스트리밍 데이터를 수집하고 처리하기 위해 AWS에서
플랫폼을 설계하려고 합니다. 솔루션은 확장 가능해야 하며 거의 실시간으로 데이터 수집을
68

IT Certification Guaranteed, The Easy Way!
지원해야 합니다. 회사는 향후 보고를 위해 Amazon S3에 데이터를 저장해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon Kinesis Data Firehose를 사용하여 스트리밍 데이터를 Amazon S3에 전달합니다.
B. AWS Glue를 사용하여 스트리밍 데이터를 Amazon S3에 전달합니다.
C. AWS Lambda를 사용하여 스트리밍 데이터를 전달하고 Amazon S3에 데이터를
저장합니다.
D. AWS Database Migration Service(AWS DMS)를 사용하여 스트리밍 데이터를 Amazon
S3에 전달합니다.
Answer: A
Explanation:
To ingest and process high-volume streaming data with the least operational overhead,
Amazon Kinesis Data Firehose is a suitable solution. Amazon Kinesis Data Firehose can
capture, transform, and deliver streaming data to Amazon S3 or other destinations. Amazon
Kinesis Data Firehose can scale automatically to match the throughput of the data and
handle any amount of data. Amazon Kinesis Data Firehose is also a fully managed service
that does not require any servers to provision or manage.
References:
* What Is Amazon Kinesis Data Firehose?
* Amazon Kinesis Data Firehose Pricing
QUESTION NO: 100
한 회사는 Amazon S3를 사용하여 고해상도 사진을 S3 버킷에 저장합니다. 애플리케이션
변경을 최소화하기 위해 회사는 사진을 S3 객체의 최신 버전으로 저장합니다. 회사는
사진에서 가장 최신 버전 2개만 유지하면 됩니다.
회사는 비용을 절감하고 싶어합니다. 회사는 S3 버킷을 큰 비용으로 식별했습니다.
최소한의 운영 오버헤드로 S3 비용을 절감할 수 있는 솔루션은 무엇입니까?
A. S3 수명 주기를 사용하여 만료된 객체 버전을 삭제하고 최신 버전 2개를 유지합니다.
B. AWS Lambda 함수를 사용하여 이전 버전을 확인하고 최신 버전 2개를 제외하고 모두
삭제합니다.
C. S3 배치 작업을 사용하여 최신이 아닌 객체 버전을 삭제하고 최신 버전 2개만 유지합니다.
D. S3 버킷에서 버전 관리를 비활성화하고 최신 버전 2개를 유지합니다.
Answer: A
Explanation:
S3 Lifecycle is a feature that allows you to automate the management of your S3 objects
based on predefined rules. You can use S3 Lifecycle to delete expired object versions and
retain the two most recent versions by creating a lifecycle configuration rule that applies to all
objects in the bucket and specifies the expiration action for noncurrent versions. This way,
you can reduce the storage costs of your S3 bucket without requiring any application
changes or manual intervention. S3 Lifecycle runs once a day and marks the eligible object
versions for deletion. You are no longer charged for the objects that are marked for deletion.
S3 Lifecycle is the most cost-effective and simple solution among the options.
B: Use an AWS Lambda function to check for older versions and delete all but the two most
recent versions.
This option is not optimal because it requires you to write, test, and maintain a custom
69

IT Certification Guaranteed, The Easy Way!
Lambda function that scans the S3 bucket for older versions and deletes them. This can incur
additional costs for Lambda invocations and increase the operational complexity and
overhead. Moreover, you need to ensure that your Lambda function has the appropriate
permissions and error handling mechanisms to perform the deletion operation.
C: Use S3 Batch Operations to delete noncurrent object versions and retain only the two
most recent versions.
This option is not ideal because S3 Batch Operations is designed for performing large-scale
operations on S3 objects, such as copying, tagging, restoring, or invoking a Lambda function.
To use S3 Batch Operations to delete noncurrent object versions, you need to provide a
manifest file that lists the object versions that you want to delete. This can be challenging and
time-consuming to generate and update. Moreover, S3 Batch Operations charges you for
each operation that you perform, which can increase your costs.
D: Deactivate versioning on the S3 bucket and retain the two most recent versions. This
option is not feasible because deactivating versioning on an S3 bucket does not delete the
existing object versions. Instead, it prevents new versions from being created. Therefore, you
still need to delete the older versions manually or use another method to do so. Additionally,
deactivating versioning can compromise the data protection and recovery capabilities of your
S3 bucket.
References:
* 1 Considering four different replication options for data in Amazon S3 | AWS Storage Blog
* 2 Using AWS Lambda with Amazon S3 batch operations - AWS Lambda
* 3 Empty an Amazon S3 bucket with a lifecycle configuration rule
* 4 Amazon S3 - Lifecycle Management - GeeksforGeeks
QUESTION NO: 101
회사에서 SQL 데이터베이스를 사용하여 공개적으로 액세스할 수 있는 영화 데이터를
저장하고 있습니다. 데이터베이스는 Amazon RDS 단일 AZ DB 인스턴스에서 실행됩니다.
스크립트는 매일 임의의 간격으로 쿼리를 실행하여 데이터베이스에 추가된 새 영화의 수를
기록합니다. 스크립트는 업무 시간 동안 최종 합계를 보고해야 합니다. 회사의 개발 팀은
스크립트가 실행 중일 때 데이터베이스 성능이 개발 작업에 적합하지 않다는 것을 알게
되었습니다. 솔루션 설계자는 이 문제를 해결하기 위한 솔루션을 권장해야 합니다. 최소한의
운영 오버헤드로 이 요구 사항을 충족하는 솔루션은 무엇입니까?
A. DB 인스턴스를 다중 AZ 배포로 수정
B. 데이터베이스의 읽기 전용 복제본 생성 읽기 전용 복제본만 쿼리하도록 스크립트 구성
C. 개발팀에 매일 일과가 끝날 때 데이터베이스의 항목을 수동으로 내보내도록 지시합니다.
D. Amazon ElastiCache를 사용하여 스크립트가 데이터베이스에 대해 실행하는 일반적인
쿼리를 캐시합니다.
Answer: B
QUESTION NO: 102
회사는 다중 계층 웹 애플리케이션에 Amazon ElastiCache를 사용할 계획입니다. 솔루션
아키텍트는 ElastiCache 클러스터용 캐시 VPC와 애플리케이션의 Amazon EC2 인스턴스용
앱 VPC를 생성합니다. 두 VPC가 모두 us-east-1 리전에 있습니다. 솔루션 아키텍트는 다음을
수행해야 합니다. 애플리케이션의 EC2 인스턴스에 ElastiCache 클러스터에 대한 액세스를
제공하는 솔루션 구현 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은
70

IT Certification Guaranteed, The Easy Way!
무엇입니까?
A. VPC 간에 피어링 연결을 생성합니다. 두 VPC 모두에서 피어링 연결을 위한 라우팅 테이블
항목을 추가합니다. ElastiCache 클러스터의 보안 그룹에 대한 인바운드 규칙을 구성하여
애플리케이션 보안 그룹의 인바운드 연결을 허용합니다.
B. 전송 VPC 생성 캐시 VPC 및 앱 VPC의 VPC 라우팅 테이블을 업데이트하여 전송 VPC를
통해 트래픽을 라우팅합니다. 애플리케이션 보안 그룹의 인바운드 연결을 허용하도록
ElastiCache 클러스터의 보안 그룹에 대한 인바운드 규칙을 구성합니다.
C. VPC 간에 피어링 연결을 생성합니다. 두 VPC 모두에서 피어링 연결을 위한 라우팅 테이블
항목을 추가합니다. 애플리케이션의 보안 그룹에서 인바운드 연결을 허용하도록 피어링
연결의 보안 그룹에 대한 인바운드 규칙을 구성합니다.
D. 전송 VPC 생성 캐시 VPC 및 앱 VPC의 VPC 라우팅 테이블을 업데이트하여 전송 VPC를
통해 트래픽을 라우팅합니다. 애플리케이션 보안 그룹의 인바운드 연결을 허용하도록 전송
VPC 보안 그룹에 대한 인바운드 규칙을 구성합니다.
Answer: A
Explanation:
Creating a peering connection between the two VPCs and configuring an inbound rule for the
ElastiCache cluster's security group to allow inbound connection from the application's
security group is the most cost- effective solution. Peering connections are free and you only
incur the cost of configuring the security group rules. The Transit VPC solution requires
additional VPCs and associated resources, which would incur additional costs.
Before Testing | AWS Certification Information and Policies | AWS
https://aws.amazon.com/certification/policies/before-testing/
QUESTION NO: 103
한 회사가 고가용성 자연어 처리(NLP) 애플리케이션을 개발하고 있습니다. 이
애플리케이션은 대량의 동시 요청을 처리합니다. 이 애플리케이션은 엔터티 인식, 감정 분석,
텍스트 데이터에 대한 핵심 구문 추출과 같은 NLP 작업을 수행합니다.
회사에서는 애플리케이션이 처리하는 데이터를 가용성과 확장성이 뛰어난 데이터베이스에
저장해야 합니다.
옵션:
A. 들어오는 요청을 처리하기 위한 Amazon API Gateway REST API 엔드포인트를 만듭니다.
각 요청에 대해 AWS Lambda 함수를 호출하도록 REST API를 구성합니다. 텍스트
데이터에서 NLP 작업을 수행하기 위해 Amazon Comprehend를 호출하도록 Lambda 함수를
구성합니다. 처리된 데이터를 Amazon DynamoDB에 저장합니다.
B. 들어오는 요청을 처리하기 위한 Amazon API Gateway HTTP API 엔드포인트를 만듭니다.
각 요청에 대해 AWS Lambda 함수를 호출하도록 HTTP API를 구성합니다. 텍스트
데이터에서 NLP 작업을 수행하기 위해 Amazon Translate를 호출하도록 Lambda 함수를
구성합니다. 처리된 데이터를 Amazon ElastiCache에 저장합니다.
C. 들어오는 요청을 버퍼링하기 위해 Amazon SQS 대기열을 만듭니다. Auto Scaling 그룹의
Amazon EC2 인스턴스에 NLP 애플리케이션을 배포합니다. Amazon Comprehend를
사용하여 NLP 작업을 수행합니다. 처리된 데이터를 Amazon RDS 데이터베이스에
저장합니다.
D. 들어오는 요청을 처리하기 위한 Amazon API Gateway WebSocket API 엔드포인트를
만듭니다. 각 요청에 대해 AWS Lambda 함수를 호출하도록 WebSocket API를 구성합니다.
71

IT Certification Guaranteed, The Easy Way!
텍스트 데이터에서 NLP 작업을 수행하기 위해 Amazon Textract를 호출하도록 Lambda
함수를 구성합니다. 처리된 데이터를 Amazon ElastiCache에 저장합니다.
Answer: A
Explanation:
* A. API Gateway + DynamoDB: Provides high scalability, low latency, and seamless
integration with Amazon Comprehend for NLP tasks.
* B. HTTP API + Translate + ElastiCache: Translate is not relevant for NLP tasks like
sentiment analysis or entity recognition. ElastiCache is unsuitable for permanent storage.
* C. SQS + EC2 + RDS: Increases complexity and operational overhead. RDS may not scale
effectively for high concurrent loads.
* D. WebSocket API + Textract: Textract is irrelevant for NLP tasks. WebSocket API is not
the optimal choice for this use case.
References: Amazon Comprehend, Amazon DynamoDB
QUESTION NO: 104
한 회사가 MySQL D6 인스턴스용 Amazon RDS를 출시했습니다. 데이터베이스에 대한
대부분의 연결은 서버리스 애플리케이션에서 이루어집니다. 데이터베이스에 대한
애플리케이션 트래픽은 무작위 간격으로 크게 변경됩니다. 수요가 많을 때 사용자는
애플리케이션에서 데이터베이스 연결 거부 오류가 발생한다고 보고합니다.
최소한의 운영 오버헤드로 이 문제를 해결할 수 있는 솔루션은 무엇입니까?
A. RDS Proxy에서 프록시 생성 RDS Proxy를 통해 DB 인스턴스를 사용하도록 사용자
애플리케이션을 구성합니다.
B. 사용자 애플리케이션과 DB 인스턴스 사이에 Memcached용 Amazon ElastCache 배포
C. DB 인스턴스를 I/O 용량이 더 높은 다른 인스턴스 클래스로 마이그레이션합니다. 새 DB
인스턴스를 사용하도록 사용자 애플리케이션을 구성합니다.
D. DB 인스턴스에 대한 다중 AZ 구성 DB 인스턴스 간에 전환하도록 사용자 애플리케이션을
구성합니다.
Answer: A
Explanation:
Many applications, including those built on modern serverless architectures, can have a large
number of open connections to the database server and may open and close database
connections at a high rate, exhausting database memory and compute resources. Amazon
RDS Proxy allows applications to pool and share connections established with the database,
improving database efficiency and application scalability.
(https://aws.amazon.com/pt/rds/proxy/)
QUESTION NO: 105
한 회사에서 여러 Microsoft Windows Server 워크로드를 us-west-1 리전에서 실행되는
Amazon EC2 인스턴스로 마이그레이션했습니다. 회사는 필요에 따라 이미지를 생성하기
위해 워크로드를 수동으로 백업합니다.
us-west-1 리전에서 자연 재해가 발생한 경우 회사는 us-west-2 리전에서 워크로드를
신속하게 복구하기를 원합니다. 회사는 EC2 인스턴스에서 24시간 이상의 데이터 손실을
원하지 않습니다. 회사는 또한 EC2 인스턴스의 모든 백업을 자동화하려고 합니다.
최소한의 관리 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? (2개를
선택하세요.)
72

IT Certification Guaranteed, The Easy Way!
A. Amazon EC2 지원 Amazon 머신 이미지(AMI) 수명 주기 정책을 생성하여 태그 기반
백업을 생성합니다. 하루에 두 번 실행되도록 백업을 예약합니다. 필요에 따라 이미지를
복사합니다.
B. Amazon EC2 지원 Amazon 머신 이미지(AMI) 수명 주기 정책을 생성하여 태그를 기반으로
백업을 생성합니다. 하루에 두 번 실행되도록 백업을 예약합니다. us-west-2 리전에 대한
복사본을 구성합니다.
C. AWS Backup을 사용하여 us-west-1 및 us-west-2에 백업 볼트를 생성합니다. 태그 값을
기반으로 EC2 인스턴스에 대한 백업 계획을 생성합니다. 백업 데이터를 us-west-2에
복사하기 위해 예약된 작업으로 실행할 AWS Lambda 함수를 생성합니다.
D. AWS Backup을 사용하여 백업 볼트를 생성합니다. AWS Backup을 사용하여 태그 값을
기반으로 EC2 인스턴스에 대한 백업 계획을 생성합니다. 사본의 대상을 us-west-2로
정의합니다. 하루에 두 번 실행할 백업 일정을 지정합니다.
E. AWS Backup을 사용하여 백업 볼트를 생성합니다. AWS Backup을 사용하여 태그 값을
기반으로 EC2 인스턴스에 대한 백업 계획을 생성합니다. 하루에 두 번 실행할 백업 일정을
지정합니다. 요청 시 us-west-2에 복사합니다.
Answer: B D
Explanation:
Option B suggests using an EC2-backed Amazon Machine Image (AMI) lifecycle policy to
automate the backup process. By configuring the policy to run twice daily and specifying the
copy to the us-west-2 Region, the company can ensure regular backups are created and
copied to the alternate region. Option D proposes using AWS Backup, which provides a
centralized backup management solution. By creating a backup vault and backup plan based
on tag values, the company can automate the backup process for the EC2 instances.
The backup schedule can be set to run twice daily, and the destination for the copy can be
defined as the us- west-2 Region.
Both options automate the backup process and include copying the backups to the us-west-2
Region, ensuring data resilience in the event of a disaster. These solutions minimize
administrative effort by leveraging automated backup and copy mechanisms provided by
AWS services.
QUESTION NO: 106
솔루션 아키텍트는 새 AWS 계정을 생성했으며 AWS 계정 루트 사용자 액세스를 보호해야
합니다.
어떤 작업 조합을 통해 이를 달성할 수 있나요? (2개를 선택하세요.)
A. 루트 사용자가 강력한 비밀번호를 사용하는지 확인하세요.
B. 루트 사용자에 대한 다중 요소 인증을 활성화합니다.
C. 암호화된 Amazon S3 버킷에 루트 사용자 액세스 키를 저장합니다.
D. 관리 권한이 포함된 그룹에 루트 사용자를 추가합니다.
E. 인라인 정책 문서를 사용하여 루트 사용자에게 필요한 권한을 적용합니다.
Answer: A B
Explanation:
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html
https://docs.aws.amazon.com/accounts/latest/reference/best-practices-root-user.html *
Enable AWS multi- factor authentication (MFA) on your AWS account root user. For more
73

IT Certification Guaranteed, The Easy Way!
information, see Using multi-factor authentication (MFA) in AWS in the IAM User Guide. *
Never share your AWS account root user password or access keys with anyone. * Use a
strong password to help protect access to the AWS Management Console.
For information about managing your AWS account root user password, see Changing the
password for the root user.
QUESTION NO: 107
회사에서 Amazon Elastic Container Service(Amazon ECS) 클러스터와 Amazon RDS DB
인스턴스를 사용하여 결제 처리 애플리케이션을 구축하고 실행하려고 합니다. 회사는 규정
준수를 위해 온프레미스 데이터 센터에서 애플리케이션을 실행합니다.
솔루션 설계자는 AWS Outposts를 솔루션의 일부로 사용하려고 합니다. 솔루션 아키텍트는
회사의 운영 팀과 협력하여 애플리케이션을 구축하고 있습니다.
회사 운영팀의 책임 활동은 무엇입니까? (3개를 선택하세요.)
A. Outposts 랙에 탄력적인 전원 및 네트워크 연결 제공
B. Outposts에서 실행되는 가상화 하이퍼바이저, 스토리지 시스템 및 AWS 서비스 관리
C. 데이터 센터 환경의 물리적 보안 및 액세스 제어
D. Outposts 랙 내 전원 공급 장치, 서버 및 네트워킹 장비를 포함한 Outposts 인프라의 가용성
E. Outposts 구성 요소의 물리적 유지 관리
F. 서버 오류 및 유지 관리 이벤트를 완화하기 위해 Amazon ECS 클러스터에 추가 용량 제공
Answer: A C F
Explanation:
These answers are correct because they reflect the customer's responsibilities for using
AWS Outposts as part of the solution. According to the AWS shared responsibility model, the
customer is responsible for providing resilient power and network connectivity to the Outposts
racks, ensuring physical security and access controls of the data center environment, and
providing extra capacity for Amazon ECS clusters to mitigate server failures and
maintenance events. AWS is responsible for managing the virtualization hypervisor, storage
systems, and the AWS services that run on Outposts, as well as the availability of the
Outposts infrastructure including the power supplies, servers, and networking equipment
within the Outposts racks, and the physical maintenance of Outposts components.
References:
* https://docs.aws.amazon.com/outposts/latest/userguide/what-is-outposts.html
* https://www.contino.io/insights/the-sandwich-responsibility-model-aws-outposts/
QUESTION NO: 108
회사는 수많은 애플리케이션이 액세스하는 Amazon S3 버킷에서 데이터 레이크를
관리합니다. S3 버킷에는 각 애플리케이션에 대한 고유한 접두사가 포함되어 있습니다.
회사는 각 애플리케이션을 특정 접두사로 제한하고 각 접두사 아래의 객체를 세부적으로
제어하기를 원합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 각 애플리케이션에 대한 전용 S3 액세스 포인트 및 액세스 포인트 정책을 생성합니다.
B. S3 버킷의 각 객체에 대한 ACL 권한을 설정하는 S3 배치 작업 작업을 생성합니다.
C. S3 버킷의 객체를 각 애플리케이션의 새 S3 버킷에 복제합니다. 접두사로 복제 규칙 만들기
D. S3 버킷의 객체를 각 애플리케이션의 새 S3 버킷에 복제합니다. 각 애플리케이션에 대한
74

IT Certification Guaranteed, The Easy Way!
전용 S3 액세스 포인트를 생성합니다.
Answer: A
Explanation:
* Understanding the Requirement: The company wants to restrict each application to its
specific prefix in an S3 bucket and have granular control over the objects under each prefix.
* Analysis of Options:
* Dedicated S3 Access Points: Provides a scalable and flexible way to manage access to S3
buckets, allowing specific policies to be attached to each access point, thereby controlling
access at the prefix level.
* S3 Batch Operations: Suitable for large-scale changes but involves more operational
overhead and does not dynamically control future access.
* Replication to new S3 buckets: Involves unnecessary duplication of data and increased
storage costs, and operational overhead for managing multiple buckets.
* Combination of replication and access points: Adds unnecessary complexity and overhead
compared to using access points directly.
* Best Solution:
* Dedicated S3 Access Points: This provides the least operational overhead while meeting
the requirements for prefix-level access control and granular management.
References:
* Amazon S3 Access Points
QUESTION NO: 109
한 회사는 최근 프라이빗 서브넷의 Amazon EC2에서 Linux 기반 애플리케이션 인스턴스를
시작하고 VPC의 퍼블릭 서브넷에서 Amazon EC2 인스턴스에서 Linux 기반 배스천 호스트를
시작했습니다. 솔루션 아키텍트는 다음을 통해 온프레미스 네트워크에서 연결해야 합니다.
배스천 호스트 및 애플리케이션 서버에 대한 회사의 인터넷 연결 솔루션 설계자는 모든 EC2
인스턴스의 보안 그룹이 해당 액세스를 허용하는지 확인해야 합니다. 이러한 요구 사항을
충족하기 위해 솔루션 설계자가 취해야 하는 단계는 무엇입니까? (2개 선택)
A. 배스천 호스트의 현재 보안 그룹을 애플리케이션 인스턴스의 인바운드 액세스만 허용하는
보안 그룹으로 교체합니다.
B. 배스천 호스트의 현재 보안 그룹을 회사의 내부 IP 범위에서 인바운드 액세스만 허용하는
보안 그룹으로 교체합니다.
C. 배스천 호스트의 현재 보안 그룹을 회사의 외부 IP 범위에서 인바운드 액세스만 허용하는
보안 그룹으로 교체합니다.
D. 애플리케이션 인스턴스의 현재 보안 그룹을 배스천 호스트의 사설 IP 주소에서만 인바운드
SSH 액세스를 허용하는 보안 그룹으로 교체합니다.
E. 애플리케이션 인스턴스의 현재 보안 그룹을 배스천 호스트의 공용 IP 주소에서만 인바운드
SSH 액세스를 허용하는 보안 그룹으로 교체합니다.
Answer: C D
Explanation:
https://digitalcloud.training/ssh-into-ec2-in-private-subnet/
QUESTION NO: 110
회사는 30일 이내에 데이터 센터에서 AWS 클라우드로 20TB의 데이터를 마이그레이션해야
합니다. 회사의 네트워크 대역폭은 15Mbps로 제한되며 사용률이 70%를 초과할 수 없습니다.
75

IT Certification Guaranteed, The Easy Way!
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. AWS Snowball을 사용하세요.
B. AWS DataSync를 사용합니다.
C. 보안 VPN 연결을 사용하세요.
D. Amazon S3 Transfer Acceleration을 사용합니다.
Answer: A
Explanation:
AWS Snowball is a secure data transport solution that accelerates moving large amounts of
data into and out of the AWS cloud. It can move up to 80 TB of data at a time, and provides a
network bandwidth of up to 50 Mbps, so it is well-suited for the task. Additionally, it is secure
and easy to use, making it the ideal solution for this migration.
QUESTION NO: 111
솔루션 아키텍트는 소프트웨어 데모 환경을 위한 아키텍처를 설계하고 있습니다. 환경은
ALB(Application Load Balancer) 뒤에 있는 Auto Scaling 그룹의 Amazon EC2 인스턴스에서
실행됩니다. 시스템은 근무 시간 동안 상당한 트래픽 증가를 경험하지만 작동할 필요는
없습니다. 주말에.
시스템이 수요에 맞게 확장될 수 있도록 솔루션 설계자는 어떤 조치 조합을 취해야 합니까?
(2개 선택)
A. AWS Auto Scaling을 사용하여 요청 속도에 따라 ALB 용량을 조정합니다.
B. AWS Auto Scaling을 사용하여 VPC 인터넷 게이트웨이의 용량 확장
C. 여러 AWS 지역에서 EC2 인스턴스를 시작하여 지역 전체에 로드를 분산합니다.
D. 대상 추적 조정 정책을 사용하여 인스턴스 CPU 사용률을 기준으로 Auto Scaling 그룹을
조정합니다.
E. 예약된 조정을 사용하여 주말 동안 Auto Scaling 그룹 최소, 최대 및 원하는 용량을 0으로
변경합니다. 주 시작 시 기본값으로 되돌립니다.
Answer: D E
Explanation:
https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-
tracking.html#target-tracking- choose-metrics A target tracking scaling policy is a type of
dynamic scaling policy that adjusts the capacity of an Auto Scaling group based on a
specified metric and a target value1. A target tracking scaling policy can automatically scale
out or scale in the Auto Scaling group to keep the actual metric value at or near the target
value1. A target tracking scaling policy is suitable for scenarios where the load on the
application changes frequently and unpredictably, such as during working hours2.
To meet the requirements of the scenario, the solutions architect should use a target tracking
scaling policy to scale the Auto Scaling group based on instance CPU utilization. Instance
CPU utilization is a common metric that reflects the demand on the application1. The
solutions architect should specify a target value that represents the ideal average CPU
utilization level for the application, such as 50 percent1. Then, the Auto Scaling group will
scale out or scale in to maintain that level of CPU utilization.
Scheduled scaling is a type of scaling policy that performs scaling actions based on a date
and time3
. Scheduled scaling is suitable for scenarios where the load on the application changes
76

IT Certification Guaranteed, The Easy Way!
periodically and predictably, such as on weekends2.
To meet the requirements of the scenario, the solutions architect should also use scheduled
scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero
for weekends. This way, the Auto Scaling group will terminate all instances on weekends
when they are not required to operate. The solutions architect should also revert to the
default values at the start of the week, so that the Auto Scaling group can resume normal
operation.
QUESTION NO: 112
소셜 미디어 회사는 웹사이트용 기능을 구축하고 있습니다. 이 기능을 통해 사용자는 사진을
업로드할 수 있습니다. 회사는 대규모 이벤트 기간 동안 수요가 크게 증가할 것으로 예상하고
웹사이트가 사용자의 업로드 트래픽을 처리할 수 있는지 확인해야 합니다.
MOST 확장성으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 사용자의 브라우저에서 애플리케이션 서버로 파일을 업로드합니다. 파일을 Amazon S3
버킷으로 전송합니다.
B. AWS Storage Gateway 파일 게이트웨이를 프로비저닝합니다. 사용자의 브라우저에서
파일 게이트웨이로 직접 파일을 업로드합니다.
C. 애플리케이션에서 Amazon S3 미리 서명된 URL을 생성합니다. 사용자 브라우저에서 S3
버킷으로 직접 파일을 업로드합니다.
D. Amazon Elastic File System(Amazon EFS) 파일 시스템 프로비저닝 사용자의
브라우저에서 파일 시스템으로 직접 파일 업로드
Answer: C
Explanation:
This approach allows users to upload files directly to S3 without passing through the
application servers, reducing the load on the application and improving scalability. It
leverages the client-side capabilities to handle the file uploads and offloads the processing to
S3.
QUESTION NO: 113
회사는 NFS를 사용하여 온프레미스 네트워크 연결 스토리지에 대용량 비디오 파일을
저장합니다. 각 비디오 파일의 크기는 1MB에서 500GB까지입니다. 총 스토리지는 70TB이며
더 이상 증가하지 않습니다. 회사는 비디오 파일을 Amazon S3로 마이그레이션하기로
결정합니다. 회사는 최소한의 네트워크 대역폭을 사용하면서 가능한 한 빨리 비디오 파일을
마이그레이션해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. S3 버킷 생성 S3 버킷에 쓸 수 있는 권한이 있는 IAM 역할을 생성합니다. AWS CLI를
사용하여 모든 파일을 S3 버킷에 로컬로 복사합니다.
B. AWS Snowball Edge 작업을 생성합니다. 온프레미스에서 Snowball Edge 디바이스를
받습니다. Snowball Edge 클라이언트를 사용하여 데이터를 디바이스로 전송합니다. AWS가
데이터를 Amazon S3로 가져올 수 있도록 장치를 반환하십시오.
C. 온프레미스에 S3 파일 게이트웨이를 배포합니다. S3 파일 게이트웨이에 연결할 공용
서비스 엔드포인트 생성 S3 버킷 생성 S3 파일 게이트웨이에 새 NFS 파일 공유 생성 새 파일
공유가 S3 버킷을 가리키도록 합니다. 기존 NFS 파일 공유에서 S3 파일 게이트웨이로
데이터를 전송합니다.
D. 온프레미스 네트워크와 AWS 간에 AWS Direct Connect 연결을 설정합니다. 온프레미스에
77

IT Certification Guaranteed, The Easy Way!
S3 파일 게이트웨이를 배포합니다. S3 File Gateway에 연결할 퍼블릭 가상 인터레이스(VIF)를
생성합니다. S3 버킷을 생성합니다. S3 File Gateway에서 새 NFS 파일 공유를 생성합니다. 새
파일 공유가 S3 버킷을 가리키도록 합니다. 기존 NFS 파일 공유에서 S3 파일 게이트웨이로
데이터를 전송합니다.
Answer: B
Explanation:
The basic difference between Snowball and Snowball Edge is the capacity they provide.
Snowball provides a total of 50 TB or 80 TB, out of which 42 TB or 72 TB is available, while
Amazon Snowball Edge provides
100 TB, out of which 83 TB is available.
QUESTION NO: 114
한 회사가 7개의 Amazon EC2 인스턴스를 사용하여 AWS에서 웹 애플리케이션을
호스팅합니다. 회사에서는 DNS 쿼리에 대한 응답으로 모든 정상 EC2 인스턴스의 IP 주소가
반환되도록 요구합니다.
이 요구 사항을 충족하려면 어떤 정책을 사용해야 합니까?
A. 단순 라우팅 정책
B. 지연 라우팅 정책
C. 다중값 라우팅 정책
D. 지리적 위치 라우팅 정책
Answer: C
Explanation:
Use a multivalue answer routing policy to help distribute DNS responses across multiple
resources. For example, use multivalue answer routing when you want to associate your
routing records with a Route 53 health check. For example, use multivalue answer routing
when you need to return multiple values for a DNS query and route traffic to multiple IP
addresses. https://aws.amazon.com/premiumsupport/knowledge-center
/multivalue-versus-simple-policies/
QUESTION NO: 115
한 회사에서 Amazon 머신 이미지(AMI)를 관리하려고 합니다. 회사는 현재 AMI가 생성된
동일한 AWS 리전에 AMI를 복사하고 있습니다. 회사는 AWS API 호출을 캡처하고 회사 계정
내에서 Amazon EC2 Createlmage API 작업이 호출될 때마다 알림을 보내는 애플리케이션을
설계해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS CloudTrail 로그를 쿼리하고 Createlmage API 호출이 감지되면 알림을 보내는 AWS
Lambda 함수를 생성합니다.
B. 업데이트된 로그가 Amazon S3로 전송될 때 발생하는 Amazon Simple 알림
서비스(Amazon SNS) 알림으로 AWS CloudTrail을 구성합니다. Amazon Athena를 사용하여
새 테이블을 생성하고 API 호출이 감지되면 Createlmage에 쿼리합니다.
C. Createlmage API 호출에 대한 Amazon EventBridge(Amazon CloudWatch Events) 규칙을
생성합니다. Createlmage API 호출이 감지되면 알림을 보내도록 대상을 Amazon
SNS(Amazon SNS) 주제로 구성합니다.
D. Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 AWS CloudTrail 로그의
대상으로 구성합니다. Createlmage API 호출이 감지되면 Amazon Simple 알림
78

IT Certification Guaranteed, The Easy Way!
서비스(Amazon SNS) 주제에 알림을 보내는 AWS Lambda 함수를 생성합니다.
Answer: C
Explanation:
https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/monitor-ami-
events.html#:~:text=For%
20example%2C%20you%20can%20create%20an%20EventBridge%20rule%20that%20dete
cts%20when%
20the%20AMI%20creation%20process%20has%20completed%20and%20then%20invokes
%20an%
20Amazon%20SNS%20topic%20to%20send%20an%20email%20notification%20to%20you.
Creating an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateImage
API call and configuring the target as an Amazon Simple Notification Service (Amazon SNS)
topic to send an alert when a CreateImage API call is detected will meet the requirements
with the least operational overhead. Amazon EventBridge is a serverless event bus that
makes it easy to connect applications together using data from your own applications,
integrated Software as a Service (SaaS) applications, and AWS services. By creating an
EventBridge rule for the CreateImage API call, the company can set up alerts whenever this
operation is called within their account. The alert can be sent to an SNS topic, which can then
be configured to send notifications to the company's email or other desired destination.
QUESTION NO: 116
회사는 Application Load Balancer 뒤의 Amazon EC2 인스턴스에서 고가용성 웹
애플리케이션을 실행합니다. 회사는 Amazon CloudWatch 지표를 사용합니다. 웹
애플리케이션에 대한 트래픽이 증가함에 따라 일부 EC2 인스턴스가 많은 미해결 요청으로
인해 과부하됩니다. CloudWatch 지표는 요청 수가 처리 시간과 일부 EC2 인스턴스로부터
응답을 받는 데 걸리는 시간이 모두 다른 EC2 인스턴스에 비해 높습니다. 회사는 이미
오버로드된 EC2 인스턴스에 새 요청이 전달되는 것을 원하지 않습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. RequestCountPerTarget 및 Active Connection Count CloudWatch 지표를 기반으로 라운드
로빈 라우팅 알고리즘을 사용합니다.
B. RequestCountPerTarget 및 ActiveConnectionCount CloudWatch 지표를 기반으로 최소
미해결 요청 알고리즘을 사용합니다.
C. RequestCount 및 TargetResponseTime CloudWatch 지표를 기반으로 라운드 로빈 라우팅
알고리즘을 사용합니다.
D. RequestCount 및 TargetResponseTime CloudWatch 지표를 기반으로 최소 미해결 요청
알고리즘을 사용합니다.
Answer: D
Explanation:
The least outstanding requests (LOR) algorithm is a load balancing algorithm that distributes
incoming requests to the target with the fewest outstanding requests. This helps to avoid
overloading any single target and improves the overall performance and availability of the
web application. The LOR algorithm can use the RequestCount and TargetResponseTime
CloudWatch metrics to determine the number of outstanding requests and the response time
of each target. These metrics measure the number of requests processed by each target and
the time elapsed after the request leaves the load balancer until a response from the target is
79

IT Certification Guaranteed, The Easy Way!
received by the load balancer, respectively. By using these metrics, the LOR algorithm can
route new requests to the targets that are less busy and more responsive, and avoid sending
requests to the targets that are already overloaded or slow. This solution meets the
requirements of the company.
References:
* Application Load Balancer now supports Least Outstanding Requests algorithm for load
balancing requests
* Target groups for your Application Load Balancers
* Elastic Load Balancing - Application Load Balancers
QUESTION NO: 117
한 회사에서 Amazon 머신 이미지(AMI)를 관리하려고 합니다. 회사는 현재 AMI가 생성된
동일한 AWS 리전에 AMI를 복사하고 있습니다. 회사는 회사 계정 내에서 Amazon EC2
CreateImage API 작업이 호출될 때마다 AWS API 호출을 캡처하고 알림을 보내는
애플리케이션을 설계해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS CloudTrail 로그를 쿼리하고 CreateImage API 호출이 감지되면 알림을 보내는 AWS
Lambda 함수를 생성합니다.
B. 업데이트된 로그가 Amazon S3로 전송될 때 발생하는 Amazon Simple 알림
서비스(Amazon SNS) 알림으로 AWS CloudTrail을 구성합니다. API 호출이 감지되면 Amazon
Athena를 사용하여 새 테이블을 생성하고 CreateImage에 대해 쿼리합니다.
C. CreateImage API 호출에 대한 Amazon EventBridge(Amazon CloudWatch Events) 규칙을
생성합니다. CreateImage API 호출이 감지되면 알림을 보내도록 대상을 Amazon
SNS(Amazon SNS) 주제로 구성합니다.
D. Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 AWS CloudTrail 로그의
대상으로 구성합니다. CreateImage API 호출이 감지되면 Amazon Simple 알림
서비스(Amazon SNS) 주제에 알림을 보내는 AWS Lambda 함수를 생성합니다.
Answer: C
Explanation:
https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/monitor-ami-
events.html#:~:text=For%
20example%2C%20you%20can%20create%20an%20EventBridge%20rule%20that%20dete
cts%20when%
20the%20AMI%20creation%20process%20has%20completed%20and%20then%20invokes
%20an%
20Amazon%20SNS%20topic%20to%20send%20an%20email%20notification%20to%20you.
QUESTION NO: 118
회사는 직원에게 기밀 및 민감한 파일에 대한 안전한 액세스를 제공해야 합니다. 회사에서는
승인된 사용자만 파일에 액세스할 수 있도록 하려고 합니다. 파일은 직원 장치에 안전하게
다운로드되어야 합니다.
파일은 온프레미스 Windows 파일 서버에 저장됩니다. 그러나 원격 사용량의 증가로 인해
파일 서버의 용량이 부족해졌습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 파일 서버를 퍼블릭 서브넷의 Amazon EC2 인스턴스로 마이그레이션합니다. 직원 .IP
80

IT Certification Guaranteed, The Easy Way!
주소에 대한 인바운드 트래픽을 제한하도록 보안 그룹을 구성합니다.
B. 파일을 Amazon FSx for Windows File Server 파일 시스템으로 마이그레이션합니다.
Amazon FSx 파일 시스템을 온프레미스 Active Directory와 통합하여 AWS 클라이언트 VPN을
구성합니다.
C. 파일을 Amazon S3로 마이그레이션하고 프라이빗 VPC 엔드포인트를 생성합니다.
다운로드를 허용하려면 서명된 URL을 만드세요.
D. 파일을 Amazon S3로 마이그레이션하고 퍼블릭 VPC 엔드포인트를 생성합니다. 직원이
AWS IAM ID 센터(AWS Sing-On)에 로그인할 수 있도록 허용합니다.
Answer: B
Explanation:
Windows file server is on-premise and we need something to replicate the data to the cloud,
the only option we have is AWS FSx for Windows File Server. Also, since the information is
confidential and sensitive, we also want to make sure that the appropriate users have access
to it in a secure manner. https://docs.aws.
amazon.com/fsx/latest/WindowsGuide/what-is.html
QUESTION NO: 119
한 회사는 현재 온프레미스 블록 스토리지 시스템에 5TB의 데이터를 저장하고 있습니다. 이
회사의 현재 스토리지 솔루션은 추가 데이터를 위한 제한된 공간을 제공합니다. 이 회사는
낮은 지연 시간으로 자주 액세스하는 데이터를 검색할 수 있어야 하는 온프레미스
애플리케이션을 실행합니다. 이 회사에는 클라우드 기반 스토리지 솔루션이 필요합니다.
이러한 요구 사항을 충족하면서 가장 운영 효율적인 솔루션은 무엇일까요?
A. 캐시된 볼륨을 iSCSt 대상으로 하는 AWS Storage Gateway 볼륨 게이트웨이를
사용합니다.
B. AWS Storage Gateway Tape Gateway를 사용합니다. Tape Gateway를 온프레미스
애플리케이션과 통합하여 Amazon S3에 가상 테이프를 저장합니다.
C. iSCSI 대상으로 저장된 볼륨이 있는 AWS Storage Gateway 볼륨 게이트웨이를
사용합니다.
D. Amazon S3 파일 게이트웨이 사용 S3 파일 게이트웨이를 온프레미스 애플리케이션과
통합하여 SMB 파일 시스템을 사용하여 파일을 저장하고 직접 검색합니다.
Answer: A
QUESTION NO: 120
회사는 레거시 애플리케이션을 사용하여 CSV 형식으로 데이터를 생성합니다. 레거시
애플리케이션은 출력 데이터를 Amazon S3에 저장합니다. 회사는 저장된 데이터를 분석하기
위해 복잡한 SQL 쿼리를 수행할 수 있는 새로운 COTS(상용 기성품) 애플리케이션을
배포하고 있습니다. Amazon Redshift 및 Amazon S3만 해당 그러나 COTS 애플리케이션은
레거시 애플리케이션이 생성하는 csv 파일을 처리할 수 없습니다. 회사는 레거시
애플리케이션을 업데이트하여 다른 형식의 데이터를 생성할 수 없습니다. 회사는 COTS
애플리케이션이 해당 데이터를 사용할 수 있도록 솔루션을 구현해야 합니다. 레거시
어플리케이터가 생성합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 일정에 따라 실행되는 AWS Glue ETL(추출, 변환 및 로드) 작업을 생성합니다. .csv 파일을
처리하고 처리된 데이터를 Amazon Redshit에 저장하도록 ETL 작업을 구성합니다.
81

IT Certification Guaranteed, The Easy Way!
B. Amazon EC2 인스턴스에서 실행되어 변환하는 Python 스크립트를 개발합니다. csv 파일을
sql 파일로 변환하면 cron 일정에 따라 Python 스크립트를 호출하여 Amazon S3에 출력
파일을 저장합니다.
C. AWS Lambda 함수와 Amazon DynamoDB 테이블을 생성합니다. S3 이벤트를 사용하여
Lambda 함수를 호출합니다. 추출 변환 및 로드(ETL) 작업을 수행하여 .csv 파일을 처리하고
처리된 데이터를 DynamoDB 테이블에 저장하도록 Lambda 함수를 구성합니다.
D. Amazon EventBridge(Amazon CloudWatch Events)를 사용하여 매주 일정에 따라 Amazon
EMR 클러스터를 시작합니다. ETL(추출, 트랙폼 및 로드) 작업을 수행하여 .csv 파일을
처리하고 처리된 데이터를 Amazon Redshift 테이블에 저장하도록 EMR 클러스터를
구성합니다.
Answer: A
Explanation:
This solution meets the requirements of implementing a solution so that the COTS
application can use the data that the legacy application produces with the least operational
overhead. AWS Glue is a fully managed service that provides a serverless ETL platform to
prepare and load data for analytics. AWS Glue can process data in various formats, including
.csv files, and store the processed data in Amazon Redshift, which is a fully managed data
warehouse service that supports complex SQL queries. AWS Glue can run ETL jobs on a
schedule, which can automate the data processing and loading process.
Option B is incorrect because developing a Python script that runs on Amazon EC2 instances
to convert the .
csv files to sql files can increase the operational overhead and complexity, and it may not
provide consistent data processing and loading for the COTS application. Option C is
incorrect because creating an AWS Lambda function and an Amazon DynamoDB table to
process the .csv files and store the processed data in the DynamoDB table does not meet
the requirement of using Amazon Redshift as the data source for the COTS application.
Option D is incorrect because using Amazon EventBridge (Amazon CloudWatch Events) to
launch an Amazon EMR cluster on a weekly schedule to process the .csv files and store the
processed data in an Amazon Redshift table can increase the operational overhead and
complexity, and it may not provide timely data processing and loading for the COTS
application.
References:
* https://aws.amazon.com/glue/
* https://aws.amazon.com/redshift/
QUESTION NO: 121
한 회사가 us-east-1 지역 내 3개의 개별 VPC에서 여러 비즈니스 애플리케이션을 실행하고
있습니다. 애플리케이션은 VPC 간에 통신할 수 있어야 합니다. 또한 애플리케이션은 단일
온프레미스 데이터 센터에서 실행되는 대기 시간에 민감한 애플리케이션에 매일 수백
기가바이트의 데이터를 일관되게 보낼 수 있어야 합니다.
솔루션 설계자는 비용 효율성을 극대화하는 네트워크 연결 솔루션을 설계해야 합니다. 어떤
솔루션이 이러한 요구 사항을 충족합니까?
A. 데이터 센터에서 AWS로 3개의 AWS Site-to-Site VPN 연결을 구성합니다. 각 VPC에 대해
하나의 VPN 연결을 구성하여 연결을 설정합니다.
B. 각 VPC에서 타사 가상 네트워크 어플라이언스를 시작합니다. 데이터 센터와 각 가상
82

IT Certification Guaranteed, The Easy Way!
어플라이언스 사이에 iPsec VPN 터널을 설정합니다.
C. 데이터 센터에서 us-east-1의 Direct Connect 게이트웨이로 3개의 AWS Direct Connect
연결을 설정합니다. Direct Connect 연결 중 하나를 사용하도록 각 VPC를 구성하여 연결을
설정합니다.
D. 데이터 센터에서 AWS로 하나의 AWS Direct Connect 연결을 설정합니다. Transit
Gateway를 생성하고 각 VPC를 Transit Gateway에 연결합니다. Direct Connect 연결과 전송
게이트웨이 간의 연결을 설정합니다.
Answer: D
Explanation:
https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-
connect-aws-transit- gateway.html
QUESTION NO: 122
솔루션 아키텍트는 2개의 퍼블릭 서브넷과 2개의 프라이빗 서브넷을 포함하는 VPC를
생성합니다. 기업 보안 의무에 따라 솔루션 설계자는 프라이빗 서브넷에서 모든 Amazon EC2
인스턴스를 시작해야 합니다. 그러나 솔루션 아키텍트가 프라이빗 서브넷의 포트 80 및
443에서 웹 서버를 실행하는 EC2 인스턴스를 시작하면 외부 인터넷 트래픽이 서버에 연결할
수 없습니다.
이 문제를 해결하려면 솔루션 아키텍트가 무엇을 해야 합니까?
A. EC2 인스턴스를 프라이빗 서브넷의 Auto Scaling 그룹에 연결합니다. 웹 사이트의 DNS
레코드가 Auto Scaling 그룹 식별자로 확인되는지 확인하세요.
B. 퍼블릭 서브넷에서 인터넷 연결 Application Load Balancer(ALB)를 프로비저닝합니다.
ALB와 연결된 대상 그룹에 EC2 인스턴스를 추가합니다. 웹 사이트의 DNS 레코드가 ALB로
확인되는지 확인하세요.
C. 프라이빗 서브넷에서 NAT 게이트웨이를 시작합니다. NAT 게이트웨이에 기본 경로를
추가하려면 프라이빗 서브넷의 라우팅 테이블을 업데이트하세요. NAT 게이트웨이에 공용
탄력적 IP 주소를 연결합니다.
D. EC2 인스턴스에 연결된 보안 그룹이 포트 80의 HTTP 트래픽과 포트 443의 HTTPS
트래픽을 허용하는지 확인합니다. 웹 사이트의 DNS 레코드가 EC2 인스턴스의 퍼블릭 IP
주소로 확인되는지 확인합니다.
Answer: B
Explanation:
An Application Load Balancer (ALB) is a type of Elastic Load Balancer (ELB) that distributes
incoming application traffic across multiple targets, such as EC2 instances, containers,
Lambda functions, and IP addresses, in multiple Availability Zones1. An ALB can be internet-
facing or internal. An internet-facing ALB has a public DNS name that clients can use to send
requests over the internet1. An internal ALB has a private DNS name that clients can use to
send requests within a VPC1. This solution meets the requirements of the question because:
* It allows external internet traffic to connect to the web server on ports 80 and 443, as the
ALB listens for requests on these ports and forwards them to the EC2 instance in the private
subnet1.
* It does not violate the corporate security mandate, as the EC2 instance is launched in a
private subnet and does not have a public IP address or a route to an internet gateway2.
* It reduces the operational overhead, as the ALB is a fully managed service that handles the
tasks of load balancing, health checking, scaling, and security1.
83

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 123
한 텔레마케팅 회사가 AWS에서 고객 콜 센터 기능을 설계하고 있습니다. 회사에는 다중 화자
인식 기능을 제공하고 녹취 파일을 생성하는 솔루션이 필요합니다. 회사는 비즈니스 패턴을
분석하기 위해 녹취 파일을 쿼리하려고 합니다. 녹취 파일은 필로스 감사를 위해 7년 동안
저장되어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 여러 화자를 인식하려면 Amazon Recognition을 사용하세요. Amazon S3에 기록 파일 저장
기록 파일 분석을 위해 기계 팀 구성 모델 사용
B. 여러 화자를 인식하려면 Amazon Transcribe를 사용하십시오. 기록 파일 분석가를 위해
Amazon Athena 사용
C. Amazon Translate 또는 다중 화자 인식을 사용합니다. Amazon Redshift에 기록 파일 저장
SQL 대기열 사용 또는 기록 파일 분석
D. 여러 화자 인식을 위해 Amazon Recognition을 사용합니다. Amazon S3에 기록 파일 저장
기록 파일 분석을 위해 Amazon Textract 사용
Answer: B
Explanation:
Amazon Transcribe now supports speaker labeling for streaming transcription. Amazon
Transcribe is an automatic speech recognition (ASR) service that makes it easy for you to
convert speech-to-text. In live audio transcription, each stream of audio may contain multiple
speakers. Now you can conveniently turn on the ability to label speakers, thus helping to
identify who is saying what in the output transcript. https://aws.
amazon.com/about-aws/whats-new/2020/08/amazon-transcribe-supports-speaker-labeling-
streaming- transcription/
QUESTION NO: 124
한 회사가 AWS 계정에 액세스할 수 있는 개인 데이터 센터에서 실행되는 타사 시스템을
제공하려고 합니다. 이 회사는 타사 시스템에서 직접 AWS API를 호출하려고 합니다. 이
회사는 디지털 인증서를 관리하기 위한 기존 프로세스를 가지고 있습니다. 이 회사는 SAML
또는 OpenID Connect(OIDC) 기능을 사용하고 싶지 않으며 장기 AWS 자격 증명을 저장하고
싶지 않습니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 통신 채널의 클라이언트와 서버 측 인증을 허용하도록 상호 TLS를 구성합니다.
B. AWS API에 대한 들어오는 HTTPS 요청을 인증하도록 AWS Signature 버전 4를
구성합니다.
C. AWS API에서 검증할 수 있는 어설션에 대한 티켓을 교환하도록 Kerberos를 구성합니다.
D. AWS Identity and Access Management(IAM) 역할을 어디에서나 구성하여 X.509 인증서를
AWS 자격 증명으로 교환하고 AWS API와 상호 작용할 수 있습니다.
Answer: D
Explanation:
* A. Mutual TLS: Provides secure communication but does not integrate with AWS credential
exchange.
* B. AWS Signature v4: Requires direct integration with AWS and is less secure for external
systems.
* C. Kerberos: Not natively supported for AWS API authentication.
84

IT Certification Guaranteed, The Easy Way!
* D. IAM Roles Anywhere: Enables AWS API access using X.509 certificates without long-
term credentials.
References: IAM Roles Anywhere
QUESTION NO: 125
예산 계획의 일부. 경영진은 사용자별로 나열된 AWS 청구 댐에 대한 보고서를 원합니다.
데이터는 부서 예산을 작성하는 데 사용됩니다. 솔루션 설계자는 이 보고서를 얻는 가장
효율적인 방법을 결정해야 합니다. 정보 어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon Athena로 쿼리를 실행하여 보고서를 생성합니다.
B. Cost Explorer에서 보고서를 생성하고 보고서를 다운로드합니다.
C. 실행 중인 대시보드에서 청구서 세부정보에 액세스하고 Via 청구서를 다운로드합니다.
D. Amazon Simple Email Service(Amazon SES)로 알림을 보내도록 AWS 예산의 비용 예산을
수정합니다.
Answer: B
Explanation:
This option is the most efficient because it uses Cost Explorer, which is a tool that allows you
to visualize, understand, and manage your AWS costs and usage over time1. You can create
a report in Cost Explorer that lists AWS billed items by user, using the user name tag as a
filter2. You can then download the report as a CSV file and use it for budget planning. Option
A is less efficient because it uses Amazon Athena, which is a serverless interactive query
service that allows you to analyze data in Amazon S3 using standard SQL3. You would need
to set up an Athena table that points to your AWS Cost and Usage Report data in S3, and
then run a query to generate the report. This would incur additional costs and complexity.
Option C is less efficient because it uses the billing dashboard, which provides a high-level
summary of your AWS costs and usage.
You can access the bill details from the billing dashboard and download them via bill, but this
would not list the billed items by user. You would need to use tags to group your costs by
user name, which would require additional steps. Option D is less efficient because it uses
AWS Budgets, which is a tool that allows you to plan your service usage, service costs, and
instance reservations. You can modify a cost budget in AWS Budgets to alert with Amazon
Simple Email Service (Amazon SES), but this would not generate a report of AWS billed
items by user. This would only notify you when your actual or forecasted costs exceed or are
expected to exceed your budgeted amount.
QUESTION NO: 126
금융 서비스 회사가 민감한 금융 거래를 처리하기 위해 AWS에서 새로운 애플리케이션을
출시할 계획입니다. 이 회사는 Amazon EC2 인스턴스에 애플리케이션을 배포할 것입니다. 이
회사는 Amazon RDS for MySQL을 데이터베이스로 사용할 것입니다. 이 회사의 보안 정책에
따라 데이터는 저장 중 및 전송 중에 암호화되어야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. AWS KMS 관리 키를 사용하여 Amazon RDS for MySQL에 대한 암호화를 구성합니다.
전송 중 암호화를 위해 AWS Certificate Manager(ACM) SSL/TLS 인증서를 구성합니다.
B. AWS KMS 관리 키를 사용하여 Amazon RDS for MySQL에 대한 휴면 암호화를
구성합니다. 전송 중 암호화를 위한 IPsec 터널을 구성합니다.
C. Amazon RDS for MySQL에 데이터를 저장하기 전에 타사 애플리케이션 수준 데이터
85

IT Certification Guaranteed, The Easy Way!
암호화를 구현합니다. 전송 중 암호화를 위해 AWS Certificate Manager(ACM) SSL/TLS
인증서를 구성합니다.
D. AWS KMS 관리 키를 사용하여 Amazon RDS for MySQL에 대한 저장 데이터 암호화를
구성합니다. 전송 중인 데이터를 암호화하기 위한 개인 연결을 활성화하기 위해 VPN 연결을
구성합니다.
Answer: A
Explanation:
This solution provides encryption at rest and in transit with the least operational overhead
while adhering to the company's security policies.
* Encryption at Rest: Amazon RDS for MySQL can be configured to encrypt data at rest by
using AWS Key Management Service (KMS) managed keys. This encryption is applied
automatically to all data stored on disk, including backups, read replicas, and snapshots. This
solution requires minimal operational overhead because AWS manages the encryption and
key management process.
* Encryption in Transit: AWS Certificate Manager (ACM) allows you to provision, manage,
and deploy SSL/TLS certificates seamlessly. These certificates can be used to encrypt data
in transit by configuring the MySQL instance to use SSL/TLS for connections. This setup
ensures that data is encrypted between the application and the database, protecting it from
interception during transmission.
* Why Not Other Options?:
* Option B (IPsec tunnels): While IPsec tunnels encrypt data in transit, they are more
complex to manage and require additional configuration and maintenance, leading to higher
operational overhead.
* Option C (Third-party application-level encryption): Implementing application-level
encryption adds complexity, requires code changes, and increases operational overhead.
* Option D (VPN for encryption): A VPN solution for encrypting data in transit is unnecessary
and adds additional complexity without providing any benefit over SSL/TLS, which is simpler
to implement and manage.
AWS References:
* Amazon RDS Encryption - Information on how to configure and use encryption for Amazon
RDS.
* AWS Certificate Manager (ACM) - Details on using ACM to manage SSL/TLS certificates for
securing data in transit.
QUESTION NO: 127
회사는 UDP를 사용하여 지리적으로 분산된 수천 개의 원격 장치로부터 데이터를 수신하는
응용 프로그램을 실행합니다. 응용 프로그램은 데이터를 즉시 처리하고 필요한 경우 장치에
다시 메시지를 보냅니다. 데이터가 저장되지 않습니다.
회사에는 장치에서 데이터 전송에 대한 대기 시간을 최소화하는 솔루션이 필요합니다.
솔루션은 또한 다른 AWS 리전에 대한 신속한 장애 조치도 제공해야 합니다. 어떤 솔루션이
이러한 요구 사항을 충족합니까?
A. Amazon Route 53 장애 조치 라우팅 정책 구성 두 리전 각각에 NLB(Network Load
Balancer) 생성 AWS Lambda 함수를 호출하여 데이터를 처리하도록 NLB 구성
B. AWS Global Accelerator를 사용하여 두 리전 각각에 NLB(Network Load Balancer)를
엔드포인트로 생성합니다. Fargate 시작 유형을 사용하여 Amazon Elastic Container
86

IT Certification Guaranteed, The Easy Way!
Service(Amazon ECS) 클러스터를 생성합니다. 클러스터에 ECS 서비스를 생성합니다. ECS
서비스를 NLB의 대상으로 설정합니다. Amazon ECS에서 데이터를 처리합니다.
C. AWS Global Accelerator를 사용하여 두 지역 각각에 엔드포인트로 ALB(Application Load
Balancer)를 생성합니다. Fargate 시작 유형을 사용하여 Amazon Elastic Container
Service(Amazon ECS) 클러스터를 생성합니다. 클러스터에 ECS 서비스를 생성합니다. ECS
서비스를 ALB의 대상으로 설정 Amazon ECS에서 데이터 처리
D. Amazon Route 53 장애 조치 라우팅 정책 구성 두 지역 각각에 ALB(Application Load
Balancer) 생성 Fargate 시작 유형을 사용하여 Amazon Elastic Container Service(Amazon
ECS) 클러스터 생성 클러스터 세트에 ECS 서비스 생성 ALB의 대상인 ECS 서비스 Amazon
ECS의 데이터 처리
Answer: B
Explanation:
To meet the requirements of minimizing latency for data transmission from the devices and
providing rapid failover to another AWS Region, the best solution would be to use AWS
Global Accelerator in combination with a Network Load Balancer (NLB) and Amazon Elastic
Container Service (Amazon ECS). AWS Global Accelerator is a service that improves the
availability and performance of applications by using static IP addresses (Anycast) to route
traffic to optimal AWS endpoints. With Global Accelerator, you can direct traffic to multiple
Regions and endpoints, and provide automatic failover to another AWS Region.
QUESTION NO: 128
회사의 애플리케이션은 데이터 수집을 위해 여러 SaaS(Software-as-a-Service) 소스와
통합됩니다. 회사는 Amazon EC2 인스턴스를 실행하여 데이터를 수신하고 분석을 위해
Amazon S3 버킷에 데이터를 업로드합니다. 데이터를 수신하고 업로드하는 동일한 EC2
인스턴스는 업로드가 완료되면 사용자에게 알림도 보냅니다. 회사에서는 애플리케이션
성능이 느려지는 것을 확인하고 성능을 최대한 개선하려고 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EC2 인스턴스가 확장될 수 있도록 Auto Scaling 그룹을 생성합니다. S3 버킷에 대한
업로드가 완료되면 Amazon Simple 알림 서비스(Amazon SNS) 주제로 이벤트를 보내도록 S3
이벤트 알림을 구성합니다.
B. 각 SaaS 소스와 S3 버킷 간에 데이터를 전송하는 Amazon AppFlow 흐름을 생성합니다. S3
버킷에 대한 업로드가 완료되면 Amazon Simple 알림 서비스(Amazon SNS) 주제로 이벤트를
보내도록 S3 이벤트 알림을 구성합니다.
C. 각 SaaS 소스에 대해 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성하여
출력 데이터를 보냅니다. S3 버킷을 규칙의 대상으로 구성합니다. S3 버킷에 대한 업로드가
완료되면 이벤트를 보내는 두 번째 EventBridge(CloudWatch 이벤트) 규칙을 생성합니다.
Amazon Simple 알림 서비스(Amazon SNS) 주제를 두 번째 규칙의 대상으로 구성합니다.
D. EC2 인스턴스 대신 사용할 Docker 컨테이너를 생성합니다. Amazon Elastic Container
Service(Amazon ECS)에서 컨테이너화된 애플리케이션을 호스팅합니다. S3 버킷에 대한
업로드가 완료되면 Amazon Simple 알림 서비스(Amazon SNS) 주제로 이벤트를 보내도록
Amazon CloudWatch Container Insights를 구성합니다.
Answer: B
Explanation:
Amazon AppFlow is a fully managed integration service that enables you to securely transfer
data between Software-as-a-Service (SaaS) applications like Salesforce, SAP, Zendesk,
87

IT Certification Guaranteed, The Easy Way!
Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, in just a
few clicks. https://aws.amazon.com/appflow/
QUESTION NO: 129
회사는 Amazon S3 버킷에 민감한 사용자 정보를 저장하고 있습니다. 회사는 VPC 내부의
Ama2on EC2 인스턴스에서 실행되는 애플리케이션 계층에서 이 버킷에 대한 보안 액세스를
제공하려고 합니다.
이를 달성하기 위해 솔루션 설계자가 수행해야 하는 단계 조합은 무엇입니까? (2개를
선택하세요.)
A. VPC 내에서 Amazon S3에 대한 VPC 게이트웨이 엔드포인트 구성
B. S3 버킷에 대한 객체를 퍼블릭으로 만드는 버킷 정책 생성
C. VPC에서 실행 중인 애플리케이션 계층으로만 액세스를 제한하는 버킷 정책 생성
D. S3 액세스 정책으로 IAM 사용자를 생성하고 IAM 자격 증명을 EC2 인스턴스에 복사
E. NAT 인스턴스를 생성하고 EC2 인스턴스가 NAT 인스턴스를 사용하여 S3 버킷에
액세스하도록 합니다.
Answer: A C
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/s3-private-connection-no-
authentication/
QUESTION NO: 130
솔루션 아키텍트는 Amazon API Gateway를 사용하여 사용자로부터 요청을 수신하는 새로운
API를 설계하고 있습니다. 요청량은 매우 다양합니다. 단 한 번의 요청도 받지 못한 채 몇
시간이 지나갈 수 있습니다. 데이터 처리는 비동기식으로 이루어지지만 요청 후 몇 초 내에
완료되어야 합니다.
최저 비용으로 요구 사항을 제공하려면 솔루션 설계자가 API 호출을 통해 어떤 컴퓨팅
서비스를 사용해야 합니까?
A. AWS Glue 작업
B. AWS Lambda 함수
C. Amazon Elastic Kubernetes Service(Amazon EKS)에서 호스팅되는 컨테이너화된 서비스
D. Amazon EC2와 함께 Amazon ECS에서 호스팅되는 컨테이너화된 서비스
Answer: B
Explanation:
API Gateway + Lambda is the perfect solution for modern applications with serverless
architecture.
QUESTION NO: 131
소매 회사는 퍼블릭 REST API에 지역 Amazon API Gateway API를 사용합니다. API Gateway
엔드포인트는 Amazon Route 53 별칭 레코드를 가리키는 사용자 지정 도메인 이름입니다.
솔루션 아키텍트는 고객에게 최소한의 영향을 미치고 데이터 손실을 최소화하는 솔루션을
생성하여 새 버전의 API를 릴리스해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. API 게이트웨이에 대한 카나리아 릴리스 배포 단계를 생성합니다. 최신 API 버전을
배포합니다. 트래픽의 적절한 비율을 카나리아 단계로 지정합니다. API 검증 후 카나리아
88

IT Certification Guaranteed, The Easy Way!
단계를 프로덕션 단계로 승격합니다.
B. OpenAPI YAML 파일 형식의 새 API 버전으로 새 API 게이트웨이 엔드포인트를
생성합니다. API Gateway의 API에 병합 모드에서 가져오기-업데이트 작업을 사용합니다.
API의 새 버전을 프로덕션 단계에 배포합니다.
C. OpenAPI JSON 파일 형식의 새 API 버전으로 새 API 게이트웨이 엔드포인트를
생성합니다. 덮어쓰기 모드에서 업데이트로 가져오기 작업을 API Gateway의 API에
사용합니다. API의 새 버전을 프로덕션 단계에 배포합니다.
D. API 정의의 새 버전으로 새 API 게이트웨이 엔드포인트를 생성합니다. 새 API Gateway
API에 대한 사용자 지정 도메인 이름을 생성합니다. Route 53 별칭 레코드가 새 API Gateway
API 사용자 지정 도메인 이름을 가리키도록 합니다.
Answer: A
Explanation:
This answer is correct because it meets the requirements of releasing the new version of
APIs with minimal effects on customers and minimal data loss. A canary release deployment
is a software development strategy in which a new version of an API is deployed for testing
purposes, and the base version remains deployed as a production release for normal
operations on the same stage. In a canary release deployment, total API traffic is separated
at random into a production release and a canary release with a pre-configured ratio.
Typically, the canary release receives a small percentage of API traffic and the production
release takes up the rest. The updated API features are only visible to API traffic through the
canary. You can adjust the canary traffic percentage to optimize test coverage or
performance. By keeping canary traffic small and the selection random, most users are not
adversely affected at any time by potential bugs in the new version, and no single user is
adversely affected all the time. After the test metrics pass your requirements, you can
promote the canary release to the production release and disable the canary from the
deployment. This makes the new features available in the production stage.
References:
* https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html
QUESTION NO: 132
회사에는 내장형 NoSQL 데이터베이스가 포함된 웹 애플리케이션이 있습니다.
애플리케이션은 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스에서
실행됩니다. 인스턴스는 단일 가용 영역의 Amazon EC2 Auto Scaling 그룹에서 실행됩니다.
최근 트래픽이 증가함에 따라 애플리케이션의 가용성이 높아야 하고 데이터베이스의 최종
일관성이 유지되어야 합니다. 어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을
충족합니까?
A. ALB를 Network Load Balancer로 교체 EC2 인스턴스의 복제 서비스를 사용하여 내장형
NoSQL 데이터베이스를 유지 관리합니다.
B. ALB를 Network Load Balancer로 교체 AWS Database Migration Service(AWS DMS)를
사용하여 내장형 NoSQL 데이터베이스를 Amazon DynamoDB로 마이그레이션합니다.
C. 3개의 가용 영역에서 EC2 인스턴스를 사용하도록 Auto Scaling 그룹을 수정합니다. EC2
인스턴스의 복제 서비스를 통해 내장형 NoSQL 데이터베이스를 유지 관리합니다.
D. 3개의 가용 영역에서 EC2 인스턴스를 사용하도록 Auto Scaling 그룹을 수정합니다. AWS
Database Migration Service(AWS DMS)를 사용하여 내장형 NoSQL 데이터베이스를 Amazon
DynamoDB로 마이그레이션합니다.
89

IT Certification Guaranteed, The Easy Way!
Answer: D
Explanation:
This solution will meet the requirements of high availability and eventual consistency with the
least operational overhead. By modifying the Auto Scaling group to use EC2 instances
across three Availability Zones, the web application can handle the increase in traffic and
tolerate the failure of one or two Availability Zones. By migrating the embedded NoSQL
database to Amazon DynamoDB, the company can benefit from a fully managed, scalable,
and reliable NoSQL database service that supports eventual consistency. AWS Database
Migration Service (AWS DMS) is a cloud service that makes it easy to migrate relational
databases, data warehouses, NoSQL databases, and other types of data stores. AWS DMS
can migrate the embedded NoSQL database to Amazon DynamoDB with minimal downtime
and zero data loss.
References: AWS Database Migration Service (AWS DMS), Amazon DynamoDB Features,
Amazon EC2 Auto Scaling
QUESTION NO: 133
회사는 온프레미스 위치에서 Amazon S3 버킷으로 100GB의 기록 데이터를
마이그레이션하려고 합니다. 회사는 사내에서 100Mbps 인터넷 연결을 보유하고 있습니다.
회사는 S3 버킷으로 전송되는 데이터를 암호화해야 합니다. 회사는 새로운 데이터를 Amazon
S3에 직접 저장합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS CLI에서 s3 sync 명령을 사용하여 데이터를 S3 버킷으로 직접 이동합니다.
B. AWS DataSync를 사용하여 온프레미스 위치에서 S3 버킷으로 데이터를
마이그레이션합니다.
C. AWS Snowball을 사용하여 데이터를 S3 버킷으로 이동합니다.
D. 온프레미스 위치에서 AWS로 IPsec VPN을 설정합니다. AWS CLI에서 s3 cp 명령을
사용하여 데이터를 S3 버킷으로 직접 이동합니다.
Answer: B
Explanation:
AWS DataSync is a data transfer service that makes it easy for you to move large amounts of
data online between on-premises storage and AWS storage services over the internet or
AWS Direct Connect. DataSync automatically encrypts your data in transit using TLS
encryption, and verifies data integrity during transfer using checksums. DataSync can
transfer data up to 10 times faster than open-source tools, and reduces operational overhead
by simplifying and automating tasks such as scheduling, monitoring, and resuming transfers.
References: https://aws.amazon.com/datasync/
QUESTION NO: 134
회사는 동일한 AWS 리전에 있는 Amazon S3 버킷에서 사진을 자주 업로드 및 다운로드해야
하는 사진 처리 애플리케이션을 실행합니다. 솔루션 설계자는 데이터 전송 비용이 증가한다는
사실을 알게 되었고 이러한 비용을 줄이기 위한 솔루션을 구현해야 합니다.
솔루션 설계자는 이 요구 사항을 어떻게 충족할 수 있습니까?
A. Amazon API Gateway를 퍼블릭 서브넷에 배포하고 이를 통해 S3 호출을 라우팅하도록
라우팅 테이블을 조정합니다.
B. NAT 게이트웨이를 퍼블릭 서브넷에 배포하고 S3 버킷에 대한 액세스를 허용하는
90

IT Certification Guaranteed, The Easy Way!
엔드포인트 정책을 연결합니다.
C. 애플리케이션을 퍼블릭 서브넷에 배포하고 인터넷 게이트웨이를 통해 라우팅하여 S3
버킷에 액세스하도록 허용합니다.
D. S3 VPC 게이트웨이 엔드포인트를 VPC에 배포하고 S3 버킷에 대한 액세스를 허용하는
엔드포인트 정책을 연결합니다.
Answer: D
Explanation:
The correct answer is Option D. Deploy an S3 VPC gateway endpoint into the VPC and
attach an endpoint policy that allows access to the S3 buckets. By deploying an S3 VPC
gateway endpoint, the application can access the S3 buckets over a private network
connection within the VPC, eliminating the need for data transfer over the internet. This can
help reduce data transfer fees as well as improve the performance of the application. The
endpoint policy can be used to specify which S3 buckets the application has access to.
QUESTION NO: 135
솔루션 설계자는 회사를 위한 고객 대상 애플리케이션을 설계하고 있습니다. 애플리케이션의
데이터베이스는 일년 내내 명확하게 정의된 액세스 패턴을 가지며 연중 시간에 따라 다양한
읽기 및 쓰기 횟수를 갖습니다. 회사는 데이터베이스에 대한 감사 기록을 7일 동안 보관해야
합니다. RPO(복구 지점 목표)는 5시간 미만이어야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Auto Scaling과 함께 Amazon DynamoDB 사용 온디맨드 백업 및 Amazon DynamoDB
Streams 사용
B. Amazon Redshift를 사용하세요. 동시성 확장을 구성합니다. 감사 로깅을 활성화합니다.
4시간마다 데이터베이스 스냅샷을 수행합니다.
C. 프로비저닝된 IOPS와 함께 Amazon RDS 사용 데이터베이스 감사 매개변수 활성화
5시간마다 데이터베이스 스냅샷 수행
D. Auto Scaling과 함께 Amazon Aurora MySQL을 사용합니다. 데이터베이스 감사 매개변수
활성화
Answer: A
Explanation:
This solution meets the requirements of a customer-facing application that has a clearly
defined access pattern throughout the year and a variable number of reads and writes that
depend on the time of year. Amazon DynamoDB is a fully managed NoSQL database service
that can handle any level of request traffic and data size. DynamoDB auto scaling can
automatically adjust the provisioned read and write capacity based on the actual workload.
DynamoDB on-demand backups can create full backups of the tables for data protection and
archival purposes. DynamoDB Streams can capture a time-ordered sequence of item-level
modifications in the tables for audit purposes.
Option B is incorrect because Amazon Redshift is a data warehouse service that is designed
for analytical workloads, not for customer-facing applications. Option C is incorrect because
Amazon RDS with Provisioned IOPS can provide consistent performance for relational
databases, but it may not be able to handle unpredictable spikes in traffic and data size.
Option D is incorrect because Amazon Aurora MySQL with auto scaling can provide high
performance and availability for relational databases, but it does not support audit logging as
a parameter.
91

IT Certification Guaranteed, The Easy Way!
References:
* https://aws.amazon.com/dynamodb/
* https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html
*
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html
* https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html
QUESTION NO: 136
보고 팀은 매일 Amazon S3 버킷으로 파일을 받습니다. 보고 팀은 Amazon QuickSight에서
사용하기 위해 매일 동시에 이 초기 S3 버킷의 파일을 수동으로 검토하고 분석 S3 버킷에
복사합니다. 추가 팀이 더 큰 크기의 더 많은 파일을 초기 S3 버킷으로 보내기 시작했습니다.
보고 팀은 파일이 초기 S3 버킷에 들어갈 때 S3 버킷을 자동으로 분석하여 파일을 이동하려고
합니다. 또한 보고 팀은 AWS Lambda 함수를 사용하여 복사된 데이터에 대해 패턴 일치
코드를 실행하려고 합니다. 또한 보고 팀은 데이터 파일을 Amazon SageMaker 파이프라인의
파이프라인으로 보내려고 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야
합니까?
A. 분석 S3 버킷에 파일을 복사하는 Lambda 함수를 생성합니다. 분석 S3 버킷에 대한 S3
이벤트 알림을 생성합니다. Lambda 및 SageMaker 파이프라인을 이벤트 알림 대상으로
구성합니다. s30objectCreated:Put을 이벤트 유형으로 구성합니다.
B. 분석 S3 버킷에 파일을 복사하는 Lambda 함수를 생성합니다. Amazon
EventBridge(Amazon CloudWatch Events)에 이벤트 알림을 보내도록 분석 S3 버킷을
구성합니다. EventBridge(CloudWatch 이벤트)에서 ObjectCreated 규칙을 구성합니다.
Lambda 및 SageMaker 파이프라인을 규칙의 대상으로 구성합니다.
C. S3 버킷 간에 S3 복제를 구성합니다. 분석 S3 버킷에 대한 S3 이벤트 알림을 생성합니다.
Lambda 및 SageMaker 파이프라인을 이벤트 알림 대상으로 구성합니다.
s30objectCreated:Put을 이벤트 유형으로 구성합니다.
D. S3 버킷 간에 S3 복제를 구성합니다. Amazon EventBridge(Amazon CloudWatch
Events)에 이벤트 알림을 보내도록 분석 S3 버킷을 구성합니다. EventBridge(CloudWatch
이벤트)에서 ObjectCreated 규칙을 구성합니다. Lambda 및 SageMaker 파이프라인을 규칙의
대상으로 구성합니다.
Answer: D
Explanation:
This solution meets the requirements of moving the files automatically, running Lambda
functions on the copied data, and sending the data files to SageMaker Pipelines with the
least operational overhead. S3 replication can copy the files from the initial S3 bucket to the
analysis S3 bucket as they arrive. The analysis S3 bucket can send event notifications to
Amazon EventBridge (Amazon CloudWatch Events) when an object is created. EventBridge
can trigger Lambda and SageMaker Pipelines as targets for the ObjectCreated rule.
Lambda can run pattern-matching code on the copied data, and SageMaker Pipelines can
execute a pipeline with the data files.
Option A is incorrect because creating a Lambda function to copy the files to the analysis S3
bucket is not necessary when S3 replication can do that automatically. It also adds
operational overhead to manage the Lambda function. Option B is incorrect because creating
a Lambda function to copy the files to the analysis S3 bucket is not necessary when S3
92

IT Certification Guaranteed, The Easy Way!
replication can do that automatically. It also adds operational overhead to manage the
Lambda function. Option C is incorrect because using S3 event notification with multiple
destinations can result in throttling or delivery failures if there are too many events.
References:
* https://aws.amazon.com/blogs/machine-learning/automate-feature-engineering-pipelines-
with-amazon- sagemaker/
* https://docs.aws.amazon.com/sagemaker/latest/dg/automating-sagemaker-with-
eventbridge.html
* https://aws.amazon.com/about-aws/whats-new/2021/04/new-options-trigger-amazon-
sagemaker- pipeline-executions/
QUESTION NO: 137
한 회사가 MySQL용 Amazon RDS에 데이터베이스를 배포했습니다. 트랜잭션 증가로 인해
데이터베이스 지원팀에서는 DB 인스턴스에 대한 읽기 속도가 느리다고 보고하고 읽기 전용
복제본을 추가할 것을 권장합니다.
이 변경 사항을 구현하기 전에 솔루션 설계자가 취해야 할 조치 조합은 무엇입니까? (2개를
선택하세요.)
A. RDS 기본 노드에서 binlog 복제를 활성화합니다.
B. 소스 DB 인스턴스의 장애 조치 우선순위를 선택합니다.
C. 원본 DB 인스턴스에서 장기 실행 트랜잭션이 완료되도록 허용합니다.
D. 글로벌 테이블을 생성하고 테이블을 사용할 수 있는 AWS 리전을 지정합니다.
E. 백업 보존 기간을 0이 아닌 값으로 설정하여 원본 인스턴스에서 자동 백업을 활성화합니다.
Answer: C E
Explanation:
"An active, long-running transaction can slow the process of creating the read replica. We
recommend that you wait for long-running transactions to complete before creating a read
replica. If you create multiple read replicas in parallel from the same source DB instance,
Amazon RDS takes only one snapshot at the start of the first create action. When creating a
read replica, there are a few things to consider. First, you must enable automatic backups on
the source DB instance by setting the backup retention period to a value other than 0.
This requirement also applies to a read replica that is the source DB instance for another
read replica"
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html
QUESTION NO: 138
한 회사는 대규모 웹 애플리케이션을 서버리스 마이크로서비스 아키텍처로 재설계하려고
합니다. 애플리케이션은 Amazon EC2 인스턴스를 사용하며 Python으로 작성되었습니다.
회사는 마이크로서비스로 테스트하기 위해 웹 애플리케이션의 한 구성 요소를 선택했습니다.
구성 요소는 초당 수백 개의 요청을 지원합니다. 회사는 Python을 지원하는 AWS 솔루션에서
마이크로서비스를 생성하고 테스트하려고 합니다. 또한 솔루션은 자동으로 확장되어야 하며
최소한의 인프라와 최소한의 운영 지원이 필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 최신 Amazon Linux 운영 체제를 실행하는 EC2 인스턴스의 자동 조정 기능이 있는 스팟
집합을 사용합니다.
B. 고가용성이 구성된 AWS Elastic Beanstalk 웹 서버 환경을 사용합니다.
93

IT Certification Guaranteed, The Easy Way!
C. Amazon Elastic Kubernetes Service(Amazon EKS)를 사용합니다. 자체 관리형 EC2
인스턴스의 Auto Scaling 그룹을 시작합니다.
D. 사용자 정의 개발 코드를 실행하는 AWS Lambda 함수를 사용합니다.
Answer: D
Explanation:
AWS Lambda is a serverless compute service that lets you run code without provisioning or
managing servers. You can use Lambda to create and test microservices that are written in
Python or other supported languages. Lambda scales automatically to handle the number of
requests per second. You only pay for the compute time you consume. Lambda also
integrates with other AWS services, such as Amazon API Gateway, Amazon S3, Amazon
DynamoDB, and Amazon SQS, to enable event-driven architectures. Lambda has minimal
infrastructure and operational overhead, as you do not need to manage servers, operating
systems, patches, or scaling policies.
The other options are not serverless solutions and require more infrastructure and
operational support. They also do not scale automatically to handle the number of requests
per second. A Spot Fleet is a collection of EC2 instances that run on spare capacity at low
prices. However, Spot Instances can be interrupted by AWS at any time, which can affect the
availability and performance of your microservice. AWS Elastic Beanstalk is a service that
automates the deployment and management of web applications on EC2 instances.
However, you still need to provision, configure, and monitor the underlying EC2 instances
and load balancers. Amazon EKS is a service that runs Kubernetes on AWS. However, you
still need to create, configure, and manage the EC2 instances that form the Kubernetes
cluster and nodes. You also need to install and update the Kubernetes software and tools.
References:
* What is AWS Lambda?
* Building Lambda functions with Python
* Create a layer for a Lambda Python function
* AWS Lambda - Function in Python
* How do I call my AWS Lambda function from a local python script?
QUESTION NO: 139
솔루션 아키텍트는 회사를 위한 사용자 인증 솔루션을 설계하고 있습니다. 솔루션은 일관되지
않은 지리적 위치에서 로그인하는 사용자에 대해 2단계 인증을 호출해야 합니다. IP 주소 또는
장치.
또한 솔루션은 수백만 명의 사용자를 수용할 수 있을 만큼 확장 가능해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족할까요?
A. 사용자 인증을 위한 Amazon Cognito 사용자 풀 구성 다중 인증(MFA)을 사용하여 nsk 기반
적응 인증 기능 활성화
B. 사용자 인증을 위한 Amazon Cognito ID 풀을 구성합니다. 다중 요소 인증(MFA)을
활성화합니다.
C. 사용자 인증을 위한 AWS Identity and Access Management(1AM) 사용자 구성
AllowManageOwnUserMFA 작업을 허용하는 1AM 정책 연결
D. 사용자 인증을 위한 AWS 1AM Identity Center(AWS Single Sign-On) 인증 구성 다중 요소
인증(MFA)을 요구하도록 권한 집합 구성
Answer: A
94

IT Certification Guaranteed, The Easy Way!
Explanation:
Amazon Cognito user pools provide a secure and scalable user directory for user
authentication and management. User pools support various authentication methods, such
as username and password, email and password, phone number and password, and social
identity providers. User pools also support multi-factor authentication (MFA), which adds an
extra layer of security by requiring users to provide a verification code or a biometric factor in
addition to their credentials. User pools can also enable risk-based adaptive authentication,
which dynamically adjusts the authentication challenge based on the risk level of the sign-in
attempt. For example, if a user tries to sign in from an unfamiliar device or location, the user
pool can require a stronger authentication factor, such as SMS or email verification code.
This feature helps to protect user accounts from unauthorized access and reduce the friction
for legitimate users. User pools can scale up to millions of users and integrate with other
AWS services, such as Amazon SNS, Amazon SES, AWS Lambda, and AWS KMS.
Amazon Cognito identity pools provide a way to federate identities from multiple identity
providers, such as user pools, social identity providers, and corporate identity providers.
Identity pools allow users to access AWS resources with temporary, limited-privilege
credentials. Identity pools do not provide user authentication or management features, such
as MFA or adaptive authentication. Therefore, option B is not correct.
AWS Identity and Access Management (IAM) is a service that helps to manage access to
AWS resources.
IAM users are entities that represent people or applications that need to interact with AWS.
IAM users can be authenticated with a password or an access key. IAM users can also
enable MFA for their own accounts, by using the AllowManageOwnUserMFA action in an
IAM policy. However, IAM users are not suitable for user authentication for web or mobile
applications, as they are intended for administrative purposes. IAM users also do not support
adaptive authentication based on risk factors. Therefore, option C is not correct.
AWS IAM Identity Center (AWS Single Sign-On) is a service that enables users to sign in to
multiple AWS accounts and applications with a single set of credentials. AWS SSO supports
various identity sources, such as AWS SSO directory, AWS Managed Microsoft AD, and
external identity providers. AWS SSO also supports MFA for user authentication, which can
be configured in the permission sets that define the level of access for each user. However,
AWS SSO does not support adaptive authentication based on risk factors.
Therefore, option D is not correct.
References:
* Amazon Cognito User Pools
* Adding Multi-Factor Authentication (MFA) to a User Pool
* Risk-Based Adaptive Authentication
* Amazon Cognito Identity Pools
* IAM Users
* Enabling MFA Devices
* AWS Single Sign-On
* How AWS SSO Works
QUESTION NO: 140
새로운 직원이 배포 엔지니어로 회사에 합류했습니다. 배포 엔지니어는 AWS CloudFormation
템플릿을 사용하여 여러 AWS 리소스를 생성합니다. 솔루션 설계자는 배포 엔지니어가 최소
95

IT Certification Guaranteed, The Easy Way!
권한 원칙을 따르면서 작업 활동을 수행하기를 원합니다.
이 목표를 달성하기 위해 솔루션 설계자는 어떤 단계를 함께 수행해야 합니까? (2개를
선택하세요.)
A. 배포 엔지니어가 AWS CloudFormation 스택 작업을 수행하기 위해 AWS 계정 루프 사용자
자격 증명을 사용하도록 합니다.
B. 배포 엔지니어를 위한 새 IAM 사용자를 생성하고 PowerUsers IAM 정책이 연결된 그룹에
IAM 사용자를 추가합니다.
C. 배포 엔지니어를 위한 새 IAM 사용자를 생성하고 IAM 정책 관리/액세스가 연결된 그룹에
IAM 사용자를 추가합니다.
D. 배포 엔지니어를 위한 새 IAM 사용자를 생성하고 AWS CloudFormation 작업만 허용하는
IAM 정책이 있는 그룹에 IAM 사용자를 추가합니다.
E. 배포 엔지니어가 IAM 역할 다이얼을 사용하여 AWS CloudFormation 스택 및 시작 스택과
관련된 권한을 명시적으로 정의할 수 있도록 IAM 역할을 생성합니다.
Answer: D E
Explanation:
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html
https://docs.aws.amazon.com/IAM/latest
/UserGuide/id_users.html
QUESTION NO: 141
회사의 웹 애플리케이션은 AWS Lambda 함수 앞의 Amazon API Gateway API와 Amazon
DynamoDB 데이터베이스로 구성됩니다. Lambda 함수는 비즈니스 로직을 처리하고
DynamoDB 테이블은 데이터를 호스팅합니다. 애플리케이션은 Amazon Cognito 사용자 풀을
사용하여 애플리케이션의 개별 사용자를 식별합니다. 솔루션 설계자는 구독이 있는 사용자만
프리미엄 콘텐츠에 액세스할 수 있도록 애플리케이션을 업데이트해야 합니다.
A. API Gateway API에서 API 캐싱 및 조절을 활성화합니다.
B. API Gateway API에 AWS WAF 설정 구독이 있는 사용자를 필터링하는 규칙 생성
C. DynamoDB 테이블의 프리미엄 콘텐츠에 세분화된 IAM 권한을 적용합니다.
D. API 사용 계획 및 API 키를 구현하여 구독하지 않은 사용자의 액세스를 제한합니다.
Answer: D
Explanation:
This option is the most efficient because it uses API usage plans and API keys, which are
features of Amazon API Gateway that allow you to control who can access your API and how
much and how fast they can access it1. It also implements API usage plans and API keys to
limit the access of users who do not have a subscription, which enables you to create
different tiers of access for your API and charge users accordingly.
This solution meets the requirement of updating the application so that only users who have
a subscription can access premium content. Option A is less efficient because it uses API
caching and throttling on the API Gateway API, which are features of Amazon API Gateway
that allow you to improve the performance and availability of your API and protect your
backend systems from traffic spikes2. However, this does not provide a way to limit the
access of users who do not have a subscription. Option B is less efficient because it uses
AWS WAF on the API Gateway API, which is a web application firewall service that helps
protect your web applications or APIs against common web exploits that may affect
96

IT Certification Guaranteed, The Easy Way!
availability, compromise security, or consume excessive resources3. However, this does not
provide a way to limit the access of users who do not have a subscription. Option C is less
efficient because it uses fine-grained IAM permissions to the premium content in the
DynamoDB table, which are permissions that allow you to control access to specific items or
attributes within a table4. However, this does not provide a way to limit the access of users
who do not have a subscription at the API level.
QUESTION NO: 142
한 비디오 게임 회사가 전 세계 사용자에게 새로운 게임 애플리케이션을 배포하고 있습니다.
회사에는 거의 실시간으로 플레이어의 리뷰와 순위를 제공할 수 있는 솔루션이 필요합니다.
솔루션 설계자는 데이터에 대한 빠른 액세스를 제공하는 솔루션을 설계해야 합니다. 또한
솔루션은 회사가 애플리케이션을 다시 시작하는 경우에도 데이터가 디스크에 유지되도록
보장해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon S3 버킷을 원본으로 사용하여 Amazon CloudFront 배포를 구성합니다. S3 버킷에
플레이어 데이터를 저장합니다.
B. 여러 AWS 리전에 Amazon EC2 인스턴스를 생성합니다. EC2 인스턴스에 플레이어
데이터를 저장합니다.
사용자를 가장 가까운 EC2 인스턴스로 안내하기 위해 지리적 위치 레코드로 Amazon Route
53을 구성합니다.
C. Redis 클러스터용 Amazon ElastiCache를 배포합니다. ElastiCache 클러스터에 플레이어
데이터를 저장합니다.
D. Memcached 클러스터용 Amazon ElastiCache를 배포합니다. ElastiCache 클러스터에
플레이어 데이터를 저장합니다.
Answer: C
Explanation:
* Requirement Analysis: The application needs near real-time access to data, persistence,
and minimal operational overhead.
* ElastiCache for Redis: Provides in-memory data storage with persistence, supporting fast
access and durability.
* Operational Overhead: Managed service reduces the burden of setup, maintenance, and
scaling.
* Implementation:
* Deploy an ElastiCache for Redis cluster.
* Configure Redis to persist data to disk using AOF (Append-Only File) or RDB (Redis
Database Backup) snapshots.
* Conclusion: ElastiCache for Redis meets the requirements for fast access, data
persistence, and low operational overhead.
References
* Amazon ElastiCache: ElastiCache for Redis Documentation
QUESTION NO: 143
회사는 Amazon EC2 인스턴스를 사용하여 내부 시스템을 호스팅합니다. 배포 작업의 일부로
관리자는 AWS CLI를 사용하여 EC2 인스턴스를 종료하려고 합니다. 그러나 관리자는
403(액세스 거부) 오류 메시지입니다.
97

IT Certification Guaranteed, The Easy Way!
관리자는 다음 IAM 정책이 연결된 IAM 역할을 사용하고 있습니다.
실패한 요청의 원인은 무엇입니까?
A. EC2 인스턴스에는 Deny 문이 포함된 리소스 기반 정책이 있습니다.
B. 정책 설명에 보안 주체가 지정되지 않았습니다.
C. "Action" 필드는 EC2 인스턴스를 종료하는 데 필요한 조치를 부여하지 않습니다.
D. EC2 인스턴스 종료 요청이 CIDR 블록 192.0.2.0/24에서 시작되지 않았거나
203.0 113.0/24
Answer: D
QUESTION NO: 144
한 회사에서 대량의 데이터를 병렬로 처리하는 애플리케이션을 배포하고 있습니다. 회사는
워크로드에 Amazon EC2 인스턴스를 사용할 계획입니다. 노드 그룹이 동일한 기본
하드웨어를 공유하지 못하도록 네트워크 아키텍처를 구성할 수 있어야 합니다.
98

IT Certification Guaranteed, The Easy Way!
이러한 요구 사항을 충족하는 네트워킹 솔루션은 무엇입니까?
A. 분산 배치 그룹에서 EC2 인스턴스를 실행합니다.
B. EC2 인스턴스를 별도의 계정으로 그룹화합니다.
C. 전용 테넌시로 EC2 인스턴스를 구성합니다.
D. 공유 테넌시로 EC2 인스턴스를 구성합니다.
Answer: A
Explanation:
it allows the company to deploy an application that processes large quantities of data in
parallel and prevent groups of nodes from sharing the same underlying hardware. By running
the EC2 instances in a spread placement group, the company can launch a small number of
instances across distinct underlying hardware to reduce correlated failures. A spread
placement group ensures that each instance is isolated from each other at the rack level.
References:
* Placement Groups
* Spread Placement Groups
QUESTION NO: 145
회사는 애플리케이션을 생성 중입니다. 회사는 여러 온프레미스 위치에 있는 애플리케이션
테스트의 데이터를 저장합니다. 회사는 온프레미스 위치를 AWS 클라우드의 AWS 지역에
있는 VPC에 연결해야 합니다. 내년 네트워크 아키텍처는 새로운 연결 관리를 단순화하고
확장 기능을 제공해야 합니다.
최소한의 관리 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. VPC 간에 피어링 연결을 생성합니다. VPC와 온프레미스 위치 간에 VPN 연결을
생성합니다.
B. Amazon EC2 인스턴스를 시작합니다. 인스턴스에 VPN 연결을 사용하여 모든 VPC와
온프레미스 위치를 연결하는 VPN 소프트웨어를 포함합니다.
C. 전송 게이트웨이 생성 VPC 연결을 위한 VPC 연결 생성 온프레미스 연결을 위한 VPN
연결을 생성합니다.
D. 온프레미스 위치와 중앙 VPC 간에 AWS Direct Connect 연결을 생성합니다. 피어링 연결을
사용하여 중앙 VPC를 다른 VPC에 연결합니다.
Answer: C
Explanation:
A transit gateway is a network transit hub that enables you to connect your VPCs and on-
premises networks in a centralized and scalable way. You can create VPC attachments to
connect your VPCs to the transit gateway, and VPN attachments to connect your on-
premises networks to the transit gateway over the internet.
The transit gateway acts as a router between the attached networks, and simplifies the
administration of new connections by reducing the number of peering or VPN connections
required. You can also use transit gateway route tables to control the routing of traffic
between the attached networks. By creating a transit gateway and using VPC and VPN
attachments, you can meet the requirements of the company with the least administrative
overhead.
References:
* AWS Transit Gateway
99

IT Certification Guaranteed, The Easy Way!
* Transit gateway attachments
* Transit gateway route tables
QUESTION NO: 146
빠르게 성장하는 글로벌 전자 상거래 회사가 AWS에서 웹 애플리케이션을 호스팅하고
있습니다. 웹 애플리케이션에는 정적 콘텐츠와 동적 콘텐츠가 포함됩니다. 웹사이트는
Amazon RDS 데이터베이스에 온라인 트랜잭션 처리(OLTP) 데이터를 저장합니다. 웹사이트
사용자의 페이지 로드 속도가 느려지고 있습니다.
이 문제를 해결하기 위해 솔루션 설계자는 어떤 조치 조합을 취해야 합니까? (2개를
선택하세요.)
A. Amazon Redshift 클러스터를 구성합니다.
B. Amazon CloudFront 배포 설정
C. Amazon S3에서 동적 웹 콘텐츠 호스팅
D. RDS DB 인스턴스에 대한 wd 복제본을 생성합니다.
E. RDS DB 인스턴스에 대한 다중 AZ 배포 구성
Answer: B D
Explanation:
To resolve the issue of slow page loads for a rapidly growing e-commerce website hosted on
AWS, a solutions architect can take the following two actions:
1. Set up an Amazon CloudFront distribution
2. Create a read replica for the RDS DB instance
Configuring an Amazon Redshift cluster is not relevant to this issue since Redshift is a data
warehousing service and is typically used for the analytical processing of large amounts of
data.
Hosting the dynamic web content in Amazon S3 may not necessarily improve performance
since S3 is an object storage service, not a web application server. While S3 can be used to
host static web content, it may not be suitable for hosting dynamic web content since S3
doesn't support server-side scripting or processing.
Configuring a Multi-AZ deployment for the RDS DB instance will improve high availability but
may not necessarily improve performance.
QUESTION NO: 147
한 회사에 Amazon RDS DB 인스턴스에서 대부분의 메타데이터를 읽는 모바일 게임이
있습니다. 게임의 인기가 높아짐에 따라 개발자들은 게임의 메타데이터 로드 시간과 관련된
속도 저하를 발견했습니다. 성능 지표에 따르면 단순히 데이터베이스를 확장하는 것은 도움이
되지 않습니다. 솔루션 설계자는 스냅샷, 복제 및 밀리초 미만의 응답 시간 기능을 포함하는
모든 옵션을 탐색해야 합니다. 솔루션 설계자가 이러한 문제를 해결하도록 권장해야 합니까?
A. Aurora 복제본을 사용하여 데이터베이스를 Amazon Aurora로 마이그레이션합니다.
B. 글로벌 테이블을 사용하여 데이터베이스를 Amazon DynamoDB로 마이그레이션합니다.
C. 데이터베이스 앞에 Redis용 Amazon ElastiCache 계층을 추가합니다.
D. 데이터베이스 앞에 Memcached용 Amazon ElastiCache 계층을 추가합니다.
Answer: C
Explanation:
This option is the most suitable way to improve the game's metadata load times without
100

IT Certification Guaranteed, The Easy Way!
migrating the database. Amazon ElastiCache for Redis is a fully managed, in-memory data
store that provides sub- millisecond latency and high throughput for read-intensive workloads.
You can use it as a caching layer in front of your RDS DB instance to store frequently
accessed metadata and reduce the load on the database.
You can also take advantage of Redis features such as snapshots, replication, and data
persistence to ensure data durability and availability. ElastiCache for Redis scales
automatically to meet your demand and integrates with other AWS services such as
CloudFormation, CloudWatch, and IAM.
Option A is not optimal because migrating the database to Amazon Aurora with Aurora
Replicas would incur additional costs and complexity. Amazon Aurora is a relational
database service that provides high performance, availability, and compatibility with MySQL
and PostgreSQL. Aurora Replicas are read-only copies of the primary database that can be
used for scaling read capacity and enhancing availability. However, migrating the database to
Aurora would require modifying the application code, testing the compatibility, and performing
the data migration. Moreover, Aurora Replicas may not provide sub-millisecond response
times as ElastiCache for Redis does.
Option B is not optimal because migrating the database to Amazon DynamoDB with global
tables would incur additional costs and complexity. Amazon DynamoDB is a NoSQL
database service that provides fast and flexible data access for any scale. Global tables are a
feature of DynamoDB that enables you to replicate your data across multiple AWS Regions
for high availability and performance. However, migrating the database to DynamoDB would
require changing the data model, modifying the application code, and performing the data
migration. Moreover, global tables may not be necessary for the game's metadata, as they
are mainly used for cross-region data access and disaster recovery.
Option D is not optimal because adding an Amazon ElastiCache for Memcached layer in
front of the database would not provide the same capabilities as ElastiCache for Redis.
Amazon ElastiCache for Memcached is another fully managed, in-memory data store that
provides high performance and scalability for caching workloads. However, Memcached does
not support snapshots, replication, or data persistence, which means that the cached data
may be lost in case of a node failure or a cache eviction. Moreover, Memcached does not
integrate with other AWS services as well as Redis does. Therefore, ElastiCache for Redis is
a better choice for this scenario. References:
* What Is Amazon ElastiCache for Redis?
* What Is Amazon Aurora?
* What Is Amazon DynamoDB?
* What Is Amazon ElastiCache for Memcached?
QUESTION NO: 148
회사는 MySQL 데이터베이스로 구동되는 온프레미스 애플리케이션을 실행합니다. 회사는
애플리케이션의 탄력성과 가용성을 높이기 위해 애플리케이션을 AWS로 마이그레이션하고
있습니다. 현재 아키텍처는 정상 작동 시간 동안 데이터베이스에서 많은 읽기 활동을
보여줍니다. 회사의 4시간마다 개발 팀은 프로덕션 데이터베이스의 전체 내보내기를 가져와
준비 환경에서 데이터베이스를 채웁니다.사용자가 허용할 수 없는 애플리케이션 대기 시간을
경험합니다. 개발 팀은 절차가 완료될 때까지 스테이징 환경을 사용할 수 없습니다. 솔루션
설계자는 애플리케이션 대기 시간 문제를 완화하는 대체 아키텍처를 권장해야 합니다. 대체
아키텍처는 또한 개발 팀이 스테이징 환경을 계속 사용할 수 있는 능력을 개발 팀에 제공해야
101

IT Certification Guaranteed, The Easy Way!
합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 프로덕션용 다중 AZ Aurora 복제본과 함께 Amazon Aurora MySQL을 사용합니다.
mysqldump 유틸리티를 사용하는 백업 및 복원 프로세스를 구현하여 스테이징
데이터베이스를 채웁니다.
B. 프로덕션용 다중 AZ Aurora 복제본과 함께 Amazon Aurora MySQL 사용 데이터베이스
복제를 사용하여 온디맨드 스테이징 데이터베이스 생성
C. Mufti AZ 배포와 함께 MySQL용 Amazon RDS 사용 및 프로덕션용 읽기 전용 복제본
스테이징 데이터베이스에 대기 인스턴스를 사용합니다.
D. 다중 AZ 배포 및 프로덕션용 읽기 전용 복제본과 함께 MySQL용 Amazon RDS를
사용합니다. mysqldump 유틸리티를 사용하는 백업 및 복원 프로세스를 구현하여 스테이징
데이터베이스를 채웁니다.
Answer: B
Explanation:
https://aws.amazon.com/blogs/aws/amazon-aurora-fast-database-cloning/
QUESTION NO: 149
회사에 Amazon DynamoDB 테이블을 기반으로 하는 애플리케이션이 있습니다. 회사의 규정
준수 요구 사항에는 데이터베이스 백업을 매달 수행하고 6개월 동안 사용할 수 있어야 하며
7년 동안 보관해야 한다고 명시되어 있습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 매월 1일에 DynamoDB 테이블을 백업하는 AWS Backup 계획을 생성합니다. 6개월 후에
백업을 콜드 스토리지로 전환하는 수명 주기 정책을 지정합니다. 각 백업의 보존 기간을
7년으로 설정합니다.
B. 매월 1일에 DynamoDB 테이블의 DynamoDB 온디맨드 백업을 생성합니다. 6개월 후에
백업을 Amazon S3 Glacier 유연한 검색으로 전환합니다. 7년이 넘은 백업을 삭제하는 S3
수명 주기 정책을 생성합니다.
C. AWS SDK를 사용하여 DynamoDB 테이블의 온디맨드 백업을 생성하는 스크립트를
개발합니다. 매월 1일에 스크립트를 실행하는 Amzon EvenlBridge 규칙을 설정합니다. 매월
둘째 날에 실행되어 6개월이 지난 DynamoDB 백업을 콜드 스토리지로 전환하고 7년이 지난
백업을 삭제하는 두 번째 스크립트를 생성합니다.
D. AWS CLI를 사용하여 DynamoDB 테이블의 온디맨드 백업 생성 cron 표현식을 사용하여
매월 1일에 명령을 실행하는 Amazon EventBridge 규칙 설정 명령에 지정하여 백업을 콜드
모드로 전환 6개월 후에 보관하고 7년 후에 백업을 삭제합니다.
Answer: A
Explanation:
This solution satisfies the requirements in the following ways:
* AWS Backup will automatically take full backups of the DynamoDB table on the schedule
defined in the backup plan (the first of each month).
* The lifecycle policy can transition backups to cold storage after 6 months, meeting that
requirement.
* Setting a 7-year retention period in the backup plan will ensure each backup is retained for
7 years as required.
* AWS Backup manages the backup jobs and lifecycle policies, requiring no custom scripting
or management.
102

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 150
회사는 PostgreSQL 데이터베이스를 포함하는 3계층 웹 애플리케이션을 호스팅합니다.
데이터베이스는 문서의 메타데이터를 저장합니다. 회사는 메타데이터에서 주요 용어를
검색하여 회사가 매달 보고서에서 검토하는 문서를 검색합니다. 문서는 Amazon S3에
저장됩니다. 문서 일반적으로 한 번만 작성되지만 자주 업데이트됩니다. 보고 프로세스는
관계형 쿼리를 사용하여 몇 시간이 걸립니다. 보고 프로세스는 문서 수정이나 새 문서 추가에
영향을 주어서는 안 됩니다.
이러한 요구 사항을 충족하는 가장 운영 효율적인 솔루션은 무엇입니까? (2개 선택)
A. 읽기 전용 복제본을 포함하는 새로운 Amazon DocumentDB(MongoDB 호환) 클러스터를
설정합니다. 읽기 전용 복제본을 확장하여 보고서를 생성합니다.
B. PostgreSQL 예약 인스턴스 및 온디맨드 읽기 전용 복제본을 위한 새로운 Amazon RDS
설정 읽기 전용 복제본을 확장하여 보고서 생성
C. 예약 인스턴스가 포함된 새로운 Amazon Aurora PostgreSQL DB 클러스터를 설정하고
Aurora 복제본이 Aurora 복제본에 대한 쿼리를 발행하여 보고서를 생성합니다.
D. PostgreSQL 다중 AZ 예약 인스턴스용 새 Amazon RDS 설정 보고 모듈이 기본 노드에
영향을 미치지 않도록 보조 RDS 노드를 쿼리하도록 보고 모듈을 구성합니다.
E. 문서를 저장할 새 Amazon DynamoDB 테이블 설정 고정된 쓰기 용량을 사용하여 새 문서
항목 지원 보고서 지원을 위해 읽기 용량 자동 확장
Answer: B C
Explanation:
These options are operationally efficient because they use Amazon RDS read replicas to
offload the reporting workload from the primary DB instance and avoid affecting any
document modifications or the addition of new documents1. They also use Reserved
Instances for the primary DB instance to reduce costs and On- Demand or Aurora Replicas
for the read replicas to scale as needed. Option A is less efficient because it uses Amazon
S3 Glacier Flexible Retrieval, which is a cold storage class that has higher retrieval costs and
longer retrieval times than Amazon S3 Standard. It also uses EventBridge rules to invoke the
job nightly, which does not meet the requirement of processing incoming data files as soon
as possible. Option D is less efficient because it uses AWS Lambda to process the files,
which has a maximum execution time of 15 minutes per invocation, which might not be
enough for processing each file that needs 3-8 minutes. It also uses S3 event notifications to
invoke the Lambda function when the files arrive, which could cause concurrency issues if
there are thousands of small data files arriving periodically. Option E is less efficient because
it uses Amazon DynamoDB, which is a NoSQL database service that does not support
relational queries, which are needed for generating the reports. It also uses fixed write
capacity, which could cause throttling or underutilization depending on the incoming data
files.
QUESTION NO: 151
한 회사가 AWS에 새로운 퍼블릭 웹 애플리케이션을 배포하고 있습니다. 애플리케이션은
애플리케이션 로드 밸런서(ALE) 뒤에서 실행됩니다. 애플리케이션은 외부 인증 기관(CA)에서
발급한 SSL/TLS 인증서로 에지에서 암호화되어야 합니다. 인증서는 만료되기 전에 매년
순환해야 합니다.
이러한 요구 사항을 충족하기 위해 솔루션 아키텍트는 무엇을 해야 할까요?
A. AWS Certificate Manager(ACM)를 사용하여 SSUTLS 인증서를 발급합니다. 인증서를
103

IT Certification Guaranteed, The Easy Way!
ALB에 적용합니다. 관리형 갱신 기능을 사용하여 인증서를 자동으로 회전합니다.
B. AWS Certificate Manager(ACM)를 사용하여 SSUTLS 인증서를 발급합니다. 인증서에서 키
자료를 가져옵니다. 인증서를 ALB에 적용합니다. 관리되는 갱신 조항을 사용하여 인증서를
자동으로 회전합니다.
C. AWS Private Certificate Authority를 ​​사용하여 루트 CA에서 SSL/TLS 인증서를 발급합니다.
인증서를 ALB에 적용합니다. 관리형 갱신 기능을 사용하여 인증서를 자동으로 회전합니다.
D. AWS Certificate Manager(ACM)를 사용하여 SSL/TLS 인증서를 가져옵니다. 인증서를
ALB_에 적용합니다. Amazon EventBridge를 사용하여 인증서가 만료에 가까워지면 알림을
보냅니다. 인증서를 수동으로 회전합니다.
Answer: D
Explanation:
To use an SSL/TLS certificate that is issued by an external CA, the certificate must be
imported to AWS Certificate Manager (ACM). ACM can send a notification when the
certificate is nearing expiration, but it cannot automatically rotate the certificate. Therefore,
the certificate must be rotated manually by importing a new certificate and applying it to the
ALB.
References:
* Importing Certificates into AWS Certificate Manager
* Renewing and Rotating Imported Certificates
* Using an ACM Certificate with an Application Load Balancer
QUESTION NO: 152
회사의 컨테이너화된 애플리케이션은 Amazon EC2 인스턴스에서 실행됩니다.
애플리케이션이 다른 비즈니스 애플리케이션과 통신하려면 먼저 보안 인증서를 다운로드해야
합니다. 회사는 인증서를 거의 실시간으로 암호화하고 해독할 수 있는 매우 안전한 솔루션을
원합니다. 또한 솔루션은 데이터가 암호화된 후 가용성이 높은 스토리지에 데이터를 저장해야
합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 암호화된 인증서에 대한 AWS Secrets Manager 비밀을 생성합니다. 필요에 따라 인증서를
수동으로 업데이트합니다. 세분화된 IAM 액세스를 사용하여 데이터에 대한 액세스를
제어합니다.
B. Python 암호화 라이브러리를 사용하여 암호화 작업을 수신하고 수행하는 AWS Lambda
함수를 생성합니다. Amazon S3 버킷에 함수를 저장합니다.
C. AWS Key Management Service(AWS KMS) 고객 관리형 키를 생성합니다. EC2 역할이
암호화 작업에 KMS 키를 사용하도록 허용합니다. Amazon S3에 암호화된 데이터를
저장합니다.
D. AWS Key Management Service(AWS KMS) 고객 관리형 키를 생성합니다. EC2 역할이
암호화 작업에 KMS 키를 사용하도록 허용합니다. Amazon Elastic Block Store(Amazon EBS)
볼륨에 암호화된 데이터를 저장합니다.
Answer: D
QUESTION NO: 153
회사는 수백 개의 AWS 계정에 걸쳐 있는 us-east-1 리전의 여러 VPC를 연결해야 합니다.
회사의 네트워킹 팀에는 클라우드 네트워크를 관리하기 위한 자체 AWS 계정이 있습니다.
VPC를 연결하기 위한 운영상 가장 효율적인 솔루션은 무엇입니까?
104

IT Certification Guaranteed, The Easy Way!
A. 각 VPC 간에 VPC 피어링 연결을 설정합니다. 연결된 각 서브넷의 라우팅 테이블을
업데이트합니다.
B. 인터넷을 통해 각 VPC를 연결하기 위해 각 VPC에 NAT 게이트웨이와 인터넷
게이트웨이를 구성합니다.
C. 네트워킹 팀의 AWS 계정에서 AWS Transit Gateway를 생성합니다. 각 VPC에서 정적
경로를 구성합니다.
D. 각 VPC에 VPN 게이트웨이를 배포합니다. 네트워킹 팀의 AWS 계정에 전송 VPC를
생성하여 각 VPC에 연결합니다.
Answer: C
Explanation:
AWS Transit Gateway is a highly scalable and centralized hub for connecting multiple VPCs,
on-premises networks, and remote networks. It simplifies network connectivity by providing a
single entry point and reducing the number of connections required. In this scenario,
deploying an AWS Transit Gateway in the networking team's AWS account allows for
efficient management and control over the network connectivity across multiple VPCs.
QUESTION NO: 154
솔루션 설계자는 회사를 위한 다중 계층 애플리케이션을 설계하고 있습니다. 애플리케이션
사용자는 모바일 장치에서 이미지를 업로드합니다. 애플리케이션은 각 이미지의 썸네일을
생성하고 이미지가 성공적으로 업로드되었음을 확인하는 메시지를 사용자에게 반환합니다.
썸네일 생성에는 최대 60초가 소요될 수 있지만, 회사에서는 원본 이미지가 수신되었음을
사용자에게 알리기 위해 더 빠른 응답 시간을 제공하고자 합니다. 솔루션 설계자는 요청을
다른 애플리케이션 계층에 비동기적으로 디스패치하도록 애플리케이션을 설계해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 사용자 지정 AWS Lambda 함수를 작성하여 썸네일을 생성하고 사용자에게 경고합니다.
이미지 업로드 프로세스를 이벤트 소스로 사용하여 Lambda 함수를 호출합니다.
B. AWS Step Functions 워크플로 생성 애플리케이션 계층 간의 오케스트레이션을 처리하고
썸네일 생성이 완료되면 사용자에게 알리도록 Step Functions를 구성합니다.
C. Amazon Simple Queue Service(Amazon SQS) 메시지 대기열을 생성합니다. 이미지가
업로드되면 썸네일 생성을 위해 SQS 대기열에 메시지를 배치하세요. 애플리케이션 메시지를
통해 이미지가 수신되었음을 사용자에게 알립니다.
D. Amazon Simple 알림 서비스(Amazon SNS) 알림 주제 및 구독 생성 이미지 업로드가
완료된 후 애플리케이션과 함께 하나의 구독을 사용하여 썸네일을 생성합니다. 두 번째
구독을 사용하여 썸네일 생성이 완료된 후 푸시 알림을 통해 사용자의 모바일 앱에 메시지를
보냅니다.
Answer: C
Explanation:
This option is the most efficient because it uses Amazon SQS, which is a fully managed
message queuing service that lets you send, store, and receive messages between software
components at any volume, without losing messages or requiring other services to be
available1. It also uses an SQS message queue to asynchronously dispatch requests to the
different application tiers, which decouples the image upload process from the thumbnail
generation process and enables scalability and reliability. It also alerts the user through an
application message that the image was received, which provides a faster response time to
105

IT Certification Guaranteed, The Easy Way!
the user than waiting for the thumbnail generation to complete. Option A is less efficient
because it uses a custom AWS Lambda function to generate the thumbnail and alert the
user, which is a way to run code without provisioning or managing servers. However, this
does not use an asynchronous dispatch mechanism to separate the image upload process
from the thumbnail generation process. It also uses the image upload process as an event
source to invoke the Lambda function, which could cause concurrency issues if there are
many images uploaded at once. Option B is less efficient because it uses AWS Step
Functions, which is a fully managed service that provides a graphical console to arrange and
visualize the components of your application as a series of steps2. However, this does not
use an asynchronous dispatch mechanism to separate the image upload process from the
thumbnail generation process. It also uses Step Functions to handle the orchestration
between the application tiers and alert the user when thumbnail generation is complete,
which could introduce additional complexity and latency. Option D is less efficient because it
uses Amazon SNS, which is a fully managed messaging service that enables you to send
messages or notifications directly to users with SMS text messages or email3. However, this
does not use an asynchronous dispatch mechanism to separate the image upload process
from the thumbnail generation process. It also uses SNS notification topics and subscriptions
to generate the thumbnail after the image upload is complete and message the user's mobile
app by way of a push notification after thumbnail generation is complete, which could
introduce additional complexity and latency.
QUESTION NO: 155
회사가 Amazon EC2 인스턴스에서 분석 소프트웨어를 실행합니다. 소프트웨어가 Amazon
S3에 업로드된 데이터를 처리하기 위해 사용자의 작업 요청을 수락합니다. 사용자가 제출된
일부 데이터가 처리되지 않는다고 보고합니다. Amazon CloudWatch에서 EC2 인스턴스의
CPU 사용률이 일관되게 유지되는 것으로 나타났습니다. 가까운
100% 회사는 시스템 성능을 개선하고 사용자 로드에 따라 시스템을 확장하기를 원합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 인스턴스 복사본을 생성합니다. 모든 인스턴스를 Application Load Balancer 뒤에
배치합니다.
B. Amazon S3용 S3 VPC 엔드포인트 생성 엔드포인트를 참조하도록 소프트웨어 업데이트
C. EC2 인스턴스를 중지합니다. 더 강력한 CPU와 더 많은 메모리를 갖춘 인스턴스 유형으로
수정합니다.
인스턴스를 다시 시작합니다.
D. 수신 요청을 Amazon Simple Queue Service(Amazon SQS)로 라우팅 대기열 크기에 따라
EC2 Auto Scaling 그룹 구성 대기열에서 읽도록 소프트웨어를 업데이트합니다.
Answer: D
Explanation:
This option is the best solution because it allows the company to decouple the analytics
software from the user requests and scale the EC2 instances dynamically based on the
demand. By using Amazon SQS, the company can create a queue that stores the user
requests and acts as a buffer between the users and the analytics software. This way, the
software can process the requests at its own pace without losing any data or overloading the
EC2 instances. By using EC2 Auto Scaling, the company can create an Auto Scaling group
that launches or terminates EC2 instances automatically based on the size of the queue. This
106

IT Certification Guaranteed, The Easy Way!
way, the company can ensure that there are enough instances to handle the load and
optimize the cost and performance of the system. By updating the software to read from the
queue, the company can enable the analytics software to consume the requests from the
queue and process the data from Amazon S3.
A: Create a copy of the instance Place all instances behind an Application Load Balancer.
This option is not optimal because it does not address the root cause of the problem, which is
the high CPU utilization of the EC2 instances. An Application Load Balancer can distribute
the incoming traffic across multiple instances, but it cannot scale the instances based on the
load or reduce the processing time of the analytics software.
Moreover, this option can incur additional costs for the load balancer and the extra instances.
B: Create an S3 VPC endpoint for Amazon S3 Update the software to reference the
endpoint. This option is not effective because it does not solve the issue of the high CPU
utilization of the EC2 instances. An S3 VPC endpoint can enable the EC2 instances to
access Amazon S3 without going through the internet, which can improve the network
performance and security. However, it cannot reduce the processing time of the analytics
software or scale the instances based on the load.
C: Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and
more memory.
Restart the instances. This option is not scalable because it does not account for the
variability of the user load. Changing the instance type to a more powerful one can improve
the performance of the analytics software, but it cannot adjust the number of instances based
on the demand. Moreover, this option can increase the cost of the system and cause
downtime during the instance modification.
References:
* 1 Using Amazon SQS queues with Amazon EC2 Auto Scaling - Amazon EC2 Auto Scaling
* 2 Tutorial: Set up a scaled and load-balanced application - Amazon EC2 Auto Scaling
* 3 Amazon EC2 Auto Scaling FAQs
QUESTION NO: 156
한 회사에는 다양한 모니터링 장치로부터 실시간 데이터를 수신하는 API가 있습니다. API는
나중에 분석하기 위해 이 데이터를 Amazon RDS DB 인스턴스에 저장합니다. 모니터링
장치가 API로 보내는 데이터의 양은 변동됩니다. 트래픽이 많은 기간에는 API가 시간 초과
오류를 반환하는 경우가 많습니다.
회사에서는 로그를 조사한 후 데이터베이스가 API에서 발생하는 쓰기 트래픽의 양을 처리할
수 없는 것으로 판단합니다. 솔루션 설계자는 데이터베이스에 대한 연결 수를 최소화해야
하며 트래픽이 많은 기간 동안 데이터가 손실되지 않도록 해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 사용 가능한 메모리가 더 많은 인스턴스 유형으로 DB 인스턴스의 크기를 늘립니다.
B. DB 인스턴스를 다중 AZ DB 인스턴스로 수정합니다. 모든 활성 RDS DB 인스턴스에
쓰도록 애플리케이션을 구성합니다.
C. 수신 데이터를 Amazon Simple Queue Service(Amazon SQS) 대기열에 쓰도록 API를
수정합니다. Amazon SQS가 호출하는 AWS Lambda 함수를 사용하여 대기열의 데이터를
데이터베이스에 씁니다.
D. 수신 데이터를 Amazon Simple Notification Service(Amazon SNS) 주제에 쓰도록 API를
수정합니다. 주제의 데이터를 데이터베이스에 쓰기 위해 Amazon SNS가 호출하는 AWS
107

IT Certification Guaranteed, The Easy Way!
Lambda 함수를 사용합니다.
Answer: C
Explanation:
Using Amazon SQS will help minimize the number of connections to the database, as the
API will write data to a queue instead of directly to the database. Additionally, using an AWS
Lambda function that Amazon SQS invokes to write data from the queue to the database will
help ensure that data is not lost during periods of heavy traffic, as the queue will serve as a
buffer between the API and the database.
QUESTION NO: 157
회사는 온프레미스 위치에서 AWS 계정으로 AWS Direct Connect 연결을 가지고 있습니다.
AWS 계정에는 동일한 AWS 지역에 30개의 서로 다른 VPC가 있습니다. VPC는 ​​프라이빗
가상 인터페이스(VIF)를 사용합니다. 각 VPC에는 다음과 겹치지 않는 CIDR 블록이 있습니다.
회사가 제어하는 ​​다른 네트워크 회사는 각 VPC가 다른 모든 VPC 및 온프레미스 네트워크와
통신할 수 있도록 허용하면서 네트워킹 아키텍처를 중앙에서 관리하기를 원합니다. 어떤
솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을 충족합니까?
A. 전송 게이트웨이를 생성하고 Direct Connect 연결을 새 전송 VIF와 연결합니다. 전송
게이트웨이의 경로 전파 기능을 켭니다.
B. Direct Connect 게이트웨이 생성 새 게이트웨이를 사용하려면 프라이빗 VIF를 다시
생성합니다. 새 가상 프라이빗 게이트웨이를 생성하여 각 VPC를 연결합니다.
C. 전송 VPC 생성 전송 VPC에 Direct Connect 연결 연결 리전의 다른 모든 VPC 간에 피닝
연결 생성 라우팅 테이블 업데이트
D. 온프레미스에서 각 VPC로 AWS Site-to-Site VPN 연결을 생성합니다. 각 연결에 대해 두
VPN 터널이 모두 작동 중인지 확인합니다. 경로 전파 기능을 켭니다.
Answer: A
Explanation:
This solution meets the following requirements:
* It is operationally efficient, as it only requires one transit gateway and one transit VIF to
connect the Direct Connect connection to all the VPCs in the same AWS Region. The transit
gateway acts as a regional network hub that simplifies the network management and reduces
the number of VIFs and gateways needed.
* It is scalable, as it can support up to 5000 attachments per transit gateway, which can
include VPCs, VPNs, Direct Connect gateways, and peering connections. The transit
gateway can also be connected to other transit gateways in different Regions or accounts
using peering connections, enabling cross- Region and cross-account connectivity.
* It is flexible, as it allows each VPC to communicate with all other VPCs and on-premises
networks using dynamic routing protocols such as Border Gateway Protocol (BGP). The
transit gateway's route propagation feature automatically propagates the routes from the
attached VPCs and VPNs to the transit gateway route table, eliminating the need to manually
update the route tables.
References:
* Transit Gateways - Amazon Virtual Private Cloud
* Working with transit gateways - AWS Direct Connect
* Amazon VPC-to-Amazon VPC connectivity options - Amazon Virtual Private Cloud
Connectivity Options
108

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 158
한 회사가 여러 가용 영역의 Amazon EC2 인스턴스에서 웹 애플리케이션을 실행합니다. EC2
인스턴스는 프라이빗 서브넷에 있습니다. 솔루션 아키텍트는 인터넷 연결 ALB(Application
Load Balancer)를 구현하고 EC2 인스턴스를 대상 그룹으로 지정합니다. 그러나 인터넷
트래픽이 EC2 인스턴스에 도달하지 않습니다.
이 문제를 해결하려면 솔루션 설계자가 아키텍처를 어떻게 재구성해야 합니까?
A. ALB를 Network Load Balancer로 교체합니다. 인터넷 트래픽을 허용하도록 퍼블릭
서브넷에서 NAT 게이트웨이를 구성합니다.
B. EC2 인스턴스를 퍼블릭 서브넷으로 이동합니다. 0.0.0.0/0으로의 아웃바운드 트래픽을
허용하도록 EC2 인스턴스의 보안 그룹에 규칙을 추가합니다.
C. 인터넷 게이트웨이 경로를 통해 0.0.0.0/0 트래픽을 보내도록 EC2 인스턴스의 서브넷에
대한 라우팅 테이블을 업데이트합니다. 0.0.0.0/0으로의 아웃바운드 트래픽을 허용하도록
EC2 인스턴스의 보안 그룹에 규칙을 추가합니다.
D. 각 가용 영역에 퍼블릭 서브넷을 생성합니다. 퍼블릭 서브넷을 ALB와 연결합니다.
프라이빗 서브넷에 대한 경로로 퍼블릭 서브넷의 라우팅 테이블을 업데이트합니다.
Answer: D
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2
/
QUESTION NO: 159
회사의 데이터 플랫폼은 Amazon Aurora MySQL 데이터베이스를 사용합니다.
데이터베이스에는 여러 가용 영역에 걸쳐 여러 읽기 전용 복제본과 여러 DB 인스턴스가
있습니다. 사용자는 최근 데이터베이스에서 너무 많은 연결이 있음을 나타내는 오류를
보고했습니다. 회사는 장애 조치 시간을 다음과 같이 줄이려고 합니다.
읽기 전용 복제본이 기본 작성자로 승격되면 20%입니다.
이 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 다중 AZ 클러스터 배포를 통해 Aurora에서 Amazon RDS로 전환합니다.
B. Aurora 데이터베이스 앞에서 Amazon RDS Proxy를 사용합니다.
C. 읽기 연결을 위해 DAX(DynamoDB Accelerator)가 있는 Amazon DynamoDB로
전환합니다.
D. 재배치 기능이 있는 Amazon Redshift로 전환합니다.
Answer: B
Explanation:
Amazon RDS Proxy is a service that provides a fully managed, highly available database
proxy for Amazon RDS and Aurora databases. It allows you to pool and share database
connections, reduce database load, and improve application scalability and availability.
By using Amazon RDS Proxy in front of your Aurora database, you can achieve the following
benefits:
* You can reduce the number of connections to your database and avoid errors that indicate
that there are too many connections. Amazon RDS Proxy handles the connection
management and multiplexing for you, so you can use fewer database connections and
resources.
* You can reduce the failover time by 20% when a read replica is promoted to primary writer.
109

IT Certification Guaranteed, The Easy Way!
Amazon RDS Proxy automatically detects failures and routes traffic to the new primary
instance without requiring changes to your application code or configuration. According to a
benchmark test, using Amazon RDS Proxy reduced the failover time from 66 seconds to 53
seconds, which is a 20% improvement.
* You can improve the security and compliance of your database access. Amazon RDS
Proxy integrates with AWS Secrets Manager and AWS Identity and Access Management
(IAM) to enable secure and granular authentication and authorization for your database
connections.
QUESTION NO: 160
회사는 여러 Amazon EC2 인스턴스에서 웹 애플리케이션을 호스팅합니다. EC2 인스턴스는
사용자 요구에 따라 확장되는 Auto Scaling 그룹에 있습니다. 회사는 장기적인 약정 없이 비용
절감을 최적화하려고 합니다. 솔루션 설계자가 선택해야 하는 EC2 인스턴스 구매 옵션은
무엇입니까? 이러한 요구 사항을 충족하는 것이 좋습니다'?
A. 전용 인스턴스만
B. 온디맨드 인스턴스만 해당
C. 온디맨드 인스턴스와 스팟 인스턴스의 혼합
D. 온디맨드 인스턴스와 예약 인스턴스의 혼합
Answer: C
Explanation:
https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-
groups.html
QUESTION NO: 161
솔루션 설계자는 스토리지 비용을 최적화해야 합니다. 솔루션 설계자는 더 이상 액세스되지
않거나 거의 액세스되지 않는 Amazon S3 버킷을 식별해야 합니다.
최소한의 운영 오버헤드로 이 목표를 달성할 수 있는 솔루션은 무엇입니까?
A. 고급 활동 지표를 위해 S3 Storage Lens 대시보드를 사용하여 버킷 액세스 패턴을
분석합니다.
B. AWS Management Console의 S3 대시보드를 사용하여 버킷 액세스 패턴을 분석합니다.
C. 버킷에 대한 Amazon CloudWatch BucketSizeBytes 지표를 활성화합니다. Amazon
Athena에서 지표 데이터를 사용하여 버킷 액세스 패턴을 분석합니다.
D. S3 객체 모니터링을 위해 AWS CloudTrail을 켭니다. Amazon CloudWatch Logs와 통합된
CloudTrail 로그를 사용하여 버킷 액세스 패턴을 분석합니다.
Answer: A
Explanation:
S3 Storage Lens is a fully managed S3 storage analytics solution that provides a
comprehensive view of object storage usage, activity trends, and recommendations to
optimize costs. Storage Lens allows you to analyze object access patterns across all of your
S3 buckets and generate detailed metrics and reports.
QUESTION NO: 162
회사는 웨어러블 기기를 사용하는 다수의 참여자로부터 데이터를 수집합니다. 회사는
데이터를 Amazon DynamoDB 테이블에 저장하고 애플리케이션을 사용하여 데이터를
분석합니다. 데이터 워크로드는 일정하고 예측 가능합니다. 회사는 DynamoDB에 대해 예상
110

IT Certification Guaranteed, The Easy Way!
예산 이하로 유지하기를 원합니다.
어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?
A. 프로비저닝 모드와 DynamoDB Standard-Infrequent Access(DynamoDB Standard-IA)를
사용합니다. 예상 워크로드에 대한 용량을 예약합니다.
B. 프로비저닝 모드 사용 RCU(읽기 용량 단위) 및 WCU(쓰기 용량 단위)를 지정합니다.
C. 주문형 모드를 사용합니다. 워크로드의 변화를 수용할 수 있을 만큼 RCU(읽기 용량 단위)
및 WCU(쓰기 용량 단위)를 높게 설정합니다.
D. 주문형 모드를 사용합니다. 예약된 용량으로 RCU(읽기 용량 단위) 및 WCU(쓰기 용량
단위)를 지정합니다.
Answer: B
Explanation:
This option is the most efficient because it uses provisioned mode, which is a read/write
capacity mode for processing reads and writes on your tables that lets you specify how much
read and write throughput you expect your application to perform1. It also specifies the read
capacity units (RCUs) and write capacity units (WCUs), which are the amount of data your
application needs to read or write per second. It also meets the requirement of staying at or
below its forecasted budget for DynamoDB, as provisioned mode has lower costs than on-
demand mode for predictable workloads. This solution meets the requirement of collecting
data from a large number of participants who use wearable devices with a constant and
predictable data workload. Option A is less efficient because it uses provisioned mode and
DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA), which is a storage class
for infrequently accessed items that require milliseconds latency2. However, this does not
meet the requirement of collecting data from a large number of participants who use
wearable devices with a constant and predictable data workload, as DynamoDB Standard-IA
is more suitable for items that are accessed less frequently than once every 30 days. Option
C is less efficient because it uses on-demand mode, which is a read/write capacity mode that
lets you pay only for what you use by automatically adjusting your table's capacity in
response to changing demand3. However, this does not meet the requirement of staying at
or below its forecasted budget for DynamoDB, as on-demand mode has higher costs than
provisioned mode for predictable workloads. Option D is less efficient because it uses on-
demand mode and specifies the RCUs and WCUs with reserved capacity, which is a way to
reserve read and write capacity for your tables in exchange for discounted hourly rates.
However, this does not meet the requirement of staying at or below its forecasted budget for
DynamoDB, as on-demand mode has higher costs than provisioned mode for predictable
workloads. Also, specifying RCUs and WCUs with reserved capacity is not possible with on-
demand mode, as it only applies to provisioned mode.
QUESTION NO: 163
애플리케이션은 프라이빗 서브넷의 Amazon EC2 인스턴스에서 실행됩니다. 애플리케이션은
Amazon DynamoDB 테이블에 액세스해야 합니다. 트래픽이 AWS 네트워크를 벗어나지
않도록 하면서 테이블에 액세스하는 가장 안전한 방법은 무엇입니까?
A. DynamoDB용 VPC 엔드포인트를 사용합니다.
B. 퍼블릭 서브넷에서 NAT 게이트웨이를 사용합니다.
C. 프라이빗 서브넷에서 NAT 인스턴스를 사용합니다.
D. VPC에 연결된 인터넷 게이트웨이를 사용합니다.
111

IT Certification Guaranteed, The Easy Way!
Answer: A
Explanation:
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-
dynamodb.html A VPC endpoint for DynamoDB enables Amazon EC2 instances in your VPC
to use their private IP addresses to access DynamoDB with no exposure to the public
internet. Your EC2 instances do not require public IP addresses, and you don't need an
internet gateway, a NAT device, or a virtual private gateway in your VPC. You use endpoint
policies to control access to DynamoDB. Traffic between your VPC and the AWS service
does not leave the Amazon network.
QUESTION NO: 164
한 회사에는 AWS에서 실행되는 인기 있는 게임 플랫폼이 있습니다. 대기 시간은 사용자
경험에 영향을 미치고 일부 플레이어에게 불공정한 이점을 가져올 수 있으므로
애플리케이션은 대기 시간에 민감합니다. 애플리케이션은 모든 AWS 리전에 배포됩니다.
이는 Application Load Balancer(ALB) 뒤에 구성된 Auto Scaling 그룹의 일부인 Amazon EC2
인스턴스에서 실행됩니다. 솔루션 설계자는 애플리케이션 상태를 모니터링하고 트래픽을
정상 엔드포인트로 리디렉션하는 메커니즘을 구현해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Global Accelerator에서 액셀러레이터를 구성합니다. 애플리케이션이 수신하는
포트에 대한 리스너를 추가하고 이를 각 리전의 리전 엔드포인트에 연결합니다. ALB를
엔드포인트로 추가합니다.
B. Amazon CloudFront 배포를 생성하고 ALB를 오리진 서버로 지정합니다. 원본 캐시 헤더를
사용하도록 캐시 동작을 구성합니다. AWS Lambda 기능을 사용하여 트래픽을
최적화하십시오.
C. Amazon CloudFront 배포를 생성하고 Amazon S3를 오리진 서버로 지정합니다. 원본 캐시
헤더를 사용하도록 캐시 동작을 구성합니다. AWS Lambda 기능을 사용하여 트래픽을
최적화하십시오.
D. 애플리케이션의 데이터 저장소 역할을 하도록 Amazon DynamoDB 데이터베이스를
구성합니다. 애플리케이션 데이터를 호스팅하는 DynamoDB의 인 메모리 캐시 역할을 하는
DynamoDB Accelerator(DAX) 클러스터를 생성합니다.
Answer: A
Explanation:
AWS Global Accelerator directs traffic to the optimal healthy endpoint based on health
checks, it can also route traffic to the closest healthy endpoint based on geographic location
of the client. By configuring an accelerator and attaching it to a Regional endpoint in each
Region, and adding the ALB as the endpoint, the solution will redirect traffic to healthy
endpoints, improving the user experience by reducing latency and ensuring that the
application is running optimally. This solution will ensure that traffic is directed to the closest
healthy endpoint and will help to improve the overall user experience.
QUESTION NO: 165
한 회사가 Amazon API Gateway REST API와 AWS Lambda를 사용하여 API를
개발했습니다. 전 세계 사용자의 대기 시간을 어떻게 줄일 수 있을까요?
A. REST API를 엣지 최적화 API 엔드포인트로 배포합니다. 캐싱을 활성화합니다. 전송 중인
데이터를 압축하기 위해 콘텐츠 인코딩을 활성화합니다.
112

IT Certification Guaranteed, The Easy Way!
B. REST API를 지역 API 엔드포인트로 배포합니다. 캐싱을 활성화합니다. 전송 중인
데이터를 압축하기 위해 콘텐츠 인코딩을 활성화합니다.
C. REST API를 엣지 최적화 API 엔드포인트로 배포합니다. 캐싱을 활성화합니다. Lambda
함수에 대한 예약된 동시성을 구성합니다.
D. REST API를 지역 API 엔드포인트로 배포합니다. 캐싱을 활성화합니다. Lambda 함수에
대한 예약된 동시성을 구성합니다.
Answer: A
* Edge-optimized API endpoints route requests through CloudFront, reducing latency for
global users.
* Option A correctly implements edge-optimization, caching, and compression to minimize
latency.
* Options B and D do not use edge optimization, leading to higher latency for global users.
* Reserved concurrency in Options C and D improves backend scaling but does not address
global latency directly.
QUESTION NO: 166
회사는 사용자 업로드 문서를 Amazon EBS 볼륨에 저장하는 단일 Amazon EC2 인스턴스를
사용하여 AWS에서 웹 애플리케이션을 호스팅하고 있습니다. 더 나은 확장성과 가용성을
위해 이 회사는 아키텍처를 복제하고 두 번째 EC2 인스턴스와 EBS 볼륨을 다른 가용 영역에
생성하여 둘 다 Application Load Balancer 뒤에 배치했습니다. 이 변경을 완료한 후 사용자는
웹 사이트를 새로 고칠 때마다 다음을 볼 수 있다고 보고했습니다. 문서의 한 하위 집합 또는
다른 하위 집합이지만 동시에 모든 문서는 아닙니다.
솔루션 설계자는 사용자가 모든 문서를 한 번에 볼 수 있도록 무엇을 제안해야 합니까?
A. 두 EBS 볼륨에 모든 문서가 포함되도록 데이터를 복사합니다.
B. 문서가 있는 서버로 사용자를 안내하도록 Application Load Balancer 구성
C. 두 EBS 볼륨의 데이터를 Amazon EFS로 복사 새 문서를 Amazon EFS에 저장하도록
애플리케이션 수정
D. 두 서버 모두에 요청을 보내도록 Application Load Balancer를 구성합니다. 올바른
서버에서 각 문서를 반환합니다.
Answer: C
Explanation:
https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works-ec2
QUESTION NO: 167
회사에는 AWS Organizations의 조직에서 실행되는 애플리케이션이 있습니다. 회사는
애플리케이션의 운영 지원을 아웃소싱합니다. 회사는 보안을 손상시키지 않고 외부 지원
엔지니어에게 액세스를 제공해야 합니다.
외부 지원 엔지니어는 AWS Management Console에 액세스해야 합니다. 외부 지원
엔지니어는 또한 프라이빗 서브넷에서 Amazon Linux를 실행하는 회사의 Amazon EC2
인스턴스 플릿에 대한 운영 체제 액세스도 필요합니다.
이러한 요구 사항을 가장 안전하게 충족할 수 있는 솔루션은 무엇입니까?
A. 모든 인스턴스에 AWS Systems Manager Agent(SSM Agent)가 설치되어 있는지
확인합니다. Systems Manager에 연결하는 데 필요한 정책이 있는 인스턴스 프로필을
할당합니다. AWS 1AM Identity Center를 사용하여 외부 지원 엔지니어 콘솔 액세스를
제공합니다. Systems Manager Session Manager를 사용하여 필요한 권한을 할당합니다.
113

IT Certification Guaranteed, The Easy Way!
B. 모든 인스턴스에 AWS Systems Manager Agent {SSM Agent}가 설치되어 있는지
확인합니다. Systems Manager에 연결하는 데 필요한 정책이 있는 인스턴스 프로필을
할당합니다. Systems Manager Session Manager를 사용하여 각 AWS 계정의 로컬 1AM
사용자 자격 증명을 외부 지원 엔지니어에게 제공하여 콘솔에 액세스합니다.
C. 모든 인스턴스에 외부 지원 엔지니어 소스 IP 주소 범위에서만 SSH 액세스를 허용하는
보안 그룹이 있는지 확인합니다. 각 AWS 계정에서 로컬 1AM 사용자 자격 증명을 외부 지원
엔지니어에게 제공하여 콘솔 액세스를 제공합니다. 각 외부 지원 엔지니어에게 애플리케이션
인스턴스에 로그인할 수 있는 SSH 키 쌍을 제공합니다.
D. 퍼블릭 서브넷에 베스천 호스트를 만듭니다. 외부 엔지니어의 IP 주소 범위에서만 액세스할
수 있도록 베스천 호스트 보안 그룹을 설정합니다. 모든 인스턴스에 베스천 호스트에서 SSH
액세스를 허용하는 보안 그룹이 있는지 확인합니다. 각 외부 지원 엔지니어에게 애플리케이션
인스턴스에 로그인할 수 있는 SSH 키 쌍을 제공합니다. 콘솔 액세스를 위해 엔지니어에게
로컬 계정 1AM 사용자 자격 증명을 제공합니다.
Answer: A
Explanation:
This solution provides the most secure access for external support engineers with the least
exposure to potential security risks.
* AWS Systems Manager (SSM) and Session Manager: Systems Manager Session Manager
allows secure and auditable access to EC2 instances without the need to open inbound SSH
ports or manage SSH keys. This reduces the attack surface significantly. The SSM Agent
must be installed and configured on all instances, and the instances must have an instance
profile with the necessary IAM permissions to connect to Systems Manager.
* IAM Identity Center: IAM Identity Center provides centralized management of access to the
AWS Management Console for external support engineers. By using IAM Identity Center, you
can control console access securely and ensure that external engineers have the appropriate
permissions based on their roles.
* Why Not Other Options?:
* Option B (Local IAM user credentials): This approach is less secure because it involves
managing local IAM user credentials and does not leverage the centralized management and
security benefits of IAM Identity Center.
* Option C (Security group with SSH access): Allowing SSH access opens up the
infrastructure to potential security risks, even when restricted by IP addresses. It also
requires managing SSH keys, which can be cumbersome and less secure.
* Option D (Bastion host): While a bastion host can secure SSH access, it still requires
managing SSH keys and opening ports. This approach is less secure and more operationally
intensive compared to using Session Manager.
AWS References:
* AWS Systems Manager Session Manager - Documentation on using Session Manager for
secure instance access.
* AWS IAM Identity Center - Overview of IAM Identity Center and its capabilities for managing
user access.
QUESTION NO: 168
최소한의 운영 비용으로 DynamoDB 데이터를 장기 분석에 어떻게 활용할 수 있을까요?
A. DynamoDB를 S3로 증분 방식으로 내보내도록 구성합니다.
114

IT Certification Guaranteed, The Easy Way!
B. DynamoDB Streams를 구성하여 S3에 레코드를 씁니다.
C. DynamoDB 데이터를 S3에 복사하도록 EMR을 구성합니다.
D. DynamoDB 데이터를 HDFS로 복사하도록 EMR을 구성합니다.
Answer: A
* Option A is the most automated and cost-efficient solution for exporting data to S3 for
analytics.
* Option B involves manual setup of Streams to S3.
* Options C and D introduce complexity with EMR.
QUESTION NO: 169
회사는 Amazon S3 버킷에 텍스트 파일로 저장된 레시피 레코드에서 재료 이름을 추출해야
합니다. 웹 애플리케이션은 재료 이름을 사용하여 Amazon DynamoDB 테이블을 쿼리하고
영양 점수를 결정합니다.
애플리케이션은 식품 이외의 기록과 오류를 처리할 수 있습니다. 회사에는 이 솔루션을
개발할 기계 학습 지식을 갖춘 직원이 없습니다. 이러한 요구 사항을 가장 비용 효율적으로
충족할 솔루션은 무엇입니까?
A. PutObject 요청이 발생할 때 S3 이벤트 알림을 사용하여 AWS Lambda 함수를 호출합니다.
Amazon Comprehend를 사용하여 객체를 분석하고 성분 이름을 추출하도록 Lambda 함수를
프로그래밍합니다. Amazon Comprehend 출력을 DynamoDB 테이블에 저장합니다.
B. Amazon EventBridge 규칙을 사용하여 PutObject 요청이 발생할 때 AWS Lambda 함수를
호출합니다.
Amazon Forecast를 사용하여 성분 이름을 추출하여 객체를 분석하도록 Lambda 함수를
프로그래밍합니다. Forecast 출력을 DynamoDB 테이블에 저장합니다.
C. PutObject 요청이 발생할 때 S3 이벤트 알림을 사용하여 AWS Lambda 함수를 호출합니다.
Amazon Polly를 사용하여 레시피 레코드의 오디오 녹음을 생성합니다. S3 버킷에 오디오
파일 저장 Amazon Simple Notification Service(Amazon SNS)를 사용하여 URL을 직원에게
메시지로 보냅니다. 직원에게 오디오 파일을 듣고 영양 점수를 계산하도록 지시합니다. 성분
이름을 DynamoDB 테이블에 저장합니다.
D. PutObject 요청이 발생할 때 Amazon EventBridge 규칙을 사용하여 AWS Lambda 함수를
호출합니다. Amazon SageMaker를 사용하여 객체를 분석하고 성분 이름을 추출하도록
Lambda 함수를 프로그래밍합니다. SageMaker 엔드포인트의 추론 출력을 DynamoDB에
저장합니다. 테이블.
Answer: A
Explanation:
This solution meets the following requirements:
* It is cost-effective, as it only uses serverless components that are charged based on usage
and do not require any upfront provisioning or maintenance.
* It is scalable, as it can handle any number of recipe records that are uploaded to the S3
bucket without any performance degradation or manual intervention.
* It is easy to implement, as it does not require any machine learning knowledge or complex
data processing logic. Amazon Comprehend is a natural language processing service that
can automatically extract entities such as ingredients from text files. The Lambda function
can simply invoke the Comprehend API and store the results in the DynamoDB table.
* It is reliable, as it can handle non-food records and errors gracefully. Amazon Comprehend
can detect the language and domain of the text files and return an appropriate response. The
115

IT Certification Guaranteed, The Easy Way!
Lambda function can also implement error handling and logging mechanisms to ensure the
data quality and integrity.
References:
* Using AWS Lambda with Amazon S3 - AWS Lambda
* What Is Amazon Comprehend? - Amazon Comprehend
* Working with Tables - Amazon DynamoDB
QUESTION NO: 170
회사는 고객이 금융 정보를 검색할 수 있도록 고객에게 API 인터페이스를 제공합니다.
회사에서는 연중 사용량이 가장 많은 시간대에 더 많은 요청이 발생할 것으로 예상합니다.
회사에서는 고객 만족을 보장하기 위해 API가 짧은 대기 시간으로 일관되게 응답하도록
요구합니다. 회사는 API에 대한 컴퓨팅 호스트를 제공해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Application Load Balancer와 Amazon Elastic Container Service(Amazon ECS)를
사용합니다.
B. 프로비저닝된 동시성을 통해 Amazon API Gateway 및 AWS Lambda 함수를 사용합니다.
C. Application Load Balancer와 Amazon Elastic Kubernetes Service(Amazon EKS)
클러스터를 사용합니다.
D. 예약된 동시성을 통해 Amazon API Gateway 및 AWS Lambda 함수를 사용합니다.
Answer: B
Explanation:
Amazon API Gateway is a fully managed service that makes it easy for developers to create,
publish, maintain, monitor, and secure APIs at any scale. AWS Lambda is a serverless
compute service that lets you run code without provisioning or managing servers. Lambda
scales automatically based on the incoming requests, but it may take some time to initialize
new instances of your function if there is a sudden increase in demand. This may result in
high latency or cold starts for your API. To avoid this, you can use provisioned concurrency,
which ensures that your function is initialized and ready to respond at any time. Provisioned
concurrency also helps you achieve consistent low latency for your API by reducing the
impact of scaling on performance. References:
https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-develop-
integrations-lambda.html https://docs.aws.amazon.com/lambda/latest/dg/configuration-
concurrency.html
QUESTION NO: 171
회사에 스토리지 용량이 부족한 온프레미스 데이터 센터가 있습니다. 회사는 대역폭 비용을
최소화하면서 스토리지 인프라를 AWS로 마이그레이션하려고 합니다. 솔루션은 추가 비용
없이 데이터를 즉시 검색할 수 있어야 합니다.
이러한 요구 사항을 어떻게 충족할 수 있습니까?
A. Amazon S3 Glacier Vault를 배포하고 빠른 검색을 활성화합니다. 워크로드에 대해
프로비저닝된 검색 용량을 활성화합니다.
B. 캐시된 볼륨을 사용하여 AWS Storage Gateway를 배포합니다. Storage Gateway를
사용하면 자주 액세스하는 데이터 하위 집합의 복사본을 로컬에 보관하면서 Amazon S3에
데이터를 저장할 수 있습니다.
C. 저장된 볼륨을 사용하여 AWS Storage Gateway를 배포하여 데이터를 로컬에 저장합니다.
116

IT Certification Guaranteed, The Easy Way!
Storage Gateway를 사용하여 데이터의 특정 시점 스냅샷을 Amazon S3에 비동기식으로
백업합니다.
D. AWS Direct Connect를 배포하여 온프레미스 데이터 센터에 연결합니다. 데이터를 로컬에
저장하도록 AWS Storage Gateway를 구성합니다. Storage Gateway를 사용하여 데이터의
특정 시점 스냅샷을 Amazon S3에 비동기식으로 백업합니다.
Answer: B
Explanation:
The solution that will meet the requirements is to deploy AWS Storage Gateway using
cached volumes and use Storage Gateway to store data in Amazon S3 while retaining copies
of frequently accessed data subsets locally. This solution will allow the company to migrate
its storage infrastructure to AWS while minimizing bandwidth costs, as it will only transfer
data that is not cached locally. The solution will also allow for immediate retrieval of data at
no additional cost, as the cached volumes will provide low-latency access to the most
recently used data. The data stored in Amazon S3 will be durable, scalable, and secure.
The other solutions are not as effective as the first one because they either do not meet the
requirements or introduce additional costs or complexity. Deploying Amazon S3 Glacier Vault
and enabling expedited retrieval will not meet the requirements, as it will incur additional
costs for both storage and retrieval.
Amazon S3 Glacier is a low-cost storage service for data archiving and backup, but it has
longer retrieval times than Amazon S3. Expedited retrieval is a feature that allows faster
access to data, but it charges a higher fee per GB retrieved. Provisioned retrieval capacity is
a feature that reserves dedicated capacity for expedited retrievals, but it also charges a
monthly fee per provisioned capacity unit. Deploying AWS Storage Gateway using stored
volumes to store data locally and use Storage Gateway to asynchronously back up point-in-
time snapshots of the data to Amazon S3 will not meet the requirements, as it will not migrate
the storage infrastructure to AWS, but only create backups. Stored volumes are volumes that
store the primary data locally and back up snapshots to Amazon S3. This solution will not
reduce the storage capacity needed on- premises, nor will it leverage the benefits of cloud
storage. Deploying AWS Direct Connect to connect with the on-premises data center and
configuring AWS Storage Gateway to store data locally and use Storage Gateway to
asynchronously back up point-in-time snapshots of the data to Amazon S3 will not meet the
requirements, as it will also not migrate the storage infrastructure to AWS, but only create
backups. AWS Direct Connect is a service that establishes a dedicated network connection
between the on-premises data center and AWS, which can reduce network costs and
increase bandwidth. However, this solution will also not reduce the storage capacity needed
on-premises, nor will it leverage the benefits of cloud storage.
References:
* AWS Storage Gateway
* Cached volumes - AWS Storage Gateway
* Amazon S3 Glacier
* Retrieving archives from Amazon S3 Glacier vaults - Amazon Simple Storage Service
* Stored volumes - AWS Storage Gateway
* AWS Direct Connect
QUESTION NO: 172
117

IT Certification Guaranteed, The Easy Way!
회사의 주문 시스템은 클라이언트의 요청을 Amazon EC2 인스턴스로 보냅니다. EC2
인스턴스는 주문을 처리하고 직원은 주문을 Amazon RDS의 데이터베이스에 저장합니다.
사용자는 시스템이 실패할 경우 주문을 다시 처리해야 한다고 보고합니다. 회사는 시스템
중단이 발생할 경우 자동으로 주문을 처리할 수 있는 탄력적인 솔루션을 원합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. EC2 인스턴스를 Auto Scaling 그룹으로 이동 Amazon Elastic Container Service(Amazon
ECS) 작업을 대상으로 하는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을
생성합니다.
B. EC2 인스턴스를 ALB(Application Load Balancer) 뒤의 Auto Scaling 그룹으로 이동합니다.
ALB 엔드포인트에 메시지를 보내도록 주문 시스템을 업데이트합니다.
C. EC2 인스턴스를 Auto Scaling 그룹으로 이동합니다. Amazon Simple Queue
Service(Amazon SQS) 대기열로 메시지를 보내도록 주문 시스템을 구성합니다. 대기열의
메시지를 사용하도록 EC2 인스턴스를 구성합니다.
D. Amazon Simple 알림 서비스(Amazon SNS) 주제 생성 AWS Lambda 함수를 생성하고
함수를 SNS 주제에 구독합니다. SNS 주제에 메시지를 보내도록 주문 시스템을 구성합니다.
EC2 인스턴스에 명령을 전송하여 처리합니다. AWS 시스템 관리자 Run Command를
사용하여 메시지
Answer: C
Explanation:
To meet the company's requirements of having a resilient solution that can process orders
automatically in case of a system outage, the solutions architect needs to implement a fault-
tolerant architecture. Based on the given scenario, a potential solution is to move the EC2
instances into an Auto Scaling group and configure the order system to send messages to an
Amazon Simple Queue Service (Amazon SQS) queue. The EC2 instances can then
consume messages from the queue.
QUESTION NO: 173
한 회사가 AWS 클라우드에서 실험적인 워크로드를 실행하려고 합니다. 회사에는 클라우드
지출에 대한 예산이 있습니다. 회사의 CFO는 각 부서의 클라우드 지출 책임에 대해 우려하고
있습니다. CFO는 지출 임계값이 예산의 60%에 도달하면 알림을 받기를 원합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS 리소스의 비용 할당 태그를 사용하여 소유자에게 레이블을 지정합니다. AWS
예산에서 사용 예산을 생성합니다. 지출이 예산의 60%를 초과할 때 알림을 받을 수 있도록
경고 임계값을 추가합니다.
B. AWS Cost Explorer 예측을 사용하여 리소스 소유자를 결정합니다. AWS 비용 이상 탐지를
사용하면 지출이 예산의 60%를 초과할 때 경고 임계값 알림을 생성할 수 있습니다.
C. AWS 리소스의 비용 할당 태그를 사용하여 소유자에게 레이블을 지정합니다. 지출이
예산의 60%를 초과할 때 AWS Trusted Advisor에서 AWS Support API를 사용하여 경고
임계값 알림 생성
D. AWS Cost Explorer 예측을 사용하여 리소스 소유자를 결정합니다. AWS 예산에서 사용
예산을 생성합니다. 지출이 예산의 60%를 초과할 때 알림을 받을 수 있도록 경고 임계값을
추가합니다.
Answer: A
Explanation:
118

IT Certification Guaranteed, The Easy Way!
This solution meets the requirements because it allows the company to track and manage its
cloud spending by using cost allocation tags to assign costs to different departments,
creating usage budgets to set spending limits, and adding alert thresholds to receive
notifications when the spending reaches a certain percentage of the budget. This way, the
company can monitor its experimental workloads and avoid overspending on the cloud.
References:
* Using Cost Allocation Tags
* Creating an AWS Budget
* Creating an Alert for an AWS Budget
QUESTION NO: 174
한 회사가 Amazon RDS 데이터베이스에 연결되는 AWS에서 애플리케이션을 구축하고
있습니다. 이 회사는 애플리케이션 구성을 관리하고 데이터베이스 및 기타 서비스에 대한
자격 증명을 안전하게 저장하고 검색하려고 합니다.
이러한 요구 사항을 가장 적은 관리 비용으로 충족할 수 있는 솔루션은 무엇일까요?
A. AWS AppConfig를 사용하여 애플리케이션 구성을 저장하고 관리합니다. AWS Secrets
Manager를 사용하여 자격 증명을 저장하고 검색합니다.
B. AWS Lambda를 사용하여 애플리케이션 구성을 저장하고 관리합니다. AWS Systems
Manager Parameter Store를 사용하여 자격 증명을 저장하고 검색합니다.
C. 암호화된 애플리케이션 구성 파일을 사용합니다. 애플리케이션 구성을 위해 Amazon S3에
파일을 저장합니다. 자격 증명을 저장하고 검색하기 위한 다른 S3 파일을 만듭니다.
D. AWS AppConfig를 사용하여 애플리케이션 구성을 저장하고 관리합니다. Amazon RDS를
사용하여 자격 증명을 저장하고 검색합니다.
Answer: A
Explanation:
This solution meets the company's requirements with minimal administrative overhead and
ensures security and ease of management.
* AWS AppConfig: AWS AppConfig is a service designed to manage application
configuration in a secure and validated way. It allows you to deploy configurations safely and
quickly without affecting the application's performance or availability.
* AWS Secrets Manager: AWS Secrets Manager is specifically designed to manage, retrieve,
and rotate credentials for databases and other services. It integrates seamlessly with AWS
services like Amazon RDS, making it an ideal solution for securely storing and retrieving
database credentials. Secrets Manager also provides automatic rotation of credentials,
reducing the operational burden.
* Why Not Other Options?:
* Option B (AWS Lambda + Parameter Store): While AWS Lambda can be used for
managing configurations and AWS Systems Manager Parameter Store can store credentials,
this approach involves more manual setup and does not offer the same level of integrated
management and security as AppConfig and Secrets Manager.
* Option C (Encrypted S3 Configuration File): Storing configuration and credentials in S3 files
involves more manual management and security considerations, increasing the
administrative overhead.
* Option D (AppConfig + RDS for credentials): RDS is not designed for storing application
credentials; it's better suited for managing database instances and their configurations.
119

IT Certification Guaranteed, The Easy Way!
AWS References:
* AWS AppConfig - Describes how to use AWS AppConfig for managing application
configurations.
* AWS Secrets Manager - Provides details on securely storing and retrieving credentials using
AWS Secrets Manager.
QUESTION NO: 175
한 회사에는 다음 달 내에 AWS 클라우드로 이동해야 하는 150TB의 보관된 이미지 데이터가
온프레미스에 저장되어 있습니다. 회사의 현재 네트워크 연결은 야간에만 이 목적으로 최대
100Mbps의 업로드를 허용합니다.
이 데이터를 이동하고 마이그레이션 기한을 맞추는 가장 비용 효율적인 메커니즘은
무엇입니까?
A. AWS Snowmobile을 사용하여 AWS로 데이터를 전송합니다.
B. 여러 AWS Snowball 디바이스를 주문하여 데이터를 AWS로 배송합니다.
C. Amazon S3 Transfer Acceleration을 활성화하고 데이터를 안전하게 업로드합니다.
D. Amazon S3 VPC 엔드포인트를 생성하고 VPN을 설정하여 데이터를 업로드합니다.
Answer: B
Explanation:
AWS Snowball is a petabyte-scale data transport service that uses secure devices to transfer
large amounts of data into and out of the AWS Cloud. Snowball addresses common
challenges with large-scale data transfers including high network costs, long transfer times,
and security concerns. AWS Snowball can transfer up to 80 TB of data per device, and
multiple devices can be used in parallel to meet the migration deadline. AWS Snowball is
more cost-effective than AWS Snowmobile, which is designed for exabyte-scale data
transfers, or Amazon S3 Transfer Acceleration, which is optimized for fast transfers over long
distances. Amazon S3 VPC endpoint does not increase the upload speed, but only provides
a secure and private connection between the VPC and S3. References: AWS Snowball,
AWS Snowmobile, Amazon S3 Transfer Acceleration, Amazon S3 VPC endpoint
QUESTION NO: 176
한 회사가 레거시 애플리케이션을 AWS로 마이그레이션할 계획입니다. 이 애플리케이션은
현재 NFS를 사용하여 온프레미스 스토리지 솔루션과 통신하여 애플리케이션 데이터를
저장합니다. 이 애플리케이션은 이 목적을 위해 NFS 이외의 다른 통신 프로토콜을
사용하도록 수정할 수 없습니다.
솔루션 아키텍트는 마이그레이션 후 어떤 스토리지 솔루션을 사용하도록 권장해야 합니까?
A. AWS 데이터 동기화
B. Amazon Elastic Block Store(Amazon EB5)
C. Amazon Elastic File System(Amazon EF5)
D. Amazon EMR 파일 시스템(Amazon EMRFS)
Answer: C
Explanation:
Amazon Elastic File System (EFS) is the ideal solution for migrating legacy applications that
require NFS (Network File System) communication. EFS provides fully managed, scalable
NFS storage in the cloud, and it supports the standard NFS protocols, allowing the legacy
application to continue using NFS without modification after migration to AWS.
120

IT Certification Guaranteed, The Easy Way!
Key AWS features:
* NFS Support: EFS natively supports the NFSv4 protocol, which makes it the best solution
for workloads that rely on NFS communication.
* Scalability and Availability: EFS automatically scales as application demands grow, making
it a highly available and reliable storage solution.
* AWS Documentation: According to AWS's best practices for file storage, EFS is
recommended for any workloads requiring NFS support in a cloud environment.
QUESTION NO: 177
회사에는 AWS Organizations 조직의 일부로 5개의 조직 단위(OU)가 있습니다. 각 OU는
회사가 소유한 5개 비즈니스와 연관되어 있습니다. 회사의 연구개발(R&D) 사업이 회사에서
분리되어 자체 조직이 필요할 것입니다. 솔루션 설계자는 이 목적을 위해 별도의 새 관리
계정을 생성합니다.
솔루션 설계자는 새 마스터 계정에서 다음에 무엇을 수행해야 합니까?
A. 전환 중에 R&D AWS 계정이 두 조직의 일부가 되도록 합니다.
B. R&D AWS 계정이 이전 조직을 떠난 후 R&D AWS 계정을 새 조직의 일부로 초대합니다.
C. 새 조직에 새 R&D AWS 계정을 생성합니다. 이전 R&D AWS 계정의 리소스를 새 R&D
AWS 계정으로 마이그레이션합니다.
D. R&D AWS 계정이 새 조직에 가입하도록 합니다. 새 마스터 계정을 이전 조직의 구성원으로
만드세요.
Answer: B
Explanation:
it allows the solutions architect to create a separate organization for the research and
development (R&D) business and move its AWS account to the new organization. By inviting
the R&D AWS account to be part of the new organization after it has left the prior
organization, the solutions architect can ensure that there is no overlap or conflict between
the two organizations. The R&D AWS account can accept or decline the invitation to join the
new organization. Once accepted, it will be subject to any policies and controls applied by the
new organization. References:
* Inviting an AWS Account to Join Your Organization
* Leaving an Organization as a Member Account
QUESTION NO: 178
웹사이트는 Auto Scaling과 EFS를 갖춘 EC2 인스턴스를 사용합니다. 회사는 어떻게 비용을
최적화할 수 있습니까?
A. 원하는 인스턴스 수를 설정하기 위해 자동 스케일링 그룹을 재구성합니다. 예약된
스케일링을 끕니다.
B. 더 큰 EC2 인스턴스를 사용하는 새로운 실행 템플릿 버전을 만듭니다.
C. 대상 추적 확장 정책을 사용하도록 자동 확장 그룹을 재구성합니다.
D. EFS 볼륨을 인스턴스 스토어 볼륨으로 바꿉니다.
Answer: C
* Option C ensures dynamic scaling based on demand using a target tracking scaling policy,
optimizing costs.
* Option A results in over-provisioning, leading to higher costs.
* Option B increases costs by using larger instances.
121

IT Certification Guaranteed, The Easy Way!
* Option D is not feasible as instance store volumes are ephemeral and unsuitable for shared
storage like EFS.
QUESTION NO: 179
한 회사가 단일 VPC의 여러 가용성 영역에 분산된 여러 Amazon EC2 인스턴스에서 미디어
스토어를 운영하고 있습니다. 이 회사는 모든 EC2 인스턴스 간에 데이터를 공유하기 위한
고성능 솔루션을 원하며, VPC 내에서만 데이터를 보관하는 것을 선호합니다.
솔루션 아키텍트는 무엇을 추천해야 할까요?
A. Amazon S3 버킷을 생성하고 각 인스턴스의 애플리케이션에서 서비스 API를 호출합니다.
B. Amazon S3 버킷을 생성하고 모든 인스턴스가 마운트된 볼륨으로 액세스하도록
구성합니다.
C. Amazon Elastic Block Store(Amazon EBS) 볼륨을 구성하고 모든 인스턴스에
마운트합니다.
D. Amazon Elastic File System(Amazon EFS) 파일 시스템을 구성하고 모든 인스턴스에
마운트합니다.
Answer: D
Explanation:
Amazon Elastic File System (EFS) is a managed file storage service that can be mounted
across multiple EC2 instances. It provides a scalable and high-performing solution to share
data among instances within a VPC.
* High Performance: EFS provides scalable performance for workloads that require high
throughput and IOPS. It is particularly well-suited for applications that need to share data
across multiple instances.
* Ease of Use: EFS can be easily mounted on multiple instances across different Availability
Zones, providing a shared file system accessible to all the instances within the VPC.
* Security: EFS can be configured to ensure that data remains within the VPC, and it
supports encryption at rest and in transit.
* Why Not Other Options?:
* Option A (Amazon S3 bucket with APIs): While S3 is excellent for object storage, it is not a
file system and does not provide the low-latency access required for shared data between
instances.
* Option B (S3 bucket as a mounted volume): S3 is not designed to be mounted as a file
system, and this approach would introduce unnecessary complexity and latency.
* Option C (EBS volume shared across instances): EBS volumes cannot be attached to
multiple instances simultaneously. It is not designed to be shared across instances like EFS.
AWS References:
* Amazon EFS - Overview of Amazon EFS and its features.
* Best Practices for Amazon EFS - Recommendations for using EFS with multiple instances.
QUESTION NO: 180
회사에 들어오는 메시지를 수집하는 응용 프로그램이 있습니다. 그런 다음 이러한 메시지는
수십 개의 다른 애플리케이션과 마이크로서비스에서 빠르게 사용됩니다.
메시지 수는 매우 다양하며 때로는 초당 최대 100,000개까지 급증합니다. 이 회사는 솔루션을
분리하고 확장성을 높이고자 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
122

IT Certification Guaranteed, The Easy Way!
A. Amazon Kinesis Data Analytics에 대한 메시지를 유지합니다. 모든 응용 프로그램은
메시지를 읽고 처리합니다.
B. CPU 지표를 기반으로 EC2 인스턴스 수를 조정하는 Auto Scaling 그룹의 Amazon EC2
인스턴스에 애플리케이션을 배포합니다.
C. 단일 샤드로 Amazon Kinesis Data Streams에 메시지를 씁니다. 모든 애플리케이션은
스트림에서 읽고 메시지를 처리합니다.
D. 하나 이상의 Amazon Simple Queue Service(Amazon SQS) 구독이 있는 Amazon Simple
Notification Service(Amazon SNS) 주제에 메시지를 게시합니다. 그러면 모든 애플리케이션이
대기열의 메시지를 처리합니다.
Answer: D
Explanation:
https://aws.amazon.com/sqs/features/
By routing incoming requests to Amazon SQS, the company can decouple the job requests
from the processing instances. This allows them to scale the number of instances based on
the size of the queue, providing more resources when needed. Additionally, using an Auto
Scaling group based on the queue size will automatically scale the number of instances up or
down depending on the workload. Updating the software to read from the queue will allow it
to process the job requests in a more efficient manner, improving the performance of the
system.
QUESTION NO: 181
한 게임 회사에서 가용성이 높은 아키텍처를 설계하고 있습니다. 이 애플리케이션은 수정된
Linux 커널에서 실행되며 UDP 기반 트래픽만 지원합니다. 회사는 가능한 최상의 사용자
경험을 제공하기 위해 프런트엔드 계층이 필요합니다. 해당 계층은 대기 시간이 짧아야 하고
트래픽을 가장 가까운 엣지 위치로 라우팅해야 하며 애플리케이션 엔드포인트에 진입하기
위한 고정 IP 주소를 제공해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 요청을 Application Load Balancer로 전달하도록 Amazon Route 53을 구성합니다. AWS
Application Auto Scaling의 애플리케이션에 AWS Lambda를 사용합니다.
B. 요청을 Network Load Balancer로 전달하도록 Amazon CloudFront를 구성합니다. AWS
Application Auto Scaling 그룹의 애플리케이션에 AWS Lambda를 사용합니다.
C. 요청을 Network Load Balancer로 전달하도록 AWS Global Accelerator를 구성합니다. EC2
Auto Scaling 그룹의 애플리케이션에 Amazon EC2 인스턴스를 사용합니다.
D. 요청을 Application Load Balancer로 전달하도록 Amazon API Gateway를 구성합니다. EC2
Auto Scaling 그룹의 애플리케이션에 Amazon EC2 인스턴스를 사용합니다.
Answer: C
Explanation:
AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS
global network and its edge locations around the world. CloudFront improves performance for
both cacheable content (such as images and videos) and dynamic content (such as API
acceleration and dynamic site delivery). Global Accelerator improves performance for a wide
range of applications over TCP or UDP by proxying packets at the edge to applications
running in one or more AWS Regions. Global Accelerator is a good fit for non-HTTP use
cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases
123

IT Certification Guaranteed, The Easy Way!
that specifically require static IP addresses or deterministic, fast regional failover. Both
services integrate with AWS Shield for DDoS protection.
QUESTION NO: 182
회사는 도망에서 역동적인 웹사이트를 자체적으로! ALB(Application Load Balancer) 뒤의
Amazon EC2 인스턴스 웹 사이트는 전 세계 고객에게 서비스를 제공하기 위해 여러 언어를
지원해야 합니다. 웹 사이트 아키텍처는 us-west-1 지역에서 실행되고 있으며 다음 지역에
있는 사용자에 대해 높은 요청 지연 시간을 보이고 있습니다. 세계의 다른 지역 웹 사이트는
사용자의 위치에 관계없이 빠르고 효율적으로 요청을 처리해야 합니다. 그러나 회사는 여러
지역에 걸쳐 기존 아키텍처를 다시 만들고 싶지 않습니다. 이러한 요구 사항을 충족하려면
솔루션 설계자가 무엇을 해야 합니까?
A. 기존 아키텍처를 Amazon S3 버킷에서 제공되는 웹 사이트로 교체 S3 버킷을 원본으로
사용하여 Amazon CloudFront 배포를 구성합니다. Accept-Language 요청 헤더를 기반으로
캐시하도록 캐시 동작 설정을 지정합니다.
B. ALB를 원본으로 사용하여 Amazon CloudFront 배포를 구성합니다. Accept-Language 요청
헤더를 기반으로 캐시하도록 캐시 동작 설정을 설정합니다.
C. ALB와 통합된 Amazon API Gateway API를 생성합니다. HTTP 통합 유형을 사용하도록
API를 구성합니다. Accept-Language 요청 헤더를 기반으로 API 캐시를 활성화하도록 API
Gateway 단계를 설정합니다.
D. 각 추가 지역에서 EC2 인스턴스를 시작하고 해당 지역의 캐시 서버 역할을 하도록
NGINX를 구성합니다. 지리적 위치 라우팅 정책을 사용하여 Amazon Route 53 레코드 세트
뒤에 모든 EC2 인스턴스와 ALB를 배치합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/header-
caching.html Configuring caching based on the language of the viewer: If you want
CloudFront to cache different versions of your objects based on the language specified in the
request, configure CloudFront to forward the Accept-Language header to your origin.
QUESTION NO: 183
회사에서 애플리케이션을 AWS로 마이그레이션하려고 합니다. 회사는 애플리케이션의 현재
가용성을 높이고 싶어합니다. 회사는 애플리케이션 아키텍처에서 AWS WAF를 사용하려고
합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 두 개의 가용 영역에 걸쳐 애플리케이션을 호스팅하는 여러 Amazon EC2 인스턴스를
포함하는 Auto Scaling 그룹을 생성합니다. Application Load Balancer(ALB)를 구성하고 Auto
Scaling 그룹을 대상으로 설정합니다. WAF를 ALB에 연결합니다.
B. 애플리케이션을 호스팅하는 여러 Amazon EC2 인스턴스를 포함하는 클러스터 배치
그룹을 생성합니다. Application Load Balancer를 구성하고 EC2 인스턴스를 대상으로
설정합니다. WAF를 배치 그룹에 연결합니다.
C. 두 개의 가용 영역에 걸쳐 애플리케이션을 호스팅하는 두 개의 Amazon EC2 인스턴스를
생성합니다. EC2 인스턴스를 ALB(Application Load Balancer)의 대상으로 구성합니다.
WAF를 ALB에 연결합니다.
D. 두 개의 가용 영역에 걸쳐 애플리케이션을 호스팅하는 여러 Amazon EC2 인스턴스를
포함하는 Auto Scaling 그룹을 생성합니다. Application Load Balancer(ALB)를 구성하고 Auto
124

IT Certification Guaranteed, The Easy Way!
Scaling 그룹을 대상으로 설정합니다. WAF를 Auto Scaling 그룹에 연결합니다.
Answer: A
Explanation:
* Understanding the Requirement: The company wants to migrate an application to AWS,
increase its availability, and use AWS WAF in the architecture.
* Analysis of Options:
* Auto Scaling group with ALB and WAF: This option provides high availability by distributing
instances across multiple Availability Zones. The ALB ensures even traffic distribution, and
AWS WAF provides security at the application layer.
* Cluster placement group with ALB and WAF: Cluster placement groups are for low-latency
networking within a single AZ, which does not provide the high availability across AZs.
* Two EC2 instances with ALB and WAF: This setup provides some availability but does not
scale automatically, missing the benefits of an Auto Scaling group.
* Auto Scaling group with WAF directly: AWS WAF cannot be directly connected to an Auto
Scaling group; it needs to be attached to an ALB, CloudFront distribution, or API Gateway.
* Best Solution:
* Auto Scaling group with ALB and WAF: This configuration ensures high availability,
scalability, and security, meeting all the requirements effectively.
References:
* Amazon EC2 Auto Scaling
* Application Load Balancer
* AWS WAF
QUESTION NO: 184
회사에는 Amazon EC2 인스턴스에서 실행되는 비즈니스에 중요한 애플리케이션이 있습니다.
애플리케이션은 Amazon DynamoDB 테이블에 데이터를 저장합니다. 회사는 지난 24시간
내의 어느 시점으로든 테이블을 되돌릴 수 있어야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 테이블에 대한 특정 시점 복구를 구성합니다.
B. 테이블에 AWS Backup을 사용합니다.
C. AWS Lambda 함수를 사용하여 매시간 테이블의 주문형 백업을 만듭니다.
D. 테이블에서 스트림을 켜서 지난 24시간 동안 테이블에 대한 모든 변경 사항에 대한 로그를
캡처합니다. 스트림 복사본을 Amazon S3 버킷에 저장합니다.
Answer: A
Explanation:
Point-in-time recovery (PITR) for DynamoDB is a feature that enables you to restore your
table data to any point in time during the last 35 days. PITR helps protect your table from
accidental write or delete operations, such as a test script writing to a production table or a
user issuing a wrong command. PITR is easy to use, fully managed, fast, and scalable. You
can enable PITR with a single click in the DynamoDB console or with a simple API call. You
can restore a table to a new table using the console, the AWS CLI, or the DynamoDB API.
PITR does not consume any provisioned table capacity and has no impact on the
performance or availability of your production applications. PITR meets the requirements of
the company with the least operational overhead, as it does not require any manual backup
creation, scheduling, or maintenance. It also provides per-second granularity for restoring the
125

IT Certification Guaranteed, The Easy Way!
table to any point within the last 24 hours.
References:
* Point-in-time recovery for DynamoDB - Amazon DynamoDB
* Amazon DynamoDB point-in-time recovery (PITR)
* Enable Point-in-Time Recovery (PITR) for Dynamodb global tables
* Restoring a DynamoDB table to a point in time - Amazon DynamoDB
* Point-in-time recovery: How it works - Amazon DynamoDB
QUESTION NO: 185
회사의 애플리케이션은 AWS에서 실행됩니다. 애플리케이션은 S3 Standard-infrequent
Access(S3 Standerd-IA) 스토리지 클래스를 사용하는 Amazon S3 버킷에 대용량 문서를
저장합니다. 회사는 데이터 저장 비용을 계속 지불하지만 총 S3 비용을 절약하려고 합니다.
회사는 승인된 외부 사용자가 밀리초 내에 문서에 액세스할 수 있기를 원합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. S3 버킷을 요청자 지불 버킷으로 구성합니다.
B. 모든 기존 객체와 향후 객체에 대해 스토리지 계층을 S3 Standard로 변경합니다.
C. S3 Docket에 대해 S3 Transfer Acceleration을 켭니다.
D. Amazon CloudFront를 사용하여 S3 버킷에 대한 모든 요청을 처리합니다.
Answer: D
Explanation:
This option is the most efficient because it uses Amazon CloudFront, which is a web service
that speeds up distribution of your static and dynamic web content, such as .html, .css, .js,
and image files, to your users1. It also uses CloudFront to handle all the requests to the S3
bucket, which reduces the S3 costs by caching the content at the edge locations and serving
it from there. It also allows authorized external users to access the documents in
milliseconds, as CloudFront delivers the content with low latency and high data transfer rates
.
This solution meets the requirement of continuing paying to store the data but saving on its
total S3 costs. Option A is less efficient because it configures the S3 bucket to be a
Requester Pays bucket, which is a way to shift the cost of data transfer and requests from
the bucket owner to the requester2. However, this does not reduce the total S3 costs, as the
company still has to pay for storing the data and for any requests made by its own users.
Option B is less efficient because it changes the storage tier to S3 Standard for all existing
and future objects, which is a way to store frequently accessed data with high durability and
availability3.
However, this does not reduce the total S3 costs, as S3 Standard has higher storage costs
than S3 Standard- IA. Option C is less efficient because it turns on S3 Transfer Acceleration
for the S3 bucket, which is a way to speed up transfers into and out of an S3 bucket by
routing requests through CloudFront edge locations4.
However, this does not reduce the total S3 costs, as S3 Transfer Acceleration has additional
charges for data transfer and requests.
QUESTION NO: 186
회사에는 us-west-2 지역에 애플리케이션이 배포된 여러 AWS 계정이 있습니다.
애플리케이션 로그는 각 계정의 Amazon S3 버킷에 저장됩니다. 회사는 단일 S3 버킷을
126

IT Certification Guaranteed, The Easy Way!
사용하는 중앙 집중식 로그 분석 솔루션을 구축하려고 합니다. 로그가 우리를 떠나서는 안
됩니다. west-2, 회사는 최소한의 운영 오버헤드를 원합니다. 이러한 요구 사항을 충족하고
가장 비용 효과적인 솔루션은 무엇입니까?
A. 애플리케이션 S3 버킷 중 하나에서 중앙 집중식 S3 버킷으로 객체를 복사하는 S3 수명
주기 정책을 생성합니다.
B. S3 동일 리전 복제를 사용하여 S3 버킷의 로그를 us-west-2의 다른 S3 버킷으로
복제합니다. 로그 분석에 이 S3 버킷을 사용합니다.
C. 매일 PutObject API 작업을 사용하여 버킷의 전체 콘텐츠를 us-west-2의 다른 S3 버킷에
복사하는 스크립트를 작성합니다. 로그 분석에 이 S3 버킷을 사용합니다.
D. 로그가 S3 버킷에 전달될 때마다 트리거되는 AWS Lambda 함수를 이 계정에
작성합니다(s3 ObjectCreated 이벤트) 로그를 us-west-2의 다른 S3 버킷에 복사합니다. 로그
분석을 위해 이 S3 버킷을 사용하세요.
Answer: B
Explanation:
This solution meets the following requirements:
* It is cost-effective, as it only charges for the storage and data transfer of the replicated
objects, and does not require any additional AWS services or custom scripts. S3 Same-
Region Replication (SRR) is a feature that automatically replicates objects across S3 buckets
within the same AWS Region. SRR can help you aggregate logs from multiple sources to a
single destination for analysis and auditing. SRR also preserves the metadata, encryption,
and access control of the source objects.
* It is operationally efficient, as it does not require any manual intervention or scheduling.
SRR replicates objects as soon as they are uploaded to the source bucket, ensuring that the
destination bucket always has the latest log data. SRR also handles any updates or deletions
of the source objects, keeping the destination bucket in sync. SRR can be enabled with a few
clicks in the S3 console or with a simple API call.
* It is secure, as it does not allow the logs to leave the us-west-2 Region. SRR only replicates
objects within the same AWS Region, ensuring that the data sovereignty and compliance
requirements are met.
SRR also supports encryption of the source and destination objects, using either server-side
encryption with AWS KMS or S3-managed keys, or client-side encryption.
References:
* Same-Region Replication - Amazon Simple Storage Service
* How do I replicate objects across S3 buckets in the same AWS Region?
* Centralized Logging on AWS | AWS Solutions | AWS Solutions Library
QUESTION NO: 187
회사는 Application Load Balancer 뒤의 Amazon Linux Amazon EC2 인스턴스에서 다중 계층
웹 애플리케이션을 호스팅합니다. 인스턴스는 여러 가용 영역의 Auto Scaling 그룹에서
실행됩니다. 이 회사는 애플리케이션의 최종 사용자가 대량의 정적 웹 콘텐츠에 액세스할 때
Auto Scaling 그룹이 더 많은 온디맨드 인스턴스를 시작하는 것을 관찰합니다. 회사는 비용을
최적화하려고 합니다.
애플리케이션을 가장 비용 효율적으로 재설계하기 위해 솔루션 설계자는 무엇을 해야
합니까?
A. 온디맨드 인스턴스 대신 예약 인스턴스를 사용하도록 Auto Scaling 그룹을
127

IT Certification Guaranteed, The Easy Way!
업데이트합니다.
B. 온디맨드 인스턴스 대신 스팟 인스턴스를 시작하여 확장하도록 Auto Scaling 그룹을
업데이트합니다.
C. Amazon S3 버킷에서 정적 웹 콘텐츠를 호스팅할 Amazon CloudFront 배포를 만듭니다.
D. Amazon API Gateway API 뒤에 AWS Lambda 함수를 생성하여 정적 웹 사이트 콘텐츠를
호스팅합니다.
Answer: C
Explanation:
This answer is correct because it meets the requirements of optimizing cost and reducing the
workload on the database. Amazon CloudFront is a content delivery network (CDN) service
that speeds up distribution of your static and dynamic web content, such as .html, .css, .js,
and image files, to your users. CloudFront delivers your content through a worldwide network
of data centers called edge locations. When a user requests content that you're serving with
CloudFront, the request is routed to the edge location that provides the lowest latency (time
delay), so that content is delivered with the best possible performance. You can create an
Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket,
which is an origin that you define for CloudFront. This way, you can offload the requests for
static web content from your EC2 instances to CloudFront, which can improve the
performance and availability of your website, and reduce the cost of running your EC2
instances.
References:
* https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html
QUESTION NO: 188
회사에서는 직원 데이터를 계층적 구조 관계로 저장하는 애플리케이션을 만들고 싶어합니다.
회사는 직원 데이터에 대한 트래픽이 많은 쿼리에 대해 최소 지연 응답이 필요하며 민감한
데이터를 보호해야 합니다. 또한 직원 데이터에 재무 정보가 있는 경우 회사는 월별 이메일
메시지를 받아야 합니다.
이러한 요구 사항을 충족하려면 솔루션 설계자가 수행해야 하는 단계 조합은 무엇입니까? (
2개를 선택하세요.)
A. Amazon Redshift를 사용하여 직원 데이터를 계층 구조로 저장합니다. 매달 Amazon S3에
데이터를 언로드합니다.
B. Amazon DynamoDB를 사용하여 직원 데이터를 계층 구조로 저장합니다. 매달 데이터를
Amazon S3로 내보냅니다.
C. AWS 계정에 대해 Amazon Macie 구성 Macie를 Amazon EventBridge와 통합하여 월간
이벤트를 AWS Lambda로 보냅니다.
D. Amazon Athena를 사용하여 Amazon S3에서 직원 데이터를 분석합니다. Athena를
Amazon QuickSight와 통합하여 분석 대시보드를 게시하고 대시보드를 사용자와 공유합니다.
E. AWS 계정에 대해 Amazon Macie를 구성합니다. Macie를 Amazon EventBridge와 통합하여
Amazon SNS(Amazon SNS) 구독을 통해 월간 알림을 보낼 수 있습니다.
Answer: B E
Explanation:
https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-hierarchical-data-
128

IT Certification Guaranteed, The Easy Way!
model/introduction.
html
QUESTION NO: 189
Amazon EC2 인스턴스에 호스팅된 회사 웹 사이트는 다음에 저장된 분류된 데이터를
처리합니다. 애플리케이션은 Amazon Elastic Block Store(Amazon EBS) 볼륨에 데이터를
기록합니다. 회사는 EBS 볼륨에 기록된 모든 데이터가 유휴 상태에서 암호화되도록 해야
합니다.
이 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EBS 암호화를 지정하는 1AM 역할을 생성합니다. EC2 인스턴스에 역할을 연결합니다.
B. EBS 볼륨을 암호화된 볼륨으로 생성합니다. EBS 볼륨을 EC2 인스턴스에 연결합니다.
C. EBS 수준에서 암호화가 필요한 모든 인스턴스에 Encrypt 키와 True Tag 값이 있는 EC2
인스턴스 태그를 생성합니다.
D. 계정에서 EBS 암호화를 시행하는 AWS Key Management Service(AWS KMS) 키 정책을
생성합니다. 키 정책이 활성 상태인지 확인하세요.
Answer: B
Explanation:
The simplest and most effective way to ensure that all data that is written to the EBS volumes
is encrypted at rest is to create the EBS volumes as encrypted volumes. You can do this by
selecting the encryption option when you create a new EBS volume, or by copying an
existing unencrypted volume to a new encrypted volume. You can also specify the AWS KMS
key that you want to use for encryption, or use the default AWS- managed key. When you
attach the encrypted EBS volumes to the EC2 instances, the data will be automatically
encrypted and decrypted by the EC2 host. This solution does not require any additional IAM
roles, tags, or policies.
References:
* Amazon EBS encryption
* Creating an encrypted EBS volume
* Encrypting an unencrypted EBS volume
QUESTION NO: 190
회사에서 인프라 모니터링 서비스를 실행합니다. 이 회사는 서비스가 고객 AWS 계정의
데이터를 모니터링할 수 있는 새로운 기능을 구축하고 있습니다. 새로운 기능은 고객
계정에서 AWS API를 호출하여 Amazon EC2 인스턴스를 설명하고 Amazon CloudWatch
지표를 읽습니다.
회사는 가장 안전한 방법으로 고객 계정에 대한 액세스 권한을 얻기 위해 무엇을 해야 합니까?
A. 고객이 회사 계정에 대한 읽기 전용 EC2 및 CloudWatch 권한과 신뢰 정책을 사용하여
계정에 1AM 역할을 생성하는지 확인하십시오.
B. 토큰 판매기를 구현하는 서버리스 API를 생성하여 읽기 전용 EC2 및 CloudWatch 권한이
있는 역할에 대한 임시 AWS 자격 증명을 제공합니다.
C. 고객이 읽기 전용 EC2 및 CloudWatch 권한이 있는 계정에서 오전 1시 사용자를
생성하는지 확인합니다. 비밀 관리 시스템에서 고객 액세스 및 비밀 키를 암호화하고
저장합니다.
D. 고객이 자신의 계정에서 Amazon Cognito 사용자를 생성하여 읽기 전용 EC2 및
CloudWatch 권한이 있는 1AM 역할을 사용하는지 확인하십시오. 암호 관리 시스템에서
129

IT Certification Guaranteed, The Easy Way!
Amazon Cognito 사용자 및 암호를 암호화하고 저장합니다.
Answer: A
Explanation:
By having customers create an IAM role with the necessary permissions in their own
accounts, the company can use AWS Identity and Access Management (IAM) to establish
cross-account access. The trust policy allows the company's AWS account to assume the
customer's IAM role temporarily, granting access to the specified resources (EC2 instances
and CloudWatch metrics) within the customer's account. This approach follows the principle
of least privilege, as the company only requests the necessary permissions and does not
require long-term access keys or user credentials from the customers.
QUESTION NO: 191
미디어 회사는 Amazon CloudFront 배포를 사용하여 인터넷을 통해 콘텐츠를 제공합니다. 이
회사는 프리미엄 고객만 미디어 스트림과 파일 콘텐츠에 액세스할 수 있기를 원합니다.
회사는 모든 콘텐츠를 Amazon S3 버킷에 저장합니다. 또한 회사는 영화 대여나 음악
다운로드와 같은 특정 목적을 위해 고객에게 주문형 콘텐츠를 제공합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. S3 서명 쿠키를 생성하고 프리미엄 고객에게 제공합니다.
B. CloudFront 서명 URL을 생성하고 프리미엄 고객에게 제공합니다.
C. 원본 액세스 제어(OAC)를 사용하여 프리미엄이 아닌 고객의 액세스를 제한합니다.
D. 프리미엄이 아닌 고객을 차단하기 위해 필드 수준 암호화를 생성하고 활성화합니다.
Answer: B
Explanation:
* CloudFront Signed URLs: These URLs allow you to provide limited access to content that is
being served through an Amazon CloudFront distribution. Signed URLs can be generated to
grant time- limited access to premium customers.
* Content Restriction:
* By using CloudFront signed URLs, you can control access to your media streams and file
content stored in S3.
* These URLs can be customized with an expiration time, ensuring that access is only
available for a specific period, which is useful for scenarios like movie rentals or music
downloads.
* Security and Flexibility:
* Signed URLs ensure that only authenticated users (premium customers) can access the
restricted content.
* This approach integrates seamlessly with CloudFront and S3, providing an efficient way to
manage access controls without additional overhead.
* Operational Efficiency: Using CloudFront signed URLs leverages AWS managed services
to handle the complexity of access control, reducing the need for custom implementation and
maintenance.
References:
* Serving Private Content with Signed URLs and Signed Cookies
QUESTION NO: 192
조사 회사는 미국 내 여러 지역에서 몇 년 동안 데이터를 수집했습니다. 이 회사는 3TB m
130

IT Certification Guaranteed, The Easy Way!
크기의 Amazon S3 버킷에서 데이터를 호스팅하고 점점 커지고 있습니다. 회사는 S3 버킷이
있는 유럽 마케팅 회사와 데이터를 공유하기 시작했습니다. 회사는 데이터 전송 비용을
가능한 한 낮게 유지하기를 원합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 회사의 S3 버킷에 요청자 지불 기능 구성
B. 회사의 S3 버킷에서 마케팅 회사의 S3 버킷 중 하나로 S3 교차 리전 복제를 구성합니다.
C. 마케팅 회사가 회사의 S3 버킷에 액세스할 수 있도록 마케팅 회사에 대한 교차 계정
액세스를 구성합니다.
D. S3 Intelligent-Tiering을 사용하도록 회사의 S3 버킷을 구성합니다. S3 버킷을 마케팅
회사의 S3 버킷 중 하나에 동기화합니다.
Answer: A
Explanation:
"Typically, you configure buckets to be Requester Pays buckets when you want to share data
but not incur charges associated with others accessing the data. For example, you might use
Requester Pays buckets when making available large datasets, such as zip code directories,
reference data, geospatial information, or web crawling data."
https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html
QUESTION NO: 193
비즈니스 애플리케이션은 Amazon EC2에서 호스팅되며 암호화된 객체 저장을 위해 Amazon
S3를 사용합니다. 최고 정보 보안 책임자(CIO)는 두 서비스 간의 애플리케이션 트래픽이 공용
인터넷을 통과해서는 안 된다고 지시했습니다.
솔루션 설계자는 규정 준수 요구 사항을 충족하기 위해 어떤 기능을 사용해야 합니까?
A. AWS 키 관리 서비스(AWS KMS)
B. VPC 엔드포인트
C. 프라이빗 서브넷
D. 가상 프라이빗 게이트웨이
Answer: B
Explanation:
https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html
QUESTION NO: 194
솔루션 설계자는 AWS에 배포되는 새 애플리케이션을 위한 클라우드 아키텍처를 설계하고
있습니다. 처리할 작업 수에 따라 필요에 따라 애플리케이션 노드를 추가 및 제거하면서
프로세스가 병렬로 실행되어야 합니다. 프로세서 응용 프로그램은 상태 비저장입니다. 솔루션
설계자는 응용 프로그램이 느슨하게 연결되어 있고 작업 항목이 영구적으로 저장되어 있는지
확인해야 합니다.
솔루션 설계자는 어떤 디자인을 사용해야 합니까?
A. 처리해야 하는 작업을 보낼 Amazon SNS 주제 생성 프로세서 애플리케이션으로 구성된
Amazon 머신 이미지(AMI) 생성 AMI를 사용하는 시작 구성 생성 시작 구성 세트를 사용하여
Auto Scaling 그룹 생성 CPU 사용량에 따라 노드를 추가 및 제거하기 위한 Auto Scaling
그룹의 조정 정책
B. 처리해야 하는 작업을 보관할 Amazon SQS 대기열 생성 프로세서 애플리케이션으로
구성된 Amazon 머신 이미지(AMI) 생성 AM을 사용하는 시작 구성 생성 시작 구성을 사용하여
Auto Scaling 그룹 생성 네트워크 사용량에 따라 노드를 추가 및 제거하도록 Auto Scaling
131

IT Certification Guaranteed, The Easy Way!
그룹에 대한 조정 정책 설정
C. 처리해야 하는 작업을 보관할 Amazon SQS 대기열 생성 프로세서 애플리케이션으로
구성된 Amazon 머신 이미지(AMI) 생성 AMI를 사용하는 시작 템플릿 생성 시작 템플릿을
사용하여 Auto Scaling 그룹 생성 Set SQS 대기열의 항목 수에 따라 노드를 추가 및 제거하는
Auto Scaling 그룹의 조정 정책
D. 처리해야 하는 작업을 보낼 Amazon SNS 주제 생성 프로세서 애플리케이션으로 구성된
Amazon 머신 이미지(AMI) 생성 AMI를 사용하는 시작 템플릿 생성 시작 템플릿을 사용하여
Auto Scaling 그룹 생성 Set SNS 주제에 게시된 메시지 수에 따라 노드를 추가 및 제거하는
Auto Scaling 그룹의 조정 정책
Answer: C
Explanation:
"Create an Amazon SQS queue to hold the jobs that needs to be processed. Create an
Amazon EC2 Auto Scaling group for the compute application. Set the scaling policy for the
Auto Scaling group to add and remove nodes based on the number of items in the SQS
queue" In this case we need to find a durable and loosely coupled solution for storing jobs.
Amazon SQS is ideal for this use case and can be configured to use dynamic scaling based
on the number of jobs waiting in the queue.
To configure this scaling you can use the backlog per instance metric with the target value
being the acceptable backlog per instance to maintain. You can calculate these numbers as
follows: Backlog per instance: To calculate your backlog per instance, start with the
ApproximateNumberOfMessages queue attribute to determine the length of the SQS queue
QUESTION NO: 195
한 회사가 AWS 클라우드에 수백 개의 Amazon EC2 Linux 기반 인스턴스를 보유하고
있습니다. 시스템 관리자는 공유 SSH 키를 사용하여 인스턴스를 관리했습니다. 최근 감사 후
회사의 보안 팀은 모든 공유 키를 제거하도록 명령했습니다. 솔루션 아키텍트는 EC2
인스턴스에 대한 보안 액세스를 제공하는 솔루션을 설계해야 합니다.
어떤 솔루션이 관리 비용을 가장 적게 들여 이 요구 사항을 충족할 수 있을까요?
A. Amazon Cognito 사용자 지정 권한 부여자를 사용하여 사용자를 인증합니다. AWS
Lambda 함수를 호출하여 임시 SSH 키를 생성합니다.
B. AWS 보안 토큰 서비스(AWS STS)를 사용하여 필요에 따라 일회용 SSH 키를 생성합니다.
C. 일련의 요새 인스턴스에 대한 공유 SSH 액세스를 허용합니다. 다른 모든 인스턴스가 요새
인스턴스에서만 SSH 액세스를 허용하도록 구성합니다.
D. AWS Systems Manager Session Manager를 사용하여 EC2 인스턴스에 연결합니다.
Answer: D
Explanation:
Session Manager is a fully managed AWS Systems Manager capability. With Session
Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances,
edge devices, on-premises servers, and virtual machines (VMs). You can use either an
interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI).
Session Manager provides secure and auditable node management without the need to open
inbound ports, maintain bastion hosts, or manage SSH keys. Session Manager also allows
you to comply with corporate policies that require controlled access to managed nodes, strict
security practices, and fully auditable logs with node access details, while providing end
users with simple one-click cross-platform access to your managed nodes.
132

IT Certification Guaranteed, The Easy Way!
https://docs.aws.amazon.com/systems-manager/latest/userguide
/session-manager.html
QUESTION NO: 196
회사에는 처리할 페이로드가 포함된 메시지를 보내는 발신자 애플리케이션과 페이로드가
포함된 메시지를 수신하기 위한 처리 애플리케이션이라는 두 가지 애플리케이션이 있습니다.
회사는 두 애플리케이션 간의 메시지를 처리하기 위해 AWS 서비스를 구현하려고 합니다.
발신자 애플리케이션은 매시간 약 1,000개의 메시지를 보낼 수 있습니다. 메시지를 처리하는
데 최대 2일이 걸릴 수 있습니다. 메시지가 처리에 실패하는 경우 나머지 메시지 처리에
영향을 주지 않도록 보관해야 합니다.
이러한 요구 사항을 충족하고 운영상 가장 효율적인 솔루션은 무엇입니까?
A. Redis 데이터베이스를 실행하는 Amazon EC2 인스턴스를 설정합니다. 인스턴스를
사용하도록 두 애플리케이션을 모두 구성합니다. 메시지를 각각 저장, 처리 및 삭제합니다.
B. Amazon Kinesis 데이터 스트림을 사용하여 발신자 애플리케이션으로부터 메시지를
수신합니다. 처리 애플리케이션을 Kinesis Client Library(KCL)와 통합합니다.
C. 발신자 및 프로세서 애플리케이션을 Amazon Simple Queue Service(Amazon SQS)
대기열과 통합합니다. 처리에 실패한 메시지를 수집하도록 배달 못한 편지 대기열을
구성합니다.
D. 처리할 알림을 받기 위해 Amazon Simple 알림 서비스(Amazon SNS) 주제에 대한 처리
애플리케이션을 구독합니다. SNS 주제에 쓰기 위해 발신자 애플리케이션을 통합합니다.
Answer: C
Explanation:
https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-
with-amazon-sqs- and-amazon-sns/
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-
dead-letter-queues.
html
QUESTION NO: 197
한 회사는 이전에 데이터 웨어하우스 솔루션을 AWS로 마이그레이션했습니다. 회사에는
AWS Direct Connect 연결도 있습니다. 회사 사무실 사용자는 시각화 도구를 사용하여 데이터
웨어하우스에 쿼리합니다. 데이터 웨어하우스에서 반환된 쿼리의 평균 크기는 50MB이고
시각화 도구에서 보낸 각 웹페이지는 약 500KB입니다. 데이터 웨어하우스에서 반환된 결과
세트는 캐시되지 않습니다.
회사에 가장 낮은 데이터 전송 송신 비용을 제공하는 솔루션은 무엇입니까?
A. 시각화 도구를 온프레미스에 호스팅하고 인터넷을 통해 직접 데이터 웨어하우스에
쿼리합니다.
B. 데이터 웨어하우스와 동일한 AWS 리전에 시각화 도구를 호스팅합니다. 인터넷을 통해
접속하세요.
C. 시각화 도구를 온프레미스에 호스팅하고 동일한 AWS 리전의 위치에서 Direct Connect
연결을 통해 직접 데이터 웨어하우스를 쿼리합니다.
D. 데이터 웨어하우스와 동일한 AWS 리전에서 시각화 도구를 호스팅하고 동일한 리전의
위치에서 Direct Connect 연결을 통해 액세스합니다.
Answer: D
Explanation:
133

IT Certification Guaranteed, The Easy Way!
https://aws.amazon.com/directconnect/pricing/
https://aws.amazon.com/blogs/aws/aws-data-transfer-prices-reduced/
QUESTION NO: 198
한 회사는 다중 계층 웹 애플리케이션에 Amazon ElastiCache를 사용할 계획입니다. 솔루션
아키텍트는 ElastiCache 클러스터용 캐시 VPC와 애플리케이션의 Amazon EC2 인스턴스용
앱 VPC를 생성합니다. 두 VPC 모두 us-east-1 리전에 있습니다.
솔루션 아키텍트는 애플리케이션의 EC2 인스턴스에 ElastiCache 클러스터에 대한 액세스를
제공하는 솔루션을 구현해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. VPC 간에 피어링 연결을 생성합니다. 두 VPC 모두에서 피어링 연결을 위한 라우팅 테이블
항목을 추가합니다. 애플리케이션 보안 그룹의 인바운드 연결을 허용하도록 ElastiCache
클러스터의 보안 그룹에 대한 인바운드 규칙을 구성합니다.
B. 전송 VPC를 생성합니다. Transit VPC를 통해 트래픽을 라우팅하도록 캐시 VPC 및 앱
VPC의 VPC 라우팅 테이블을 업데이트합니다. 애플리케이션 보안 그룹의 인바운드 연결을
허용하도록 ElastiCache 클러스터의 보안 그룹에 대한 인바운드 규칙을 구성합니다.
C. VPC 간에 피어링 연결을 생성합니다. 두 VPC 모두에서 피어링 연결을 위한 라우팅 테이블
항목을 추가합니다. 애플리케이션 보안 그룹의 인바운드 연결을 허용하도록 피어링 연결의
보안 그룹에 대한 인바운드 규칙을 구성합니다.
D. 전송 VPC를 생성합니다. Transit VPC를 통해 트래픽을 라우팅하도록 캐시 VPC 및 앱
VPC의 VPC 라우팅 테이블을 업데이트합니다. 애플리케이션 보안 그룹의 인바운드 연결을
허용하도록 Transit VPC의 보안 그룹에 대한 인바운드 규칙을 구성합니다.
Answer: A
Explanation:
Creating a peering connection between the VPCs allows the application's EC2 instances to
communicate with the ElastiCache cluster directly and efficiently. This is the most cost-
effective solution as it does not involve creating additional resources such as a Transit VPC,
and it does not incur additional costs for traffic passing through the Transit VPC. Additionally,
it is also more secure as it allows you to configure a more restrictive security group rule to
allow inbound connection from only the application's security group.
QUESTION NO: 199
한 회사가 Amazon RDS for MySQL에서 프로덕션 데이터베이스를 운영합니다. 이 회사는
보안 규정 준수를 위해 데이터베이스 버전을 업그레이드하려고 합니다. 데이터베이스에
중요한 데이터가 포함되어 있기 때문에 이 회사는 데이터를 잃지 않고 기능을 업그레이드하고
테스트할 수 있는 빠른 솔루션을 원합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. RDS 수동 스냅샷을 만듭니다. MySQL용 Amazon RDS의 새 버전으로 업그레이드합니다.
B. 네이티브 백업 및 복원을 사용합니다. 데이터를 업그레이드된 새로운 버전의 Amazon RDS
for MySQL로 복원합니다.
C. AWS Database Migration Service(AWS DMS)를 사용하여 데이터를 업그레이드된 새로운
버전의 Amazon RDS for MySQL로 복제합니다.
D. Amazon RDS Blue/Green 배포를 사용하여 프로덕션 변경 사항을 배포하고 테스트하세요.
Answer: D
Explanation:
134

IT Certification Guaranteed, The Easy Way!
Amazon RDS Blue/Green Deployments is the ideal solution for upgrading the database
version with minimal operational overhead and no data loss. Blue/Green Deployments allows
you to create a separate, fully managed "green" environment with the upgraded database
version. You can test the new version in the green environment while the "blue" environment
continues serving production traffic. Once testing is complete, you can seamlessly switch
traffic to the green environment without downtime.
This solution provides:
* Fast, non-disruptive upgrade: Traffic is only switched to the new environment after testing,
ensuring zero data loss.
* Minimal operational overhead: AWS handles the infrastructure management, reducing
manual intervention.
* Option A (Manual snapshot): This requires manual intervention and involves more
operational overhead.
* Option B (Native backup/restore): This approach is more labor-intensive and slower than
Blue/Green Deployments.
* Option C (DMS): AWS DMS adds unnecessary complexity for a simple version upgrade
when Blue
/Green Deployments can handle the task more efficiently.
AWS References:
* Amazon RDS Blue/Green Deployments
QUESTION NO: 200
어떤 회사가 사진을 저장하고 공유하는 애플리케이션을 운영합니다. 사용자는 사진을
Amazon S3 버킷에 업로드합니다. 사용자는 매일 약 150장의 사진을 업로드합니다. 이 회사는
각 새 사진의 썸네일을 만들고 두 번째 S3 버킷에 썸네일을 저장하는 솔루션을 설계하려고
합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. Amazon EventBridge 예약 규칙을 구성하여 장기 실행 Amazon EMR 클러스터에서 매분
스크립트를 호출합니다. 썸네일이 없는 사진에 대한 썸네일을 생성하도록 스크립트를
구성합니다. 썸네일을 두 번째 S3 버킷에 업로드하도록 스크립트를 구성합니다.
B. 항상 켜져 있는 메모리 최적화 Amazon EC2 인스턴스에서 매분 스크립트를 호출하도록
Amazon EventBridge 예약 규칙을 구성합니다. 썸네일이 없는 사진에 대한 썸네일을
생성하도록 스크립트를 구성합니다. 썸네일을 두 번째 S3 버킷에 업로드하도록 스크립트를
구성합니다.
C. 사용자가 애플리케이션에 새 사진을 업로드할 때마다 AWS Lambda 함수를 호출하도록 S3
이벤트 알림을 구성합니다. Lambda 함수를 구성하여 섬네일을 생성하고 두 번째 S3 버킷에
섬네일을 업로드합니다.
D. 사용자가 애플리케이션에 새 사진을 업로드할 때마다 AWS Lambda 함수를 호출하도록 S3
Storage Lens를 구성합니다. Lambda 함수를 구성하여 섬네일을 생성하고 두 번째 S3 버킷에
섬네일을 업로드합니다.
Answer: C
Explanation:
The most cost-effective and scalable solution for generating thumbnails when photos are
uploaded to an S3 bucket is to use S3 event notifications to trigger an AWS Lambda function.
This approach avoids the need for a long-running EC2 instance or EMR cluster, making it
135

IT Certification Guaranteed, The Easy Way!
highly cost-effective because Lambda only charges for the time it takes to process each
event.
* S3 Event Notifications: Automatically triggers the Lambda function when a new photo is
uploaded to the S3 bucket.
* AWS Lambda: A serverless compute service that scales automatically and only charges for
execution time, which makes it the most economical choice when dealing with periodic
events like photo uploads.
* The Lambda function can generate the thumbnail and upload it to a second S3 bucket,
fulfilling the requirement efficiently.
* Option A and Option B (EMR or EC2 with scheduled scripts)**: These are less cost-
effective as they involve continuously running infrastructure, which incurs unnecessary costs.
* Option D (S3 Storage Lens): S3 Storage Lens is a tool for storage analytics and is not
designed for event-based photo processing.
AWS References:
* Amazon S3 Event Notifications
* AWS Lambda Pricing
QUESTION NO: 201
Amazon S3 버킷에 업로드된 모든 객체가 암호화되도록 솔루션 아키텍트는 무엇을 해야
합니까?
A. PutObject에 s3 x-amz-acl 헤더 세트가 없는 경우 거부하도록 버킷 정책을 업데이트합니다.
B. PutObject에 비공개로 설정된 s3:x-amz-aci 헤더가 없는 경우 거부하도록 버킷 정책을
업데이트합니다.
C. PutObject에 true로 설정된 aws SecureTransport 헤더가 없는 경우 거부하도록 버킷 정책을
업데이트합니다.
D. PutObject에 x-amz-server-side-encryption 헤더 세트가 없는 경우 거부하도록 버킷 정책을
업데이트합니다.
Answer: D
Explanation:
https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-
amazon-s3/#:~:
text=Solution%20overview,console%2C%20CLI%2C%20or%20SDK.&text=To%20encrypt%
20an%
20object%20at,S3%2C%20or%20SSE%2DKMS.
QUESTION NO: 202
회사에는 Microsoft Windows 공유 파일 저장소가 필요한 온프레미스에서 실행되는 대규모
Microsoft SharePoint 배포가 있습니다. 회사는 이 워크로드를 AWS 클라우드로
마이그레이션하기를 원하며 다양한 스토리지 옵션을 고려하고 있습니다. 저장소 솔루션은
액세스 제어를 위해 고가용성 및 Active Directory와 통합되어야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon EFS 스토리지 구성 및 인증을 위한 Active Directory 도메인 설정
B. 두 가용 영역의 AWS Storage Gateway 타일 게이트웨이에 SMB Me 공유 생성
C. Amazon S3 버킷을 생성하고 볼륨으로 탑재하도록 Microsoft Windows Server를
구성합니다.
136

IT Certification Guaranteed, The Easy Way!
D. AWS에서 Windows 파일 서버용 Amazon FSx 파일 시스템 생성 및 인증을 위한 Active
Directory 도메인 설정
Answer: D
QUESTION NO: 203
회사는 2주 이내에 MySQL 데이터베이스를 온프레미스 데이터 센터에서 AWS로
마이그레이션해야 합니다. 데이터베이스 크기는 20TB입니다. 회사는 가동 중지 시간을
최소화하면서 마이그레이션을 완료하려고 합니다.
가장 비용 효율적으로 데이터베이스를 마이그레이션하는 솔루션은 무엇입니까?
A. AWS Snowball Edge Storage Optimized 디바이스를 주문합니다. AWS SCT(AWS Schema
Conversion Tool)와 함께 AWS Database Migration Service(AWS DMS)를 사용하여 지속적인
변경 사항을 복제하면서 데이터베이스를 마이그레이션합니다. Snowball Edge 디바이스를
AWS로 보내 마이그레이션을 완료하고 지속적인 복제를 계속합니다.
B. AWS Snowmobile 차량을 주문하세요. AWS SCT(AWS Schema Conversion Tool)와 함께
AWS Database Migration Service(AWS DMS)를 사용하여 지속적인 변경 사항에 따라
데이터베이스를 마이그레이션합니다. Snowmobile 차량을 AWS로 다시 보내 마이그레이션을
완료하고 지속적인 복제를 계속합니다.
C. GPU 디바이스가 포함된 AWS Snowball Edge Compute Optimized를 주문하세요. AWS
SCT(AWS Schema Conversion Tool)와 함께 AWS Database Migration Service(AWS DMS)를
사용하여 지속적인 변경 사항이 포함된 데이터베이스를 마이그레이션합니다. Snowball
디바이스를 AWS로 보내 마이그레이션을 완료하고 지속적인 복제를 계속합니다.
D. 데이터 센터와의 연결을 설정하려면 1GB 전용 AWS Direct Connect 연결을 주문하세요.
AWS SCT(AWS Schema Conversion Tool)와 함께 AWS Database Migration Service(AWS
DMS)를 사용하여 지속적인 변경 사항을 복제하면서 데이터베이스를 마이그레이션합니다.
Answer: A
Explanation:
This answer is correct because it meets the requirements of migrating a 20 TB MySQL
database within 2 weeks with minimal downtime and cost-effectively. The AWS Snowball
Edge Storage Optimized device has up to 80 TB of usable storage space, which is enough to
fit the database. The AWS Database Migration Service (AWS DMS) can migrate data from
MySQL to Amazon Aurora, Amazon RDS for MySQL, or MySQL on Amazon EC2 with
minimal downtime by continuously replicating changes from the source to the target. The
AWS Schema Conversion Tool (AWS SCT) can convert the source schema and code to a
format compatible with the target database. By using these services together, the company
can migrate the database to AWS with minimal downtime and cost. The Snowball Edge
device can be shipped back to AWS to finish the migration and continue the ongoing
replication until the database is fully migrated.
References:
* https://docs.aws.amazon.com/snowball/latest/developer-guide/device-differences.html
* https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html
*
https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/CHAP_Source.MySQ
L.htm
QUESTION NO: 204
137

IT Certification Guaranteed, The Easy Way!
Amazon EC2 인스턴스에 호스팅된 애플리케이션은 Amazon S3 버킷에 액세스해야 합니다.
트래픽은 인터넷을 통과해서는 안 됩니다. 솔루션 아키텍트는 이러한 요구 사항을 충족하기
위해 액세스를 어떻게 구성해야 합니까?
A. Amazon Route 53을 사용하여 프라이빗 호스팅 영역 생성
B. VPC에서 Amazon S3에 대한 게이트웨이 VPC 엔드포인트 설정
C. NAT 게이트웨이를 사용하여 S3 버킷에 액세스하도록 EC2 인스턴스를 구성합니다.
D. VPC와 S3 버킷 간에 AWS Site-to-Site VPN 연결을 설정합니다.
Answer: B
Explanation:
This option is the most efficient because it uses a gateway VPC endpoint for Amazon S3,
which provides reliable connectivity to Amazon S3 without requiring an internet gateway or a
NAT device for the VPC1. A gateway VPC endpoint routes traffic from the VPC to Amazon
S3 using a prefix list for the service and does not leave the AWS network2. This meets the
requirement of not traversing the internet. Option A is less efficient because it uses a private
hosted zone by using Amazon Route 53, which is a DNS service that allows you to create
custom domain names for your resources within your VPC3. However, this does not provide
connectivity to Amazon S3 without an internet gateway or a NAT device. Option C is less
efficient because it uses a NAT gateway to access the S3 bucket, which is a highly available,
managed Network Address Translation (NAT) service that enables instances in a private
subnet to connect to the internet or other AWS services, but prevents the internet from
initiating a connection with those instances4. However, this does not meet the requirement of
not traversing the internet. Option D is less efficient because it uses an AWS Site-to- Site
VPN connection between the VPC and the S3 bucket, which is a secure and encrypted
network connection between your on-premises network and your VPC. However, this does
not meet the requirement of not traversing the internet.
QUESTION NO: 205
병원은 Amazon S3 버킷에 환자 기록을 저장해야 합니다. 병원의 규정 준수 팀은 모든 보호
건강 정보(PHI)가 전송 중이거나 저장되지 않았는지 확인해야 합니다. 규정 준수 팀은 저장
데이터의 암호화 키를 관리해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Certificate Manager(ACM)에서 공개 SSL/TLS 인증서를 생성합니다. 인증서를
Amazon S3와 연결합니다. AWS KMS 키(SSE-KMS)로 서버 측 암호화를 사용하도록 각 S3
버킷에 대한 기본 암호화를 구성합니다. KMS 키를 관리할 규정 준수 팀을 할당합니다.
B. S3 버킷 정책에서 aws:SecureTransport 조건을 사용하여 HTTPS(TLS)를 통한 암호화된
연결만 허용합니다. S3 관리형 암호화 키(SSE-S3)로 서버 측 암호화를 사용하도록 각 S3
버킷에 대한 기본 암호화를 구성합니다. SSE-S3 키를 관리할 규정 준수 팀을 할당합니다.
C. S3 버킷 정책에서 aws:SecureTransport 조건을 사용하여 HTTPS(TLS)를 통한 암호화된
연결만 허용합니다. AWS KMS 키(SSE-KMS)로 서버 측 암호화를 사용하도록 각 S3 버킷에
대한 기본 암호화를 구성합니다. KMS 키를 관리할 규정 준수 팀을 할당합니다.
D. S3 버킷 정책에서 aws:SecureTransport 조건을 사용하여 HTTPS(TLS)를 통한 암호화된
연결만 허용합니다. Amazon Macie를 사용하여 Amazon S3에 저장된 민감한 데이터를
보호하세요. Macie를 관리할 규정 준수 팀을 할당합니다.
Answer: C
138

IT Certification Guaranteed, The Easy Way!
Explanation:
it allows the compliance team to manage the KMS keys used for server-side encryption,
thereby providing the necessary control over the encryption keys. Additionally, the use of the
"aws:SecureTransport" condition on the bucket policy ensures that all connections to the S3
bucket are encrypted in transit.
QUESTION NO: 206
회사에는 트랜잭션 데이터를 처리하는 온프레미스 MySQL 데이터베이스가 있습니다. 회사는
데이터베이스를 AWS 클라우드로 마이그레이션하고 있습니다. 마이그레이션된
데이터베이스는 해당 데이터베이스를 사용하는 회사의 애플리케이션과 호환성을 유지해야
합니다. 또한 마이그레이션된 데이터베이스는 수요가 증가하는 기간 동안 자동으로
확장되어야 합니다.
이러한 요구 사항을 충족하는 마이그레이션 솔루션은 무엇입니까?
A. 기본 MySQL 도구를 사용하여 데이터베이스를 MySQL용 Amazon RDS로
마이그레이션합니다. 탄력적 스토리지 확장을 구성합니다.
B. mysqldump 유틸리티를 사용하여 데이터베이스를 Amazon Redshift로
마이그레이션합니다. Amazon Redshift 클러스터에 대해 Auto Scaling을 활성화합니다.
C. AWS Database Migration Service(AWS DMS)를 사용하여 데이터베이스를 Amazon
Aurora로 마이그레이션합니다. Aurora Auto Scaling을 활성화합니다.
D. AWS Database Migration Service(AWS DMS)를 사용하여 데이터베이스를 Amazon
DynamoDB로 마이그레이션합니다. Auto Scaling 정책을 구성합니다.
Answer: C
Explanation:
To migrate a MySQL database to AWS with compatibility and scalability, Amazon Aurora is a
suitable option. Aurora is compatible with MySQL and can scale automatically with Aurora
Auto Scaling. AWS Database Migration Service (AWS DMS) can be used to migrate the
database from on-premises to Aurora with minimal downtime.
References:
* What Is Amazon Aurora?
* Using Amazon Aurora Auto Scaling with Aurora Replicas
* What Is AWS Database Migration Service?
QUESTION NO: 207
한 회사의 웹사이트에서는 매일 수백만 건의 요청을 처리하고 있으며 요청 건수는 계속
증가하고 있습니다. 솔루션 아키텍트는 웹 애플리케이션의 응답 시간을 개선해야 합니다.
솔루션 설계자는 Amazon DynamoDB 테이블에서 제품 세부 정보를 검색할 때
애플리케이션이 지연 시간을 줄여야 한다고 결정합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. DynamoDB Accelerator(DAX) 클러스터를 설정합니다. 모든 읽기 요청을 DAX를 통해
라우팅합니다.
B. DynamoDB 테이블과 웹 애플리케이션 사이에 Redis용 Amazon ElastiCache를
설정합니다. Redis를 통해 모든 읽기 요청을 라우팅합니다.
C. DynamoDB 테이블과 웹 애플리케이션 사이에 Memcached용 Amazon ElastiCache를
설정합니다. Memcached를 통해 모든 읽기 요청을 라우팅합니다.
D. 테이블에 Amazon DynamoDB 스트림을 설정하고, AWS Lambda가 테이블에서 읽고
139

IT Certification Guaranteed, The Easy Way!
Amazon ElastiCache를 채우도록 합니다. ElastiCache를 통해 모든 읽기 요청을
라우팅합니다.
Answer: A
Explanation:
it allows the company to improve the response time of the web application and decrease
latency when retrieving product details from the Amazon DynamoDB table. By setting up a
DynamoDB Accelerator (DAX) cluster, the company can use a fully managed, highly
available, in-memory cache for DynamoDB that delivers up to a 10x performance
improvement. By routing all read requests through DAX, the company can reduce the
number of read operations on the DynamoDB table and improve the user experience.
References:
* Amazon DynamoDB Accelerator (DAX)
* Using DAX with DynamoDB
QUESTION NO: 208
한 회사에서는 Amazon API Gateway REST API와 AWS Lambda 함수를 사용하여 API를
개발했습니다.
API는 전 세계 사용자에게 정적 및 동적 콘텐츠를 제공합니다. 이 회사는 API 요청에 대한
콘텐츠 전송 대기 시간을 줄이고자 합니다.
옵션:
A. REST API를 엣지 최적화 API 엔드포인트로 배포합니다. 캐싱을 활성화합니다. API
정의에서 콘텐츠 인코딩을 활성화하여 전송 중인 애플리케이션 데이터를 압축합니다.
B. REST API를 지역 API 엔드포인트로 배포합니다. 캐싱을 활성화합니다. API 정의에서
콘텐츠 인코딩을 활성화하여 전송 중인 애플리케이션 데이터를 압축합니다.
C. REST API를 엣지 최적화 API 엔드포인트로 배포합니다. 캐싱을 활성화합니다. Lambda
함수에 대한 예약된 동시성을 구성합니다.
D. REST API를 지역 API 엔드포인트로 배포합니다. 캐싱을 활성화합니다. Lambda 함수에
대한 예약된 동시성을 구성합니다.
Answer: A
Explanation:
* A. Edge-optimized API + Caching: Reduces latency by using Amazon CloudFront for edge
locations and enables caching for dynamic content. Compression reduces data transfer
latency.
* B. Regional API + Caching: Increases latency for global users due to the lack of edge
locations.
* C. Edge-optimized API + Reserved Concurrency: Reserved concurrency ensures Lambda
availability but does not address latency for dynamic content.
* D. Regional API + Reserved Concurrency: Lacks edge optimization, increasing latency for
global users.
References: Amazon API Gateway
QUESTION NO: 209
회사는 AWS Organizations를 사용하여 여러 AWS 계정 내에서 워크로드를 실행합니다. 태그
지정 정책은 회사가 태그를 생성할 때 AWS 리소스에 부서 태그를 추가합니다.
회계팀은 Amazon EC2 소비에 대한 지출을 결정해야 합니다. 회계팀은 AWS 계정에 관계없이
140

IT Certification Guaranteed, The Easy Way!
어느 부서가 비용을 담당하는지 결정해야 합니다. 회계팀은 조직 내 모든 AWS 계정에 대해
AWS Cost Explorer에 액세스할 수 있고 AWS의 모든 보고서에 액세스해야 합니다. 비용
탐색기.
운영상 가장 효율적인 방식으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 조직 마스터 계정 청구 콘솔에서 부서라는 사용자 정의 비용 할당 태그를 활성화합니다.
Cost Explorer에서 태그 이름별로 그룹화하여 하나의 비용 보고서를 생성하고 EC2로
필터링합니다.
B. 조직 마스터 계정 결제 콘솔에서 Department라는 AWS 정의 비용 할당 태그를
활성화합니다. Cost Explorer에서 태그 이름별로 그룹화하여 하나의 비용 보고서를 생성하고
EC2별로 필터링합니다.
C. 조직 회원 계정 청구 콘솔에서 부서라는 사용자 정의 비용 할당 태그를 활성화합니다. Cost
Explorer에서 태그 이름별로 그룹화하여 하나의 비용 보고서를 생성하고 EC2로
필터링합니다.
D. 조직 회원 계정 결제 콘솔에서 Department라는 AWS 정의 비용 할당 태그를 활성화합니다.
Cost Explorer에서 태그 이름별로 그룹화하고 EC2별로 필터링하여 하나의 비용 보고서를
생성합니다.
Answer: B
Explanation:
This solution meets the following requirements:
* It is operationally efficient, as it only requires one activation of the cost allocation tag and
one creation of the cost report from the management account, which has access to all the
member accounts' data and billing preferences.
* It is consistent, as it uses the AWS-defined cost allocation tag named department, which is
automatically applied to resources when the company creates tags using the tagging policy
enforced by AWS Organizations. This ensures that the tag name and value are the same
across all the resources and accounts, and avoids any discrepancies or errors that might
arise from user-defined tags.
* It is informative, as it creates one cost report in Cost Explorer grouping by the tag name,
and filters by EC2. This allows the accounting team to see the breakdown of EC2
consumption and costs by department, regardless of the AWS account. The team can also
use other features of Cost Explorer, such as charts, filters, and forecasts, to analyze and
optimize the spending.
References:
* Using AWS cost allocation tags - AWS Billing
* User-defined cost allocation tags - AWS Billing
* Cost Tagging and Reporting with AWS Organizations
QUESTION NO: 210
한 회사는 PostgreSQL Singfe-AZ DB 인스턴스용 Amazon RDS에 모든 주문을 저장하는
온라인 쇼핑 애플리케이션을 호스팅합니다. 경영진은 C^ilure의 단일 지점을 제거하기를
원하며 솔루션 설계자에게 애플리케이션 코드를 변경할 필요 없이 데이터베이스 가동 중지
시간을 최소화하는 접근 방식을 권장하도록 요청했습니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 데이터베이스 인스턴스를 수정하고 다중 AZ 옵션을 지정하여 기존 데이터베이스
인스턴스를 다중 AZ 배포로 변환합니다.
141

IT Certification Guaranteed, The Easy Way!
B. 새로운 RDS 다중 AZ 배포를 생성합니다. 현재 RDS 인스턴스의 스냅샷을 만들고 해당
스냅샷을 사용하여 새 다중 AZ 배포를 복원합니다.
C. 다른 가용 영역에 PostgreSQL 데이터베이스의 읽기 전용 복제본을 생성합니다. Amazon
Route 53 가중치 기반 레코드 세트를 사용하여 데이터베이스 전체에 요청을 분산시킵니다.
D. PostgreSQL용 RDS 데이터베이스를 최소 그룹 크기가 2인 Amazon EC2 Auto Scaling
그룹에 배치합니다. Amazon Route 53 가중치 기반 레코드 세트를 사용하여 인스턴스 전체에
요청을 분산시킵니다.
Answer: A
Explanation:
https://aws.amazon.com/rds/features/multi-az/ To convert an existing Single-AZ DB Instance
to a Multi-AZ deployment, use the "Modify" option corresponding to your DB Instance in the
AWS Management Console.
QUESTION NO: 211
회사의 애플리케이션은 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스에서
실행됩니다. 인스턴스는 여러 가용 영역에 걸쳐 Amazon EC2 Auto Scaling 그룹에서
실행됩니다. 매월 첫날 자정. 월말 재무 계산 목욕이 실행되면 응용 프로그램이 훨씬
느려집니다. 이로 인해 EC2 인스턴스의 CPU 사용률이 즉시 100%에 도달하여
애플리케이션이 중단됩니다.
애플리케이션이 워크로드를 처리하고 가동 중지 시간을 방지할 수 있도록 솔루션 설계자는
무엇을 권장해야 합니까?
A. ALB에서 Amazon CloudFront 배포를 구성합니다.
B. CPU 사용률을 기반으로 EC2 Auto Scaling 단순 조정 정책을 구성합니다.
C. 월별 일정에 따라 EC2 Auto Scaling 예약 조정 정책을 구성합니다.
D. EC2 인스턴스에서 일부 워크로드를 제거하도록 Amazon ElasticCache를 구성합니다.
Answer: C
Explanation:
Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule is
the best option because it allows for the proactive scaling of the EC2 instances before the
monthly batch run begins. This will ensure that the application is able to handle the increased
workload without experiencing downtime. The scheduled scaling policy can be configured to
increase the number of instances in the Auto Scaling group a few hours before the batch run
and then decrease the number of instances after the batch run is complete. This will ensure
that the resources are available when needed and not wasted when not needed. The most
appropriate solution to handle the increased workload during the monthly batch run and avoid
downtime would be to configure an EC2 Auto Scaling scheduled scaling policy based on the
monthly schedule.
https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-scheduled-
scaling.html
QUESTION NO: 212
회사는 Amazon RDS 데이터베이스를 기반으로 하는 Amazon EC2에서 매우 민감한
애플리케이션을 실행하고 있습니다. 규정 준수 규정에 따라 모든 개인 식별 정보(Pll)는 저장
시 암호화되어야 합니다.
인프라에 대한 변경 사항을 최소화하면서 이 요구 사항을 충족하기 위해 솔루션 설계자는
142

IT Certification Guaranteed, The Easy Way!
어떤 솔루션을 권장해야 합니까?
A. AWS Certificate Manager를 배포하여 인증서 생성 인증서를 사용하여 데이터베이스 볼륨
암호화
B. AWS CloudHSM을 배포합니다. 암호화 키를 생성하고 해당 키를 사용하여 데이터베이스
볼륨을 암호화합니다.
C. AWS Key Management Service {AWS KMS) 키를 사용하여 SSL 암호화를 구성하여
데이터베이스 볼륨을 암호화합니다.
D. 인스턴스 및 데이터베이스 볼륨을 암호화하도록 AWS KMS(AWS Key Management
Service) 키를 사용하여 Amazon Elastic Block Store(Amazon EBS) 암호화 및 Amazon RDS
암호화를 구성합니다.
Answer: D
Explanation:
* EBS Encryption:
* Default EBS Encryption: Can be enabled for new EBS volumes.
* Use of AWS KMS: Specify AWS KMS keys to handle encryption and decryption of data
transparently.
* Amazon RDS Encryption:
* RDS Encryption: Encrypts the underlying storage for RDS instances using AWS KMS.
* Configuration: Enable encryption when creating the RDS instance or modify an existing
instance to enable encryption.
* Least Amount of Changes:
* Both EBS and RDS support seamless encryption with AWS KMS, requiring minimal
changes to the existing infrastructure.
* Enables compliance with regulatory requirements without modifying the application.
* Operational Efficiency: Using AWS KMS for both EBS and RDS ensures a consistent,
managed approach to encryption, simplifying key management and enhancing security.
References:
* Amazon EBS Encryption
* Amazon RDS Encryption
* AWS Key Management Service
QUESTION NO: 213
한 회사는 Amazon EC2 인스턴스에서 실행되는 지연 시간에 민감한 애플리케이션을 위해
인메모리 데이터베이스를 실행하려고 합니다. 이 애플리케이션은 분당 100,000건 이상의
트랜잭션을 처리하며 높은 네트워크 처리량이 필요합니다. 솔루션 설계자는 데이터 전송
비용을 최소화하는 비용 효율적인 네트워크 설계를 제공해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 동일한 AWS 리전 내의 동일한 가용 영역에서 모든 EC2 인스턴스를 시작합니다. EC2
인스턴스를 시작할 때 클러스터 전략으로 배치 그룹을 지정합니다.
B. 동일한 AWS 리전 내의 다양한 가용 영역에서 모든 EC2 인스턴스를 시작합니다. EC2
인스턴스를 시작할 때 파티션 전략으로 배치 그룹을 지정합니다.
C. Auto Scaling 그룹을 배포하여 네트워크 사용률 목표에 따라 다양한 가용 영역에서 EC2
인스턴스를 시작합니다.
D. 다양한 가용 영역에서 EC2 인스턴스를 시작하기 위한 단계적 조정 정책을 사용하여 Auto
Scaling 그룹을 배포합니다.
143

IT Certification Guaranteed, The Easy Way!
Answer: A
Explanation:
* Launching instances within a single AZ and using a cluster placement group provides the
lowest network latency and highest bandwidth between instances. This maximizes
performance for an in-memory database and high-throughput application.
* Communications between instances in the same AZ and placement group are free,
minimizing data transfer charges. Inter-AZ and public IP traffic can incur charges.
* A cluster placement group enables the instances to be placed close together within the AZ,
allowing the high network throughput required. Partition groups span AZs, reducing
bandwidth.
* Auto Scaling across zones could launch instances in AZs that increase data transfer
charges. It may reduce network throughput, impacting performance.
QUESTION NO: 214
솔루션 설계자는 Amazon S3 버킷을 저장용으로 사용하여 문서 검토 애플리케이션을
구현하고 있습니다. 솔루션은 실수로 문서가 삭제되는 것을 방지하고 문서의 모든 버전을
사용할 수 있도록 보장해야 합니다. 사용자는 문서를 다운로드, 수정, 업로드할 수 있어야
합니다.
이러한 요구 사항을 충족하려면 어떤 조치 조합을 취해야 합니까? (2개를 선택하세요.)
A. 읽기 전용 버킷 ACL을 활성화합니다.
B. 버킷의 버전 관리를 활성화합니다.
C. IAM 정책을 버킷에 연결합니다.
D. 버킷에서 MFA 삭제를 활성화합니다.
E. AWS KMS를 사용하여 버킷을 암호화합니다.
Answer: B D
Explanation:
Versioning is a feature of Amazon S3 that allows users to keep multiple versions of the same
object in a bucket. It can help prevent accidental deletion of the documents and ensure that
all versions of the documents are available1. MFA Delete is a feature of Amazon S3 that
adds an extra layer of security by requiring two forms of authentication to delete a version or
change the versioning state of a bucket. It can help prevent unauthorized or accidental
deletion of the documents2. By enabling both versioning and MFA Delete on the bucket, the
solution can meet the requirements.
A: Enable a read-only bucket ACL. This solution will not meet the requirement of allowing
users to download, modify, and upload documents, as a read-only bucket ACL will prevent
write access to the bucket3.
C: Attach an IAM policy to the bucket. This solution will not meet the requirement of
preventing accidental deletion of the documents and ensuring that all versions of the
documents are available, as an IAM policy is used to grant or deny permissions to users or
roles, not to enable versioning or MFA Delete4.
E: Encrypt the bucket using AWS KMS. This solution will not meet the requirement of
preventing accidental deletion of the documents and ensuring that all versions of the
documents are available, as encrypting the bucket using AWS KMS is a method of protecting
data at rest, not enabling versioning or MFA Delete.
Reference URL: https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html
144

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 215
솔루션 설계자가 애플리케이션을 생성 중입니다. 애플리케이션은 VPC의 여러 가용 영역에
걸쳐 프라이빗 서브넷의 Amazon EC2 인스턴스에서 실행됩니다. EC2 인스턴스는 기밀
정보가 포함된 대용량 파일에 자주 액세스합니다. 이러한 파일은 처리를 위해 Amazon S3
버킷에 저장됩니다. 솔루션 설계자는 데이터 전송 비용을 최소화하기 위해 네트워크
아키텍처를 최적화해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. VPC에서 Amazon S3에 대한 게이트웨이 엔드포인트를 생성합니다. 프라이빗 서브넷의
라우팅 테이블에서 게이트웨이 엔드포인트에 대한 항목을 추가합니다.
B. 퍼블릭 서브넷에 단일 NAT 게이트웨이를 생성합니다. 프라이빗 서브넷의 라우팅
테이블에서 NAT 게이트웨이를 가리키는 기본 경로를 추가합니다.
C. VPC에서 Amazon S3에 대한 AWS PrivateLink 인터페이스 엔드포인트를 생성합니다.
프라이빗 서브넷의 라우팅 테이블에서 인터페이스 엔드포인트에 대한 항목을 추가합니다.
D. 퍼블릭 서브넷의 각 가용 영역에 대해 하나의 NAT 게이트웨이를 생성합니다. 프라이빗
서브넷의 각 경로 레이블에 동일한 가용 영역의 NAT 게이트웨이를 가리키는 기본 경로를
추가합니다.
Answer: A
Explanation:
* Understanding the Requirement: The application running on EC2 instances in private
subnets needs frequent access to large confidential files stored in S3, minimizing data
transfer costs.
* Analysis of Options:
* Gateway Endpoint for S3: Provides a secure, scalable, and cost-effective way for instances
in private subnets to access S3 without using the internet or NAT gateways, thus minimizing
data transfer costs.
* Single NAT Gateway: Incurs additional costs for data transfer through the NAT gateway,
which is not cost-effective.
* PrivateLink Interface Endpoint for S3: Primarily used for accessing AWS services over a
private connection but is more complex and costly compared to a gateway endpoint for S3.
* Multiple NAT Gateways: Increases costs significantly and adds complexity without offering
the cost benefits of a gateway endpoint.
* Best Solution:
* Gateway Endpoint for S3: This solution provides the required access with the least data
transfer costs and minimal complexity.
References:
* VPC Endpoints for Amazon S3
* Gateway Endpoints
QUESTION NO: 216
개발팀은 다른 팀이 액세스할 수 있는 웹사이트를 호스팅해야 합니다. 웹 사이트 콘텐츠는
HTML, CSS, 클라이언트 측 JavaScript 및 이미지로 구성됩니다. 웹 사이트를 호스팅하는 데
가장 비용 효율적인 방법은 무엇입니까?
A. 웹사이트를 컨테이너화하고 AWS Fargate에서 호스팅합니다.
B. Amazon S3 버킷을 생성하고 거기에서 웹사이트를 호스팅합니다.
145

IT Certification Guaranteed, The Easy Way!
C. Amazon EC2 인스턴스에 웹 서버를 배포하여 웹 사이트를 호스팅합니다.
D. Express js 프레임워크를 사용하는 AWS Lambda 대상으로 Application Load Balancer를
구성합니다.
Answer: B
Explanation:
In Static Websites, Web pages are returned by the server which are prebuilt.
They use simple languages such as HTML, CSS, or JavaScript.
There is no processing of content on the server (according to the user) in Static Websites.
Web pages are returned by the server with no change therefore, static Websites are fast.
There is no interaction with databases.
Also, they are less costly as the host does not need to support server-side processing with
different languages.
In Dynamic Websites, Web pages are returned by the server which are processed during
runtime means they are not prebuilt web pages but they are built during runtime according to
the user's demand.
These use server-side scripting languages such as PHP, Node.js, ASP.NET and many more
supported by the server.
So, they are slower than static websites but updates and interaction with databases are
possible.
QUESTION NO: 217
한 회사는 가까운 미래에 급속한 성장을 기대하고 있습니다. 솔루션 아키텍트는 기존
사용자를 구성하고 AWS의 새 사용자에게 권한을 부여해야 합니다. 솔루션 설계자는 1AM
그룹을 만들기로 결정했습니다. 솔루션 설계자는 부서를 기반으로 1AM 그룹에 새 사용자를
추가합니다.
새로운 사용자에게 권한을 부여하는 가장 안전한 방법은 무엇입니까?
A. 서비스 제어 정책(SCP)을 적용하여 액세스 권한을 관리합니다.
B. 최소 권한 권한이 있는 IAM 역할을 생성합니다. 1AM 그룹에 역할을 연결합니다.
C. 최소 권한 권한을 부여하는 IAM 정책을 생성합니다. 정책을 1AM 그룹에 연결합니다.
D. 오전 1시 역할을 만듭니다. 최대 권한을 정의하는 권한 경계와 역할을 연결합니다.
Answer: C
Explanation:
An IAM policy is a document that defines the permissions for an IAM identity (such as a user,
group, or role).
You can use IAM policies to grant permissions to existing users and groups based on
department. You can create an IAM policy that grants least privilege permission, which
means that you only grant the minimum permissions required for the users to perform their
tasks. You can then attach the policy to the IAM groups, which will apply the policy to all the
users in those groups. This solution will reduce operational costs and simplify configuration
and management of permissions. References: https://docs.aws.amazon.com/IAM/latest
/UserGuide/access_policies.html
QUESTION NO: 218
Amazon EC2 인스턴스는 새 VPC의 프라이빗 서브넷에 있습니다. 이 서브넷에는 아웃바운드
인터넷 액세스가 없지만 EC2 인스턴스에는 외부 공급업체로부터 월간 보안 업데이트를
146

IT Certification Guaranteed, The Easy Way!
다운로드할 수 있는 기능이 필요합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 인터넷 게이트웨이를 생성하고 이를 VPC에 연결합니다. 인터넷 게이트웨이를 기본 경로로
사용하도록 프라이빗 서브넷 라우팅 테이블을 구성합니다.
B. NAT 게이트웨이를 생성하고 이를 퍼블릭 서브넷에 배치합니다. NAT 게이트웨이를 기본
경로로 사용하도록 프라이빗 서브넷 라우팅 테이블을 구성합니다.
C. NAT 인스턴스를 생성하고 EC2 인스턴스가 있는 동일한 서브넷에 배치합니다. NAT
인스턴스를 기본 경로로 사용하도록 프라이빗 서브넷 라우팅 테이블을 구성합니다.
D. 인터넷 게이트웨이를 생성하고 이를 VPC에 연결합니다. NAT 인스턴스를 생성하고 이를
EC2 인스턴스가 있는 동일한 서브넷에 배치합니다. 인터넷 게이트웨이를 기본 경로로
사용하도록 프라이빗 서브넷 라우팅 테이블을 구성합니다.
Answer: B
Explanation:
This approach will allow the EC2 instance to access the internet and download the monthly
security updates while still being located in a private subnet. By creating a NAT gateway and
placing it in a public subnet, it will allow the instances in the private subnet to access the
internet through the NAT gateway. And then, configure the private subnet route table to use
the NAT gateway as the default route. This will ensure that all outbound traffic is directed
through the NAT gateway, allowing the EC2 instance to access the internet while still
maintaining the security of the private subnet.
QUESTION NO: 219
회사에서는 맨 웹 애플리케이션 중 하나의 콘텐츠에 대한 액세스를 제한하고 AWS에서
사용할 수 있는 인증 기술을 사용하여 콘텐츠를 보호하려고 합니다. 회사는 서버리스
아키텍처를 구현하여 100명 미만의 사용자를 위한 인증 솔루션을 종료하려고 합니다.
솔루션은 기본 웹 애플리케이션과 통합되어야 하며 웹 콘텐츠를 전 세계적으로 제공해야
합니다. 또한 솔루션은 회사의 사용자 기반이 증가함에 따라 확장되는 동시에 가능한 가장
낮은 로그인 대기 시간을 제공해야 합니다.
A. Amazon Cognito 인증을 사용합니다. Lambda#Edge tor 인증 사용 Amazon CloudFront
10을 사용하여 전 세계적으로 웹 애플리케이션 제공
B. Microsoft Active Directory 인증을 위해 AWS Directory Service를 사용합니다. 인증을 위해
AWS Lambda를 사용합니다. Application Load Balancer를 사용하여 웹 애플리케이션을
전역적으로 제공합니다.
C. 인증을 위해 Amazon Cognito를 사용합니다. AWS Lambda to 인증을 사용합니다. Amazon
S3 Transfer Acceleration 10을 사용하여 웹 애플리케이션을 전 세계적으로 제공합니다.
D. 인증을 위해 Microsoft Active Directory용 AWS Directory Service를 사용합니다. 인증을
위해 Lambda@Edge를 사용합니다. AWS Elastic Beanstalk를 사용하여 웹 애플리케이션을
제공합니다.
Answer: A
Explanation:
https://aws.amazon.com/blogs/networking-and-content-delivery/adding-http-security-headers-
using- lambdaedge-and-amazon-cloudfront/ Amazon CloudFront is a global content delivery
network (CDN) service that can securely deliver web content, videos, and APIs at scale. It
integrates with Cognito for authentication and with Lambda@Edge for authorization, making
147

IT Certification Guaranteed, The Easy Way!
it an ideal choice for serving web content globally. Lambda@Edge is a service that lets you
run AWS Lambda functions globally closer to users, providing lower latency and faster
response times. It can also handle authorization logic at the edge to secure content in
CloudFront. For this scenario, Lambda@Edge can provide authorization for the web
application while leveraging the low-latency benefit of running at the edge.
QUESTION NO: 220
회사는 AWS로 마이그레이션하고 애플리케이션에 Amazon EC2 온디맨드 인스턴스를 사용할
계획입니다. 마이그레이션 테스트 단계에서 기술 팀은 애플리케이션이 완전히 생산되기 위해
메모리를 실행하고 로드하는 데 오랜 시간이 걸린다는 사실을 관찰했습니다.
다음 테스트 단계에서 애플리케이션 실행 시간을 단축할 솔루션은 무엇입니까?
A. 두 개 이상의 EC2 온디맨드 인스턴스를 시작합니다. Auto Scaling 기능을 활성화하고 다음
테스트 단계에서 EC2 온디맨드 인스턴스를 사용할 수 있도록 하십시오.
B. EC2 스팟 인스턴스를 시작하여 애플리케이션을 지원하고 다음 테스트 단계에서 사용할 수
있도록 애플리케이션을 확장합니다.
C. 최대 절전 모드를 활성화한 상태에서 EC2 온디맨드 인스턴스를 시작합니다. 다음 테스트
단계에서 EC2 Auto Scaling 웜 풀을 구성합니다.
D. 용량 예약을 통해 EC2 온디맨드 인스턴스를 시작합니다. 다음 테스트 단계에서 추가 EC2
인스턴스를 시작하십시오.
Answer: C
Explanation:
The solution that will reduce the launch time of the application during the next testing phase
is to launch the EC2 On-Demand Instances with hibernation turned on and configure EC2
Auto Scaling warm pools. This solution allows the application to resume from a hibernated
state instead of starting from scratch, which can save time and resources. Hibernation
preserves the memory (RAM) state of the EC2 instances to the root EBS volume and then
stops the instances. When the instances are resumed, they restore their memory state from
the EBS volume and become productive quickly. EC2 Auto Scaling warm pools can be used
to maintain a pool of pre-initialized instances that are ready to scale out when needed. Warm
pools can also support hibernated instances, which can further reduce the launch time and
cost of scaling out.
The other solutions are not as effective as the first one because they either do not reduce the
launch time, do not guarantee availability, or do not use On-Demand Instances as required.
Launching two or more EC2 On- Demand Instances with auto scaling features does not
reduce the launch time of the application, as each instance still has to go through the
initialization process. Launching EC2 Spot Instances does not guarantee availability, as Spot
Instances can be interrupted by AWS at any time when there is a higher demand for
capacity. Launching EC2 On-Demand Instances with Capacity Reservations does not reduce
the launch time of the application, as it only ensures that there is enough capacity available
for the instances, but does not pre- initialize them.
References:
* Hibernating your instance - Amazon Elastic Compute Cloud
* Warm pools for Amazon EC2 Auto Scaling - Amazon EC2 Auto Scaling
QUESTION NO: 221
148

IT Certification Guaranteed, The Easy Way!
한 회사에서는 승인된 Amazon Machine Images(AMI)의 사용을 제한하기 위해 개발팀에 대한
새로운 보안 규정 준수 요구 사항을 구현하려고 합니다.
이 회사는 모든 Amazon EC2 인스턴스에 대해 승인된 운영 체제와 소프트웨어에만 액세스를
제공하고자 합니다. 이 회사는 솔루션이 EC2 인스턴스를 시작하기 위한 리드 타임을
최소화하기를 원합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 승인된 AMI로 시작된 EC2 인스턴스만 포함하는 AWS Service Catalog를 사용하여
포트폴리오를 만듭니다. 필요한 모든 소프트웨어가 AMI에 사전 설치되어 있는지 확인합니다.
개발자가 포트폴리오를 사용할 수 있도록 필요한 권한을 만듭니다.
B. EC2 Image Builder를 사용하여 승인된 운영 체제와 소프트웨어가 포함된 AMI를 만듭니다.
개발자에게 해당 AMI에 대한 액세스 권한을 부여하여 EC2 인스턴스를 시작합니다.
C. 승인된 운영 체제를 포함하는 AMI를 만듭니다. 개발자에게 승인된 AMI를 사용하도록
지시합니다. 새 EC2 인스턴스가 시작될 때 AWS Systems Manager 스크립트를 실행하기
위한 Amazon EventBridge 규칙을 만듭니다. 리포지토리에서 필요한 소프트웨어를
설치하도록 스크립트를 구성합니다.
D. 승인되지 않은 AMI로 EC2 인스턴스 시작을 감지하는 AWS Config 규칙을 만듭니다. 해당
인스턴스를 종료하고 승인된 AMI로 인스턴스를 다시 시작하는 수정 규칙을 연결합니다. AWS
Systems Manager를 사용하여 EC2 인스턴스 시작 시 승인된 소프트웨어를 자동으로
설치합니다.
Answer: A
Explanation:
AWS Service Catalog is designed to allow organizations to manage a catalog of approved
products (including AMIs) that users can deploy. By creating a portfolio that contains only
EC2 instances launched with preapproved AMIs, the company can enforce compliance with
the approved operating system and software for all EC2 instances. Service Catalog also
streamlines the process of launching EC2 instances, reducing the lead time while ensuring
that developers use only the approved configurations.
* Option B (EC2 Image Builder): While EC2 Image Builder helps in creating and managing
AMIs, it doesn't provide the enforcement mechanism that Service Catalog does.
* Option C (EventBridge rule and Systems Manager script): This solution is reactive and
involves more operational complexity compared to Service Catalog.
* Option D (AWS Config rule): This option is reactive (it terminates non-compliant instances
after launch) and introduces additional operational overhead.
AWS References:
* AWS Service Catalog
QUESTION NO: 222
회사에 고객을 위한 모바일 앱이 있습니다. 앱의 데이터는 민감하며 저장 시 암호화되어야
합니다. 회사는 AWS Key Management Service(AWS KMS)를 사용합니다. 회사에는 실수로
KMS 키가 삭제되는 것을 방지하는 솔루션이 필요합니다. 솔루션은 Amazon Simple 알림을
사용해야 합니다. 사용자가 KMS 키를 삭제하려고 시도할 때 관리자에게 이메일 알림을
보내는 서비스(Amazon SNS) 어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을
충족합니까?
A. 사용자가 KMS 키 삭제를 시도할 때 반응하는 Amazon EventBndge 규칙을 생성합니다.
KMS 키 삭제를 취소하는 AWS Config 규칙을 구성합니다. AWS Config 규칙을 EventBridge
149

IT Certification Guaranteed, The Easy Way!
규칙의 대상으로 추가합니다. SNS 생성 관리자에게 알리는 주제
B. KMS 키 삭제를 방지하는 사용자 지정 논리가 있는 AWS Lambda 함수를 생성합니다.
사용자가 KMS 키를 삭제하려고 할 때 활성화되는 Amazon CloudWatch 경보를 생성합니다.
작업이 수행됩니다. SNS 주제 생성 관리자에게 알리는 SNS 메시지를 게시하도록
EventBndge 규칙을 구성합니다.
C. KMS DeleteKey 작업이 수행될 때 반응하는 Amazon EventBndge 규칙을 생성합니다.
AWS 시스템 관리자 자동화 Runbook을 시작하도록 규칙을 구성합니다. KMS 키 삭제를
취소하도록 Runbook을 구성합니다. SNS 주제 생성 EventBndge 규칙을 구성합니다.
관리자에게 알리는 SNS 메시지를 게시합니다.
D. AWS CloudTrail 추적 생성 새 Amazon CloudWatch 로그 그룹에 로그를 전달하는 추적
구성 CloudWatch 로그 그룹에 대한 지표 필터를 기반으로 CloudWatch 경보 생성 Amazon
SNS를 사용하여 다음과 같은 경우 관리자에게 알리도록 경보를 구성합니다. KMS DeleteKey
작업이 수행됩니다.
Answer: C
Explanation:
This solution meets the requirements with the least operational overhead because it uses
AWS services that are fully managed and scalable. The EventBridge rule can detect the
DeleteKey operation from the AWS KMS API and trigger the Systems Manager Automation
runbook, which can execute a predefined workflow to cancel the key deletion. The
EventBridge rule can also publish an SNS message to the topic that sends an email
notification to the administrators. This way, the company can prevent the accidental deletion
of KMS keys and notify the administrators of any attempts to delete them.
Option A is not a valid solution because AWS Config rules are used to evaluate the
configuration of AWS resources, not to cancel the deletion of KMS keys. Option B is not a
valid solution because it requires creating and maintaining a custom Lambda function that
has logic to prevent KMS key deletion, which adds operational overhead. Option D is not a
valid solution because it only notifies the administrators of the DeleteKey operation, but does
not cancel it.
References:
* Using Amazon EventBridge rules to trigger Systems Manager Automation workflows - AWS
Systems Manager
* Using Amazon SNS for system-to-administrator communications - Amazon Simple
Notification Service
* Deleting AWS KMS keys - AWS Key Management Service
QUESTION NO: 223
한 회사는 us-west-2 리전의 NLB(Network Load Balancer) 뒤에 있는 3개의 Amazon EC2
인스턴스에 자체 관리형 DNS 솔루션을 구현했습니다. 회사 사용자의 대부분은 미국과 유럽에
있습니다. 회사는 솔루션의 성능과 가용성을 개선하려고 합니다. 회사는 eu-west-1 지역에서
3개의 EC2 인스턴스를 시작 및 구성하고 EC2 인스턴스를 새 NLB의 대상으로 추가합니다.
회사는 트래픽을 모든 EC2 인스턴스로 라우팅하기 위해 어떤 솔루션을 사용할 수 있습니까?
A. 두 NLB 중 하나로 요청을 라우팅하는 Amazon Route 53 지리적 위치 라우팅 정책을
생성합니다. Amazon CloudFront 배포를 생성합니다. Route 53 레코드를 배포 원본으로
사용합니다.
B. AWS Global Accelerator에서 표준 액셀러레이터를 생성합니다. us-west-2 및 eu-west-1에
150

IT Certification Guaranteed, The Easy Way!
엔드포인트 그룹을 생성합니다. 두 개의 NLB를 엔드포인트 그룹의 엔드포인트로 추가합니다.
C. 탄력적 IP 주소를 6개의 EC2 인스턴스에 연결합니다. 6개의 EC2 인스턴스 중 하나로
요청을 라우팅하는 Amazon Route 53 지리적 위치 라우팅 정책을 생성합니다. Amazon
CloudFront 배포를 생성합니다. Route 53 레코드를 배포 원본으로 사용합니다.
D. 두 개의 NLB를 두 개의 ALB(Application Load Balancer)로 교체합니다. 두 개의 ALB 중
하나로 요청을 라우팅하는 Amazon Route 53 지연 시간 라우팅 정책을 생성합니다. Amazon
CloudFront 배포를 생성합니다. Route 53 레코드를 배포 원본으로 사용합니다.
Answer: B
Explanation:
For standard accelerators, Global Accelerator uses the AWS global network to route traffic to
the optimal regional endpoint based on health, client location, and policies that you configure,
which increases the availability of your applications. Endpoints for standard accelerators can
be Network Load Balancers, Application Load Balancers, Amazon EC2 instances, or Elastic
IP addresses that are located in one AWS Region or multiple Regions.
https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html
QUESTION NO: 224
한 회사에서 Amazon S3 Standard 스토리지를 사용하여 백업 파일을 저장하고 있습니다. 해당
파일은 1개월간 자주 접속됩니다. 단, 1개월이 지나면 해당 파일에 접근할 수 없습니다. 회사는
해당 파일을 무기한 보관해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 스토리지 솔루션은 무엇입니까?
A. 객체를 자동으로 마이그레이션하도록 S3 Intelligent-Tiering을 구성합니다.
B. 1개월 후에 S3 Standard에서 S3 Glacier Deep Archive로 객체를 전환하기 위한 S3 수명
주기 구성을 생성합니다.
C. 1개월 후에 객체를 S3 Standard에서 S3 Standard-Infrequent Access(S3 Standard-IA)로
전환하는 S3 수명 주기 구성을 생성합니다.
D. 1개월 후에 객체를 S3 Standard에서 S3 One Zone-Infrequent Access(S3 One Zone-IA)로
전환하는 S3 수명 주기 구성을 생성합니다.
Answer: B
Explanation:
The storage solution that will meet these requirements most cost-effectively is B: Create an
S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive
after 1 month. Amazon S3 Glacier Deep Archive is a secure, durable, and extremely low-cost
Amazon S3 storage class for long-term retention of data that is rarely accessed and for which
retrieval times of several hours are acceptable. It is the lowest-cost storage option in Amazon
S3, making it a cost-effective choice for storing backup files that are not accessed after 1
month. You can use an S3 Lifecycle configuration to automatically transition objects from S3
Standard to S3 Glacier Deep Archive after 1 month. This will minimize the storage costs for
the backup files that are not accessed frequently.
QUESTION NO: 225
솔루션 설계자는 은행에 대한 신용 카드 데이터 유효성 검사 요청을 처리하기 위한 비동기
애플리케이션을 설계하고 있습니다. 애플리케이션은 안전해야 하며 각 요청을 한 번 이상
처리할 수 있어야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
151

IT Certification Guaranteed, The Easy Way!
A. AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service(Amazon
SQS) 표준 대기열을 이벤트 소스로 설정합니다. 암호화에는 AWS Key Management
Service(SSE-KMS)를 사용합니다. Lambda 실행 역할에 대한 kms:Decrypt 권한을
추가합니다.
B. AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service(Amazon
SQS) FIFO 대기열을 이벤트 소스로 사용합니다. 암호화에는 SQS 관리 암호화 키(SSE-
SQS)를 사용합니다. Lambda 함수에 대한 암호화 키 호출 권한을 추가합니다.
C. AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service(Amazon
SQS) FIFO 대기열을 이벤트 소스로 설정합니다. AWS KMS 키(SSE-KMS)를 사용합니다.
Lambda 실행 역할에 kms:Decrypt 권한을 추가합니다.
D. AWS Lambda 이벤트 소스 매핑을 사용합니다. Amazon Simple Queue Service(Amazon
SQS) 표준 대기열을 이벤트 소스로 설정합니다. 암호화에는 AWS KMS 키(SSE-KMS)를
사용합니다. Lambda 함수에 대한 암호화 키 호출 권한을 추가합니다.
Answer: B
Explanation:
* Requirement Analysis: The application must process each credit card data validation
request at least once, securely and cost-effectively.
* SQS FIFO Queues: Ensures that each message is processed exactly once and in the exact
order sent.
* AWS Lambda: Using Lambda for event-driven processing ensures scalability and cost-
efficiency.
* SSE-SQS: Provides encryption at rest using SQS-managed keys, simplifying encryption
management.
* Implementation:
* Set up SQS FIFO queues as the event source for Lambda.
* Enable SSE-SQS for encryption.
* Ensure the Lambda execution role has the necessary permissions to use the encryption
keys.
* Conclusion: This combination meets the requirements of security, exact-once processing,
and cost- effectiveness.
References
* Amazon SQS: Amazon SQS Documentation
* AWS Lambda with SQS: Using AWS Lambda with Amazon SQS
QUESTION NO: 226
한 회사는 최근 AWS 계정의 Amazon EC2 인스턴스에서 다양한 새로운 워크로드를
시작했습니다. 회사는 인스턴스에 원격으로 안전하게 액세스하고 관리하기 위한 전략을
수립해야 합니다. 회사는 기본 AWS 서비스와 작동하고 AWS Well-Architected 프레임워크를
따르는 반복 가능한 프로세스를 구현해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EC2 직렬 콘솔을 사용하여 관리를 위해 각 인스턴스의 터미널 인터페이스에 직접
액세스합니다.
B. 각 기존 인스턴스와 새 인스턴스에 적절한 IAM 역할을 연결합니다. AWS 시스템 관리자
세션 관리자를 사용하여 원격 SSH 세션을 설정합니다.
C. 관리 SSH 키 쌍을 생성합니다. 공개 키를 각 EC2 인스턴스에 로드합니다. 퍼블릭 서브넷에
152

IT Certification Guaranteed, The Easy Way!
배스천 호스트를 배포하여 각 인스턴스 관리를 위한 터널을 제공합니다.
D. AWS Site-to-Site VPN 연결을 설정합니다. VPN 터널을 통해 SSH 키를 사용하여 로컬
온프레미스 머신을 사용하여 인스턴스에 직접 연결하도록 관리자에게 지시합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-launch-managed-
instance.html
QUESTION NO: 227
애플리케이션 개발 팀은 큰 이미지를 더 작은 압축 이미지로 변환하는 마이크로서비스를
설계하고 있습니다. 사용자가 웹 인터페이스를 통해 이미지를 업로드하면 마이크로서비스는
Amazon S3 버킷에 이미지를 저장하고, AWS Lambda 함수를 사용하여 이미지를 처리 ​​및
압축한 다음, 다른 S3 버킷에 압축된 형식으로 이미지를 저장해야 합니다.
솔루션 아키텍트는 내구성이 뛰어난 상태 비저장 구성 요소를 사용하여 이미지를 자동으로
처리하는 솔루션을 설계해야 합니다.
이러한 요구 사항을 충족하는 작업 조합은 무엇입니까? (2개를 선택하세요.)
A. Amazon Simple Queue Service(Amazon SQS) 대기열 생성 이미지가 S3 버킷에 업로드될
때 SQS 대기열에 알림을 보내도록 S3 버킷을 구성합니다.
B. Amazon Simple Queue Service(Amazon SQS) 대기열을 호출 소스로 사용하도록 Lambda
함수를 구성합니다. SQS 메시지가 성공적으로 처리되면 대기열에서 메시지를 삭제합니다.
C. 새 업로드를 위해 S3 버킷을 모니터링하도록 Lambda 함수를 구성합니다. 업로드된
이미지가 감지되면 파일 이름을 메모리의 텍스트 파일에 쓰고 텍스트 파일을 사용하여 처리된
이미지를 추적합니다.
D. Amazon EC2 인스턴스를 시작하여 Amazon Simple Queue Service(Amazon SQS)
대기열을 모니터링합니다. 대기열에 항목이 추가되면 EC2 인스턴스의 텍스트 파일에 파일
이름을 기록하고 Lambda 함수를 호출합니다.
E. 이미지가 업로드될 때 S3 버킷을 모니터링하도록 Amazon EventBridge(Amazon
CloudWatch Events) 이벤트를 구성합니다. 추가 처리를 위해 애플리케이션 소유자의 이메일
주소가 포함된 Amazon Simple 알림 서비스(Amazon SNS) 주제에 알림을 보냅니다.
Answer: A B
Explanation:
* Creating an Amazon Simple Queue Service (SQS) queue and configuring the S3 bucket to
send a notification to the SQS queue when an image is uploaded to the S3 bucket will ensure
that the Lambda function is triggered in a stateless and durable manner.
* Configuring the Lambda function to use the SQS queue as the invocation source, and
deleting the message in the queue after it is successfully processed will ensure that the
Lambda function processes the image in a stateless and durable manner.
Amazon SQS is a fully managed message queuing service that enables you to decouple and
scale microservices, distributed systems, and serverless applications. SQS eliminates the
complexity and overhead associated with managing and operating-message oriented
middleware, and empowers developers to focus on differentiating work. When new images
are uploaded to the S3 bucket, SQS will trigger the Lambda function to process the image
and compress it. Once the image is processed, the SQS message is deleted, ensuring that
the Lambda function is stateless and durable.
153

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 228
회사에는 Amazon RDS의 데이터베이스에 목록을 저장하는 자동차 판매 웹 사이트가
있습니다. 자동차가 판매되면 해당 목록을 웹 사이트에서 제거해야 하며 데이터를 여러 대상
시스템으로 전송해야 합니다.
솔루션 아키텍트는 어떤 디자인을 추천해야 할까요?
A. Amazon RDS의 데이터베이스가 업데이트될 때 트리거되는 AWS Lambda 함수를
생성하여 대상이 사용할 Amazon Simple Queue Service(Amazon SQS> 대기열에 정보를
보냅니다.
B. Amazon RDS의 데이터베이스가 업데이트될 때 트리거되는 AWS Lambda 함수를
생성하여 대상이 사용할 Amazon Simple Queue Service(Amazon SQS) FIFO 대기열에
정보를 보냅니다.
C. RDS 이벤트 알림을 구독하고 Amazon Simple Queue Service(Amazon SQS) 대기열을
여러 Amazon Simple Notification Service(Amazon SNS) 주제로 팬아웃하여 보냅니다. AWS
Lambda 함수를 사용하여 대상 업데이트
D. RDS 이벤트 알림을 구독하고 여러 Amazon Simple Queue Service(Amazon SQS)
대기열에 팬아웃된 Amazon Simple Notification Service(Amazon SNS) 주제를 보냅니다.
AWS Lambda 함수를 사용하여 대상을 업데이트합니다.
Answer: D
Explanation:
https://docs.aws.amazon.com/lambda/latest/dg/services-rds.html
https://docs.aws.amazon.com/lambda/latest
/dg/with-sns.html
QUESTION NO: 229
한 회사에서 콘텐츠 관리 시스템을 제공하는 웹 애플리케이션을 구축하고 있습니다. 콘텐츠
관리 시스템은 Application Load Balancer(Al B) 뒤의 Amazon EC2 인스턴스에서 실행됩니다.
FC? 인스턴스는 여러 가용성 항목에 걸쳐 Auto Scaling 그룹에서 실행됩니다. 사용자는
콘텐츠 관리 시스템에 파일, 블로그 및 기타 웹사이트 자산을 지속적으로 추가하고
업데이트하고 있습니다.
솔루션 아키텍트는 모든 EC2 인스턴스가 지연 시간을 최소화하면서 최신 웹 사이트 콘텐츠를
공유하는 솔루션을 구현해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Auto Scaling 그룹 수명 주기 정책의 EC2 사용자 데이터를 업데이트하여 가장 최근에
시작된 EC2 인스턴스의 웹 사이트 자산을 복사합니다. 최신 EC2 인스턴스에서만 웹 사이트
자산을 변경하도록 ALB를 구성합니다.
B. 웹 사이트 자산을 Amazon Elastic File System(Amazon EFS) 파일 시스템에 복사합니다.
EFS 파일 시스템을 로컬로 탑재하도록 각 EC2 인스턴스를 구성합니다.
EFS 파일 시스템에 저장된 웹 사이트 자산을 참조하도록 웹 사이트 호스팅 애플리케이션을
구성합니다.
C. 웹 사이트 자산을 Amazon S3 버킷에 복사합니다. 각 EC2 인스턴스가 S3 버킷에서 연결된
Amazon Elastic Block Store(Amazon EBS) 볼륨으로 웹 사이트 자산을 다운로드하는지
확인하십시오. 파일을 최신 상태로 유지하려면 매 시간마다 S3 sync 명령을 실행하세요.
D. 웹 사이트 자산을 사용하여 Amazon Elastic Block Store(Amazon EBS) 스냅샷을
복원합니다. 새 CC2 인스턴스가 시작되면 EBS 스냅샷을 보조 EBS 볼륨으로 연결합니다.
154

IT Certification Guaranteed, The Easy Way!
보조 EDS 볼륨에 저장된 웹 사이트 자산을 참조하도록 웹 사이트 호스팅 애플리케이션을
구성합니다.
Answer: B
Explanation:
* Understanding the Requirement: The company needs all EC2 instances to share up-to-date
website content with minimal lag time, running behind an Application Load Balancer.
* Analysis of Options:
* EC2 User Data with ALB: Complex and not scalable as it requires updating each instance
manually.
* Amazon EFS: Provides a scalable, shared file storage solution that can be mounted by
multiple EC2 instances, ensuring all instances have access to the same up-to-date content.
* Amazon S3 with EC2 Sync: Involves periodic synchronization which introduces lag and
complexity.
* Amazon EBS Snapshots: Not suitable for dynamic and frequent updates required by a
content management system.
* Best Solution:
* Amazon EFS: Ensures all EC2 instances have access to a consistent and up-to-date set of
website assets with minimal lag time, meeting the requirements effectively.
References:
* Amazon Elastic File System (EFS)
* Mounting EFS File Systems on EC2 Instances
QUESTION NO: 230
회사에 수명이 다한 온프레미스 볼륨 백업 솔루션이 있습니다. 회사는 AWS를 새로운 백업
솔루션의 일부로 사용하고 AWS에 백업되는 동안 모든 데이터에 대한 로컬 액세스를
유지하려고 합니다. 회사는 AWS에 백업된 데이터가 자동으로 안전하게 전송되기를
원합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Snowball을 사용하여 온프레미스 솔루션의 데이터를 Amazon S3로
마이그레이션합니다. 데이터에 대한 로컬 액세스를 제공하기 위해 Snowball S3 엔드포인트를
탑재하도록 온프레미스 시스템을 구성합니다.
B. AWS Snowball Edge를 사용하여 온프레미스 솔루션의 데이터를 Amazon S3로
마이그레이션합니다. Snowball Edge 파일 인터페이스를 사용하여 온프레미스 시스템에
데이터에 대한 로컬 액세스를 제공합니다.
C. AWS Storage Gateway를 사용하고 캐시된 볼륨 게이트웨이를 구성합니다. Storage
Gateway 소프트웨어 애플리케이션을 온프레미스에서 실행하고 로컬로 캐시할 데이터 비율을
구성합니다. 데이터에 대한 로컬 액세스를 제공하기 위해 게이트웨이 스토리지 볼륨을
마운트합니다.
D. AWS Storage Gateway를 사용하고 저장된 볼륨 게이트웨이를 구성합니다.
온프레미스에서 스토리지 소프트웨어 애플리케이션을 실행하고 게이트웨이 스토리지 볼륨을
온프레미스 스토리지에 매핑합니다. 데이터에 대한 로컬 액세스를 제공하기 위해 게이트웨이
스토리지 볼륨을 마운트합니다.
Answer: D
Explanation:
This option is the most efficient because it uses AWS Storage Gateway, which is a service
155

IT Certification Guaranteed, The Easy Way!
that connects an on- premises software appliance with cloud-based storage to provide
seamless integration with data security features between your on-premises IT environment
and the AWS storage infrastructure1. It also uses a stored volume gateway, which is a type
of volume gateway that stores your primary data locally and asynchronously backs up point-
in-time snapshots of your data to Amazon S32. It also runs the Storage Gateway software
application on premises and maps the gateway storage volumes to on-premises storage,
which enables you to use your existing storage hardware and network infrastructure. It also
mounts the gateway storage volumes to provide local access to the data, which ensures that
your data is available for low latency access on premises while also getting backed up to
AWS. This solution meets the requirement of maintaining local access to all the data while it
is backed up on AWS and ensuring that the data backed up on AWS is automatically and
securely transferred. Option A is less efficient because it uses AWS Snowball, which is a
physical device that lets you transfer large amounts of data into and out of AWS3. However,
this does not provide a periodic backup solution, as it requires manual handling and shipping
of the device. It also configures on-premises systems to mount the Snowball S3 endpoint to
provide local access to the data, which could introduce additional complexity and latency.
Option B is less efficient because it uses AWS Snowball Edge, which is a physical device
that has onboard storage and compute capabilities for select AWS capabilities. However, this
does not provide a periodic backup solution, as it requires manual handling and shipping of
the device. It also uses the Snowball Edge file interface to provide on-premises systems with
local access to the data, which could introduce additional complexity and latency. Option C is
less efficient because it uses AWS Storage Gateway and configures a cached volume
gateway, which is a type of volume gateway that stores your primary data in Amazon S3 and
retains a copy of frequently accessed data subsets locally. However, this does not provide
local access to all the data, as only some data subsets are cached locally. It also configures
a percentage of data to cache locally, which could incur higher costs and complexity than
using a stored volume gateway.
QUESTION NO: 231
한 회사에는 공통 Amazon RDS MySQL Multi-AZ DB 인스턴스에 자주 액세스해야 하는 여러
개의 웹 서버가 있습니다. 이 회사는 사용자 자격 증명을 자주 순환해야 하는 보안 요구 사항을
충족하는 동시에 웹 서버가 데이터베이스에 연결할 수 있는 안전한 방법을 원합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Secrets Manager에 데이터베이스 사용자 자격 증명을 저장합니다. 웹 서버가 AWS
Secrets Manager에 액세스할 수 있도록 필요한 IAM 권한을 부여합니다.
B. AWS Systems Manager OpsCenter에 데이터베이스 사용자 자격 증명을 저장합니다. 웹
서버가 OpsCenter에 액세스할 수 있도록 필요한 IAM 권한을 부여합니다.
C. 데이터베이스 사용자 자격 증명을 안전한 Amazon S3 버킷에 저장합니다. 웹 서버가 자격
증명을 검색하고 데이터베이스에 액세스할 수 있도록 필요한 IAM 권한을 부여합니다.
D. 웹 서버 파일 시스템의 AWS Key Management Service(AWS KMS)로 암호화된 파일에
데이터베이스 사용자 자격 증명을 저장합니다. 웹 서버는 파일을 해독하고 데이터베이스에
액세스할 수 있어야 합니다.
Answer: A
Explanation:
AWS Secrets Manager helps you protect secrets needed to access your applications,
156

IT Certification Guaranteed, The Easy Way!
services, and IT resources. The service enables you to easily rotate, manage, and retrieve
database credentials, API keys, and other secrets throughout their lifecycle.
https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html
Secrets Manager enables you to replace hardcoded credentials in your code, including
passwords, with an API call to Secrets Manager to retrieve the secret programmatically. This
helps ensure the secret can't be compromised by someone examining your code, because
the secret no longer exists in the code. Also, you can configure Secrets Manager to
automatically rotate the secret for you according to a specified schedule. This enables you to
replace long-term secrets with short-term ones, significantly reducing the risk of compromise.
QUESTION NO: 232
한 회사가 AWS에서 민감한 데이터를 처리하는 애플리케이션을 설계하고 있습니다. a.
애플리케이션은 여러 고객의 재무 데이터를 저장하고 처리합니다.
규정 준수 요구 사항을 충족하려면 각 고객의 데이터는 안전한 중앙 집중형 키 관리 솔루션을
사용하여 별도로 암호화해야 합니다. 이 회사는 AWS Key Management Service(AWS
KMS)를 사용하여 암호화를 구현하려고 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. 각 고객에 대해 고유한 암호화 키를 생성합니다. Amazon S3 버킷에 키를 저장합니다. 서버
측 암호화를 활성화합니다.
B. 고객이 제공한 암호화 키를 안전하게 저장하는 AWS 환경에 하드웨어 보안 어플라이언스를
배포합니다. 보안 어플라이언스를 AWS KMS와 통합하여 애플리케이션의 민감한 데이터를
암호화합니다.
C. 애플리케이션 전반의 모든 민감한 데이터를 암호화하기 위해 단일 AWS KMS 키를
생성합니다.
D. 세부적인 액세스 제어 및 로깅이 활성화된 각 고객 데이터에 대해 별도의 AWS KMS 키를
생성합니다.
Answer: D
Explanation:
This solution meets the requirement of encrypting each customer's data separately with the
least operational overhead by leveraging AWS Key Management Service (KMS).
* Separate AWS KMS Keys: By creating separate KMS keys for each customer, you can
ensure that each customer's data is encrypted with a unique key. This approach satisfies the
compliance requirement for separate encryption and provides fine-grained control over
access to the keys.
* Granular Access Control: AWS KMS allows you to define key policies and use IAM policies
to grant specific permissions to the keys. This ensures that only authorized users or services
can access the keys, thereby maintaining the principle of least privilege.
* Logging and Monitoring: AWS KMS integrates with AWS CloudTrail, which logs all key
usage and management activities. This provides an audit trail that is essential for meeting
compliance requirements.
* Why Not Other Options?:
* Option A (Store keys in S3): Storing keys in S3 is not recommended because it does not
provide the same level of security, access control, or integration with AWS services as KMS
does.
* Option B (Hardware security appliance): Deploying a hardware security appliance adds
157

IT Certification Guaranteed, The Easy Way!
significant operational overhead and complexity, which is unnecessary given that KMS
already provides a secure and centralized key management solution.
* Option C (Single KMS key for all data): Using a single KMS key does not meet the
requirement of encrypting each customer's data separately.
AWS References:
* AWS Key Management Service (KMS) - Overview of KMS, its features, and best practices
for key management.
* Using AWS KMS for Multi-Tenant Applications - Guidance on how to design applications
using KMS for multi-tenancy.
QUESTION NO: 233
한 회사에는 단일 가용 영역의 MySQL용 Amazon RDS DB 인스턴스에 온라인 광고
비즈니스를 위한 대규모 데이터 세트가 저장되어 있습니다. 회사는 프로덕션 DB 인스턴스에
대한 쓰기 작업에 영향을 주지 않고 비즈니스 보고 쿼리가 실행되기를 원합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. RDS 읽기 전용 복제본을 배포하여 비즈니스 보고 쿼리를 처리합니다.
B. DB 인스턴스를 Elastic Load Balancer 뒤에 배치하여 수평으로 확장합니다.
C. 쓰기 작업 및 쿼리를 처리하기 위해 DB 인스턴스를 더 큰 인스턴스 유형으로 확장합니다.
D. 비즈니스 보고 쿼리를 처리하기 위해 여러 가용 영역에 OB 거리를 배포합니다.
Answer: A
Explanation:
Read replica use cases - You have a production database that is taking on normal load & You
want to run a reporting application to run some analytics * You create a Read Replica to run
the new workload there * The production application is unaffected * Read replicas are used
for SELECT (=read) only kind of statements (not INSERT, UPDATE, DELETE)
QUESTION NO: 234
한 회사가 현재 Microsoft Windows Server를 사용하여 온프레미스 주식 거래 애플리케이션을
실행하고 있습니다. 이 회사는 애플리케이션을 AWS 클라우드로 마이그레이션하려고 합니다.
이 회사는 여러 가용성 영역에 걸쳐 블록 스토리지에 대한 저지연 액세스를 제공하는
고가용성 솔루션을 설계해야 합니다. 어떤 솔루션이 최소한의 구현 노력으로 이러한 요구
사항을 충족할까요?
A. Amazon EC2 인스턴스에서 두 개의 가용성 영역에 걸쳐 있는 Windows Server 클러스터를
구성합니다. 두 클러스터 노드에 애플리케이션을 설치합니다. 두 클러스터 노드 간의 공유
저장소로 Amazon FSx for Windows File Server를 사용합니다.
B. Amazon EC2 인스턴스에서 두 개의 가용성 영역에 걸쳐 있는 Windows Server 클러스터를
구성합니다. 두 클러스터 노드에 애플리케이션을 설치합니다. EC2 인스턴스에 연결된
스토리지로 Amazon Elastic Block Store(Amazon EBS) 범용 SSD(gp3) 볼륨을 사용합니다.
애플리케이션 수준 복제를 설정하여 한 가용성 영역의 한 EBS 볼륨에서 두 번째 가용성
영역의 다른 EBS 볼륨으로 데이터를 동기화합니다.
C. 두 개의 가용성 영역에 있는 Amazon EC2 인스턴스에 애플리케이션을 배포합니다. 한 EC2
인스턴스를 활성 모드로 구성하고 두 번째 EC2 인스턴스를 대기 모드로 구성합니다. Amazon
FSx for NetApp ONTAP Multi-AZ 파일 시스템을 사용하여 iSCSI(Internet Small Computer
Systems Interface) 프로토콜을 사용하여 데이터에 액세스합니다.
D. 두 개의 가용성 영역에 있는 Amazon EC2 인스턴스에 애플리케이션을 배포합니다. 한 EC2
158

IT Certification Guaranteed, The Easy Way!
인스턴스를 활성 모드로 구성하고 두 번째 EC2 인스턴스를 대기 모드로 구성합니다. Amazon
Elastic Block Store(Amazon EBS) Provisioned IOPS SSD(io2) 볼륨을 EC2 인스턴스에
연결된 스토리지로 사용합니다. Amazon EBS 수준 복제를 설정하여 한 가용성 영역의 한 io2
볼륨에서 두 번째 가용성 영역의 다른 io2 볼륨으로 데이터를 동기화합니다.
Answer: A
Explanation:
This solution is designed to provide high availability and low-latency access to block storage
across multiple Availability Zones with minimal implementation effort.
* Windows Server Cluster Across AZs: Configuring a Windows Server Failover Cluster
(WSFC) that spans two Availability Zones ensures that the application can failover from one
instance to another in case of a failure, meeting the high availability requirement.
* Amazon FSx for Windows File Server: FSx for Windows File Server provides fully managed
Windows file storage that is accessible via the SMB protocol, which is suitable for Windows-
based applications. It offers high availability and can be used as shared storage between the
cluster nodes, ensuring that both nodes have access to the same data with low latency.
* Why Not Other Options?:
* Option B (EBS with application-level replication): This requires complex configuration and
management, as EBS volumes cannot be directly shared across AZs. Application-level
replication is more complex and prone to errors.
* Option C (FSx for NetApp ONTAP with iSCSI): While this is a viable option, it introduces
additional complexity with iSCSI and requires more specialized knowledge for setup and
management.
* Option D (EBS with EBS-level replication): EBS-level replication is not natively supported
across AZs, and setting up a custom replication solution would increase the implementation
effort.
AWS References:
* Amazon FSx for Windows File Server - Overview and benefits of using FSx for Windows File
Server.
* Windows Server Failover Clustering on AWS - Guide on setting up a Windows Server
cluster on AWS.
QUESTION NO: 235
회사에서 계층적 구조 관계로 직원 데이터를 저장하는 애플리케이션을 만들고자 합니다.
회사는 직원 데이터에 대한 트래픽이 많은 쿼리에 대한 최소 대기 시간 응답이 필요하며
민감한 데이터를 보호해야 합니다. 회사는 또한 직원 데이터에 재무 정보가 있는 경우 월별
이메일 메시지를 받아야 합니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 단계 조합을 수행해야 합니까?
(2개를 선택하세요.)
A. Amazon Redshift를 사용하여 직원 데이터를 계층에 저장하십시오. 매월 Amazon S3에
데이터를 언로드합니다.
B. Amazon DynamoDB를 사용하여 직원 데이터를 계층에 저장합니다. 매월 데이터를
Amazon S3로 내보냅니다.
C. AWS 계정에 대해 Amazon fvlacie를 구성합니다. Macie를 Amazon EventBridge와
통합하여 월별 이벤트를 AWS Lambda로 전송합니다.
D. Amazon Athena를 사용하여 Amazon S3에서 직원 데이터를 분석합니다. Athena를
159

IT Certification Guaranteed, The Easy Way!
Amazon QuickSight와 통합하여 분석 대시보드를 게시하고 사용자와 대시보드를 공유합니다.
E. AWS 계정에 대해 Amazon Macie 구성 Macie를 Amazon EventBridge와 통합하여 Amazon
Simple Notification Service(Amazon SNS) 구독을 통해 월별 알림을 보냅니다.
Answer: B E
Explanation:
Generally, for building a hierarchical relationship model, a graph database such as Amazon
Neptune is a better choice. In some cases, however, DynamoDB is a better choice for
hierarchical data modeling because of its flexibility, security, performance, and scale.
https://docs.aws.amazon.com/prescriptive-guidance/latest
/dynamodb-hierarchical-data-model/introduction.html
QUESTION NO: 236
한 회사가 VPC의 컨테이너에서 실행되는 애플리케이션을 만들고 있습니다. 애플리케이션은
Amazon S3 버킷에 데이터를 저장하고 액세스합니다. 개발 단계에서 애플리케이션은 매일
Amazon S3에 1TB의 데이터를 저장하고 액세스합니다. 회사는 비용을 최소화하고 가능할
때마다 트래픽이 인터넷을 통과하는 것을 방지하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. S3 버킷에 대해 S3 Intelligent-Tiering을 활성화합니다.
B. S3 버킷에 대해 S3 Transfer Acceleration을 활성화합니다.
C. Amazon S3용 게이트웨이 VPC 엔드포인트를 생성합니다. 이 엔드포인트를 VPC의 모든
라우팅 테이블과 연결합니다.
D. VPC에서 Amazon S3에 대한 인터페이스 엔드포인트를 생성합니다. 이 엔드포인트를
VPC의 모든 라우팅 테이블과 연결합니다.
Answer: C
Explanation:
A gateway VPC endpoint for Amazon S3 enables private connections between the VPC and
Amazon S3 that do not require an internet gateway or NAT device. This minimizes costs and
prevents traffic from traversing the internet. A gateway VPC endpoint uses a prefix list as the
route target in a VPC route table to route traffic privately to Amazon S31. Associating the
endpoint with all route tables in the VPC ensures that all subnets can access Amazon S3
through the endpoint.
Option A is incorrect because S3 Intelligent-Tiering is a storage class that optimizes storage
costs by automatically moving objects between two access tiers based on changing access
patterns. It does not affect the network traffic between the VPC and Amazon S32.
Option B is incorrect because S3 Transfer Acceleration is a feature that enables fast, easy,
and secure transfers of files over long distances between clients and an S3 bucket. It does
not prevent traffic from traversing the internet3.
Option D is incorrect because an interface VPC endpoint for Amazon S3 is powered by AWS
PrivateLink, which requires an elastic network interface (ENI) with a private IP address in
each subnet. This adds complexity and cost to the solution. Moreover, an interface VPC
endpoint does not support cross-Region access to Amazon S3. Reference URL: 1:
https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints- s3.html 2:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-
dynamic-data- access 3: https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer
-acceleration.html : https://aws.
160

IT Certification Guaranteed, The Easy Way!
amazon.com/blogs/architecture/choosing-your-vpc-endpoint-strategy-for-amazon-s3/
QUESTION NO: 237
보안 요구 사항을 충족하려면 회사는 Amazon RDS MySQL DB 인스턴스와 통신하는 동안
전송 중인 모든 애플리케이션 데이터를 암호화해야 합니다. 최근 보안 감사에서는 AWS Key
Management Service(AWS KMS)를 사용하여 저장 데이터 암호화가 활성화되었지만 전송
중인 데이터는 활성화되지 않은 것으로 나타났습니다.
솔루션 설계자는 보안 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 데이터베이스에서 오전 1시 데이터베이스 인증을 활성화합니다.
B. 자체 서명된 인증서를 제공합니다. RDS 인스턴스에 대한 모든 연결에 인증서를
사용하십시오.
C. RDS 인스턴스의 스냅샷을 찍습니다. 암호화가 활성화된 새 인스턴스로 스냅샷을
복원합니다.
D. AWS에서 제공하는 루트 인증서를 다운로드합니다. RDS 인스턴스에 대한 모든 연결에
인증서를 제공하십시오.
Answer: D
Explanation:
To satisfy the security requirements, the solutions architect should download AWS-provided
root certificates and provide the certificates in all connections to the RDS instance. This will
enable SSL/TLS encryption for data in transit between the application and the RDS instance.
SSL/TLS encryption provides a layer of security by encrypting data that moves between the
client and the server. Amazon RDS creates an SSL certificate and installs the certificate on
the DB instance when the instance is provisioned. The application can use the AWS- provided
root certificates to verify the identity of the DB instance and establish a secure connection1.
The other options are not correct because they do not enable encryption for data in transit or
are not relevant for the use case. Enabling IAM database authentication on the database is
not correct because this option only provides a method of authentication, not encryption. IAM
database authentication allows users to use AWS Identity and Access Management (IAM)
users and roles to access a database, instead of using a database user name and
password2. Providing self-signed certificates is not correct because this option is not secure
or reliable. Self-signed certificates are certificates that are signed by the same entity that
issued them, instead of by a trusted certificate authority (CA). Self-signed certificates can be
easily forged or compromised, and are not recognized by most browsers and applications3.
Taking a snapshot of the RDS instance and restoring it to a new instance with encryption
enabled is not correct because this option only enables encryption at rest, not encryption in
transit. Encryption at rest protects data that is stored on disk, but does not protect data that is
moving between the client and the server4.
References:
* Using SSL/TLS to encrypt a connection to a DB instance - Amazon Relational Database
Service
* IAM database authentication for MySQL and PostgreSQL - Amazon Relational Database
Service
* What are self-signed certificates?
* Encrypting Amazon RDS resources - Amazon Relational Database Service
161

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 238
한 회사가 AWS에 웹 애플리케이션을 배포했습니다. 이 회사는 호출 요구 사항을 지원하기
위해 기본 DB 인스턴스와 5개의 읽기 전용 복제본을 사용하여 MySQL용 Amazon RDS에서
백엔드 데이터베이스를 호스팅합니다. 읽기 전용 복제본은 기본 DB 인스턴스보다 1초 이상
지연되어서는 안 됩니다. 데이터베이스는 정기적으로 예약된 저장 프로시저를 실행합니다.
웹 사이트의 트래픽이 증가함에 따라 복제본은 최대 로드 기간 동안 추가 지연을 경험합니다.
솔루션 설계자는 복제 지연을 최대한 줄여야 합니다. 솔루션 설계자는 애플리케이션 코드에
대한 변경을 최소화해야 하며 지속적인 운영 오버헤드를 최소화해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 데이터베이스를 Amazon Aurora MySQL로 마이그레이션합니다. 읽기 전용 복제본을
Aurora 복제본으로 교체하고 Aurora Auto Scaling을 구성합니다. 저장 프로시저를 Aurora
MySQL 기본 함수로 바꿉니다.
B. 데이터베이스 앞에 Amazon ElasticCache for Redis 클러스터를 배포합니다.
애플리케이션이 데이터베이스를 쿼리하기 전에 캐시를 확인하도록 애플리케이션을
수정합니다. 저장 프로시저를 AWS Lambda 함수로 바꿉니다.
C. Amazon EC2 인스턴스에서 실행되는 MySQL 데이터베이스로 데이터베이스를
마이그레이션합니다. 모든 복제본 노드에 대해 컴퓨팅에 최적화된 대규모 EC2 인스턴스를
선택하세요. EC2 인스턴스에서 저장 프로시저를 유지 관리합니다.
D. 데이터베이스를 Amazon DynamicDB로 마이그레이션하여 필요한 처리량을 지원하기 위해
다수의 읽기 용량 단위(RCU)를 프로비저닝하고 온디맨드 용량 확장을 구성합니다. 저장
프로시저를 DynamoDB 스트림으로 교체
Answer: A
Explanation:
Option A is the most appropriate solution for reducing replication lag without significant
changes to the application code and minimizing ongoing operational overhead. Migrating the
database to Amazon Aurora MySQL allows for improved replication performance and higher
scalability compared to Amazon RDS for MySQL. Aurora Replicas provide faster replication,
reducing the replication lag, and Aurora Auto Scaling ensures that there are enough Aurora
Replicas to handle the incoming traffic. Additionally, Aurora MySQL native functions can
replace the stored procedures, reducing the load on the database and improving
performance.
QUESTION NO: 239
회사에는 데이터베이스에 주문을 기록하고 결제를 처리하기 위해 서비스를 호출하는
전자상거래 결제 워크플로가 있습니다. 결제 프로세스 중에 사용자에게 시간 초과가
발생합니다. 사용자가 결제 양식을 다시 제출하면 원하는 동일한 거래에 대해 여러 개의 고유
주문이 생성됩니다.
솔루션 설계자는 여러 주문이 생성되는 것을 방지하기 위해 이 워크플로를 어떻게
리팩터링해야 합니까?
A. Amazon Kinesis Data Firehose에 주문 메시지를 보내도록 웹 애플리케이션을 구성합니다.
Kinesis Data Firehose에서 메시지를 검색하고 주문을 처리하도록 결제 서비스를 설정합니다.
B. AWS CloudTrail에서 기록된 애플리케이션 경로 요청을 기반으로 AWS Lambda 함수를
호출하는 규칙을 생성합니다. Lambda를 사용하여 데이터베이스를 쿼리하고, 결제 서비스를
호출하고, 주문 정보를 전달합니다.
162

IT Certification Guaranteed, The Easy Way!
C. 주문을 데이터베이스에 저장합니다. Amazon Simple 알림 서비스(Amazon SNS)에 주문
번호가 포함된 메시지를 보냅니다. Amazon SNS를 폴링하도록 결제 서비스를 설정합니다.
메시지를 검색하고 주문을 처리합니다.
D. 주문을 데이터베이스에 저장합니다. 주문 번호가 포함된 메시지를 Amazon Simple Queue
Service(Amazon SQS) FIFO 대기열로 보냅니다. 메시지를 검색하고 주문을 처리하도록 결제
서비스를 설정합니다. 대기열에서 메시지를 삭제합니다.
Answer: D
Explanation:
This approach ensures that the order creation and payment processing steps are separate
and atomic. By sending the order information to an SQS FIFO queue, the payment service
can process the order one at a time and in the order they were received. If the payment
service is unable to process an order, it can be retried later, preventing the creation of
multiple orders. The deletion of the message from the queue after it is processed will prevent
the same message from being processed multiple times.
QUESTION NO: 240
한 회사가 AWS에서 쇼핑 애플리케이션을 구축하고 있습니다. 애플리케이션은 매달 한 번씩
변경되고 트래픽 양에 따라 확장되어야 하는 카탈로그를 제공합니다. 회사는 애플리케이션의
대기 시간이 최소화되기를 원합니다. 각 사용자의 쇼핑 칼에서 얻은 데이터는 가용성이
높아야 합니다. 사용자의 연결이 끊어졌다가 다시 연결되더라도 사용자 세션 데이터를 사용할
수 있어야 합니다.
장바구니 데이터가 항상 보존되도록 솔루션 설계자는 무엇을 해야 합니까?
A. Amazon Aurora의 카탈로그에 액세스하기 위해 고정 세션 기능(세션 선호도)을
활성화하도록 Application Load Balancer를 구성합니다.
B. Amazon DynamoDB의 카탈로그 데이터와 사용자 세션의 쇼핑 칼 데이터를 캐시하도록
Redis용 Amazon ElastiCacJie를 구성합니다.
C. Amazon DynamoDB의 카탈로그 데이터와 사용자 세션의 장바구니 데이터를 캐시하도록
Amazon OpenSearch Service를 구성합니다.
D. 카탈로그 및 장바구니용 Amazon Elastic Block Store(Amazon EBS) 스토리지로 Amazon
EC2 인스턴스를 구성합니다. 자동 스냅샷을 구성합니다.
Answer: B
Explanation:
To ensure that the shopping cart data is preserved at all times, a solutions architect should
configure Amazon ElastiCache for Redis to cache catalog data from Amazon DynamoDB and
shopping cart data from the user's session. This solution has the following benefits:
* It offers the lowest possible latency from the application, as ElastiCache for Redis is a
blazing fast in- memory data store that provides sub-millisecond latency to power internet
-scale real-time applications1.
* It scales with traffic volume, as ElastiCache for Redis supports horizontal scaling by adding
more nodes or shards to the cluster, and vertical scaling by changing the node type2.
* It is highly available, as ElastiCache for Redis supports replication across multiple
Availability Zones and automatic failover in case of a primary node failure3.
* It preserves user session data even if the user is disconnected and reconnects, as
ElastiCache for Redis can store session data, such as user login information and shopping
cart contents, in a persistent and durable manner using snapshots or AOF (append-only file)
163

IT Certification Guaranteed, The Easy Way!
persistence4.
References:
* 1: https://aws.amazon.com/elasticache/redis/
* 2: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Scaling.html
* 3: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.html
* 4: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/backups.html
QUESTION NO: 241
금융 회사는 AWS에서 웹 애플리케이션을 호스팅합니다. 애플리케이션은 Amazon API
Gateway 지역 API 엔드포인트를 사용하여 사용자에게 현재 주가를 검색할 수 있는 기능을
제공합니다. 회사 보안팀에서는 API 요청 수가 증가하는 것을 확인했습니다. 보안 팀은 HTTP
플러드 공격으로 인해 애플리케이션이 오프라인 상태가 될 수 있다는 점을 우려하고
있습니다.
솔루션 설계자는 이러한 유형의 공격으로부터 애플리케이션을 보호하기 위한 솔루션을
설계해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 최대 TTL이 24시간인 API Gateway 지역 API 엔드포인트 앞에 Amazon CloudFront 배포를
생성합니다.
B. 비율 기반 규칙을 사용하여 지역 AWS WAF 웹 ACL을 생성합니다. 웹 ACL을 API
게이트웨이 단계와 연결합니다.
C. Amazon CloudWatch 지표를 사용하여 Count 지표를 모니터링하고 사전 정의된 비율에
도달하면 보안 팀에 알립니다.
D. API Gateway 지역 API 엔드포인트 앞에 Lambda@Edge를 사용하여 Amazon CloudFront
배포를 생성합니다. 사전 정의된 속도를 초과하는 IP 주소의 요청을 차단하는 AWS Lambda
함수를 생성합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html
A rate-based rule in AWS WAF allows the security team to configure thresholds that trigger
rate-based rules, which enable AWS WAF to track the rate of requests for a specified time
period and then block them automatically when the threshold is exceeded. This provides the
ability to prevent HTTP flood attacks with minimal operational overhead.
QUESTION NO: 242
그룹은 Amazon S3 버킷을 나열하고 해당 버킷에서 객체를 삭제할 권한이 필요합니다.
관리자가 다음 1AM 정책을 생성하여 버킷에 대한 액세스를 제공하고 해당 정책을 그룹에
적용했습니다. 그룹은 버킷에서 객체를 삭제할 수 없습니다. 회사는 최소 권한 액세스 규칙을
따릅니다.
에이)
164

IT Certification Guaranteed, The Easy Way!
비)
기음)
A. 옵션 A
B. 옵션 B
C. 옵션 C
D. 옵션 D
Answer: D
Explanation:
{
"Version": "2012-10-17",
"Statement": [
{
"Action": [
"s3:ListBucket",
"s3:DeleteObject"
],
"Resource": [
"arn:aws:s3:::<bucket-name>"
],
"Effect": "Allow",
},
{
"Action": "s3:*DeleteObject",
"Resource": [
165

IT Certification Guaranteed, The Easy Way!
"arn:aws:s3:::<bucket-name>/*" # <- The policy clause kludge "added" to match the solution
(Q248.1) example
],
"Effect": "Allow"
}
]
}
QUESTION NO: 243
회사의 애플리케이션은 Auto Scaling 그룹의 Amazon EC2 인스턴스에서 실행됩니다. 회사는
애플리케이션에서 임의의 요일에 트래픽이 갑자기 증가하는 것을 발견했습니다. 회사는
트래픽이 갑자기 증가하는 동안에도 애플리케이션 성능을 유지하려고 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 수동 스케일링을 사용하여 Auto Scaling 그룹의 크기를 변경합니다.
B. 예측 조정을 사용하여 Auto Scaling 그룹의 크기를 변경합니다.
C. 동적 스케일링을 사용하여 Auto Scaling 그룹의 크기를 변경합니다.
D. 일정 조정을 사용하여 Auto Scaling 그룹의 크기를 변경합니다.
Answer: C
Explanation:
Dynamic scaling is a type of autoscaling that automatically adjusts the number of EC2
instances in an Auto Scaling group based on demand or load. It uses CloudWatch alarms to
trigger scaling actions when a specified metric crosses a threshold. It can scale out (add
instances) or scale in (remove instances) as needed1. By using dynamic scaling, the solution
can maintain application performance during sudden traffic increases most cost- effectively.
A: Use manual scaling to change the size of the Auto Scaling group. This solution will not
meet the requirement of maintaining application performance during sudden traffic increases,
as manual scaling requires users to manually increase or decrease the number of instances
through a CLI or console. It does not respond automatically to changes in demand or load2.
B: Use predictive scaling to change the size of the Auto Scaling group. This solution will not
meet the requirement of most cost-effectiveness, as predictive scaling uses machine learning
and artificial intelligence tools to evaluate traffic loads and anticipate when more or fewer
resources are needed. It performs scheduled scaling actions based on the prediction, which
may not match the actual demand or load at any given time. Predictive scaling is more
suitable for scenarios where there are predictable traffic patterns or known changes in traffic
loads3.
D: Use schedule scaling to change the size of the Auto Scaling group. This solution will not
meet the requirement of maintaining application performance during sudden traffic increases,
as schedule scaling performs scaling actions at specific times that users schedule. It does
not respond automatically to changes in demand or load. Schedule scaling is more suitable
for scenarios where there are predictable traffic drops or spikes at specific times of the day.
Reference URL: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-
on-demand.html
QUESTION NO: 244
회사에는 Amazon EC2 인스턴스에서 실행되고 Amazon Aurora 데이터베이스를 사용하는
166

IT Certification Guaranteed, The Easy Way!
애플리케이션이 있습니다. EC2 인스턴스는 파일에 로컬로 저장된 사용자 이름과 암호를
사용하여 데이터베이스에 연결합니다. 회사는 자격 증명 관리의 운영 오버헤드를
최소화하려고 합니다.
이 목표를 달성하려면 솔루션 아키텍트가 무엇을 해야 합니까?
A. AWS Secrets Manager를 사용하세요. 자동 회전을 켭니다.
B. AWS 시스템 관리자 매개변수 저장소를 사용합니다. 자동 회전을 켭니다.
C. AWS Key C. Management Service(AWS KMS) 암호화 키로 암호화된 객체를 저장하는
Amazon S3 버킷을 생성합니다. 자격 증명 파일을 S3 버킷으로 마이그레이션합니다.
애플리케이션이 S3 버킷을 가리키도록 합니다.
D. 암호화된 Amazon Elastic Block Store(Amazon EBS) 볼륨(또는 각 EC2 인스턴스)을
생성합니다. 새 EBS 볼륨을 각 EC2 인스턴스에 연결합니다. 자격 증명 파일을 새 EBS
볼륨으로 마이그레이션합니다. 애플리케이션이 새 EBS 볼륨을 가리키도록 합니다. .
Answer: A
Explanation:
https://aws.amazon.com/cn/blogs/security/how-to-connect-to-aws-secrets-manager-service-
within-a-virtual-private-cloud/
https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-
automatically-with-aws-secrets-manager/
QUESTION NO: 245
회사에 AWS Glue 추출이 있습니다. 매일 동시에 실행되는 변환 및 로드(ETL) 작업입니다.
작업은 Amazon S3 버킷에 있는 XML 데이터를 처리합니다.
매일 S3 버킷에 새로운 데이터가 추가됩니다. 솔루션 아키텍트는 AWS Glue가 각 실행 중에
모든 데이터를 처리하고 있음을 확인합니다.
AWS Glue가 오래된 데이터를 재처리하는 것을 방지하려면 솔루션 아키텍트가 무엇을 해야
합니까?
A. 작업 북마크를 사용하도록 작업을 편집합니다.
B. 데이터가 처리된 후 데이터를 삭제하도록 작업을 편집합니다.
C. NumberOfWorkers 필드를 1로 설정하여 작업을 편집합니다.
D. FindMatches 기계 학습(ML) 변환을 사용합니다.
Answer: C
Explanation:
This is the purpose of bookmarks: "AWS Glue tracks data that has already been processed
during a previous run of an ETL job by persisting state information from the job run. This
persisted state information is called a job bookmark. Job bookmarks help AWS Glue maintain
state information and prevent the reprocessing of old data."
https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html
QUESTION NO: 246
개발 팀은 개발, 스테이징 및 프로덕션 환경에 여러 AWS 계정을 사용합니다.
팀원들은 활용도가 낮은 대규모 Amazon EC2 인스턴스를 출시해 왔습니다. 솔루션
아키텍트는 모든 계정에서 대규모 인스턴스가 시작되지 않도록 해야 합니다.
솔루션 설계자는 어떻게 최소한의 운영 오버헤드로 이 요구 사항을 충족할 수 있습니까?
A. 대규모 EC2 인스턴스의 시작을 거부하도록 오전 1시 정책을 업데이트합니다. 모든
167

IT Certification Guaranteed, The Easy Way!
사용자에게 정책을 적용합니다.
B. 대규모 EC2 인스턴스의 시작을 방지하는 리소스를 AWS Resource Access Manager에서
정의합니다.
C. 각 계정에 대규모 EC2 인스턴스의 시작을 거부하는 AM 역할을 생성합니다. 개발자에게
권한을 부여합니다.
역할에 대한 오전 1시 그룹 액세스입니다.
D. 기본 정책을 사용하여 마스터 계정의 AWS Organizations에 조직을 생성합니다. 대규모
EC2 인스턴스의 시작을 거부하는 서비스 제어 정책(SCP)을 생성하고 이를 AWS 계정에
적용합니다.
Answer: D
Explanation:
* Understanding the Requirement: The development team needs to prevent the launch of
large EC2 instances across multiple AWS accounts used for development, staging, and
production environments.
* Analysis of Options:
* IAM Policies: Would need to be applied individually to each user in every account, leading
to significant operational overhead.
* AWS Resource Access Manager: Used for sharing resources, not for enforcing restrictions
on resource creation.
* IAM Role in Each Account: Requires creating and managing roles in each account, leading
to higher operational overhead compared to using a centralized approach.
* Service Control Policy (SCP) with AWS Organizations: Provides a centralized way to
enforce policies across multiple AWS accounts, ensuring that large EC2 instances cannot be
launched in any account.
* Best Solution:
* Service Control Policy (SCP) with AWS Organizations: This solution offers the least
operational overhead by allowing centralized management and enforcement of policies
across all accounts, effectively preventing the launch of large EC2 instances.
References:
* AWS Organizations and SCPs
QUESTION NO: 247
한 기상 스타트업 회사는 사용자에게 온라인으로 날씨 데이터를 판매하는 맞춤형 웹
애플리케이션을 보유하고 있습니다. 이 회사는 Amazon DynamoDB를 사용하여 데이터를
저장하고 새로운 기상 현상이 기록될 때마다 4개의 내부 팀 관리자에게 알림을 보내는 새로운
서비스를 구축하려고 합니다. 회사는 현재 애플리케이션의 성능에 영향을 미치는 진정한
새로운 서비스를 원하지 않습니다. 솔루션 설계자는 최소한의 운영 오버헤드로 이러한 요구
사항을 충족하려면 어떻게 해야 합니까?
A. DynamoDB 트랜잭션을 사용하여 테이블에 새 이벤트 데이터를 씁니다. 내부 팀에
알리도록 트랜잭션을 구성합니다.
B. 현재 애플리케이션이 4개의 Amazon Simple 알림 서비스(Amazon SNS) 주제에 메시지를
게시하도록 합니다. 각 팀이 하나의 주제를 구독하게 합니다.
C. 테이블에서 Amazon DynamoDB 스트림을 활성화합니다. 트리거를 사용하여 팀이 구독할
수 있는 혼합 Amazon Simple 알림 서비스(Amazon SNS) 주제에 글을 쓰세요.
D. 새 항목에 플래그를 지정하려면 각 레코드에 사용자 정의 속성을 추가합니다. 매분마다
168

IT Certification Guaranteed, The Easy Way!
테이블에서 새로운 항목을 검색하고 팀이 구독할 수 있는 Amazon Simple Queue
Service(Amazon SOS) 대기열에 알리는 크론 작업을 작성합니다.
Answer: C
Explanation:
The best solution to meet these requirements with the least amount of operational overhead
is to enable Amazon DynamoDB Streams on the table and use triggers to write to a single
Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.
This solution requires minimal configuration and infrastructure setup, and Amazon
DynamoDB Streams provide a low-latency way to capture changes to the DynamoDB table.
The triggers automatically capture the changes and publish them to the SNS topic, which
notifies the internal teams.
QUESTION NO: 248
회사는 모든 기능이 활성화된 AWS Organizations를 사용하고 ap-southeast-2 지역에서 여러
Amazon EC2 워크로드를 실행합니다. 회사에는 다른 리전에서 리소스가 생성되지 않도록
하는 서비스 제어 정책(SCP)이 있습니다. 보안 정책에 따라 회사는 모든 저장 데이터를
암호화해야 합니다.
감사 결과 직원들이 볼륨을 암호화하지 않고 EC2 인스턴스용 Amazon Elastic Block
Store(Amazon EBS) 볼륨을 생성한 것으로 나타났습니다. 회사는 암호화된 EBS 볼륨을
사용하기 위해 1AM 사용자 또는 루트 사용자가 ap-southeast-2에서 시작하는 모든 새 EC2
인스턴스를 원합니다. 회사는 EBS 볼륨을 생성하는 직원에게 최소한의 영향을 미치는
솔루션을 원합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.)
A. Amazon EC2 콘솔에서 EBS 암호화 계정 속성을 선택하고 기본 암호화 키를 정의합니다.
B. 오전 1시 권한 경계를 만듭니다. 루트 조직 단위(OU)에 권한 경계를 연결합니다.
ec2:Encrypted 조건이 false일 때 ec2:CreateVolume 작업을 거부하도록 경계를 정의합니다.
C. SCR을 만듭니다. SCP를 루트 조직 단위(OU)에 연결합니다. ec2:Encrypted 조건이 false인
경우 ec2:CreateVolume 작업을 거부하도록 SCP를 정의합니다.
D. ec2:Encrypted 조건이 false인 경우 ec2:CreateVolume 작업을 거부하도록 각 계정의 오전
1시 정책을 업데이트합니다.
E. 조직 마스터 계정에서 기본 EBS 볼륨 암호화 설정을 지정합니다.
Answer: C
Explanation:
A service control policy (SCP) is a type of policy that you can use to manage permissions in
your organization. SCPs offer central control over the maximum available permissions for all
accounts in your organization, allowing you to ensure your accounts stay within your
organization's access control guidelines.
You can use an SCP to deny the ec2:CreateVolume action when the ec2:Encrypted condition
equals false, which means that any user or role in the accounts under the root OU will not be
able to create unencrypted EBS volumes. This solution will have minimal effect on
employees who create EBS volumes, as they can still create encrypted volumes as needed.
References: https://docs.aws.amazon.com/organizations/latest/userguide
/orgs_manage_policies_scps.html
QUESTION NO: 249
169

IT Certification Guaranteed, The Easy Way!
제조 회사에는 Amazon S3 버킷에 .csv 파일을 업로드하는 기계 센서가 있습니다. 이러한 .csv
파일은 이미지로 변환되어야 하며 그래픽 보고서의 자동 생성을 위해 가능한 한 빨리 사용할
수 있어야 합니다.
이미지는 1개월이 지나면 관련이 없게 되지만 1년에 두 번 기계 학습(ML) 모델을 훈련시키기
위해 .csv 파일을 보관해야 합니다. ML 교육 및 감사는 몇 주 전에 미리 계획됩니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 단계 조합은 무엇입니까? (2개를
선택하세요.)
A. 매시간 .csv 파일을 다운로드하고 이미지 파일을 생성하며 이미지를 S3 버킷에 업로드하는
Amazon EC2 스팟 인스턴스를 시작합니다.
B. .csv 파일을 이미지로 변환하고 이미지를 S3 버킷에 저장하는 AWS Lambda 함수를
설계합니다. .csv 파일이 업로드되면 Lambda 함수를 호출합니다.
C. S3 버킷의 .csv 파일 및 이미지 파일에 대한 S3 수명 주기 규칙을 생성합니다. .csv 파일을
업로드하고 1일 후에 S3 Standard에서 S3 Glacier로 전환합니다. 30일 후에 이미지 파일을
만료하십시오.
D. S3 버킷의 .csv 파일 및 이미지 파일에 대한 S3 수명 주기 규칙을 생성합니다. 업로드 1일
후 .csv 파일을 S3 Standard에서 S3 One Zone-Infrequent Access(S3 One Zone-IA)로
전환합니다. 30일 후에 이미지 파일을 만료하십시오.
E. S3 버킷의 .csv 파일 및 이미지 파일에 대한 S3 수명 주기 규칙을 생성합니다. 업로드 1일
후 .csv 파일을 S3 Standard에서 S3 Standard-Infrequent Access(S3 Standard-IA)로
전환합니다. RRS(Reduced Redundancy Storage)에 이미지 파일을 보관합니다.
Answer: B C
Explanation:
These answers are correct because they meet the requirements of converting the .csv files
into images, making them available as soon as possible, and minimizing the storage costs.
AWS Lambda is a service that lets you run code without provisioning or managing servers.
You can use AWS Lambda to design a function that converts the .csv files into images and
stores the images in the S3 bucket. You can invoke the Lambda function when a .csv file is
uploaded to the S3 bucket by using an S3 event notification. This way, you can ensure that
the images are generated and made available as soon as possible for the graphical reports.
S3 Lifecycle is a feature that enables you to manage your objects so that they are stored cost
effectively throughout their lifecycle. You can create S3 Lifecycle rules for .csv files and
image files in the S3 bucket to transition them to different storage classes or expire them
based on your business needs. You can transition the .csv files from S3 Standard to S3
Glacier 1 day after they are uploaded, since they are only needed twice a year for ML
trainings and audits that are planned weeks in advance. S3 Glacier is a storage class for data
archiving that offers secure, durable, and extremely low-cost storage with retrieval times
ranging from minutes to hours. You can expire the image files after 30 days, since they
become irrelevant after 1 month.
References:
* https://docs.aws.amazon.com/lambda/latest/dg/welcome.html
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-
glacier
170

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 250
회사는 새 문서가 Amazon S3 버킷에 업로드될 때 AWS Lambda 기능을 호출하는 서버리스
애플리케이션을 배포했습니다. 애플리케이션은 Lambda 기능을 사용하여 문서를 처리합니다.
최근 마케팅 캠페인 이후 회사는 애플리케이션이 많은 문서를 처리하지 않는다는 사실을
발견했습니다. 문서 솔루션 설계자는 이 애플리케이션의 아키텍처를 개선하기 위해 무엇을
해야 합니까?
A. Lambda 함수의 런타임 제한 시간 값을 15분으로 설정합니다.
B. S3 버킷 복제 정책 구성 나중에 처리할 수 있도록 S3 버킷에 문서를 준비합니다.
C. 추가 Lambda 함수 배포 두 Lambda 함수에 걸쳐 문서 처리의 로드 밸런싱
D. Amazon Simple Queue Service(Amazon SOS) 대기열을 생성합니다. 대기열에 요청을
보냅니다. 대기열을 Lambda에 대한 이벤트 소스로 구성합니다.
Answer: D
Explanation:
To improve the architecture of this application, the best solution would be to use Amazon
Simple Queue Service (Amazon SQS) to buffer the requests and decouple the S3 bucket
from the Lambda function. This will ensure that the documents are not lost and can be
processed at a later time if the Lambda function is not available. This will ensure that the
documents are not lost and can be processed at a later time if the Lambda function is not
available. By using Amazon SQS, the architecture is decoupled and the Lambda function can
process the documents in a scalable and fault-tolerant manner
QUESTION NO: 251
회사에서 전자상거래 애플리케이션을 구축 중이며 중요한 고객 정보를 저장해야 합니다.
회사는 고객에게 웹사이트에서 구매 거래를 완료할 수 있는 기능을 제공해야 합니다. 또한
회사는 데이터베이스 관리자로부터도 민감한 고객 데이터가 보호되는지 확인해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 민감한 데이터를 Amazon Elastic Block Store(Amazon EBS) 볼륨에 저장합니다. EBS
암호화를 사용하여 데이터를 암호화합니다. IAM 인스턴스 역할을 사용하여 액세스를
제한합니다.
B. MySQL용 Amazon RDS에 민감한 데이터를 저장합니다. AWS Key Management
Service(AWS KMS) 클라이언트 측 암호화를 사용하여 데이터를 암호화합니다.
C. 민감한 데이터를 Amazon S3에 저장합니다. AWS Key Management Service(AWS KMS)
서버 측 암호화를 사용하여 데이터를 암호화합니다. S3 버킷 정책을 사용하여 액세스를
제한합니다.
D. Windows Server용 Amazon FSx에 민감한 데이터를 저장합니다. 응용 프로그램 서버에
파일 공유를 탑재합니다. Windows 파일 권한을 사용하여 액세스를 제한하세요.
Answer: B
Explanation:
it allows the company to store sensitive customer information in a managed AWS service and
give customers the ability to complete purchase transactions on the website. By using AWS
Key Management Service (AWS KMS) client-side encryption, the company can encrypt the
data before sending it to Amazon RDS for MySQL. This ensures that sensitive customer data
is protected, even from database administrators, as only the application has access to the
encryption keys. References:
171

IT Certification Guaranteed, The Easy Way!
* Using Encryption with Amazon RDS for MySQL
* Encrypting Amazon RDS Resources
QUESTION NO: 252
회사는 Amazon S3에 기밀 데이터를 저장할 준비를 하고 있습니다. 규정 준수를 위해
데이터를 저장 시 암호화해야 합니다. 암호화 키 사용은 감사 목적으로 기록되어야 합니다.
열쇠는 매년 교체해야 합니다.
이러한 요구 사항과 운영상 가장 효율적인 솔루션은 무엇입니까?
A. 고객 제공 키를 사용한 서버 측 암호화(SSE-C)
B. Amazon S3 관리형 키(SSE-S3)를 사용한 서버 측 암호화
C. 수동 교체가 포함된 AWS KMS(SSE-KMS) 고객 마스터 키(CMK)를 사용한 서버 측 암호화
D. 자동 교체 기능이 있는 AWS KMS(SSE-KMS) 고객 마스터 키(CMK)를 사용한 서버 측
암호화
Answer: D
Explanation:
https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html
When you enable automatic key rotation for a customer managed key, AWS KMS generates
new cryptographic material for the KMS key every year. AWS KMS also saves the KMS key's
older cryptographic material in perpetuity so it can be used to decrypt data that the KMS key
encrypted.
Key rotation in AWS KMS is a cryptographic best practice that is designed to be transparent
and easy to use.
AWS KMS supports optional automatic key rotation only for customer managed CMKs.
Enable and disable key rotation. Automatic key rotation is disabled by default on customer
managed CMKs. When you enable (or re-enable) key rotation, AWS KMS automatically
rotates the CMK 365 days after the enable date and every
365 days thereafter.
QUESTION NO: 253
회사는 온프레미스 데이터 센터의 Kubernetes 클러스터에서 컨테이너화된 애플리케이션을
실행합니다. 회사는 데이터 저장을 위해 MongoDB 데이터베이스를 사용하고 있습니다.
회사는 이러한 환경 중 일부를 AWS로 마이그레이션하려고 하지만 현재로서는 코드 변경이나
배포 방법 변경이 불가능합니다. 회사에는 운영 오버헤드를 최소화하는 솔루션이 필요합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 컴퓨팅을 위해 Amazon EC2 작업자 노드와 함께 Amazon Elastic Container
Service(Amazon ECS)를 사용하고 데이터 저장을 위해 EC2의 MongoDB를 사용합니다.
B. 컴퓨팅에는 AWS Fargate, 데이터 저장에는 Amazon DynamoDB와 함께 Amazon Elastic
Container Service(Amazon ECS)를 사용하세요.
C. 컴퓨팅을 위해 Amazon EC2 작업자 노드와 함께 Amazon Elastic Kubernetes
Service(Amazon EKS)를 사용하고 데이터 저장을 위해 Amazon DynamoDB를 사용합니다.
D. 컴퓨팅을 위해 AWS Fargate와 데이터 저장을 위해 Amazon DocumentDB(MongoDB
호환)와 함께 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용합니다.
Answer: D
Explanation:
Amazon DocumentDB (with MongoDB compatibility) is a fast, reliable, and fully managed
172

IT Certification Guaranteed, The Easy Way!
database service.
Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible
databases in the cloud. With Amazon DocumentDB, you can run the same application code
and use the same drivers and tools that you use with MongoDB.
https://docs.aws.amazon.com/documentdb/latest/developerguide/what-is.html
QUESTION NO: 254
한 회사가 Amazon EC2 인스턴스와 Amazon RDS에서 2계층 애플리케이션을 호스팅합니다.
애플리케이션의 수요는 하루 중 시간에 따라 다릅니다. 업무 시간 이후와 주말에는 부하가
최소화됩니다. EC2 인스턴스는 최소 2개, 최대 5개의 인스턴스로 구성된 EC2 Auto Scaling
그룹에서 실행됩니다. 애플리케이션은 항상 사용할 수 있어야 하지만 회사는 전반적인 비용을
걱정합니다.
가용성 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 모든 EC2 스팟 인스턴스를 사용합니다. RDS 데이터베이스를 사용하지 않을 때는
중지하십시오.
B. 5개의 EC2 인스턴스를 보장하는 EC2 Instance Savings Plan을 구매하세요. RDS 예약 DB
인스턴스 구매
C. EC2 예약 인스턴스 2개 구매 필요에 따라 최대 3개의 추가 EC2 스팟 인스턴스를
사용합니다. RDS 데이터베이스를 사용하지 않을 때는 중지하십시오.
D. EC2 인스턴스 2개를 보장하는 EC2 Instance Savings Plan을 구매하세요. 필요에 따라
최대 3개의 추가 EC2 온디맨드 인스턴스를 사용하세요. RDS 예약 DB 인스턴스를
구매하세요.
Answer: C
Explanation:
This solution meets the requirements of a two-tier application that has a variable demand
based on the time of day and must be available at all times, while minimizing the overall cost.
EC2 Reserved Instances can provide significant savings compared to On-Demand Instances
for the baseline level of usage, and they can guarantee capacity reservation when needed.
EC2 Spot Instances can provide up to 90% savings compared to On- Demand Instances for
any additional capacity that the application needs during peak hours. Spot Instances are
suitable for stateless applications that can tolerate interruptions and can be replaced by other
instances.
Stopping the RDS database when it is not in use can reduce the cost of running the database
tier.
Option A is incorrect because using all EC2 Spot Instances can affect the availability of the
application if there are not enough spare capacity or if the Spot price exceeds the maximum
price. Stopping the RDS database when it is not in use can reduce the cost of running the
database tier, but it can also affect the availability of the application. Option B is incorrect
because purchasing EC2 Instance Savings Plans to cover five EC2 instances can lock in a
fixed amount of compute usage per hour, which may not match the actual usage pattern of
the application. Purchasing an RDS Reserved DB Instance can provide savings for the
database tier, but it does not allow stopping the database when it is not in use. Option D is
incorrect because purchasing EC2 Instance Savings Plans to cover two EC2 instances can
lock in a fixed amount of compute usage per hour, which may not match the actual usage
pattern of the application. Using up to three additional EC2 On-Demand Instances as needed
173

IT Certification Guaranteed, The Easy Way!
can incur higher costs than using Spot Instances.
References:
* https://aws.amazon.com/ec2/pricing/reserved-instances/
* https://aws.amazon.com/ec2/spot/
* https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_StopInstance.html
QUESTION NO: 255
한 회사가 상당한 양의 민감한 고객 데이터를 저장하는 AWS 환경의 보안을 강화하고
있습니다. 이 회사는 여러 Amazon S3 버킷에 저장된 민감한 데이터를 자동으로 식별하고
분류하는 솔루션이 필요합니다. 이 솔루션은 데이터 침해에 자동으로 대응하고, 규정을
준수하지 않는 데이터가 발견되면 즉시 이메일을 통해 회사 보안 팀에 경고해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon GuardDuty를 사용합니다. AWS Lambda 함수를 구성하여 알림을 Amazon Simple
Notification Service(Amazon SNS) 주제로 라우팅합니다. 보안 팀을 SNS 주제에 구독합니다.
B. Amazon GuardDuty를 사용합니다. AWS Lambda 함수를 구성하여 알림을 Amazon Simple
Queue Service(Amazon SQS) 대기열로 라우팅합니다. 두 번째 Lambda 함수를 구성하여
SQS 대기열을 주기적으로 폴링하고 Amazon Simple Email Service(Amazon SES)를
사용하여 보안 팀에 이메일을 보냅니다.
C. Amazon Macie를 사용합니다. Amazon EventBridge를 Macie와 통합하고 EventBridge를
구성하여 Amazon Simple Notification Service(Amazon SNS) 토픽에 알림을 보냅니다. 보안
팀을 SNS 토픽에 구독합니다.
D. Amazon Macie를 사용합니다. Amazon EventBridge를 Macie와 통합하고 EventBridge를
구성하여 알림을 Amazon Simple Queue Service(Amazon SQS) 대기열로 라우팅합니다.
AWS Lambda 함수를 구성하여 SQS 대기열을 주기적으로 폴링하고 Amazon Simple Email
Service(Amazon SES)를 사용하여 보안 팀에 알림을 보냅니다.
Answer: C
Explanation:
* A & B. GuardDuty: Designed for threat detection, not for identifying or classifying sensitive
data in S3 buckets.
* C. Macie with EventBridge + SNS: Automatically identifies sensitive data, triggers alerts,
and uses SNS for immediate notification via email.
* D. Macie with EventBridge + SQS: Introduces latency due to periodic polling and adds
unnecessary complexity.
References: Amazon Macie, Amazon EventBridge
QUESTION NO: 256
한 회사에는 개발 AWS 계정에서 실행되는 여러 Amazon RDS DB 인스턴스가 있습니다. 모든
인스턴스에는 개발 리소스로 식별하는 태그가 있습니다. 이 회사는 개발 DB 인스턴스가 영업
시간 동안만 일정에 따라 실행되기를 원합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. 중지해야 할 RDS 인스턴스를 식별하기 위해 Amazon CloudWatch 알람을 만듭니다. RDS
인스턴스를 시작 및 중지하기 위한 AWS Lambda 함수를 만듭니다.
B. 시작 및 중지할 RDS 인스턴스를 식별하기 위해 AWS Trusted Advisor 보고서를 만듭니다.
RDS 인스턴스를 시작 및 중지하기 위한 AWS Lambda 함수를 만듭니다.
C. RDS 인스턴스를 시작 및 중지하기 위해 AWS Systems Manager State Manager 연결을
174

IT Certification Guaranteed, The Easy Way!
생성합니다.
D. RDS 인스턴스를 시작 및 중지하기 위해 AWS Lambda 함수를 호출하는 Amazon
EventBridge 규칙을 만듭니다.
Answer: D
Explanation:
To run RDS instances only during business hours with the least operational overhead, you
can use Amazon EventBridge to schedule events that invoke AWS Lambda functions. The
Lambda functions can be configured to start and stop the RDS instances based on the
specified schedule (business hours). EventBridge rules allow you to define recurring events
easily, and Lambda functions provide a serverless way to manage RDS instance start and
stop operations, reducing administrative overhead.
* Option A: While CloudWatch alarms could be used, they are more suited for monitoring,
and using Lambda with EventBridge is simpler.
* Option B (Trusted Advisor): Trusted Advisor is not ideal for scheduling tasks.
* Option C (Systems Manager): Systems Manager could also work, but EventBridge and
Lambda offer a more streamlined and lower-overhead solution.
AWS References:
* Amazon EventBridge Scheduler
* AWS Lambda
QUESTION NO: 257
회사는 단일 Amazon EC2 인스턴스에서 실행되는 콘텐츠 관리 시스템을 사용하고 있습니다.
EC2 인스턴스에는 웹 서버와 데이터베이스 소프트웨어가 모두 포함되어 있습니다. 회사는 웹
사이트 플랫폼의 가용성을 높여야 하며 사용자 요구에 맞게 웹 사이트를 확장할 수 있어야
합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 권장해야 합니까?
A. 데이터베이스를 Amazon RDS로 이동하고 자동 백업을 활성화합니다. 동일한 가용
영역에서 다른 EC2 인스턴스를 수동으로 시작합니다. 가용 영역에서 Application Load
Balancer를 구성하고 두 인스턴스를 대상으로 설정합니다.
B. 기존 EC2 인스턴스와 동일한 가용 영역에 읽기 전용 복제본이 있는 Amazon Aurora
인스턴스로 데이터베이스를 마이그레이션합니다. 동일한 가용 영역에서 다른 EC2
인스턴스를 수동으로 시작합니다. Application Load Balancer를 구성하고 두 개의 EC2
인스턴스를 대상으로 설정합니다.
C. 다른 가용 영역에 읽기 전용 복제본이 있는 Amazon Aurora로 데이터베이스를 이동합니다.
EC2 인스턴스에서 Amazon 머신 이미지(AMI)를 생성합니다. 두 개의 가용 영역에 Application
Load Balancer를 구성합니다. 두 개의 가용 영역에서 AMI를 사용하는 Auto Scaling 그룹을
연결합니다.
D. 데이터베이스를 별도의 EC2 인스턴스로 이동하고 Amazon S3에 백업을 예약합니다. 원본
EC2 인스턴스에서 Amazon 머신 이미지(AMI)를 생성합니다. 두 개의 가용 영역에 Application
Load Balancer를 구성합니다. 두 개의 가용 영역에서 AMI를 사용하는 Auto Scaling 그룹을
연결합니다.
Answer: C
Explanation:
This approach will provide both high availability and scalability for the website platform. By
moving the database to Amazon Aurora with a read replica in another availability zone, it will
175

IT Certification Guaranteed, The Easy Way!
provide a failover option for the database. The use of an Application Load Balancer and an
Auto Scaling group across two availability zones allows for automatic scaling of the website
to meet increased user demand. Additionally, creating an AMI from the original EC2 instance
allows for easy replication of the instance in case of failure.
QUESTION NO: 258
이미지 호스팅 회사는 객체를 Amazon S3 버킷에 저장합니다. 회사는 S3 버킷에 있는 객체가
실수로 대중에게 노출되는 것을 방지하려고 합니다. 전체 AWS 계정의 모든 S3 객체는
비공개로 유지되어야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon GuardDuty를 사용하여 S3 버킷 정책 모니터링 AWS Lambda 함수를 사용하여
객체를 공개하는 모든 변경 사항을 수정하는 자동 수정 작업 규칙을 생성합니다.
B. AWS Trusted Advisor를 사용하여 공개적으로 액세스 가능한 S3 Dockets 찾기 이메일 알림
구성 Trusted Advisor에서 변경 사항이 감지되면 공개 액세스를 허용하는 경우 S3 버킷 정책을
수동으로 변경합니다.
C. AWS Resource Access Manager를 사용하여 공개적으로 액세스할 수 있는 S3 버킷 찾기
변경 사항이 감지되면 Amazon Simple 알림 서비스(Amazon SNS)를 사용하여 AWS Lambda
함수를 호출합니다. 프로그래밍 방식으로 변경 사항을 해결하는 Lambda 함수를 배포합니다.
D. 계정 수준에서 S3 공개 액세스 차단 기능을 사용합니다. AWS Organizations를 사용하여
IAM 사용자가 설정을 변경하지 못하도록 하는 서비스 제어 정책(SCP)을 생성합니다. 연결
계정에 연결 SCP 적용
Answer: D
Explanation:
The S3 Block Public Access feature allows you to restrict public access to S3 buckets and
objects within the account. You can enable this feature at the account level to prevent any S3
bucket from being made public, regardless of the bucket policy settings. AWS Organizations
can be used to apply a Service Control Policy (SCP) to the account to prevent IAM users
from changing this setting, ensuring that all S3 objects remain private. This is a
straightforward and effective solution that requires minimal operational overhead.
QUESTION NO: 259
회사는 AWS 클라우드에서 워크로드를 실행합니다. 회사는 보안 데이터를 중앙에서 수집하여
회사 전체의 보안을 평가하고 워크로드 보호를 개선하려고 합니다.
최소한의 개발 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Lake Formation에서 데이터 레이크 구성 AWS Glue 크롤러를 사용하여 보안
데이터를 데이터 레이크로 수집합니다.
B. csv 형식으로 보안 데이터를 수집하도록 AWS Lambda 함수를 구성합니다. Amazon S3
버킷에 데이터 업로드
C. Amazon Security Lake에서 보안 데이터를 수집하도록 데이터 레이크를 구성합니다.
Amazon S3 버킷에 데이터를 업로드합니다.
D. 보안 데이터를 Amazon RDS 클러스터에 로드하도록 AWS Database Migration
Service(AWS DMS) 복제 인스턴스를 구성합니다.
Answer: C
Explanation:
* Understanding the Requirement: The company wants to centrally collect security data with
minimal development effort to assess and improve security across all workloads.
176

IT Certification Guaranteed, The Easy Way!
* Analysis of Options:
* Amazon Security Lake: This is a purpose-built service for centralizing security data from
across AWS services and third-party sources into a data lake. It provides native integration
and requires minimal development effort to set up.
* AWS Lake Formation with AWS Glue: While this can be used to create a data lake, it
requires more development effort to set up and configure Glue crawlers for ingestion.
* AWS Lambda with S3: This approach involves custom development to collect and process
security data before storing it in S3, which requires more effort.
* AWS DMS to RDS: AWS Database Migration Service is typically used for database
migrations and is not suited for collecting and analyzing security data.
* Best Option for Minimal Development Effort:
* Amazon Security Lake provides the least development effort for setting up a centralized
repository for security data. It simplifies data ingestion and management, making it the most
efficient solution for this use case.
References:
* Amazon Security Lake
* AWS Lake Formation
* AWS Glue
QUESTION NO: 260
회사는 온프레미스 NAS(Network Attached Storage) 시스템에서 AWS 클라우드로 600TB의
데이터를 전송해야 합니다. 데이터 전송은 2주 이내에 완료되어야 합니다. 데이터는
중요하므로 전송 중에 암호화되어야 합니다. 회사의 인터넷 연결은 100Mbps의 업로드 속도를
지원할 수 있습니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. Amazon S3 멀티파트 업로드 기능을 사용하여 HTTPS를 통해 수수료를 전송합니다.
B. 온프레미스 NAS 시스템과 가장 가까운 AWS 리전 간에 VPN 연결을 생성합니다. VPN
연결을 통해 데이터를 전송합니다.
C. AWS Snow Family 콘솔을 사용하여 여러 AWS Snowball Edge Storage Optimized
디바이스를 주문합니다. 디바이스를 사용하여 Amazon S3에 데이터를 전송합니다.
D. 회사 위치와 가장 가까운 AWS 지역 간에 10Gbps AWS Direct Connect 연결을 설정합니다.
VPN 연결을 통해 데이터를 지역으로 전송하여 Amazon S3에 데이터를 저장합니다.
Answer: C
Explanation:
The best option is to use the AWS Snow Family console to order several AWS Snowball
Edge Storage Optimized devices and use the devices to transfer the data to Amazon S3.
Snowball Edge is a petabyte-scale data transfer device that can help transfer large amounts
of data securely and quickly. Using Snowball Edge can be the most cost-effective solution for
transferring large amounts of data over long distances and can help meet the requirement of
transferring 600 TB of data within two weeks.
QUESTION NO: 261
회사는 온프레미스 LDAP 디렉터리 서비스를 사용하여 AWS Management Console에
사용자를 인증해야 합니다. 디렉터리 서비스는 SAML(Security Assertion Markup
Language)과 호환되지 않습니다.
177

IT Certification Guaranteed, The Easy Way!
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS와 온프레미스 LDAP 간에 AWS 1AM Identity Center(AWS Single Sign-On)를
활성화합니다.
B. AWS 자격 증명을 사용하는 오전 1시 정책을 생성하고 해당 정책을 LDAP에 통합합니다.
C. LDAP 자격 증명이 업데이트될 때마다 IAM 자격 증명을 교체하는 프로세스를 설정합니다.
D. AWS Security Token Service(AWS STS)를 사용하여 단기 자격 증명을 얻는 온프레미스
사용자 지정 자격 증명 브로커 애플리케이션 또는 프로세스를 개발합니다.
Answer: D
Explanation:
The solution that meets the requirements is to develop an on-premises custom identity broker
application or process that uses AWS Security Token Service (AWS STS) to get short-lived
credentials. This solution allows the company to use its existing LDAP directory service to
authenticate its users to the AWS Management Console, without requiring SAML
compatibility. The custom identity broker application or process can act as a proxy between
the LDAP directory service and AWS STS, and can request temporary security credentials
for the users based on their LDAP attributes and roles. The users can then use these
credentials to access the AWS Management Console via a sign-in URL generated by the
identity broker. This solution also enhances security by using short-lived credentials that
expire after a specified duration.
The other solutions do not meet the requirements because they either require SAML
compatibility or do not provide access to the AWS Management Console. Enabling AWS IAM
Identity Center (AWS Single Sign- On) between AWS and the on-premises LDAP would
require the LDAP directory service to support SAML
2.0, which is not the case for this scenario. Creating an IAM policy that uses AWS credentials
and integrating the policy into LDAP would not provide access to the AWS Management
Console, but only to the AWS APIs. Setting up a process that rotates the IAM credentials
whenever LDAP credentials are updated would also not provide access to the AWS
Management Console, but only to the AWS CLI. Therefore, these solutions are not suitable
for the given requirements.
QUESTION NO: 262
회사는 VPC에서 실행되는 Amazon EC2 인스턴스를 배포합니다. EC2 인스턴스는 나중에
데이터를 처리할 수 있도록 소스 데이터를 Amazon S3 버킷에 로드합니다. 규정 준수 법률에
따라 데이터는 공용 인터넷을 통해 전송되어서는 안 됩니다. 회사의 온프레미스 데이터
센터에 있는 서버는 LC2 인스턴스에서 실행되는 애플리케이션의 출력을 사용합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon EC2용 인터페이스 VPC 엔드포인트를 배포합니다. 회사와 VPC 간에 AWS Site-
to-Site VPN 연결을 생성합니다.
B. Amazon S3용 게이트웨이 VPC 엔드포인트를 배포합니다. 온프레미스 네트워크와 VPC
간에 AWS Direct Connect 연결을 설정합니다.
C. VPC에서 S3 버킷으로의 AWS Transit Gateway 연결을 설정합니다. 회사와 VPC 간에
AWS Site-to-Site VPN 연결을 생성합니다.
D. NAT 게이트웨이에 대한 경로가 있는 프록시 EC2 인스턴스를 설정합니다. 프록시 EC2
인스턴스를 구성하여 S3 데이터를 가져오고 애플리케이션 인스턴스에 공급합니다.
178

IT Certification Guaranteed, The Easy Way!
Answer: B
Explanation:
* Understanding the Requirement: EC2 instances need to upload data to S3 without using
the public internet, and on-premises servers need to consume this data.
* Analysis of Options:
* Interface VPC Endpoint for EC2: Not relevant for accessing S3.
* Gateway VPC Endpoint for S3 and Direct Connect: Provides private connectivity from EC2
instances to S3 and from on-premises to AWS, ensuring compliance with the requirement to
avoid public internet.
* Transit Gateway and Site-to-Site VPN: Adds unnecessary complexity and does not provide
the same level of performance as Direct Connect.
* Proxy EC2 Instances with NAT Gateways: Increases complexity and costs compared to a
direct connection using VPC endpoints and Direct Connect.
* Best Solution:
* Gateway VPC Endpoint for S3 and Direct Connect: This solution ensures secure, private
data transfer both within AWS and between on-premises and AWS, meeting the compliance
requirements effectively.
References:
* Amazon VPC Endpoints for S3
* AWS Direct Connect
QUESTION NO: 263
회사에서 데이터 저장을 위해 Amazon DynamoDB 테이블을 사용할 계획입니다. 회사는 비용
최적화에 대해 우려하고 있습니다. 대부분의 아침에는 테이블을 사용하지 않습니다. 저녁에는
읽기 및 쓰기 트래픽이 예측할 수 없는 경우가 많습니다. 트래픽 급증이 발생하면 매우 빠르게
발생합니다.
솔루션 아키텍트는 무엇을 추천해야 합니까?
A. 온디맨드 용량 모드에서 DynamoDB 테이블을 생성합니다.
B. 글로벌 보조 인덱스가 있는 DynamoDB 테이블을 생성합니다.
C. 프로비저닝된 용량 및 Auto Scaling으로 DynamoDB 테이블을 생성합니다.
D. 프로비저닝된 용량 모드에서 DynamoDB 테이블을 생성하고 전역 테이블로 구성합니다.
Answer: A
Explanation:
Provisioned capacity is best if you have relatively predictable application traffic, run
applications whose traffic is consistent, and ramps up or down gradually. On-demand
capacity mode is best when you have unknown workloads, unpredictable application traffic
and also if you only want to pay exactly for what you use. The on-demand pricing model is
ideal for bursty, new, or unpredictable workloads whose traffic can spike in seconds or
minutes, and when under-provisioned capacity would impact the user experience.
https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-
lens/capacity.html
QUESTION NO: 264
회사는 여러 AWS 계정의 AWS CloudTrail 로그를 중앙 집중식 계정의 Amazon S3 버킷으로
보냅니다. 회사는 CloudTrail 로그를 보관해야 합니다. 회사는 언제든지 CloudTrail 로그를
179

IT Certification Guaranteed, The Easy Way!
쿼리할 수 있어야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 중앙 집중식 계정의 CloudTraiI 이벤트 기록을 사용하여 Amazon Athena 테이블을
생성합니다. Athena에서 CloudTrail 로그를 쿼리합니다.
B. CloudTrail 로그를 관리하도록 Amazon Neptune 인스턴스를 구성합니다. Neptune에서
CloudTraiI 로그를 쿼리합니다.
C. 로그를 Amazon DynamoDB 테이블로 보내도록 CloudTrail을 구성합니다. Amazon
QulCkSight에서 대시보드를 생성하여 테이블의 로그를 쿼리합니다.
D. Amazon Athena를 사용하여 Athena 노트북을 생성합니다. 로그를 노트북으로 보내도록
CloudTrail을 구성합니다. Athena에서 쿼리를 실행합니다.
Answer: A
Explanation:
it allows the company to keep the CloudTrail logs and query them at any time. By using the
CloudTrail event history in the centralized account, the company can view, filter, and
download recent API activity across multiple AWS accounts. By creating an Amazon Athena
table from the CloudTrail event history, the company can use a serverless interactive query
service that makes it easy to analyze data in S3 using standard SQL. By querying the
CloudTrail logs from Athena, the company can gain insights into user activity and resource
changes. References:
* Viewing Events with CloudTrail Event History
* Querying AWS CloudTrail Logs
* Amazon Athena
QUESTION NO: 265
한 회사는 금융 위험 모델링을 위해 AWS의 고성능 컴퓨팅(HPC) 인프라를 사용하려고
합니다. 회사의 HPC 워크로드는 Linux에서 실행됩니다. 각 HPC 워크플로는 수백 개의
Amazon EC2 스팟 인스턴스에서 실행되고, 수명이 짧으며, 분석 및 향후 장기적 사용을 위해
최종적으로 영구 스토리지에 저장되는 수천 개의 출력 파일을 생성합니다.
회사는 모든 EC2 인스턴스에서 데이터를 처리할 수 있도록 온프레미스 데이터를 장기 영구
스토리지로 복사할 수 있는 클라우드 스토리지 솔루션을 찾고 있습니다. 또한 솔루션은
데이터세트와 출력 파일을 읽고 쓰기 위해 영구 스토리지와 통합된 고성능 파일 시스템이어야
합니다.
이러한 요구 사항을 충족하는 AWS 서비스 조합은 무엇입니까?
A. Amazon S3와 통합된 Lustre용 Amazon FSx
B. Amazon S3와 통합된 Windows 파일 서버용 Amazon FSx
C. Amazon Elastic Block Store(Amazon EBS)와 통합된 Amazon S3 Glacier
D. Amazon Elastic Block Store(Amazon EBS) 범용 SSD(gp2) 볼륨과 통합된 VPC
엔드포인트가 있는 Amazon S3 버킷
Answer: A
Explanation:
https://aws.amazon.com/fsx/lustre/
Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-
performance, scalable storage for compute workloads. Many workloads such as machine
learning, high performance computing (HPC), video rendering, and financial simulations
depend on compute instances accessing the same set of data through high-performance
180

IT Certification Guaranteed, The Easy Way!
shared storage.
QUESTION NO: 266
솔루션 설계자가 애플리케이션을 위한 새로운 Amazon CloudFront 배포를 생성하고
있습니다. 사용자가 제출한 정보 중 일부는 민감한 정보입니다. 애플리케이션은 HTTPS를
사용하지만 다른 보안 계층이 필요합니다. 민감한 정보는 전체 애플리케이션 스택에서
보호되어야 하며 정보에 대한 액세스는 특정 애플리케이션으로 제한되어야 합니다.
솔루션 아키텍트는 어떤 조치를 취해야 합니까?
A. CloudFront 서명된 URL을 구성합니다.
B. CloudFront 서명 쿠키를 구성합니다.
C. CloudFront 필드 수준 암호화 프로필을 구성합니다.
D. CloudFront를 구성하고 뷰어 프로토콜 정책에 대해 오리진 프로토콜 정책 설정을 HTTPS
전용으로 설정합니다.
Answer: C
Explanation:
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-
encryption.html
"With Amazon CloudFront, you can enforce secure end-to-end connections to origin servers
by using HTTPS.
Field-level encryption adds an additional layer of security that lets you protect specific data
throughout system processing so that only certain applications can see it."
QUESTION NO: 267
회사는 기존 3계층 웹 아키텍처의 비용을 절감하려고 합니다. 웹, 애플리케이션 및
데이터베이스 서버는 개발, 테스트 및 프로덕션 환경을 위한 Amazon EC2 인스턴스에서
실행됩니다. EC2 인스턴스의 평균 CPU 사용률은 사용량이 많은 시간에는 30%이고 사용량이
많지 않은 시간에는 10%입니다.
프로덕션 EC2 인스턴스는 하루 24시간 실행됩니다. 개발 및 테스트 EC2 인스턴스는 매일
최소 8시간 동안 실행됩니다. 회사는 개발을 중지하고 사용하지 않을 때 EC2 인스턴스를
테스트하는 자동화를 구현할 계획입니다.
어떤 EC2 인스턴스 구매 솔루션이 가장 비용 효율적으로 회사의 요구 사항을 충족합니까?
A. 프로덕션 EC2 인스턴스에 스팟 인스턴스를 사용합니다. EC2 인스턴스 개발 및 테스트에
예약 인스턴스를 사용합니다.
B. 프로덕션 EC2 인스턴스에 예약 인스턴스를 사용합니다. 개발 및 테스트 EC2 인스턴스에
온디맨드 인스턴스를 사용합니다.
C. 프로덕션 EC2 인스턴스에 스팟 블록을 사용합니다. EC2 인스턴스 개발 및 테스트에 예약
인스턴스를 사용합니다.
D. 프로덕션 EC2 인스턴스에 온디맨드 인스턴스를 사용합니다. 개발 및 테스트 EC2
인스턴스에 스팟 블록을 사용합니다.
Answer: B
QUESTION NO: 268
한 회사가 AWS에서 새로운 애플리케이션을 구현하고 있습니다. 이 회사는 여러 AWS 지역
내의 여러 가용성 영역에 걸쳐 여러 Amazon EC2 인스턴스에서 애플리케이션을 실행합니다.
이 애플리케이션은 인터넷을 통해 사용할 수 있습니다. 사용자는 전 세계에서 이
181

IT Certification Guaranteed, The Easy Way!
애플리케이션에 액세스할 것입니다.
회사에서는 애플리케이션에 액세스하는 각 사용자가 사용자 위치에 가장 가까운 EC2
인스턴스로 전송되도록 하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon Route 53 지리적 위치 라우팅 정책을 구현합니다. 인터넷 연결 애플리케이션 로드
밸런서를 사용하여 동일한 지역 내의 모든 가용성 영역에 트래픽을 분산합니다.
B. Amazon Route 53 지리적 근접 라우팅 정책을 구현합니다. 인터넷 연결 네트워크 로드
밸런서를 사용하여 동일한 지역 내의 모든 가용성 영역에 트래픽을 분산합니다.
C. Amazon Route 53 다중값 답변 라우팅 정책을 구현합니다. 인터넷에 연결된 애플리케이션
부하 분산 장치를 사용하여 동일한 지역 내 모든 가용성 영역에 트래픽을 분산합니다.
D. Amazon Route 53 가중 라우팅 정책을 구현합니다. 인터넷 연결 네트워크 로드 밸런서를
사용하여 동일한 지역 내의 모든 가용성 영역에 트래픽을 분산합니다.
Answer: A
Explanation:
The requirement is to route users to the nearest AWS Region where the application is
deployed. The best solution is to use Amazon Route 53 with a geolocation routing policy,
which routes traffic based on the geographic location of the user making the request.
* Geolocation Routing: This routing policy ensures that users are directed to the resources (in
this case, EC2 instances) that are geographically closest to them, thereby reducing latency
and improving the user experience.
* Application Load Balancer (ALB): Within each Region, an internet-facing Application Load
Balancer (ALB) is used to distribute incoming traffic across multiple EC2 instances in different
Availability Zones. ALBs are designed to handle HTTP/HTTPS traffic and provide advanced
features like content- based routing, SSL termination, and user authentication.
* Why Not Other Options?:
* Option B (Geoproximity + NLB): Geoproximity routing is similar but more complex as it
requires fine-tuning the proximity settings. A Network Load Balancer (NLB) is better suited for
TCP/UDP traffic rather than HTTP/HTTPS.
* Option C (Multivalue Answer Routing + ALB): Multivalue answer routing does not direct
traffic based on user location but rather returns multiple values and lets the client choose.
This does not meet the requirement for geographically routing users.
* Option D (Weighted Routing + NLB): Weighted routing splits traffic based on predefined
weights and does not consider the user's geographic location. NLB is not ideal for this
scenario due to its focus on lower-level protocols.
AWS References:
* Amazon Route 53 Routing Policies - Detailed explanation of the various routing policies
available in Route 53, including geolocation.
* Elastic Load Balancing - Information on the different types of load balancers in AWS and
when to use them.
QUESTION NO: 269
마케팅 회사는 마케팅 캠페인을 통해 Amazon S3에서 대량의 새로운 클릭스트림 데이터를
수신합니다. 회사는 Amazon S3의 클릭스트림 데이터를 신속하게 분석해야 합니다. 그런 다음
회사는 데이터 파이프라인에서 데이터를 추가로 처리할지 여부를 결정해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
182

IT Certification Guaranteed, The Easy Way!
A. Spark 카탈로그에 외부 테이블 생성 데이터를 쿼리하도록 AWS Glue에서 작업 구성
B. 데이터를 크롤링하도록 AWS Glue 크롤러를 구성합니다. 데이터를 쿼리하도록 Amazon
Athena를 구성합니다.
C. Hive 메타스토어에 외부 테이블을 생성합니다. 데이터를 쿼리하도록 Amazon EMR에서
Spark 작업을 구성합니다.
D. 데이터를 크롤링하도록 AWS Glue 크롤러를 구성합니다. SQL을 사용하여 데이터를
쿼리하도록 Amazon Kinesis Data Analytics 구성
Answer: B
Explanation:
* AWS Glue Crawler: AWS Glue is a fully managed ETL (Extract, Transform, Load) service
that makes it easy to prepare and load data for analytics. A Glue crawler can automatically
discover new data and schema in Amazon S3, making it easy to keep the data catalog up-to-
date.
* Crawling the Data:
* Set up an AWS Glue crawler to scan the S3 bucket containing the clickstream data.
* The crawler will automatically detect the schema and create/update the tables in the AWS
Glue Data Catalog.
* Amazon Athena:
* Athena is an interactive query service that makes it easy to analyze data in Amazon S3
using standard SQL.
* Once the data catalog is updated by the Glue crawler, use Athena to query the clickstream
data directly in S3.
* Operational Efficiency: This solution leverages fully managed services, reducing operational
overhead.
Glue crawlers automate data cataloging, and Athena provides a serverless, pay-per-query
model for quick data analysis without the need to set up or manage infrastructure.
References:
* AWS Glue
* Amazon Athena
QUESTION NO: 270
응용 프로그램을 사용하면 회사 본사의 사용자가 제품 데이터에 액세스할 수 있습니다. 제품
데이터는 Amazon RDS MySQL DB 인스턴스에 저장됩니다. 운영 팀은 애플리케이션 성능
저하를 격리하고 쓰기 트래픽에서 읽기 트래픽을 분리하려고 합니다. 솔루션 설계자는
애플리케이션의 성능을 신속하게 최적화해야 합니다.
솔루션 설계자는 무엇을 권장해야 합니까?
A. 기존 데이터베이스를 다중 AZ 배포로 변경합니다. 기본 가용 영역에서 읽기 요청을
제공합니다.
B. 기존 데이터베이스를 다중 AZ 배포로 변경합니다. 보조 가용 영역에서 읽기 요청을
제공합니다.
C. 데이터베이스에 대한 읽기 전용 복제본을 생성합니다. 컴퓨팅 및 스토리지 리소스의 절반을
원본 데이터베이스로 사용하여 읽기 전용 복제본을 구성합니다.
D. 데이터베이스에 대한 읽기 전용 복제본을 생성합니다. 원본 데이터베이스 와 동일한
컴퓨팅 및 스토리지 리소스로 읽기 전용 복제본을 구성 합니다.
183

IT Certification Guaranteed, The Easy Way!
Answer: D
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_MySQL.Replication.Rea
dReplicas.html
QUESTION NO: 271
어떤 회사는 2주 안에 온프레미스 데이터 센터에서 AWS로 MySQL 데이터베이스를
마이그레이션해야 합니다.
데이터베이스 크기는 180TB입니다. 회사는 데이터베이스를 분할할 수 없습니다.
회사는 마이그레이션 중 다운타임을 최소화하려고 합니다. 회사의 인터넷 연결 속도는
100Mbps.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. AWS Snowball Edge Storage Optimized 장치를 주문합니다. AWS Database Migration
Service(AWS DMS)와 AWS Schema Conversion Tool(AWS SCT)을 사용하여
데이터베이스를 Amazon RDS for MySQL로 마이그레이션하고 진행 중인 변경 사항을
복제합니다. Snowball Edge 장치를 AWS로 다시 보내 마이그레이션을 완료합니다. 진행 중인
변경 사항을 계속 복제합니다.
B. 데이터 센터와 AWS 간에 AWS Site-to-Site VPN 연결을 설정합니다. AWS Database
Migration Service(AWS DMS)와 AWS Schema Conversion Tool(AWS SCT)을 사용하여
데이터베이스를 Amazon RDS to MySQL로 마이그레이션하고 진행 중인 변경 사항을
복제합니다.
C. 데이터 센터와 AWS 간에 10Gbps 전용 AWS Direct Connect 연결을 설정합니다. AWS
DataSync를 사용하여 데이터베이스를 Amazon S3에 복제합니다. Amazon S3에서 새로운
Amazon RDS for MySQL 데이터베이스 인스턴스로 데이터를 가져오는 스크립트를 만듭니다.
D. 회사의 기존 인터넷 연결을 사용합니다. AWS DataSync를 사용하여 데이터베이스를
Amazon S3로 복제합니다. Amazon S3에서 MySQL 데이터베이스 인스턴스용 새 Amazon
RDS로 데이터를 가져오는 스크립트를 만듭니다.
Answer: A
Explanation:
Given the large size (180 TB) of the database and the time constraint, AWS Snowball Edge
Storage Optimized is the best solution. Snowball Edge allows for the physical transfer of
large datasets to AWS efficiently without relying on slow internet connections. AWS DMS and
SCT can be used to perform ongoing replication of any changes made during the migration,
ensuring minimal downtime.
* Option B (VPN): Using a 100 Mbps internet connection would take far too long to transfer
180 TB.
* Option C (Direct Connect): Establishing a 10 Gbps Direct Connect link might not be feasible
within the 2-week timeframe.
* Option D (DataSync over internet): With the existing internet connection, DataSync would
also take too long.
AWS References:
* AWS Snowball Edge
* AWS DMS
QUESTION NO: 272
184

IT Certification Guaranteed, The Easy Way!
회사에서 데이터 저장을 위해 Amazon DynamoDB 테이블을 사용할 계획입니다. 회사는 비용
최적화에 대해 우려하고 있습니다. 대부분의 아침에는 테이블을 사용하지 않습니다. 저녁에는
읽기 및 쓰기 트래픽이 예측할 수 없는 경우가 많습니다. 트래픽 급증이 발생하면 매우 빠르게
발생합니다.
솔루션 아키텍트는 무엇을 추천해야 합니까?
A. 온디맨드 용량 모드에서 DynamoDB 테이블을 생성합니다.
B. 글로벌 보조 인덱스가 있는 DynamoDB 테이블을 생성합니다.
C. 프로비저닝된 용량 및 Auto Scaling으로 DynamoDB 테이블을 생성합니다.
D. 프로비저닝된 용량 모드에서 DynamoDB 테이블을 생성하고 전역 테이블로 구성합니다.
Answer: A
QUESTION NO: 273
한 회사는 비디오 콘텐츠를 게시하고 모든 모바일 플랫폼에서 사용할 수 있도록
트랜스코딩하기 위한 온라인 서비스를 제공합니다. 애플리케이션 아키텍처는 Amazon Elastic
File System(Amazon EFS) 표준을 사용하여 여러 Amazon EC2 Linux 인스턴스가 처리를 위해
비디오 콘텐츠에 액세스할 수 있도록 비디오를 수집하고 저장합니다. 시간이 지남에 따라
서비스의 인기가 높아짐에 따라 스토리지 비용도 너무 높아졌습니다. 값비싼.
가장 비용 효율적인 스토리지 솔루션은 무엇입니까?
A. 파일에 AWS Storage Gateway를 사용하여 비디오 콘텐츠를 저장하고 처리합니다.
B. 볼륨에 AWS Storage Gateway를 사용하여 비디오 콘텐츠를 저장하고 처리합니다.
C. 비디오 콘텐츠 저장에 Amazon EFS를 사용합니다. 처리가 완료되면 파일을 Amazon
Elastic Block Store(Amazon EBS)로 전송합니다.
D. 비디오 콘텐츠 저장에 Amazon S3를 사용합니다. 처리를 위해 서버에 연결된 Amazon
Elastic Block Store(Amazon EBS) 볼륨으로 파일을 일시적으로 이동합니다.
Answer: D
Explanation:
* Amazon S3 for large-scale, durable, and inexpensive storage of the video content. S3
storage costs are significantly lower than EFS. * Amazon EBS only temporarily during
processing. By mounting an EBS volume only when a video needs to be processed, and
unmounting it after, the time the content spends on the higher-cost EBS storage is
minimized. * The EBS volume can be sized to match the workload needs for active
processing, keeping costs lower. The volume does not need to store the entire video library
long-term.
QUESTION NO: 274
회사는 REST API로 검색하기 위해 주문 배송 통계를 제공하는 애플리케이션을 개발
중입니다.
이 회사는 배송 통계를 추출하고 데이터를 읽기 쉬운 HTML 형식으로 구성하고 매일 아침
여러 이메일 주소로 보고서를 보내려고 합니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 단계 조합을 취해야 합니까? (2개를
선택하세요.)
A. 데이터를 Amazon Kinesis Data Firehose로 보내도록 애플리케이션을 구성합니다.
B. Amazon Simple Email Service(Amazon SES)를 사용하여 데이터 형식을 지정하고
보고서를 이메일로 보냅니다.
185

IT Certification Guaranteed, The Easy Way!
C. 데이터에 대한 애플리케이션의 API를 쿼리하기 위해 AWS Glue 작업을 호출하는 Amazon
EventBridge(Amazon CloudWatch Events) 예약 이벤트를 생성합니다.
D. AWS Lambda 함수를 호출하여 데이터에 대한 애플리케이션의 API를 쿼리하는 Amazon
EventBridge(Amazon CloudWatch Events) 예약 이벤트를 생성합니다.
E. Amazon S3에 애플리케이션 데이터를 저장합니다. 보고서를 보낼 S3 이벤트 대상으로
Amazon Simple Notification Service(Amazon SNS) 주제를 생성합니다.
Answer: B D
Explanation:
https://docs.aws.amazon.com/ses/latest/dg/send-email-formatted.html
D: Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that
invokes an AWS Lambda function to query the application's API for the data. This step can
be done using AWS Lambda to extract the shipping statistics and organize the data into an
HTML format.
B: Use Amazon Simple Email Service (Amazon SES) to format the data and send the report
by email. This step can be done by using Amazon SES to send the report to multiple email
addresses at the same time every morning.
Therefore, options D and B are the correct choices for this question. Option A is incorrect
because Kinesis Data Firehose is not necessary for this use case. Option C is incorrect
because AWS Glue is not required to query the application's API. Option E is incorrect
because S3 event notifications cannot be used to send the report by email.
QUESTION NO: 275
한 회사는 사용자가 모바일 장치에서 슬로우 모션 비디오 클립을 스트리밍할 수 있는 모바일
앱을 만들고 싶어합니다. 현재 앱은 비디오 클립을 캡처하고 원시 형식의 비디오 클립을
Amazon S3 버킷에 업로드합니다. 앱은 S3 버킷에서 직접 이러한 비디오 클립을 검색합니다.
그러나 비디오는 원본 형식이 큽니다.
사용자가 모바일 장치에서 버퍼링 및 재생 문제를 겪고 있습니다. 회사는 운영 오버헤드를
최소화하면서 앱의 성능과 확장성을 최대화하는 솔루션을 구현하려고 합니다.
이러한 요구 사항을 충족하는 솔루션 조합은 무엇입니까? (2개를 선택하세요.)
A. 콘텐츠 전송 및 캐싱을 위해 Amazon CloudFront 배포
B. AWS DataSync를 사용하여 다른 S3 버킷의 AWS 리전에 걸쳐 비디오 파일을 복제합니다.
C. Amazon Elastic Transcoder를 사용하여 비디오 파일을 보다 적절한 형식으로 변환합니다.
D. 콘텐츠 전송 및 캐싱을 위해 로컬 영역에 Amazon EC2 인스턴스의 Auto Scaling 그룹 배포
E. Amazon EC2 인스턴스의 Auto Scaling 그룹을 배포하여 비디오 파일을 보다 적절한
형식으로 변환합니다.
Answer: A C
Explanation:
* Understanding the Requirement: The mobile app captures and uploads raw video clips to
S3, but users experience buffering and playback issues due to the large size of these videos.
* Analysis of Options:
* Amazon CloudFront: A content delivery network (CDN) that can cache and deliver content
globally with low latency. It helps reduce buffering by delivering content from edge locations
closer to the users.
* AWS DataSync: Primarily used for data transfer and replication across AWS Regions,
186

IT Certification Guaranteed, The Easy Way!
which does not directly address the video size and buffering issue.
* Amazon Elastic Transcoder: A media transcoding service that can convert raw video files
into formats and resolutions more suitable for streaming, reducing the size and improving
playback performance.
* EC2 Instances in Local Zones: While this could provide content delivery and caching, it
involves more operational overhead compared to using CloudFront.
* EC2 Instances for Transcoding: Involves setting up and maintaining infrastructure, leading
to higher operational overhead compared to using Elastic Transcoder.
* Best Combination of Solutions:
* Deploy Amazon CloudFront: This optimizes the performance by caching content at edge
locations, reducing latency and buffering for users.
* Use Amazon Elastic Transcoder: This reduces the file size and converts videos into formats
better suited for streaming on mobile devices.
References:
* Amazon CloudFront
* Amazon Elastic Transcoder
QUESTION NO: 276
회사는 AWS 클라우드에서 애플리케이션을 호스팅합니다. 이 애플리케이션은 Amazon
DynamoDB 테이블과 함께 Auto Scaling 그룹의 Elastic Load Balancer 뒤에 있는 Amazon
EC2 인스턴스에서 실행됩니다. 회사는 다운타임을 최소화하면서 다른 AWS 리전에서
애플리케이션을 사용할 수 있기를 원합니다.
가동 중지 시간을 최소화하면서 이러한 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야
합니까?
A. 재해 복구 지역에 Auto Scaling 그룹과 로드 밸런서를 생성합니다. DynamoDB 테이블을
전역 테이블로 구성합니다. 새 재해 복구 리전의 로드 밸런서를 가리키도록 DNS 장애 조치를
구성합니다.
B. 필요할 때 실행할 EC2 인스턴스, 로드 밸런서 및 DynamoDB 테이블을 생성하는 AWS
CloudFormation 템플릿을 생성합니다. 새 재해 복구 리전의 로드 밸런서를 가리키도록 DNS
장애 조치를 구성합니다.
C. AWS CloudFormation 템플릿을 생성하여 EC2 인스턴스와 필요할 때 실행할 로드 밸런서를
생성합니다. DynamoDB 테이블을 전역 테이블로 구성합니다. 새 재해 복구 리전의 로드
밸런서를 가리키도록 DNS 장애 조치를 구성합니다.
D. 재해 복구 지역에 Auto Scaling 그룹 및 로드 밸런서를 생성합니다. DynamoDB 테이블을
전역 테이블로 구성합니다. 재해 복구 로드 밸런서를 가리키는 Amazon Route 53을
업데이트하는 AWS Lambda 함수를 트리거하는 Amazon CloudWatch 경보를 생성합니다.
Answer: A
Explanation:
This answer is correct because it meets the requirements of securely migrating the existing
data to AWS and satisfying the new regulation. AWS DataSync is a service that makes it
easy to move large amounts of data online between on-premises storage and Amazon S3.
DataSync automatically encrypts data in transit and verifies data integrity during transfer.
AWS CloudTrail is a service that records AWS API calls for your account and delivers log
files to Amazon S3. CloudTrail can log data events, which show the resource operations
performed on or within a resource in your AWS account, such as S3 object-level API activity.
187

IT Certification Guaranteed, The Easy Way!
By using CloudTrail to log data events, you can audit access at all levels of the stored data.
References:
* https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html
* https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-
cloudtrail.html
QUESTION NO: 277
한 회사가 온프레미스 데이터 센터의 서버에서 Node.js 기능을 실행합니다. 데이터 센터는
PostgreSQL 데이터베이스에 데이터를 저장합니다. 회사는 서버의 환경 변수에 있는 연결
문자열에 자격 증명을 저장합니다. 회사는 애플리케이션을 AWS로 마이그레이션하고 Node.js
애플리케이션 서버를 AWS Lambd a로 교체하려고 합니다. 또한 회사는 Amazon RDS for
PostgreSQL로 마이그레이션하고 데이터베이스 자격 증명이 안전하게 관리되도록 하려고
합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. AWS Systems Manager Parameter Store에 데이터베이스 자격 증명을 매개변수로
저장합니다. Parameter Store를 구성하여 30일마다 비밀을 자동으로 순환합니다. Lambda
함수를 업데이트하여 매개변수에서 자격 증명을 검색합니다.
B. AWS Secrets Manager에서 데이터베이스 자격 증명을 비밀로 저장합니다. Secrets
Manager가 30일마다 자격 증명을 자동으로 순환하도록 구성합니다. Lambda 함수를
업데이트하여 비밀에서 자격 증명을 검색합니다.
C. 데이터베이스 자격 증명을 암호화된 Lambda 환경 변수로 저장합니다. 자격 증명을
순환하기 위한 사용자 지정 Lambda 함수를 작성합니다. Lambda 함수가 30일마다
실행되도록 예약합니다.
D. AWS Key Management Service(AWS KMS)에 데이터베이스 자격 증명을 키로
저장합니다. 키에 대한 자동 로테이션을 구성합니다. Lambda 함수를 업데이트하여 KMS
키에서 자격 증명을 검색합니다.
Answer: B
Explanation:
AWS Secrets Manager is designed specifically to securely store and manage sensitive
information such as database credentials. It integrates seamlessly with AWS services like
Lambda and RDS, and it provides automatic credential rotation with minimal operational
overhead.
* AWS Secrets Manager: By storing the database credentials in Secrets Manager, you
ensure that the credentials are securely stored, encrypted, and managed. Secrets Manager
provides a built-in mechanism to automatically rotate credentials at regular intervals (e.g.,
every 30 days), which helps in maintaining security best practices without requiring additional
manual intervention.
* Lambda Integration: The Lambda function can be easily configured to retrieve the
credentials from Secrets Manager using the AWS SDK, ensuring that the credentials are
accessed securely at runtime.
* Why Not Other Options?:
* Option A (Parameter Store with Rotation): While Parameter Store can store parameters
securely, Secrets Manager is more tailored for secrets management and automatic rotation,
offering more features and less operational overhead.
* Option C (Encrypted Lambda environment variable): Storing credentials directly in Lambda
188

IT Certification Guaranteed, The Easy Way!
environment variables, even when encrypted, requires custom code to manage rotation,
which increases operational complexity.
* Option D (KMS with automatic rotation): KMS is for managing encryption keys, not for
storing and rotating secrets like database credentials. This option would require more custom
implementation to manage credentials securely.
AWS References:
* AWS Secrets Manager - Detailed documentation on how to store, manage, and rotate
secrets using AWS Secrets Manager.
* Using Secrets Manager with AWS Lambda - Guidance on integrating Secrets Manager with
Lambda for secure credential management.
QUESTION NO: 278
회사의 웹 애플리케이션은 VPC의 Application Load Balancer 뒤에서 실행되는 여러 Amazon
EC2 인스턴스로 구성됩니다. MySQL용 Amazon RDS DB 인스턴스에는 데이터가 포함되어
있습니다. 회사에는 AWS 환경에서 의심스럽거나 예상치 못한 동작을 자동으로 감지하고
대응하는 기능이 필요합니다. 이 회사는 이미 자사 아키텍처에 AWS WAF를 추가했습니다.
솔루션 설계자는 위협으로부터 보호하기 위해 다음으로 무엇을 해야 합니까?
A. Amazon GuardDuty를 사용하여 위협 탐지를 수행합니다. GuardDuty 결과를 필터링하고
AWS Lambda 함수를 호출하여 AWS WAF 규칙을 조정하도록 Amazon EventBridge를
구성합니다.
B. AWS Firewall Manager를 사용하여 위협 탐지를 수행합니다. Firewall Manager 결과를
필터링하고 AWS Lambda 함수를 호출하여 AWS WAF 웹 ACL을 조정하도록 Amazon
EventBridge를 구성합니다.
C. Amazon Inspector를 사용하여 위협 탐지를 수행하고 AWS WAF 규칙을 업데이트합니다.
웹 애플리케이션에 대한 액세스를 제한하려면 VPC 네트워크 ACL을 만듭니다.
D. Amazon Macie를 사용하여 위협 탐지를 수행하고 AWS WAF 규칙을 업데이트합니다. 웹
애플리케이션에 대한 액세스를 제한하려면 VPC 네트워크 ACL을 만듭니다.
Answer: A
Explanation:
* Understanding the Requirement: The company needs to automatically detect and respond
to suspicious or unexpected behavior in its AWS environment, beyond the existing AWS
WAF setup.
* Analysis of Options:
* Amazon GuardDuty: Provides continuous monitoring and threat detection across AWS
accounts and resources, including integration with AWS WAF for automated response.
* AWS Firewall Manager: Manages firewall rules across multiple accounts but is more
focused on central management than threat detection.
* Amazon Inspector: Focuses on security assessments and vulnerability management rather
than real-time threat detection.
* Amazon Macie: Primarily used for data security and privacy, not comprehensive threat
detection.
* Best Solution:
* Amazon GuardDuty with EventBridge and Lambda: This combination ensures continuous
threat detection and automated response by updating AWS WAF rules based on GuardDuty
findings.
189

IT Certification Guaranteed, The Easy Way!
References:
* Amazon GuardDuty
* Amazon EventBridge
* AWS Lambda
QUESTION NO: 279
솔루션 설계자는 MySQL 데이터베이스를 사용하여 복잡한 Java 애플리케이션을 구현하고
있습니다. Java 애플리케이션은 Apache Tomcat에 배포되어야 하며 가용성이 높아야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. AWS Lambda에 애플리케이션을 배포합니다. Lambda 함수와 연결하도록 Amazon API
Gateway API를 구성합니다.
B. AWS Elastic Beanstalk를 사용하여 애플리케이션을 배포합니다. 부하 분산 환경과 롤링
배포 정책을 구성합니다.
C. 데이터베이스를 Amazon ElastiCache로 마이그레이션합니다. 애플리케이션의 액세스를
허용하도록 ElastiCache 보안 그룹을 구성합니다.
D. Amazon EC2 인스턴스를 시작합니다. EC2 인스턴스에 MySQL 서버를 설치합니다.
서버에서 애플리케이션을 구성합니다. AMI를 생성합니다. AMI를 사용하여 Auto Scaling
그룹이 포함된 시작 템플릿을 생성합니다.
Answer: B
Explanation:
AWS Elastic Beanstalk provides an easy and quick way to deploy, manage, and scale
applications. It supports a variety of platforms, including Java and Apache Tomcat. By using
Elastic Beanstalk, the solutions architect can upload the Java application and configure the
environment to run Apache Tomcat.
QUESTION NO: 280
회사의 웹사이트는 대중에게 제품을 판매하는 데 사용됩니다. 이 사이트는 ALB(Application
Load Balancer) 뒤에 있는 Auto Scaling 그룹의 Amazon EC2 인스턴스에서 실행됩니다.
Amazon CloudFront 배포판도 있으며 AWS WAF는 SQL 주입 공격으로부터 보호하는 데
사용되고 있습니다. ALB는 CloudFront 배포의 오리진입니다. 최근 보안 로그를 검토한 결과,
해당 웹사이트에 대한 접근을 차단해야 할 외부 악성 IP가 발견되었습니다.
솔루션 설계자는 애플리케이션을 보호하기 위해 무엇을 해야 합니까?
A. CloudFront 배포의 네트워크 ACL을 수정하여 악성 IP 주소에 대한 거부 규칙을
추가합니다.
B. AWS WAF 구성을 수정하여 악성 IP 주소를 차단하는 IP 일치 조건을 추가합니다.
C. ALB 뒤의 대상 그룹에 있는 EC2 인스턴스에 대한 네트워크 ACL을 수정하여 악성 IP
주소를 거부합니다.
D. 악성 IP 주소를 거부하도록 ALB 뒤의 대상 그룹에 있는 EC2 인스턴스에 대한 보안 그룹을
수정합니다.
Answer: B
Explanation:
AWS WAF is a web application firewall that helps protect web applications from common web
exploits that could affect application availability, compromise security, or consume excessive
resources. AWS WAF allows users to create rules that block, allow, or count web requests
based on customizable web security rules.
190

IT Certification Guaranteed, The Easy Way!
One of the types of rules that can be created is an IP match rule, which allows users to
specify a list of IP addresses or IP address ranges that they want to allow or block. By
modifying the configuration of AWS WAF to add an IP match condition to block the malicious
IP address, the solution architect can prevent the attacker from accessing the website
through the CloudFront distribution and the ALB.
The other options are not correct because they do not effectively block the malicious IP
address from accessing the website. Modifying the network ACL on the CloudFront
distribution or the EC2 instances in the target groups behind the ALB will not work because
network ACLs are stateless and do not evaluate traffic at the application layer. Modifying the
security groups for the EC2 instances in the target groups behind the ALB will not work
because security groups are stateful and only evaluate traffic at the instance level, not at the
load balancer level.
References:
* AWS WAF
* How AWS WAF works
* Working with IP match conditions
QUESTION NO: 281
연구실에서는 약 8TB의 데이터를 처리해야 합니다. 실험실에서는 스토리지 하위 시스템에
대해 밀리초 미만의 지연 시간과 6GBps의 최소 처리량이 필요합니다. Amazon Linux를
실행하는 수백 개의 Amazon EC2 인스턴스가 데이터를 배포하고 처리합니다. 어떤 솔루션이
성능을 충족할 것입니까? 요구 사항?
A. NetApp ONTAP 파일 시스템용 Amazon FSx 생성 각 볼륨의 계층화 정책을 ALL로 설정
원시 데이터를 파일 시스템으로 가져오기 EC2 인스턴스에 파일 시스템 탑재
B. 원시 데이터를 저장하기 위한 Amazon S3 버킷 생성 영구 SSD 스토리지를 사용하는
Lustre용 Amazon FSx 파일 시스템 생성 Amazon S3에서 데이터를 가져오고 내보내는 옵션
선택 EC2 인스턴스에 파일 시스템 탑재
C. 원시 데이터를 저장할 Amazon S3 버킷 생성 영구 HDD 스토리지를 사용하는 Lustre용
Amazon FSx 파일 시스템 생성 Amazon S3에서 데이터를 가져오고 내보내는 옵션 선택 EC2
인스턴스에 파일 시스템 탑재
D. NetApp ONTAP 파일 시스템용 Amazon FSx를 생성합니다. 각 볼륨의 연결 정책을
NONE으로 설정합니다. 원시 데이터를 파일 시스템으로 가져오기 EC2 인스턴스에 파일
시스템 탑재
Answer: B
Explanation:
Create an Amazon S3 bucket to store the raw data Create an Amazon FSx for Lustre file
system that uses persistent SSD storage Select the option to import data from and export
data to Amazon S3 Mount the file system on the EC2 instances. Amazon FSx for Lustre uses
SSD storage for sub-millisecond latencies and up to 6 GBps throughput, and can import data
from and export data to Amazon S3. Additionally, the option to select persistent SSD storage
will ensure that the data is stored on the disk and not lost if the file system is stopped.
QUESTION NO: 282
전자 상거래 회사는 AWS 클라우드에서 분석 애플리케이션을 호스팅합니다. 이
애플리케이션은 매달 약 300MB의 데이터를 생성합니다. 데이터는 JSON 형식으로
191

IT Certification Guaranteed, The Easy Way!
저장됩니다. 회사는 데이터를 백업하기 위한 재해 복구 솔루션을 평가하고 있습니다. 필요한
경우 데이터에 밀리초 내에 액세스할 수 있어야 하며 데이터는 30일 동안 보관되어야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. Amazon OpenSearch 서비스(Amazon Elasticsearch Service)
B. Amazon S3 빙하
C. Amazon S3 표준
D. PostgreSQL용 Amazon RDS
Answer: C
Explanation:
This solution meets the requirements of a disaster recovery solution to back up the data that
is generated by an analytics application, stored in JSON format, and must be accessible in
milliseconds if it is needed. Amazon S3 Standard is a durable and scalable storage class for
frequently accessed data. It can store any amount of data and provide high availability and
performance. It can also support millisecond access time for data retrieval.
Option A is incorrect because Amazon OpenSearch Service (Amazon Elasticsearch Service)
is a search and analytics service that can index and query data, but it is not a backup solution
for data stored in JSON format.
Option B is incorrect because Amazon S3 Glacier is a low-cost storage class for data
archiving and long-term backup, but it does not support millisecond access time for data
retrieval. Option D is incorrect because Amazon RDS for PostgreSQL is a relational database
service that can store and query structured data, but it is not a backup solution for data
stored in JSON format.
References:
* https://aws.amazon.com/s3/storage-classes/
* https://aws.amazon.com/s3/faqs/#Durability_and_data_protection
QUESTION NO: 283
한 제약회사에서 신약을 개발하고 있습니다. 지난 몇 달 동안 회사에서 생성되는 데이터의
양이 기하급수적으로 증가했습니다. 회사의 연구원들은 최소한의 지연으로 즉시 사용할 수
있도록 전체 데이터 세트의 하위 집합을 정기적으로 요구합니다. 그러나 전체 데이터 세트에
매일 액세스할 필요는 없습니다. 현재 모든 데이터는 온프레미스 스토리지 어레이에 상주하고
있으며 회사는 지속적인 자본 비용을 줄이고 싶어합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 스토리지 솔루션을 권장해야
합니까?
A. AWS DataSync를 예약된 크론 작업으로 실행하여 지속적으로 데이터를 Amazon S3
버킷으로 마이그레이션합니다.
B. Amazon S3 버킷을 대상 스토리지로 사용하여 AWS Storage Gateway 파일 게이트웨이를
배포합니다. 데이터를 Storage Gateway 어플라이언스로 마이그레이션합니다.
C. Amazon S3 버킷을 대상 스토리지로 사용하여 캐시된 볼륨이 있는 AWS Storage Gateway
볼륨 게이트웨이를 배포합니다. 데이터를 Storage Gateway 어플라이언스로
마이그레이션합니다.
D. 온프레미스 환경에서 AWS로 AWS Site-to-Site VPN 연결을 구성합니다. Amazon Elastic
File System(Amazon EFS) 파일 시스템으로 데이터를 마이그레이션합니다.
Answer: C
192

IT Certification Guaranteed, The Easy Way!
Explanation:
AWS Storage Gateway is a hybrid cloud storage service that allows you to seamlessly
integrate your on- premises applications with AWS cloud storage. Volume Gateway is a type
of Storage Gateway that presents cloud-backed iSCSI block storage volumes to your on-
premises applications. Volume Gateway operates in either cache mode or stored mode. In
cache mode, your primary data is stored in Amazon S3, while retaining your frequently
accessed data locally in the cache for low latency access. In stored mode, your primary data
is stored locally and your entire dataset is available for low latency access on premises while
also asynchronously getting backed up to Amazon S3.
For the pharmaceutical company's use case, cache mode is the most suitable option, as it
meets the following requirements:
* It reduces the need to scale the on-premises storage infrastructure, as most of the data is
stored in Amazon S3, which is scalable, durable, and cost-effective.
* It provides low latency access to the subset of the data that the researchers regularly
require, as it is cached locally in the Storage Gateway appliance.
* It does not require the entire dataset to be accessed on a daily basis, as it is stored in
Amazon S3 and can be retrieved on demand.
* It offers flexible data protection and recovery options, as it allows taking point-in-time copies
of the volumes using AWS Backup, which are stored in AWS as Amazon EBS snapshots.
Therefore, the solutions architect should recommend deploying an AWS Storage Gateway
volume gateway with cached volumes with an Amazon S3 bucket as the target storage and
migrating the data to the Storage Gateway appliance.
References:
* Volume Gateway | Amazon Web Services
* How Volume Gateway works (architecture) - AWS Storage Gateway
* AWS Storage Volume Gateway - Cached volumes - Stack Overflow
QUESTION NO: 284
어떤 기업이 CompanyConfidential Amazon S3 버킷에 접근할 권한이 없어야 하는 새로운
클라우드 엔지니어를 채용했습니다. 클라우드 엔지니어는 AdminTools라는 S3 버킷에 대한
읽기 및 쓰기 권한이 있어야 합니다.
이러한 기준을 충족하는 IAM 정책은 무엇입니까?
A.
193

IT Certification Guaranteed, The Easy Way!
B.
194

IT Certification Guaranteed, The Easy Way!
D.
Answer: A
195

IT Certification Guaranteed, The Easy Way!
Explanation:
https://docs.amazonaws.cn/en_us/IAM/latest/UserGuide/reference_policies_examples_s3_rw
-bucket.html The policy is separated into two parts because the ListBucket action requires
permissions on the bucket while the other actions require permissions on the objects in the
bucket. You must use two different Amazon Resource Names (ARNs) to specify bucket-level
and object-level permissions. The first Resource element specifies arn:aws:s3:::AdminTools
for the ListBucket action so that applications can list all objects in the AdminTools bucket.
QUESTION NO: 285
한 회사에서 인기 노래 클립으로 만든 벨소리를 판매합니다. 벨소리가 포함된 파일은 Amazon
S3 Standard에 저장되며 크기는 최소 128KB입니다. 회사에는 수백만 개의 파일이 있지만
90일이 지난 벨소리는 다운로드되는 경우가 거의 없습니다. 회사는 가장 많이 액세스하는
파일을 사용자가 쉽게 사용할 수 있도록 유지하면서 스토리지 비용을 절약해야 합니다.
이러한 요구 사항을 가장 비용 효과적으로 충족하기 위해 회사는 어떤 조치를 취해야 합니까?
A. 객체의 초기 스토리지 계층에 대해 S3 Standard-Infrequent Access(S3 Standard-IA)
스토리지를 구성합니다.
B. 파일을 S3 Intelligent-Tiering으로 이동하고 90일 후에 객체를 더 저렴한 스토리지 계층으로
이동하도록 구성합니다.
C. 객체를 관리하고 90일 후에 S3 Standard-Infrequent Access(S3 Standard-1A)로
이동하도록 S3 인벤토리를 구성합니다.
D. 90일 후에 객체를 S3 Standard에서 S3 Standard-Infrequent Access(S3 Standard-1A)로
이동하는 S3 수명 주기 정책을 구현합니다.
Answer: D
Explanation:
This solution meets the requirements of saving money on storage while keeping the most
accessed files readily available for the users. S3 Lifecycle policy can automatically move
objects from one storage class to another based on predefined rules. S3 Standard-IA is a
lower-cost storage class for data that is accessed less frequently, but requires rapid access
when needed. It is suitable for ringtones older than 90 days that are downloaded infrequently.
Option A is incorrect because configuring S3 Standard-IA for the initial storage tier of the
objects can incur higher costs for frequent access and retrieval fees. Option B is incorrect
because moving the files to S3 Intelligent-Tiering can incur additional monitoring and
automation fees that may not be necessary for ringtones older than 90 days. Option C is
incorrect because using S3 inventory to manage objects and move them to S3 Standard-IA
can be complex and time-consuming, and it does not provide automatic cost savings.
References:
* https://aws.amazon.com/s3/storage-classes/
* https://aws.amazon.com/s3/cloud-storage-cost-optimization-ebook/
QUESTION NO: 286
한 회사는 AWS 계정의 모든 애플리케이션에 걸쳐 Amazon EC2 Auto Scaling 이벤트를
보고하는 솔루션을 구축하고 있습니다. 회사는 Amazon S3에 EC2 Auto Scaling 상태
데이터를 저장하기 위해 서버리스 솔루션을 사용해야 합니다. 그런 다음 회사는 Amazon S3의
데이터를 사용하여 대시보드에 거의 실시간 업데이트를 제공합니다. 솔루션은 EC2 인스턴스
시작 속도에 영향을 주어서는 안 됩니다.
196

IT Certification Guaranteed, The Easy Way!
이러한 요구 사항을 충족하려면 회사에서 데이터를 Amazon S3로 어떻게 이동해야 합니까?
A. Amazon CloudWatch 지표 스트림을 사용하여 EC2 Auto Scaling 상태 데이터를 Amazon
Kinesis Data Firehose로 보냅니다. Amazon S3에 데이터를 저장합니다.
B. Amazon EMR 클러스터를 시작하여 EC2 Auto Scaling 상태 데이터를 수집하고 Amazon
Kinesis Data Firehose로 데이터를 보냅니다. Amazon S3에 데이터를 저장합니다.
C. 일정에 따라 AWS Lambda 함수를 호출하는 Amazon EventBridge 규칙을 생성합니다. EC2
Auto Scaling 상태 데이터를 Amazon S3에 직접 보내도록 Lambda 함수를 구성합니다.
D. EC2 인스턴스를 시작하는 동안 부트스트랩 스크립트를 사용하여 Amazon Kinesis
에이전트를 설치합니다. EC2 Auto Scaling 상태 데이터를 수집하고 Amazon Kinesis Data
Firehose로 데이터를 보내도록 Kinesis 에이전트를 구성합니다. Amazon S3에 데이터를
저장합니다.
Answer: A
Explanation:
You can use metric streams to continually stream CloudWatch metrics to a destination of
your choice, with near-real-time delivery and low latency. One of the use cases is Data Lake:
create a metric stream and direct it to an Amazon Kinesis Data Firehose delivery stream that
delivers your CloudWatch metrics to a data lake such as Amazon S3.
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch- Metric
-Streams.html
QUESTION NO: 287
회사에는 이미지 공유를 위한 3계층 애플리케이션이 있습니다. 애플리케이션은 프런트엔드
계층에 Amazon EC2 인스턴스를 사용하고, 애플리케이션 계층에 또 다른 EC2 인스턴스를
사용하며, MySQL 데이터베이스에 세 번째 EC2 인스턴스를 사용합니다. 솔루션 설계자는
애플리케이션 변경이 최소화되는 확장 가능하고 가용성이 높은 솔루션을 설계해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon S3를 사용하여 프런트엔드 계층을 호스팅합니다. 애플리케이션 계층에 AWS
Lambda 함수를 사용합니다. 데이터베이스를 Amazon DynamoDB 테이블로 이동합니다.
Amazon S3를 사용하여 사용자 이미지를 저장하고 제공합니다.
B. 프런트엔드 계층과 애플리케이션 계층에 로드 밸런싱된 다중 AZ AWS Elastic Beanstalk
환경을 사용합니다. 사용자의 이미지를 제공하기 위해 여러 읽기 복제본이 있는 Amazon RDS
DB 인스턴스로 데이터베이스를 이동합니다.
C. Amazon S3를 사용하여 프런트엔드 계층을 호스팅합니다. 애플리케이션 계층에 대한 Auto
Scaling 그룹의 EC2 인스턴스 플릿을 사용합니다. 사용자의 이미지를 저장하고 제공하기 위해
데이터베이스를 메모리 최적화 인스턴스 유형으로 이동합니다.
D. 프런트엔드 계층과 애플리케이션 계층에 로드 밸런싱된 다중 AZ AWS Elastic Beanstalk
환경을 사용합니다. 데이터베이스를 Amazon RDS 다중 AZ DB 인스턴스로 이동합니다.
Amazon S3를 사용하여 사용자 이미지를 저장하고 제공합니다.
Answer: D
Explanation:
for "Highly available": Multi-AZ & for "least amount of changes to the application": Elastic
Beanstalk automatically handles the deployment, from capacity provisioning, load balancing,
auto-scaling to application health monitoring
197

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 288
한 금융 서비스 회사는 두 개의 데이터 센터를 폐쇄하고 100TB가 넘는 데이터를 AWS로
마이그레이션하려고 합니다. 데이터는 하위 폴더의 깊은 계층에 저장된 수백만 개의 작은
파일로 구성된 복잡한 디렉터리 구조를 가지고 있습니다. 대부분의 데이터는 비정형이며
회사의 파일 스토리지는 여러 공급업체의 SMB 기반 스토리지 유형으로 구성됩니다. 회사는
마이그레이션 후 데이터에 액세스하기 위해 애플리케이션을 변경하고 싶지 않습니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야
합니까?
A. AWS Direct Connect를 사용하여 데이터를 Amazon S3로 마이그레이션합니다.
B. AWS DataSync를 사용하여 데이터를 Amazon FSx for Lustre로 마이그레이션합니다.
C. AWS DataSync를 사용하여 데이터를 Amazon FSx for Windows File Server로
마이그레이션합니다.
D. AWS Direct Connect를 사용하여 온프레미스 데이터 스토리지를 AWS Storage Gateway
볼륨 게이트웨이로 마이그레이션합니다.
Answer: C
Explanation:
AWS DataSync is a data transfer service that simplifies, automates, and accelerates moving
data between on- premises storage systems and AWS storage services over the internet or
AWS Direct Connect1. AWS DataSync can transfer data to Amazon FSx for Windows File
Server, which is a fully managed file system that is accessible over the industry-standard
Server Message Block (SMB) protocol. Amazon FSx for Windows File Server is built on
Windows Server, delivering a wide range of administrative features such as user quotas,
end-user file restore, and Microsoft Active Directory (AD) integration2. This solution meets
the requirements of the question because:
* It can migrate more than 100 TB of data to AWS within a reasonable time frame, as AWS
DataSync is optimized for high-speed and efficient data transfer1.
* It can preserve the intricate directory structure and the millions of small files stored in deep
hierarchies of subfolders, as AWS DataSync can handle complex file structures and
metadata, such as file names, permissions, and timestamps1.
* It can avoid changing the applications to access the data after migration, as Amazon FSx
for Windows File Server supports the same SMB protocol and Windows Server features that
the company's on- premises file storage uses2.
* It can reduce the operational overhead, as AWS DataSync and Amazon FSx for Windows
File Server are fully managed services that handle the tasks of setting up, configuring, and
maintaining the data transfer and the file system12.
QUESTION NO: 289
보안 감사 결과 Amazon EC2 인스턴스가 정기적으로 패치되지 않는 것으로 나타났습니다.
솔루션 아키텍트는 대규모 EC2 인스턴스 전체에 걸쳐 정기적인 보안 검색을 실행하는
솔루션을 제공해야 합니다. 또한 솔루션은 정기적으로 EC2 인스턴스를 패치하고 각
인스턴스의 패치 상태에 대한 보고서를 제공해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. EC2 인스턴스에서 소프트웨어 취약성을 검색하도록 Amazon Macie를 설정합니다.
정기적인 일정에 따라 인스턴스에 패치를 적용하려면 각 EC2 인스턴스에 크론 작업을
설정하세요.
198

IT Certification Guaranteed, The Easy Way!
B. 계정에서 Amazon GuardDuty를 활성화합니다. EC2 인스턴스에서 소프트웨어 취약성을
검색하도록 GuardDuty를 구성합니다. 정기적으로 EC2 인스턴스를 패치하도록 AWS
Systems Manager Session Manager를 설정합니다.
C. EC2 인스턴스에서 소프트웨어 취약성을 검색하도록 Amazon Detective를 설정합니다.
정기적으로 EC2 인스턴스를 패치하도록 Amazon EventBridge 예약 규칙을 설정합니다.
D. 계정에서 Amazon Inspector를 활성화합니다. EC2 인스턴스에서 소프트웨어 취약성을
검색하도록 Amazon Inspector를 구성합니다. 정기적으로 EC2 인스턴스를 패치하도록 AWS
Systems Manager Patch Manager를 설정합니다.
Answer: D
Explanation:
Amazon Inspector is an automated security assessment service that helps improve the
security and compliance of applications deployed on AWS. Amazon Inspector automatically
assesses applications for exposure, vulnerabilities, and deviations from best practices. After
performing an assessment, Amazon Inspector produces a detailed list of security findings
prioritized by level of severity1. Amazon Inspector can scan the EC2 instances for software
vulnerabilities and provide a report of each instance's patch status. AWS Systems Manager
Patch Manager is a capability of AWS Systems Manager that automates the process of
patching managed nodes with both security-related updates and other types of updates.
Patch Manager uses patch baselines, which include rules for auto-approving patches within
days of their release, in addition to optional lists of approved and rejected patches. Patch
Manager can patch fleets of Amazon EC2 instances, edge devices, on-premises servers, and
virtual machines (VMs) by operating system type2. Patch Manager can patch the EC2
instances on a regular schedule and provide a report of each instance's patch status.
Therefore, the combination of Amazon Inspector and AWS Systems Manager Patch Manager
will meet the requirements of the question.
The other options are not valid because:
* Amazon Macie is a security service that uses machine learning to automatically discover,
classify, and protect sensitive data in AWS. Amazon Macie does not scan the EC2 instances
for software vulnerabilities, but rather for data classification and protection3. A cron job is a
Linux command for scheduling a task to be executed sometime in the future. A cron job is not
a reliable way to patch the EC2 instances on a regular schedule, as it may fail or be
interrupted by other processes4.
* Amazon GuardDuty is a threat detection service that continuously monitors for malicious
activity and unauthorized behavior to protect your AWS accounts and workloads. Amazon
GuardDuty does not scan the EC2 instances for software vulnerabilities, but rather for
network and API activity anomalies5.
AWS Systems Manager Session Manager is a fully managed AWS Systems Manager
capability that lets you manage your Amazon EC2 instances, edge devices, on-premises
servers, and virtual machines (VMs) through an interactive one-click browser-based shell or
the AWS Command Line Interface (AWS CLI). Session Manager does not patch the EC2
instances on a regular schedule, but rather provides secure and auditable node
management2.
* Amazon Detective is a security service that makes it easy to analyze, investigate, and
quickly identify the root cause of potential security issues or suspicious activities. Amazon
Detective does not scan the EC2 instances for software vulnerabilities, but rather collects and
199

IT Certification Guaranteed, The Easy Way!
analyzes data from AWS sources such as Amazon GuardDuty, Amazon VPC Flow Logs, and
AWS CloudTrail. Amazon EventBridge is a serverless event bus that makes it easy to
connect applications using data from your own applications, integrated Software-as-a-Service
(SaaS) applications, and AWS services. EventBridge delivers a stream of real-time data from
event sources, such as Zendesk, Datadog, or Pagerduty, and routes that data to targets like
AWS Lambda. EventBridge does not patch the EC2 instances on a regular schedule, but
rather triggers actions based on events.
References: Amazon Inspector, AWS Systems Manager Patch Manager, Amazon Macie,
Cron job, Amazon GuardDuty, [Amazon Detective], [Amazon EventBridge]
QUESTION NO: 290
한 회사가 온프레미스 데이터 센터에서 AWS로 레거시 애플리케이션을 마이그레이션하고
있습니다. 이 애플리케이션은 하루 종일 다양한 반복 일정으로 1~20분 동안 실행되는 수백
개의 크론 작업에 의존합니다.
이 회사는 최소한의 리팩토링으로 AWS에서 크론 작업을 예약하고 실행할 수 있는 솔루션을
원합니다. 이 솔루션은 미래의 이벤트에 대한 응답으로 크론 작업을 실행하는 것을 지원해야
합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Cron 작업에 대한 컨테이너 이미지를 만듭니다. Amazon EventBridge Scheduler를
사용하여 반복 일정을 만듭니다. Cron 작업 작업을 AWS Lambda 함수로 실행합니다.
B. cron 작업에 대한 컨테이너 이미지를 만듭니다. Amazon Elastic Container Service(Amazon
ECS)에서 AWS Batch를 스케줄링 정책과 함께 사용하여 cron 작업을 실행합니다.
C. cron 작업에 대한 컨테이너 이미지를 만듭니다. Amazon EventBridge Scheduler를
사용하여 반복 일정을 만듭니다. AWS Fargate에서 cron 작업 작업을 실행합니다.
D. cron 작업에 대한 컨테이너 이미지를 만듭니다. 지정된 시간에 cron 작업을 실행하기 위해
Wait 상태를 사용하는 AWS Step Functions에서 워크플로를 만듭니다. RunTask 작업을
사용하여 AWS Fargate에서 cron 작업 태스크를 실행합니다.
Answer: C
Explanation:
This solution is the most suitable for running cron jobs on AWS with minimal refactoring,
while also supporting the possibility of running jobs in response to future events.
* Container Image for Cron Jobs: By containerizing the cron jobs, you can package the
environment and dependencies required to run the jobs, ensuring consistency and ease of
deployment across different environments.
* Amazon EventBridge Scheduler: EventBridge Scheduler allows you to create a recurring
schedule that can trigger tasks (like running your cron jobs) at specific times or intervals. It
provides fine-grained control over scheduling and integrates seamlessly with AWS services.
* AWS Fargate: Fargate is a serverless compute engine for containers that removes the need
to manage EC2 instances. It allows you to run containers without worrying about the
underlying infrastructure.
Fargate is ideal for running jobs that can vary in duration, like cron jobs, as it scales
automatically based on the task's requirements.
* Why Not Other Options?:
* Option A (Lambda): While AWS Lambda could handle short-running cron jobs, it has
limitations in terms of execution duration (maximum of 15 minutes) and might not be suitable
200

IT Certification Guaranteed, The Easy Way!
for jobs that run up to 20 minutes.
* Option B (AWS Batch on ECS): AWS Batch is more suitable for batch processing and
workloads that require complex job dependencies or orchestration, which might be more than
what is needed for simple cron jobs.
* Option D (Step Functions with Wait State): While Step Functions provide orchestration
capabilities, this approach would introduce unnecessary complexity and overhead compared
to the straightforward scheduling with EventBridge and running on Fargate.
AWS References:
* Amazon EventBridge Scheduler - Details on how to schedule tasks using Amazon
EventBridge Scheduler.
* AWS Fargate - Information on how to run containers in a serverless manner using AWS
Fargate.
QUESTION NO: 291
회사는 AWS Organizations를 사용합니다. 회사는 다양한 예산으로 일부 AWS 계정을
운영하려고 합니다. 회사는 특정 기간 동안 할당된 예산 임계값이 충족되면 알림을 받고 AWS
계정에 대한 추가 리소스 프로비저닝을 자동으로 방지하려고 합니다.
이러한 요구 사항을 충족하는 솔루션 조합은 무엇입니까? (3개를 선택하세요.)
A. AWS 예산을 사용하여 예산을 생성합니다. 필요한 AWS 계정의 비용 및 사용 보고서
섹션에서 예산 금액을 설정합니다.
B. AWS 예산을 사용하여 예산을 생성합니다. 필요한 AWS 계정의 결제 대시보드에서 예산
금액을 설정합니다.
C. AWS Budgets에 대한 오전 1시 사용자를 생성하여 필요한 권한으로 예산 작업을
실행합니다.
D. AWS Budgets에 대한 오전 1시 역할을 생성하여 필요한 권한으로 예산 작업을 실행합니다.
E. 각 계정이 예산 임계값을 충족할 때 회사에 알리는 경고를 추가합니다. 추가 리소스
프로비저닝을 방지하기 위해 적절한 구성 규칙으로 생성된 오전 1시 ID를 선택하는 예산
작업을 추가합니다.
F. 각 계정이 예산 한도를 충족할 때 회사에 알리는 경고를 추가합니다. 추가 리소스
프로비저닝을 방지하기 위해 적절한 SCP(서비스 제어 정책)로 생성된 1AM ID를 선택하는
예산 작업을 추가합니다.
Answer: B D F
Explanation:
To use AWS Budgets to create and manage budgets for different AWS accounts, the
company needs to do the following steps:
* Use AWS Budgets to create a budget for each AWS account that needs a different budget
amount. The budget can be based on cost or usage metrics, and can have different time
periods, filters, and thresholds. The company can set the budget amount under the Billing
dashboards of the required AWS accounts1.
* Create an IAM role for AWS Budgets to run budget actions with the required permissions. A
budget action is a response that AWS Budgets initiates when a budget exceeds a specified
threshold. The IAM role allows AWS Budgets to perform actions on behalf of the company,
such as applying an IAM policy or a service control policy (SCP) to restrict the provisioning of
additional resources2.
* Add an alert to notify the company when each account meets its budget threshold. The alert
201

IT Certification Guaranteed, The Easy Way!
can be sent via email or Amazon SNS. The company can also add a budget action that
selects the IAM role created and the appropriate SCP to prevent provisioning of additional
resources. An SCP is a type of policy that can be applied to an AWS account or an
organizational unit (OU) within AWS Organizations. An SCP can limit the actions that users
and roles can perform in the account or OU3.
References:
* 4: https://aws.amazon.com/budgets/
* 1: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-create.html
* 2: https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-controls.html
* 3:
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.ht
ml
QUESTION NO: 292
한 회사는 온프레미스 데이터 센터에서 AWS로 상용 기성 애플리케이션을 마이그레이션할
계획입니다. 소프트웨어에는 예측 가능한 용량 및 가동 시간 요구 사항을 갖춘 소켓과 코어를
사용하는 소프트웨어 라이선스 모델이 있습니다. 회사는 올해 초에 구입한 기존 라이센스를
사용하려고 합니다.
가장 비용 효율적인 Amazon EC2 요금 옵션은 무엇입니까?
A. 전용 예약 호스트
B. 전용 주문형 호스트
C. 전용 예약 인스턴스
D. 전용 On-Oemand 인스턴스
Answer: A
Explanation:
https://aws.amazon.com/ec2/dedicated-hosts/ Amazon EC2 Dedicated Hosts allow you to
use your eligible software licenses from vendors such as Microsoft and Oracle on Amazon
EC2, so that you get the flexibility and cost effectiveness of using your own licenses, but with
the resiliency, simplicity and elasticity of AWS.
QUESTION NO: 293
스타트업 회사가 Amazon EC2 인스턴스에서 고객을 위한 웹사이트를 호스팅하고 있습니다.
웹사이트는 무상태 Python 애플리케이션과 MySQL 데이터베이스로 구성되어 있습니다.
웹사이트는 소량의 트래픽만 처리합니다. 이 회사는 인스턴스의 안정성에 대해 우려하고
있으며 고가용성 아키텍처로 마이그레이션해야 합니다. 이 회사는 애플리케이션 코드를
수정할 수 없습니다.
솔루션 아키텍트는 웹사이트의 고가용성을 달성하기 위해 어떤 작업 조합을 취해야 합니까?
(2가지를 선택하세요.)
A. 사용 중인 각 가용성 영역에 인터넷 게이트웨이를 제공합니다.
B. 데이터베이스를 Amazon RDS for MySQL Multi-AZ DB 인스턴스로 마이그레이션합니다.
C. 데이터베이스를 Amazon DynamoDB로 마이그레이션하고 DynamoDB 자동 크기 조정을
활성화합니다.
D. AWS DataSync를 사용하여 여러 EC2 인스턴스의 데이터베이스 데이터를 동기화합니다.
E. 두 개의 가용성 영역에 분산된 EC2 인스턴스의 자동 크기 조정 그룹에 트래픽을
분산시키는 애플리케이션 부하 분산 장치를 생성합니다.
202

IT Certification Guaranteed, The Easy Way!
Answer: B E
Explanation:
To achieve high availability for the website, two key actions should be taken:
* Amazon RDS for MySQL Multi-AZ: By migrating the database to an RDS for MySQL Multi-
AZ deployment, the database becomes highly available. Multi-AZ provides automatic failover
from the primary database to a standby replica in another Availability Zone, ensuring
database availability even in the case of an AZ failure.
* Application Load Balancer and Auto Scaling: Deploying an Application Load Balancer (ALB)
in front of the EC2 instances ensures that traffic is evenly distributed across the instances.
Configuring an Auto Scaling group to run EC2 instances across multiple Availability Zones
ensures that the application remains available even if one instance or one AZ becomes
unavailable. This setup enhances fault tolerance and improves reliability.
* Why Not Other Options?:
* Option A (Internet Gateway per AZ): Internet Gateways are region-wide resources and do
not need to be provisioned per Availability Zone. This option does not contribute to high
availability.
* Option C (DynamoDB + Auto Scaling): DynamoDB would require changes to the application
code to switch from MySQL, which is not possible per the question's constraints.
* Option D (DataSync): AWS DataSync is used for data transfer and synchronization, not for
achieving high availability for a database.
AWS References:
* Amazon RDS Multi-AZ Deployments - Explanation of how Multi-AZ deployments work in
Amazon RDS.
* Application Load Balancing - Details on how to configure and use ALB for distributing traffic
across multiple instances.
QUESTION NO: 294
금융 회사가 거래 플랫폼을 AWS로 이전하고 있습니다. 거래 플랫폼은 대량의 시장 데이터를
처리하고 주식 거래를 처리합니다. 이 회사는 온프레미스 데이터 센터에서 AWS로 일관되고
지연 시간이 짧은 네트워크 연결을 구축해야 합니다.
회사는 VPC에서 리소스를 호스팅합니다. 솔루션은 퍼블릭 인터넷을 사용해서는 안 됩니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. AWS 클라이언트 VPN을 사용하여 온프레미스 데이터 센터를 AWS에 연결합니다.
B. AWS Direct Connect를 사용하여 온프레미스 데이터 센터에서 AWS로 연결을 설정합니다.
C. AWS PrivateLink를 사용하여 온프레미스 데이터 센터에서 AWS로 연결을 설정합니다.
D. AWS 사이트 간 VPN을 사용하여 온프레미스 데이터 센터를 AWS에 연결합니다.
Answer: B
Explanation:
AWS Direct Connect is the best solution for establishing a consistent, low-latency connection
from an on- premises data center to AWS without using the public internet. Direct Connect
offers dedicated, high- throughput, and low-latency network connections, which are ideal for
performance-sensitive applications like a trading platform that processes high volumes of
market data and stock trades.
Direct Connect provides a private connection to your AWS VPC, ensuring that data doesn't
traverse the public internet, which enhances both security and performance consistency.
203

IT Certification Guaranteed, The Easy Way!
AWS References:
* AWS Direct Connect provides a dedicated network connection to AWS services with
consistent, low- latency performance.
* Best Practices for High Performance on AWS for performance-sensitive workloads like
trading platforms.
Why the other options are incorrect:
* A. AWS Client VPN: While this offers secure connectivity, it's over the public internet and is
not designed for the low-latency, high-performance needs of a trading platform.
* C. AWS PrivateLink: PrivateLink is used for connecting VPCs and services within AWS, but
it is not designed for connecting on-premises data centers to AWS.
* D. AWS Site-to-Site VPN: Although this provides secure connectivity, it uses the public
internet, which can introduce latency and doesn't meet the low-latency requirements of the
use case.
QUESTION NO: 295
회사에는 관리 및 프로덕션이라는 두 개의 VPC가 있습니다. 관리 VPC는 ​​고객 게이트웨이를
통해 VPN을 사용하여 데이터 센터의 단일 디바이스에 연결합니다. 프로덕션 VPC는 ​​가상
프라이빗 게이트웨이 AWS Direct Connect 연결을 사용합니다. 관리 및 프로덕션 VPC는 ​​모두
단일 VPC 피어링 연결을 사용하여 이 아키텍처의 단일 실패 지점을 완화하기 위해 솔루션
설계자가 수행해야 하는 작업은 무엇입니까?
A. 관리 및 프로덕션 VPC 사이에 VPN 세트를 추가합니다.
B. 두 번째 가상 프라이빗 게이트웨이를 추가하고 이를 관리 VPC에 연결합니다.
C. 두 번째 고객 게이트웨이 디바이스에서 관리 VPC에 두 번째 VPN 세트를 추가합니다.
D. 관리 VPC와 프로덕션 VPC 사이에 두 번째 VPC 피어링 연결을 추가합니다.
Answer: C
Explanation:
This answer is correct because it provides redundancy for the VPN connection between the
Management VPC and the data center. If one customer gateway device or one VPN tunnel
becomes unavailable, the traffic can still flow over the second customer gateway device and
the second VPN tunnel. This way, the single point of failure in the VPN connection is
mitigated.
References:
* https://docs.aws.amazon.com/vpn/latest/s2svpn/vpn-redundant-connection.html
* https://www.trendmicro.com/cloudoneconformity/knowledge-base/aws/VPC/vpn-tunnel-
redundancy.
html
QUESTION NO: 296
한 게임 회사가 공개 점수판을 데이터 센터에서 AWS 클라우드로 옮기고 있습니다. 이 회사는
Application Load Balancer 뒤에 있는 Amazon EC2 Windows Server 인스턴스를 사용하여
동적 애플리케이션을 호스팅합니다. 회사는 해당 애플리케이션을 위한 가용성이 뛰어난
스토리지 솔루션이 필요합니다. 애플리케이션은 정적 파일과 동적 서버측 코드로 구성됩니다.
이러한 요구 사항을 충족하려면 솔루션 설계자가 수행해야 하는 단계 조합은 무엇입니까?
(2개를 선택하세요.)
A. Amazon S3에 정적 파일을 저장합니다. Amazon CloudFront를 사용하여 엣지에서 객체를
204

IT Certification Guaranteed, The Easy Way!
캐시합니다.
B. Amazon S3에 정적 파일을 저장합니다. Amazon ElastiCache를 사용하여 엣지에서 객체를
캐시합니다.
C. Amazon Elastic File System(Amazon EFS)에 서버 측 코드를 저장합니다. 파일을
공유하려면 각 EC2 인스턴스에 EFS 볼륨을 탑재하세요.
D. Windows 파일 서버용 Amazon FSx에 서버 측 코드를 저장합니다. 파일을 공유하려면 각
EC2 인스턴스에 FSx for Windows File Server 볼륨을 탑재하세요.
E. 범용 SSD(gp2) Amazon Elastic Block Store(Amazon EBS) 볼륨에 서버 측 코드를
저장합니다. 파일을 공유하려면 각 EC2 인스턴스에 EBS 볼륨을 탑재하세요.
Answer: A D
Explanation:
A because Elasticache, despite being ideal for leaderboards per Amazon, doesn't cache at
edge locations. D because FSx has higher performance for low latency needs.
https://www.techtarget.com/searchaws/tip
/Amazon-FSx-vs-EFS-Compare-the-AWS-file-services "FSx is built for high performance and
submillisecond latency using solid-state drive storage volumes. This design enables users to
select storage capacity and latency independently. Thus, even a subterabyte file system can
have 256 Mbps or higher throughput and support volumes up to 64 TB." Amazon S3 is an
object storage service that can store static files such as images, videos, documents, etc.
Amazon EFS is a file storage service that can store files in a hierarchical structure and
supports NFS protocol.
Amazon FSx for Windows File Server is a file storage service that can store files in a
hierarchical structure and supports SMB protocol. Amazon EBS is a block storage service
that can store data in fixed-size blocks and attach to EC2 instances.
Based on these definitions, the combination of steps that should be taken to meet the
requirements are:
A: Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the
edge. D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx
for Windows File Server volume on each EC2 instance to share the files.
QUESTION NO: 297
솔루션 설계자는 엔지니어링 도면을 저장하고 보는 데 사용되는 새로운 웹 애플리케이션용
스토리지 아키텍처를 설계하고 있습니다. 모든 애플리케이션 구성 요소는 AWS 인프라에
배포됩니다.
응용 프로그램 설계는 사용자가 엔지니어링 도면이 로드될 때까지 기다리는 시간을
최소화하기 위해 캐싱을 지원해야 합니다. 애플리케이션은 페타바이트 단위의 데이터를
저장할 수 있어야 합니다. 솔루션 설계자는 어떤 스토리지와 캐싱 조합을 사용해야 합니까?
A. Amazon CloudFront를 갖춘 Amazon S3
B. Amazon ElastiCache를 사용한 Amazon S3 Glacier
C. Amazon CloudFront를 사용한 Amazon Elastic Block Store(Amazon EBS) 볼륨
D. Amazon ElastiCache를 사용하는 AWS Storage Gateway
Answer: A
Explanation:
To store and view engineering drawings with caching support, Amazon S3 and Amazon
205

IT Certification Guaranteed, The Easy Way!
CloudFront are suitable solutions. Amazon S3 can store any amount of data with high
durability, availability, and performance. Amazon CloudFront can distribute the engineering
drawings to edge locations closer to the users, which can reduce the latency and improve the
user experience. Amazon CloudFront can also cache the engineering drawings at the edge
locations, which can minimize the amount of time that users wait for the drawings to load.
References:
* What Is Amazon S3?
* What Is Amazon CloudFront?
QUESTION NO: 298
한 회사가 Windows 컨테이너 아래의 .NET 6 Framework에서 실행되는 Windows 작업을
컨테이너화했습니다. 회사는 AWS 클라우드에서 이 작업을 실행하려고 합니다. 작업은
10분마다 실행됩니다. 작업 실행 시간은 1분에서 3분 사이입니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 작업의 컨테이너 이미지를 기반으로 AWS Lambda 함수를 생성합니다. 10분마다 함수를
호출하도록 Amazon EventBridge를 구성합니다.
B. AWS Batch를 사용하여 AWS Fargate 리소스를 사용하는 작업을 생성합니다. 10분마다
실행되도록 작업 예약을 구성합니다.
C. AWS Fargate에서 Amazon Elastic Container Service(Amazon ECS)를 사용하여 작업을
실행합니다. 10분마다 실행할 작업의 컨테이너 이미지를 기반으로 예약된 작업을 만듭니다.
D. AWS Fargate에서 Amazon Elastic Container Service(Amazon ECS)를 사용하여 작업을
실행합니다. 작업의 컨테이너 이미지를 기반으로 독립형 작업을 생성합니다. Windows 작업
스케줄러를 사용하여 10분마다 작업을 실행합니다.
Answer: A
Explanation:
AWS Lambda supports container images as a packaging format for functions. You can use
existing container development workflows to package and deploy Lambda functions as
container images of up to 10 GB in size.
You can also use familiar tools such as Docker CLI to build, test, and push your container
images to Amazon Elastic Container Registry (Amazon ECR). You can then create an AWS
Lambda function based on the container image of your job and configure Amazon
EventBridge to invoke the function every 10 minutes using a cron expression. This solution
will be cost-effective as you only pay for the compute time you consume when your function
runs. References: https://docs.aws.amazon.com/lambda/latest/dg/images-create.
html https://docs.aws.amazon.com/eventbridge/latest/userguide/run-lambda-schedule.html
QUESTION NO: 299
회사는 AWS에 사용자 장치에서 센서 데이터를 수집하는 3계층 환경을 가지고 있습니다.
트래픽은 NIB(Network Load Balancer)를 거쳐 웹 계층용 Amazon EC2 인스턴스로,
마지막으로 애플리케이션 계층용 EC2 인스턴스로 흐릅니다. 데이터베이스 호출 솔루션
설계자는 웹 계층으로 전송되는 데이터의 보안을 개선하기 위해 무엇을 해야 합니까?
A. TLS 수신기를 구성하고 NLB에 서버 인증서를 추가합니다.
B. AWS Shield Advanced 구성 및 NLB에서 AWS WAF 활성화
C. 로드 밸런서를 Application Load Balancer로 변경하고 여기에 AWS WAF를 연결합니다.
D. AWS Key Management Service(AWS KMS)를 사용하여 EC2 인스턴스에서 Amazon
206

IT Certification Guaranteed, The Easy Way!
Elastic Block Store(Amazon EBS) 볼륨 암호화
Answer: A
A: How do you protect your data in transit?
Best Practices:
Implement secure key and certificate management: Store encryption keys and certificates
securely and rotate them at appropriate time intervals while applying strict access control; for
example, by using a certificate management service, such as AWS Certificate Manager
(ACM).
Enforce encryption in transit: Enforce your defined encryption requirements based on
appropriate standards and recommendations to help you meet your organizational, legal, and
compliance requirements.
Automate detection of unintended data access: Use tools such as GuardDuty to
automatically detect attempts to move data outside of defined boundaries based on data
classification level, for example, to detect a trojan that is copying data to an unknown or
untrusted network using the DNS protocol.
Authenticate network communications: Verify the identity of communications by using
protocols that support authentication, such as Transport Layer Security (TLS) or IPsec.
https://wa.aws.amazon.com/wat.question.SEC_9.en.html
QUESTION NO: 300
한 회사의 솔루션 아키텍트가 AWS Organizations를 사용하는 AWS 다중 계정 솔루션을
설계하고 있습니다.
솔루션 아키텍트는 회사 계정을 조직 단위(OU)로 구성했습니다.
솔루션 아키텍트는 OU 계층 구조의 변경 사항을 식별하는 솔루션이 필요합니다. 솔루션은
또한 회사의 운영 팀에 변경 사항을 알려야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. AWS Control Tower를 사용하여 AWS 계정을 프로비저닝합니다. 계정 드리프트 알림을
사용하여 OU 계층 구조의 변경 사항을 식별합니다.
B. AWS Control Tower를 사용하여 AWS 계정을 프로비저닝합니다. AWS Config 집계 규칙을
사용하여 OU 계층의 변경 사항을 식별합니다.
C. AWS Service Catalog를 사용하여 Organizations에서 계정을 만듭니다. AWS CloudTrail
조직 트레일을 사용하여 OU 계층 구조의 변경 사항을 식별합니다.
D. AWS CloudFormation 템플릿을 사용하여 Organizations에서 계정을 만듭니다. 스택에서
드리프트 감지 작업을 사용하여 OU 계층의 변경 사항을 식별합니다.
Answer: A
Explanation:
* Understanding the Requirement: The company needs to monitor and notify changes to the
OU hierarchy with minimal operational overhead.
* Analysis of Options:
* AWS Control Tower with Account Drift Notifications: AWS Control Tower provides
automated account provisioning and governance, including drift detection and notifications for
changes in the OU hierarchy.
* AWS Control Tower with AWS Config: AWS Config provides resource configuration tracking
but is more complex compared to drift notifications directly available in Control Tower.
* AWS Service Catalog with CloudTrail: While CloudTrail tracks changes, setting up
207

IT Certification Guaranteed, The Easy Way!
notification mechanisms involves more operational overhead.
* AWS CloudFormation with Drift Detection: Suitable for tracking configuration changes but
less efficient for monitoring OU hierarchy changes compared to Control Tower's built-in
features.
* Best Solution:
* AWS Control Tower with Account Drift Notifications: Provides a streamlined and efficient
way to detect and notify changes in the OU hierarchy with minimal operational overhead.
References:
* AWS Control Tower
* AWS Control Tower Drift Detection
QUESTION NO: 301
A manufacturing company runs an order processing application in its VPC. The company
wants to securely send messages from the application to an external Salesforce system that
uses Open Authorization (OAuth).
A solutions architect needs to integrate the company's order processing application with the
external Salesforce system.
Which solution will meet these requirements?
A. Create an Amazon Simple Notification Service (Amazon SNS) topic in a fanout
configuration that pushes data to an HTTPS endpoint. Configure the order processing
application to publish messages to the SNS topic.
B. Create an Amazon Simple Notification Service (Amazon SNS) topic in a fanout
configuration that pushes data to an Amazon Data Firehose delivery stream that has a HTTP
destination. Configure the order processing application to publish messages to the SNS
topic.
C. Create an Amazon EventBridge rule and configure an Amazon EventBridge API
destination partner Configure the order processing application to publish messages to
Amazon EventBridge.
D. Create an Amazon Managed Streaming for Apache Kafka (Amazon MSK) topic that has
an outbound MSK Connect connector. Configure the order processing application to publish
messages to the MSK topic.
Answer: C
Explanation:
Amazon EventBridge API destinations allow you to send data from AWS to external systems,
like Salesforce, using HTTP APIs, including those secured with OAuth. This provides a
secure and scalable solution for sending messages from the order processing application to
Salesforce.
* Option A and B (SNS): SNS is not ideal for OAuth-secured external APIs and lacks the
necessary OAuth integration.
* Option D (MSK): Amazon MSK is a Kafka-based streaming solution, which is overkill for
simple message forwarding to Salesforce.
AWS References:
* Amazon EventBridge API Destinations
QUESTION NO: 302
208

IT Certification Guaranteed, The Easy Way!
소셜 미디어 회사는 사용자가 AWS 클라우드에서 호스팅되는 애플리케이션에 이미지를
업로드할 수 있도록 허용하려고 합니다. 회사는 이미지가 여러 장치 유형에 표시될 수 있도록
이미지 크기를 자동으로 조정하는 솔루션이 필요합니다. 애플리케이션은 하루 종일 예측할 수
없는 트래픽 패턴을 경험합니다. 회사는 확장성을 극대화하는 고가용성 솔루션을 찾고
있습니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 이미지 크기를 조정하고 이미지를 Amazon S3 버킷에 저장하기 위해 AWS Lambda 함수를
호출하는 Amazon S3에서 호스팅되는 정적 웹 사이트를 생성합니다.
B. AWS Step Functions를 호출하여 이미지 크기를 조정하고 Amazon RDS 데이터베이스에
이미지를 저장하는 Amazon CloudFront에서 호스팅되는 정적 웹 사이트를 생성합니다.
C. Amazon EC2 인스턴스에서 실행되는 웹 서버에서 호스팅되는 동적 웹 사이트 생성 EC2
인스턴스에서 실행되는 프로세스를 구성하여 이미지 크기를 조정하고 이미지를 Amazon S3
버킷에 저장합니다.
D. Amazon Simple Queue Service(Amazon SQS)에서 크기 조정 작업을 생성하는 자동 확장
Amazon Elastic Container Service(Amazon ECS) 클러스터에서 호스팅되는 동적 웹 사이트를
생성합니다. 크기 조정 작업을 처리하기 위해 Amazon EC2 인스턴스에서 실행되는 이미지
크기 조정 프로그램 설정
Answer: A
Explanation:
By using Amazon S3 and AWS Lambda together, you can create a serverless architecture
that provides highly scalable and available image resizing capabilities. Here's how the
solution would work: Set up an Amazon S3 bucket to store the original images uploaded by
users. Configure an event trigger on the S3 bucket to invoke an AWS Lambda function
whenever a new image is uploaded. The Lambda function can be designed to retrieve the
uploaded image, perform the necessary resizing operations based on device requirements,
and store the resized images back in the S3 bucket or a different bucket designated for
resized images. Configure the Amazon S3 bucket to make the resized images publicly
accessible for serving to users.
QUESTION NO: 303
회사는 회사의 기본 AWS 계정에 있는 VPC에서 실행될 AWS Lambda 함수를 생성해야
합니다. Lambda 함수는 회사가 Amazon Elastic File System(Amazon EFS) 파일 시스템에
저장하는 파일에 액세스해야 합니다. EFS 파일 시스템은 보조 AWS 계정에 있습니다. 회사가
파일 시스템에 파일을 추가함에 따라 솔루션은 수요를 충족하도록 확장되어야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 기본 계정에 새 EPS 파일 시스템 생성 AWS DataSync를 사용하여 원래 EPS 파일
시스템의 콘텐츠를 새 EPS 파일 시스템에 복사
B. 기본 계정과 보조 계정에 있는 VPC 간에 VPC 피어링 연결을 생성합니다.
C. 파일 시스템에 대해 구성된 마운트가 있는 보조 계정에서 두 번째 Lambda 함수를
생성합니다. 기본 계정의 Lambda 함수를 사용하여 보조 계정의 Lambda 함수 호출
D. 파일 시스템의 콘텐츠를 Lambda 계층으로 이동합니다. 회사의 보조 계정이 Lambda
계층을 사용할 수 있도록 Lambda 계층의 권한을 구성합니다.
Answer: B
Explanation:
209

IT Certification Guaranteed, The Easy Way!
This option is the most cost-effective and scalable way to allow the Lambda function in the
primary account to access the EFS file system in the secondary account. VPC peering
enables private connectivity between two VPCs without requiring gateways, VPN
connections, or dedicated network connections. The Lambda function can use the VPC
peering connection to mount the EFS file system as a local file system and access the files
as needed. The solution does not incur additional data transfer or storage costs, and it
leverages the existing EFS file system without duplicating or moving the data.
Option A is not cost-effective because it requires creating a new EFS file system and using
AWS DataSync to copy the data from the original EFS file system. This would incur additional
storage and data transfer costs, and it would not provide real-time access to the files.
Option C is not scalable because it requires creating a second Lambda function in the
secondary account and configuring cross-account permissions to invoke it from the primary
account. This would add complexity and latency to the solution, and it would increase the
Lambda invocation costs.
Option D is not feasible because Lambda layers are not designed to store large amounts of
data or provide file system access. Lambda layers are used to share common code or
libraries across multiple Lambda functions.
Moving the contents of the EFS file system to a Lambda layer would exceed the size limit of
250 MB for a layer, and it would not allow the Lambda function to read or write files to the
layer. References:
* What Is VPC Peering?
* Using Amazon EFS file systems with AWS Lambda
* What Are Lambda Layers?
QUESTION NO: 304
회사에서는 AWS 인프라에 대해 월별 유지 관리를 수행합니다. 이러한 유지 관리 활동 중에
회사는 여러 AWS 리전에 걸쳐 MySQL 데이터베이스에 대한 Amazon ROS에 대한 자격
증명을 교체해야 합니다. 어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을
충족합니까?
A. 자격 증명을 AWS Secrets Manager에 비밀로 저장합니다. 필요한 리전에 대해 다중 리전
비밀 복제를 사용합니다. Secrets Manager를 구성하여 일정에 따라 비밀을 교체합니다.
B. 보안 문자열 매개변수를 생성하여 AWS 시스템 관리자에 자격 증명을 비밀로 저장합니다.
필요한 리전에 대해 다중 리전 비밀 복제를 사용합니다. 일정에 따라 비밀을 교체하도록
시스템 관리자를 구성합니다.
C. 서버 측 암호화(SSE)가 활성화된 Amazon S3 버킷에 자격 증명을 저장합니다. Amazon
EventBridge(Amazon CloudWatch Events)를 사용하여 AWS Lambda 함수를 호출하여 자격
증명을 교체합니다.
D. AWS Key Management Service(AWS KMS) 다중 리전 고객 관리형 키를 사용하여 자격
증명을 비밀로 암호화합니다. Amazon DynamoDB 글로벌 테이블에 비밀을 저장합니다. AWS
Lambda 함수를 사용하여 DynamoDB에서 비밀을 검색합니다. RDS API를 사용하여 비밀을
회전시키세요.
Answer: A
Explanation:
https://aws.amazon.com/blogs/security/how-to-replicate-secrets-aws-secrets-manager-
multiple-regions/
210

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 305
회사에 각각 크기가 약 5MB인 많은 수의 파일을 생성하는 응용 프로그램이 있습니다. 파일은
Amazon S3에 저장됩니다. 회사 정책에 따라 파일을 삭제하려면 4년 동안 보관해야 합니다.
파일에는 재생산이 쉽지 않은 중요한 비즈니스 데이터가 포함되어 있으므로 즉각적인
액세스가 항상 필요합니다. 파일은 객체 생성 후 처음 30일 동안 자주 액세스되지만 처음 30일
후에는 거의 액세스되지 않습니다. 다음 중 가장 비용 효율적인 스토리지 솔루션은
무엇입니까?
A. 객체 생성 30일 후 Mm을 S3 Standard에서 S3 Glacier로 이동하는 S3 버킷 수명 주기 정책
생성 객체 생성 4년 후 Tiles 삭제
B. S3 Standard에서 S3 One Zone-infrequent Access(S3 One Zone-IA)로 타일을 이동하는 S3
버킷 수명 주기 정책을 생성합니다. 객체 생성 후 30일 객체 생성 후 4년 동안 요금 삭제
C. S3 Standard-infrequent Access(S3 Standard)에서 파일을 이동하는 S3 버킷 수명 주기
정책 생성
-lA) 객체 생성에서 30. 객체 생성 4년 후 관계 삭제
D. S3 버킷 수명 주기 정책을 생성하여 S3 Standard에서 S3 Standard-Infrequent Access(S3
Standard-IA)로 파일을 이동합니다. 객체 생성 후 30일 객체 상자에 넣은 후 4년 후에 파일을
S3 Glacier로 이동합니다.
Answer: B
Explanation:
https://aws.amazon.com/s3/storage-classes/?trk=66264cd8-3b73-416c-9693-
ea7cf4fe846a&sc_channel=ps&s_kwcid=AL!4422!3!536452716950!p!!g!!aws%20s3%
20pricing&ef_id=Cj0KCQjwnbmaBhD-
ARIsAGTPcfVHUZN5_BMrzl5zBcaC8KnqpnNZvjbZzqPkH6k7q4JcYO5KFLx0YYgaAm6nEA
Lw_wcB:G:
s&s_kwcid=AL!4422!3!536452716950!p!!g!!aws%20s3%20pricing
QUESTION NO: 306
회사는 데이터 센터에서 SMB 파일 서버를 실행하고 있습니다. 파일 서버는 파일이 생성된 후
처음 며칠 동안 자주 액세스하는 대용량 파일을 저장합니다. 7일이 지나면 파일에 거의
액세스하지 않습니다.
총 데이터 크기가 증가하고 있으며 회사의 총 저장 용량에 가깝습니다. 솔루션 설계자는 가장
최근에 액세스한 파일에 대한 저지연 액세스를 잃지 않으면서 회사의 사용 가능한 저장
공간을 늘려야 합니다. 솔루션 설계자는 향후 스토리지 문제를 방지하기 위해 파일 수명 주기
관리도 제공해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS DataSync를 사용하여 SMB 파일 서버에서 AWS로 7일이 지난 데이터를 복사합니다.
B. Amazon S3 파일 게이트웨이를 생성하여 회사의 저장 공간을 확장합니다. S3 수명 주기
정책을 생성하여 7일 후에 데이터를 S3 Glacier Deep Archive로 전환합니다.
C. Amazon FSx for Windows 파일 서버 파일 시스템을 생성하여 회사의 저장 공간을
확장합니다.
D. 각 사용자의 컴퓨터에 유틸리티를 설치하여 Amazon S3에 액세스합니다. S3 수명 주기
정책을 생성하여 7일 후 데이터를 S3 Glacier Flexible Retrieval로 전환합니다.
Answer: B
Explanation:
211

IT Certification Guaranteed, The Easy Way!
Amazon S3 File Gateway is a hybrid cloud storage service that enables on-premises
applications to seamlessly use Amazon S3 cloud storage. It provides a file interface to
Amazon S3 and supports SMB and NFS protocols. It also supports S3 Lifecycle policies that
can automatically transition data from S3 Standard to S3 Glacier Deep Archive after a
specified period of time. This solution will meet the requirements of increasing the company's
available storage space without losing low-latency access to the most recently accessed files
and providing file lifecycle management to avoid future storage issues.
Reference:
https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html
QUESTION NO: 307
회사에서는 암호화 키 자료 제어를 유지하면서 백업, 복구 및 보관을 위한 클라우드 기반
솔루션이 필요합니다.
이러한 요구 사항을 충족시킬 솔루션 조합은 무엇입니까? (2개 선택)
A. 키 자료 없이 AWS Key Management Service(AWS KMS) 키를 만듭니다. 회사의 키 자료를
KMS 키로 가져옵니다.
B. AWS KMS에서 생성한 키 자료가 포함된 AWS KMS 암호화 키를 생성합니다.
C. Amazon S3 Standard-Infrequent Access(S3 Standard-IA)에 데이터를 저장합니다. AWS
KMS 키와 함께 S3 버킷 키를 사용합니다.
D. Amazon S3 Glacier 스토리지 클래스에 데이터를 저장합니다. 고객 제공 키(SSE-C)를
사용하여 서버 측 암호화를 사용합니다.
E. AWS Snowball 디바이스에 데이터를 저장합니다. AWS KMS 키(SSE-KMS)를 사용하여
서버 측 암호화를 사용합니다.
Answer: A D
* Option A allows importing your own encryption keys into AWS KMS, ensuring control over
key material.
* Option D uses S3 Glacier with SSE-C, where the customer controls the encryption keys,
meeting compliance needs.
* Option B uses AWS-managed key material, violating the requirement for key material
control.
* Option C and E are not fully compliant with the control requirement.
QUESTION NO: 308
한 회사가 Amazon Linux EC2 인스턴스 그룹에서 애플리케이션을 실행합니다. 규정 준수를
위해 회사는 모든 애플리케이션 로그 파일을 7년 동안 보관해야 합니다. 로그 파일은 모든
파일에 동시에 액세스할 수 있어야 하는 보고 도구로 분석됩니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 스토리지 솔루션은 무엇입니까?
A. Amazon Elastic Block Store(Amazon EBS)
B. Amazon Elastic File System(Amazon EFS)
C. Amazon EC2 인스턴스 스토어
D. 아마존 S3
Answer: D
Explanation:
https://docs.aws.amazon.com/efs/latest/ug/transfer-data-to-efs.html
212

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 309
전자상거래 회사는 Microsoft SQL Server Enterprise Edition을 실행하는 Amazon RDS DB
인스턴스에 대한 재해 복구 솔루션을 원합니다. 회사의 현재 복구 지점 목표(RPO)와 복구
시간 목표(RTO)는 24시간입니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. 지역 간 읽기 복제본을 생성하고 읽기 복제본을 기본 인스턴스로 승격합니다.
B. AWS Database Migration Service(AWS DMS)를 사용하여 RDS 지역 간 복제를
생성합니다.
C. 24시간마다 크로스 리전 복제를 사용하여 네이티브 백업을 Amazon S3 버킷으로
복사합니다.
D. 24시간마다 자동 스냅샷을 다른 지역으로 복사합니다.
Answer: D
Explanation:
This solution is the most cost-effective and meets the RPO and RTO requirements of 24
hours.
* Automatic Snapshots: Amazon RDS automatically creates snapshots of your DB instance
at regular intervals. By copying these snapshots to another AWS Region every 24 hours, you
ensure that you have a backup available in a different geographic location, providing disaster
recovery capability.
* RPO and RTO: Since the company's RPO and RTO are both 24 hours, copying snapshots
daily to another Region is sufficient. In the event of a disaster, you can restore the DB
instance from the most recent snapshot in the target Region.
* Why Not Other Options?:
* Option A (Cross-Region Read Replica): This could provide a faster recovery time but is
more costly due to the ongoing replication and resource usage in another Region.
* Option B (DMS Cross-Region Replication): While effective for continuous replication, it
introduces complexity and cost that isn't necessary given the 24-hour RPO/RTO.
* Option C (Cross-Region Native Backup Copy): This involves more manual steps and
doesn't offer as straightforward a solution as automated snapshot copying.
AWS References:
* Amazon RDS Automated Backups and Snapshots - Details on automated backups and
snapshots in RDS.
* Copying an Amazon RDS DB Snapshot - How to copy DB snapshots to another Region.
QUESTION NO: 310
솔루션 설계자는 회사의 재해 복구(DR) 아키텍처를 설계하고 있습니다. 이 회사에는 예약된
백업을 통해 프라이빗 서브넷의 Amazon EC2 인스턴스에서 실행되는 MySQL
데이터베이스가 있습니다. 여러 AWS 지역을 포함하는 DR 설계.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. MySQL 데이터베이스를 여러 EC2 인스턴스로 마이그레이션합니다. DR 지역에서 대기
EC2 인스턴스를 구성합니다. 복제를 활성화합니다.
B. MySQL 데이터베이스를 Amazon RDS로 마이그레이션합니다. 다중 AZ 배포를
사용합니다. 다양한 가용 영역에서 기본 DB 인스턴스에 대한 읽기 복제를 활성화합니다.
C. MySQL 데이터베이스를 Amazon Aurora 글로벌 데이터베이스로 마이그레이션합니다.
기본 리전에서 기본 DB 클러스터를 호스팅합니다. DR 지역에서 보조 DB 클러스터를
213

IT Certification Guaranteed, The Easy Way!
호스팅합니다.
D. S3 교차 리전 복제(CRR)용으로 구성된 Amazon S3 버킷에 MySQL 데이터베이스의 예약
백업을 저장합니다. 데이터 백업을 사용하여 DR 지역의 데이터베이스를 복원합니다.
Answer: C
Explanation:
Migrate MySQL database to an Amazon Aurora global database is the best solution because
it requires minimal operational overhead. Aurora is a managed service that provides
automatic failover, so standby instances do not need to be manually configured. The primary
DB cluster can be hosted in the primary Region, and the secondary DB cluster can be hosted
in the DR Region. This approach ensures that the data is always available and up-to-date in
multiple Regions, without requiring significant manual intervention.
QUESTION NO: 311
회사에서 Amazon ECS를 사용하여 애플리케이션을 실행합니다. 애플리케이션은 원본
이미지의 크기가 조정된 버전을 생성한 다음 Amazon S3 API를 호출하여 크기가 조정된
이미지를 Amazon S3에 저장합니다.
솔루션 설계자는 애플리케이션에 Amazon $3에 액세스할 수 있는 권한이 있는지 어떻게
확인할 수 있습니까?
A. Amazon ECS에서 읽기/쓰기 액세스를 허용하도록 AWS IAM에서 S3 역할을 업데이트한
다음 컨테이너를 다시 시작합니다.
B. S3 권한이 있는 IAM 역할을 생성한 다음 작업 정의에서 해당 역할을 taskRoleArn으로
지정합니다.
C. Amazon ECS에서 Amazon $3로의 액세스를 허용하는 보안 그룹을 생성하고 ECS
클러스터에서 사용하는 시작 구성을 업데이트합니다.
D. S3 권한이 있는 IAM 사용자를 생성한 다음 이 계정으로 로그인한 동안 ECS 클러스터에
대한 Amazon EC2 인스턴스를 다시 시작합니다.
Answer: B
Explanation:
This answer is correct because it allows the application to access Amazon S3 by using an
IAM role that is associated with the ECS task. The task role grants permissions to the
containers running in the task, and can be used to make AWS API calls from the application
code. The taskRoleArn is a parameter in the task definition that specifies the IAM role to use
for the task.
References:
* https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html
* https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_TaskDefinition.html
QUESTION NO: 312
회사에서 온프레미스 데이터 센터를 AWS로 마이그레이션하려고 합니다. 데이터 캔터는 NFS
기반 파일 시스템에 데이터를 저장하는 SFTP 서버를 호스팅합니다. 서버에는 전송해야 하는
200GB의 데이터가 저장되어 있습니다. 서버는 Amazon Elastic File System(Amazon EFS)
파일 시스템을 사용하는 Amazon EC2 인스턴스에서 호스팅되어야 합니다. 솔루션
아키텍트는 이 작업을 자동화하기 위해 어떤 단계를 조합하여 수행해야 합니까? (2개 선택)
A. EFS 파일 시스템과 동일한 가용 영역에서 EC2 인스턴스를 시작합니다.
B. 온프레미스 데이터 센터에 AWS DataSync 에이전트를 설치합니다.
214

IT Certification Guaranteed, The Easy Way!
C. 데이터를 저장하기 위해 EC2 인스턴스에 보조 Amazon Elastic Block Store(Amazon EBS)
볼륨을 생성합니다.
D. 운영 체제 복사 명령을 수동으로 사용하여 데이터를 EC2 인스턴스에 푸시합니다.
E. AWS DataSync를 사용하여 온프레미스 SFTP 서버에 적합한 위치 구성을 생성합니다.
Answer: B E
Explanation:
AWS DataSync is an online data movement and discovery service that simplifies data
migration and helps users quickly, easily, and securely move their file or object data to, from,
and between AWS storage services1
. Users can use AWS DataSync to transfer data between on-premises and AWS storage
services. To use AWS DataSync, users need to install an AWS DataSync agent in the on-
premises data center. The agent is a software appliance that connects to the source or
destination storage system and handles the data transfer to or from AWS over the network2.
Users also need to use AWS DataSync to create a suitable location configuration for the on-
premises SFTP server. A location is a logical representation of a storage system that
contains files or objects that users want to transfer using DataSync. Users can create
locations for NFS shares, SMB shares, HDFS file systems, self-managed object storage,
Amazon S3 buckets, Amazon EFS file systems, Amazon FSx for Windows File Server file
systems, Amazon FSx for Lustre file systems, Amazon FSx for OpenZFS file systems,
Amazon FSx for NetApp ONTAP file systems, and AWS Snowcone devices3.
QUESTION NO: 313
한 회사는 Amazon EC2 인스턴스와 Amazon Elastic Block Store(Amazon EBS)를 사용하여
자체 관리형 데이터베이스를 실행합니다. 이 회사는 모든 EBS 볼륨에 걸쳐 350TB의 데이터를
보유하고 있습니다. 회사는 EBS 스냅샷을 매일 촬영하여 1개월간 보관합니다. Dally 변경률은
EBS 볼륨의 5%입니다.
새로운 규정으로 인해 회사는 월별 스냅샷을 7년 동안 보관해야 합니다. 회사는 새로운 규정을
준수하고 관리 노력을 최소화하면서 데이터를 사용할 수 있도록 백업 전략을 변경해야
합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. EBS 스냅샷 표준 계층에 일일 스냅샷을 1개월 동안 보관합니다. 보존 기간이 7년인
Amazon S3 Glacier Deep Archive에 월별 스냅샷을 복사합니다.
B. 현재 EBS 스냅샷 정책을 계속합니다. 보존 기간이 7년인 Amazon EBS 스냅샷 아카이브로
월간 스냅샷을 이동하는 새 정책을 추가합니다.
C. EBS 스냅샷 표준 계층에 일일 스냅샷을 1개월 동안 보관합니다. 월간 스냅샷을 표준
계층에 7년 동안 보관합니다. 증분 스냅샷을 사용합니다.
D. EBS 스냅샷 표준 계층에 일일 스냅샷을 유지합니다. EBS 다이렉트 API를 사용하여 매달
모든 EBS 볼륨의 스냅샷을 찍습니다. Infrequent Access 계층의 Amazon S3 버킷에 스냅샷을
7년 동안 저장합니다.
Answer: B
Explanation:
* Understanding the Requirement: The company needs to keep daily EBS snapshots for 1
month and retain monthly snapshots for 7 years due to new regulations.
* Analysis of Options:
* S3 Glacier Deep Archive: Moving snapshots to S3 Glacier Deep Archive involves additional
215

IT Certification Guaranteed, The Easy Way!
complexity and might not be the most straightforward approach for EBS snapshots.
* EBS Snapshots Archive: This is a cost-effective solution designed specifically for long-term
storage of EBS snapshots.
* Standard Tier for 7 Years: Keeping snapshots in the standard tier for 7 years is more
expensive and does not optimize costs.
* EBS Direct APIs to S3: This involves additional operational overhead and is not the most
cost- effective approach compared to using EBS Snapshots Archive.
* Best Solution:
* EBS Snapshots Archive: Adding a policy to move monthly snapshots to the EBS Snapshots
Archive for long-term retention is the most cost-effective and administratively simple solution.
References:
* Amazon EBS Snapshots
* Amazon EBS Snapshots Archive
QUESTION NO: 314
솔루션 설계자는 가용성이 높은 Amazon ElastiCache for Redis 기반 솔루션을 설계하고
있습니다. 솔루션 설계자는 장애로 인해 로컬 및 AWS 리전 내에서 성능 저하 또는 데이터
손실이 발생하지 않도록 해야 합니다. 솔루션은 노드 수준과 지역 수준에서 고가용성을
제공해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 여러 노드가 포함된 샤드와 함께 다중 AZ Redis 복제 그룹을 사용합니다.
B. Redis AOF(추가 전용 파일)가 설정된 여러 노드를 포함하는 Redis 샤드를 사용합니다.
C. 복제 그룹에 둘 이상의 읽기 전용 복제본이 있는 다중 AZ Redis 클러스터를 사용합니다.
D. Auto Scaling이 켜진 여러 노드를 포함하는 Redis 샤드를 사용합니다.
Answer: A
Explanation:
This answer is correct because it provides high availability at the node level and at the
Region level for the ElastiCache for Redis solution. A Multi-AZ Redis replication group
consists of a primary cluster and up to five read replica clusters, each in a different
Availability Zone. If the primary cluster fails, one of the read replicas is automatically
promoted to be the new primary cluster. A Redis replication group with shards enables
partitioning of the data across multiple nodes, which increases the scalability and
performance of the solution. Each shard can have one or more replicas to provide
redundancy and read scaling.
References:
* https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html
* https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Shards.html
QUESTION NO: 315
회사에는 Amazon S3 데이터 레이크가 있습니다. 회사에는 데이터 레이크의 데이터를
변환하고 매일 데이터 웨어하우스에 데이터를 로드하는 솔루션이 필요합니다. 데이터
웨어하우스에는 MPP(대규모 병렬 처리) 기능이 있어야 합니다.
그런 다음 데이터 분석가는 데이터에 대해 SQL 명령을 사용하여 기계 학습(ML) 모델을
생성하고 훈련해야 합니다. 솔루션은 가능한 경우 서버리스 AWS 서비스를 사용해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
216

IT Certification Guaranteed, The Easy Way!
A. 매일 Amazon EMR 작업을 실행하여 데이터를 변환하고 Amazon Redshift에 데이터를
로드합니다. Amazon Redshift ML을 사용하여 ML 모델을 생성하고 교육합니다.
B. 매일 Amazon EMR 작업을 실행하여 데이터를 변환하고 Amazon Aurora Serverless에
데이터를 로드합니다. Amazon Aurora ML을 사용하여 ML 모델 생성 및 교육
C. 매일 AWS Glue 작업을 실행하여 데이터를 변환하고 Amazon Redshift Serverless에
데이터를 로드합니다. Amazon Redshift ML을 사용하여 ML 모델을 생성하고 트램합니다.
D. 매일 AWS Glue 작업을 실행하여 데이터를 변환하고 Amazon Athena 테이블에 데이터를
로드합니다. Amazon Athena ML을 사용하여 ML 모델 생성 및 훈련
Answer: C
Explanation:
AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to
prepare and load your data for analytics. AWS Glue can automatically discover your data in
Amazon S3 and catalog it, so you can query and search the data using SQL. AWS Glue can
also run serverless ETL jobs using Apache Spark and Python to transform and load your
data into various destinations, such as Amazon Redshift, Amazon Athena, or Amazon
Aurora. AWS Glue is a serverless service, so you only pay for the resources consumed by
the jobs, and you don't need to provision or manage any infrastructure.
Amazon Redshift is a fully managed, petabyte-scale data warehouse service that enables
you to use standard SQL and your existing business intelligence (BI) tools to analyze your
data. Amazon Redshift also supports massively parallel processing (MPP), which means it
can distribute and execute queries across multiple nodes in parallel, delivering fast
performance and scalability. Amazon Redshift Serverless is a new option that automatically
scales query compute capacity based on the queries being run, so you don't need to manage
clusters or capacity. You only pay for the query processing time and the storage consumed
by your data.
Amazon Redshift ML is a feature that enables you to create, train, and deploy machine
learning (ML) models using familiar SQL commands. Amazon Redshift ML can automatically
discover the best model and hyperparameters for your data, and store the model in Amazon
SageMaker, a fully managed service that provides a comprehensive set of tools for building,
training, and deploying ML models. You can then use SQL functions to apply the model to
your data in Amazon Redshift and generate predictions.
The combination of AWS Glue, Amazon Redshift Serverless, and Amazon Redshift ML
meets the requirements of the question, as it provides a serverless, scalable, and SQL-based
solution to transform, load, and analyze the data from the Amazon S3 data lake, and to
create and train ML models on the data.
Option A is not correct, because Amazon EMR is not a serverless service. Amazon EMR is a
managed service that simplifies running Apache Spark, Apache Hadoop, and other big data
frameworks on AWS. Amazon EMR requires you to launch and configure clusters of EC2
instances to run your ETL jobs, which adds complexity and cost compared to AWS Glue.
Option B is not correct, because Amazon Aurora Serverless is not a data warehouse service,
and it does not support MPP. Amazon Aurora Serverless is an on-demand, auto-scaling
configuration for Amazon Aurora, a relational database service that is compatible with
MySQL and PostgreSQL. Amazon Aurora Serverless can automatically adjust the database
capacity based on the traffic, but it does not distribute the data and queries across multiple
nodes like Amazon Redshift does. Amazon Aurora Serverless is more suitable for
217

IT Certification Guaranteed, The Easy Way!
transactional workloads than analytical workloads.
Option D is not correct, because Amazon Athena is not a data warehouse service, and it
does not support MPP. Amazon Athena is an interactive query service that enables you to
analyze data in Amazon S3 using standard SQL. Amazon Athena is serverless, so you only
pay for the queries you run, and you don't need to load the data into a database. However,
Amazon Athena does not store the data in a columnar format, compress the data, or optimize
the query execution plan like Amazon Redshift does. Amazon Athena is more suitable for ad-
hoc queries than complex analytics and ML.
References:
* AWS Glue
* Amazon Redshift
* Amazon Redshift Serverless
* Amazon Redshift ML
* Amazon EMR
* Amazon Aurora Serverless
* Amazon Athena
QUESTION NO: 316
회사는 Amazon RDS가 지원하는 웹 애플리케이션을 실행합니다. 새로운 데이터베이스
관리자가 데이터베이스 테이블의 정보를 실수로 편집하여 데이터 손실을 초래했습니다.
이러한 유형의 사고를 복구하기 위해 회사에서는 지난 30일 이내에 변경 사항이 발생하기 5분
전의 상태로 데이터베이스를 복원하는 기능을 원합니다.
이 요구 사항을 충족하려면 솔루션 설계자가 설계에 어떤 기능을 포함해야 합니까?
A. 복제본 읽기
B. 수동 스냅샷
C. 자동 백업
D. 다중 AZ 배포
Answer: C
Explanation:
https://aws.amazon.com/rds/features/backup/
Automated backups, will meet the requirement. Amazon RDS allows you to automatically
create backups of your DB instance. Automated backups enable point-in-time recovery
(PITR) for your DB instance down to a specific second within the retention period, which can
be up to 35 days. By setting the retention period to 30 days, the company can restore the
database to its state from up to 5 minutes before any change within the last
30 days.
QUESTION NO: 317
한 회사가 AWS에서 주문 관리 애플리케이션을 실행합니다. 이 애플리케이션을 통해 고객은
주문을 하고 신용 카드로 지불할 수 있습니다. 이 회사는 Amazon CloudFront 배포를 사용하여
애플리케이션을 제공합니다.
보안팀은 모든 수신 요청에 대한 로깅을 설정했습니다. 보안팀은 사용자가 로깅 구성을
수정하면 알림을 생성하는 솔루션이 필요합니다.
옵션(두 개 선택):
A. 사용자가 CloudFront 배포를 생성하거나 수정할 때 호출되는 Amazon EventBridge 규칙을
218

IT Certification Guaranteed, The Easy Way!
구성합니다. AWS Lambda 함수를 EventBridge 규칙의 대상으로 추가합니다.
B. 애플리케이션 로드 밸런서(ALB)를 만듭니다. ALB에 대한 AWS WAF 규칙을 활성화합니다.
보안 위반을 감지하도록 AWS Config 규칙을 구성합니다.
C. CloudFront 배포 로깅의 변경 사항을 감지하는 AWS Lambda 함수를 만듭니다. Lambda
함수를 구성하여 Amazon Simple Notification Service(Amazon SNS)를 사용하여 보안 팀에
알림을 보냅니다.
D. Amazon GuardDuty를 설정합니다. GuardDuty를 구성하여 CloudFront 배포에서 결과를
모니터링합니다. 결과를 처리하기 위한 AWS Lambda 함수를 만듭니다.
E. Amazon API Gateway에서 개인 API를 만듭니다. AWS WAF 규칙을 사용하여 개인 API를
일반적인 보안 문제로부터 보호합니다.
Answer: A C
Explanation:
Detailed Explanation:
* A. EventBridge Rule: Detects modifications to CloudFront distributions in real time and
triggers the Lambda function for further action.
* B. ALB + Config: Focuses on ALB security violations, not relevant for CloudFront logging
changes.
* C. Lambda + SNS: Provides real-time notifications about changes in logging configuration.
* D. GuardDuty: Focuses on threat detection, not logging configuration changes.
* E. API Gateway + WAF: Unrelated to CloudFront logging changes.
References: Amazon EventBridge, Amazon SNS
QUESTION NO: 318
한 미디어 회사는 rts 시스템을 AWS 클라우드로 이전할 가능성을 평가하고 있습니다. 이
회사는 비디오 처리를 위해 가능한 최대 I/O 성능을 갖춘 최소 10TB의 스토리지가
필요합니다. 미디어 콘텐츠를 저장하기 위한 매우 내구성이 뛰어난 300TB의 스토리지와 더
이상 사용되지 않는 보관 미디어의 요구 사항을 충족하는 900TB의 스토리지 이러한 요구
사항을 충족하기 위해 솔루션 설계자가 권장해야 하는 서비스 세트는 무엇입니까?
A. 최대 성능을 위한 Amazon EBS, 내구성 있는 데이터 스토리지를 위한 Amazon S3,
아카이브 스토리지를 위한 Amazon S3 Glacier
B. 최대 성능을 위한 Amazon EBS, 내구성 있는 데이터 스토리지를 위한 Amazon EFS,
아카이브 스토리지를 위한 Amazon S3 Glacier
C. 최대 성능을 위한 Amazon EC2 인스턴스 스토어. 내구성 있는 데이터 스토리지를 위한
Amazon EFS 및 아카이브 스토리지를 위한 Amazon S3
D. 최대 성능을 위한 Amazon EC2 인스턴스 스토어. 내구성 있는 데이터 스토리지를 위한
Amazon S3 및 아카이브 스토리지를 위한 Amazon S3 Glacier
Answer: A
Explanation:
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html
QUESTION NO: 319
회사에서는 3계층 웹 애플리케이션을 사용하여 신입 직원에게 교육을 제공합니다. 매일
12시간 동안만 애플리케이션에 액세스할 수 있습니다. 이 회사는 MySQL DB 인스턴스용
Amazon RDS를 사용하여 정보를 저장하고 있으며 비용을 최소화하려고 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
219

IT Certification Guaranteed, The Easy Way!
A. AWS Systems Manager Session Manager에 대한 IAM 정책을 구성합니다. 정책에 대한
IAM 역할을 생성합니다. 역할의 신뢰 관계를 업데이트합니다. DB 인스턴스의 자동 시작 및
중지를 설정합니다.
B. DB 인스턴스가 중지되었을 때 사용자가 캐시의 데이터에 액세스할 수 있는 기능을
제공하는 Redis용 Amazon ElastiCache 캐시 클러스터를 생성합니다. DB 인스턴스가 시작된
후 캐시를 무효화합니다.
C. Amazon EC2 인스턴스를 시작합니다. Amazon RDS에 대한 액세스 권한을 부여하는 IAM
역할을 생성합니다. EC2 인스턴스에 역할을 연결합니다. 원하는 일정에 따라 EC2 인스턴스를
시작하고 중지하도록 cron 작업을 구성합니다.
D. DB 인스턴스를 시작하고 중지하는 AWS Lambda 함수를 생성합니다. Amazon
EventBridge(Amazon CloudWatch Events) 예약 규칙을 생성하여 Lambda 함수를
호출합니다. Lambda 함수를 규칙의 이벤트 대상으로 구성
Answer: D
Explanation:
In a typical development environment, dev and test databases are mostly utilized for 8 hours
a day and sit idle when not in use. However, the databases are billed for the compute and
storage costs during this idle time. To reduce the overall cost, Amazon RDS allows instances
to be stopped temporarily. While the instance is stopped, you're charged for storage and
backups, but not for the DB instance hours. Please note that a stopped instance will
automatically be started after 7 days. This post presents a solution using AWS Lambda and
Amazon EventBridge that allows you to schedule a Lambda function to stop and start the idle
databases with specific tags to save on compute costs. The second post presents a solution
that accomplishes stop and start of the idle Amazon RDS databases using AWS Systems
Manager.
QUESTION NO: 320
회사에서 Amazon Elastic Container Service(Amazon ECS)를 사용할 컨테이너화된
애플리케이션을 설계하고 있습니다. 애플리케이션은 내구성이 뛰어나고 RPO(복구 지점
목표)가 8시간인 다른 AWS 리전에 데이터를 복구할 수 있는 공유 파일 시스템에 액세스해야
합니다. 파일 시스템은 리전 내의 각 가용 영역에 탑재 대상을 제공해야 합니다.
솔루션 설계자는 AWS Backup을 사용하여 다른 리전에 대한 복제를 관리하려고 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. '다중 AZ 배포가 있는 Windows 파일 서버용 Amazon FSx
B. 다중 AZ 배포가 있는 NetApp ONTAP용 Amazon FSx
C. '표준 스토리지 클래스가 있는 Amazon Elastic File System(Amazon EFS)
D. OpenZFS용 Amazon FSx
Answer: B
Explanation:
This answer is correct because it meets the requirements of accessing a shared file system
that is highly durable, can recover data to another AWS Region, and can provide a mount
target in each Availability Zone within a Region. Amazon FSx for NetApp ONTAP is a fully
managed service that provides enterprise-grade data management and storage for your
Windows and Linux applications. You can use Amazon FSx for NetApp ONTAP to create file
systems that span multiple Availability Zones within an AWS Region, providing high
220

IT Certification Guaranteed, The Easy Way!
availability and durability. You can also use AWS Backup to manage the replication of your
file systems to another AWS Region, with a recovery point objective (RPO) of 8 hours or less.
AWS Backup is a fully managed backup service that automates and centralizes backup of
data over AWS services. You can use AWS Backup to create backup policies and monitor
activity for your AWS resources in one place.
References:
* https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is.html
* https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html
QUESTION NO: 321
한 회사가 프로덕션 애플리케이션의 새 버전을 출시했습니다. 회사의 워크로드는 Amazon
EC2를 사용합니다. AWS 람다. AWS 파게이트. 그리고 아마존 세이지메이커. 이제 회사는
사용량이 안정된 상태이므로 워크로드 비용을 최적화하려고 합니다. 회사는 최소한의 저축
계획으로 가장 많은 서비스를 보장하기를 원합니다. 이러한 요구 사항을 충족하는 저축 계획
조합은 무엇입니까? (2개를 선택하세요.)
A. Amazon EC2 및 SageMaker에 대한 EC2 인스턴스 Savings Plan을 구매하세요.
B. Amazon EC2용 Compute Savings Plan을 구매하세요. 람다 및 SageMaker
C. SageMaker 저축 플랜 구매
D. Lambda, Fargate 및 Amazon EC2에 대한 Compute Savings Plan 구매
E. Amazon EC2 및 Fargate에 대한 EC2 인스턴스 Savings Plan 구매
Answer: B D
Explanation:
* Understanding the Requirement: The company wants to cost-optimize their workload that
uses EC2, Lambda, Fargate, and SageMaker, covering the most services with the fewest
savings plans.
* Analysis of Options:
* EC2 Instance Savings Plan: Limited to EC2 and SageMaker, missing coverage for Lambda
and Fargate.
* Compute Savings Plan: Provides the most flexibility, covering a broad range of compute
services, including EC2, Lambda, Fargate, and SageMaker.
* SageMaker Savings Plan: Specifically for SageMaker, missing coverage for EC2, Lambda,
and Fargate.
* Combination of plans: The Compute Savings Plan is versatile and can be combined to
cover different services efficiently.
* Best Solution:
* Compute Savings Plan for EC2, Lambda, and SageMaker: Covers the primary compute
services efficiently.
* Compute Savings Plan for Lambda, Fargate, and EC2: Covers the remaining services,
ensuring broad coverage with minimal plans.
References:
* AWS Savings Plans
* Compute Savings Plans
QUESTION NO: 322
온라인 학습 회사가 AWS 클라우드로 마이그레이션하고 있습니다. 회사는 PostgreSQL
221

IT Certification Guaranteed, The Easy Way!
데이터베이스에 학생 기록을 유지합니다. 회사에는 여러 AWS 리전에서 데이터를 항상
온라인으로 사용할 수 있는 솔루션이 필요합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. PostgreSQL 데이터베이스를 Amazon EC2 인스턴스의 PostgreSQL 클러스터로
마이그레이션합니다.
B. 다중 AZ 기능이 활성화된 PostgreSQL 데이터베이스를 PostgreSQL용 Amazon RDS DB
인스턴스로 마이그레이션합니다.
C. PostgreSQL 데이터베이스를 PostgreSQL DB 인스턴스용 Amazon RDS로
마이그레이션합니다. 다른 리전에 읽기 전용 복제본을 생성합니다.
D. PostgreSQL 데이터베이스를 PostgreSQL DB 인스턴스용 Amazon RDS로
마이그레이션합니다. 다른 리전에 복사할 DB 스냅샷을 설정합니다.
Answer: C
Explanation:
" online across multiple AWS Regions at all times". Currently only Read Replica supports
cross-regions , Multi-AZ does not support cross-region (it works only in same region)
https://aws.amazon.com/about-aws
/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/
QUESTION NO: 323
회사는 AWS Organizations를 사용하여 각 사업부에 대한 전용 AWS 계정을 생성하고 요청 시
각 사업부의 계정을 독립적으로 관리합니다. 루트 이메일 수신자는 한 계정의 루트 사용자
이메일 주소로 전송된 알림을 놓쳤습니다. 회사는 향후 모든 알림을 놓치지 않기를 원합니다.
향후 알림은 계정 관리자에게만 제공되어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS 계정 루트 사용자 이메일 주소로 전송된 알림 ​​이메일 메시지를 조직의 모든
사용자에게 전달하도록 회사의 이메일 서버를 구성합니다.
B. 모든 AWS 계정 루트 사용자 이메일 주소를 알림에 응답할 수 있는 몇몇 관리자에게
전달되는 배포 목록으로 구성합니다. AWS Organizations 콘솔에서 또는 프로그래밍 방식으로
AWS 계정 대체 연락처를 구성합니다.
C. 모든 AWS 계정 루트 사용자 이메일 메시지가 경고 모니터링을 담당하고 해당 경고를
적절한 그룹에 전달하는 관리자 한 명에게 전송되도록 구성합니다.
D. 동일한 루트 사용자 이메일 주소를 사용하도록 모든 기존 AWS 계정과 새로 생성된 모든
계정을 구성합니다. AWS Organizations 콘솔에서 또는 프로그래밍 방식으로 AWS 계정 대체
연락처를 구성합니다.
Answer: B
Explanation:
Use a group email address for the management account's root user
https://docs.aws.amazon.com/organizations
/latest/userguide/orgs_best-practices_mgmt-acct.html#best-practices_mgmt-acct_email-
address
QUESTION NO: 324
한 글로벌 기업이 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스에서 웹
애플리케이션을 호스팅합니다. 웹 애플리케이션에는 정적 데이터와 동적 데이터가 있습니다.
회사는 정적 데이터를 Amazon S3 버킷에 저장합니다. 회사는 정적 데이터와 동적 데이터에
222

IT Certification Guaranteed, The Easy Way!
대한 성능을 향상하고 대기 시간을 줄이고 싶어합니다. 회사는 Amazon Route 53에 등록된
자체 도메인 이름을 사용하고 있습니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. S3 버킷과 ALB를 오리진으로 포함하는 Amazon CloudFront 배포를 생성합니다. 트래픽을
CloudFront 배포로 라우팅하도록 Route 53을 구성합니다.
B. ALB를 원본으로 사용하는 Amazon CloudFront 배포를 생성합니다. S3 버킷을
엔드포인트로 사용하는 AWS Global Accelerator 표준 액셀러레이터를 생성합니다. 트래픽을
CloudFront 배포로 라우팅하도록 Route 53을 구성합니다.
C. S3 버킷을 오리진으로 포함하는 Amazon CloudFront 배포 생성 ALB 및 CloudFront 배포를
엔드포인트로 포함하는 AWS Global Accelerator 표준 액셀러레이터 생성 가속기 DNS 이름을
가리키는 사용자 지정 도메인 이름 생성 사용자 지정 도메인 사용 웹 애플리케이션의
엔드포인트로 이름을 지정합니다.
D. ALB를 원본으로 포함하는 Amazon CloudFront 배포를 생성합니다. C. S3 버킷을
엔드포인트로 포함하는 AWS Global Accelerator 표준 액셀러레이터를 생성합니다. 두 개의
도메인 이름을 생성합니다. 동적 콘텐츠의 경우 하나의 도메인 이름을 CloudFront DNS
이름으로 가리키고, 다른 도메인 이름으로 정적 콘텐츠의 가속기 DNS 이름을 가리키도록
도메인 이름을 웹 애플리케이션의 엔드포인트로 사용합니다.
Answer: C
Explanation:
Static content can be cached at Cloud front Edge locations from S3 and dynamic content
EC2 behind the ALB whose performance can be improved by Global Accelerator whose one
endpoint is ALB and other Cloud front. So with regards to custom domain name endpoint is
web application is R53 alias records for the custom domain point to web application
https://aws.amazon.com/blogs/networking-and-content-delivery/improving- availability-and
-performance-for-application-load-balancers-using-one-click-integration-with-aws-global-
accelerator/
QUESTION NO: 325
한 미디어 회사가 AWS에서 웹 사이트를 호스팅하고 있습니다. 웹 사이트 애플리케이션의
아키텍처에는 ALB(Application Load Balancer) 뒤에 있는 Amazon EC2 인스턴스 집합과
Amazon Aurora에서 호스팅되는 데이터베이스가 포함되어 있습니다. 회사의 사이버 보안
팀은 해당 애플리케이션이 SOL 주입에 취약하다고 보고했습니다.
회사는 이 문제를 어떻게 해결해야 할까요?
A. ALB 앞에 AWS WAF를 사용합니다. 적절한 웹 ACL을 AWS WAF와 연결합니다.
B. 고정된 응답으로 SQL 주입에 응답하는 ALB 리스너 규칙을 생성합니다.
C. 모든 SQL 주입 시도를 자동으로 차단하려면 AWS Shield Advanced를 구독하세요.
D. 모든 SOL 주입 시도를 자동으로 차단하도록 Amazon Inspector를 설정합니다.
Answer: A
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/waf-block-common-
attacks/#:~:text=To%
20protect%20your%20applications%20against,%2C%20query%20string%2C%20or%20URI.
-----------------------------------------------------------------------------------------------------------------------
Protect against SQL injection and cross-site scripting To protect your applications against
223

IT Certification Guaranteed, The Easy Way!
SQL injection and cross- site scripting (XSS) attacks, use the built-in SQL injection and cross
-site scripting engines. Remember that attacks can be performed on different parts of the
HTTP request, such as the HTTP header, query string, or URI. Configure the AWS WAF
rules to inspect different parts of the HTTP request against the built-in mitigation engines.
QUESTION NO: 326
회사는 AWS에서 결제 애플리케이션을 실행하려고 합니다. 애플리케이션은 모바일
장치로부터 결제 알림을 받습니다. 결제 알림은 추가 처리를 위해 전송되기 전에 기본 검증이
필요합니다. 백엔드 처리 애플리케이션은 오랫동안 실행되며 조정하려면 컴퓨팅 및 메모리가
필요합니다. 회사는 그렇지 않습니다. 인프라를 관리하고 싶습니다. 어떤 솔루션이 최소한의
운영 오버헤드로 이러한 요구 사항을 충족합니까?
A. Amazon Simple Queue Service(Amazon SQS) 대기열 생성 대기열을 Amazon
EventBndge 규칙과 통합하여 모바일 장치에서 결제 알림 수신 결제 알림을 검증하고 백엔드
애플리케이션에 알림을 전송하도록 규칙 구성 백엔드 배포 Amazon Elastic Kubernetes
Service(Amazon EKS)의 애플리케이션 독립 실행형 클러스터 생성
B. Amazon API Gateway API 생성 API를 AWS Step Functions 상태 시스템과 통합하여
모바일 장치로부터 결제 알림 수신 상태 시스템을 호출하여 결제 알림을 검증하고 알림을
백엔드 애플리케이션에 보냅니다. Amazon에 백엔드 애플리케이션 배포 탄력적 Kubernetes
Sen/ice(Amazon EKS). 자체 관리형 노드로 EKS 클러스터를 구성합니다.
C. Amazon Simple Queue Sen/ice(Amazon SQS) 대기열을 생성합니다. 대기열을 Amazon
EventBridge 규칙과 통합하여 모바일 장치에서 결제 알림을 받습니다. 결제 알림을 검증하고
백엔드 애플리케이션에 알림을 보내도록 규칙을 구성합니다. Amazon EC2 스팟 인스턴스의
백엔드 애플리케이션 기본 할당 전략으로 스팟 집합을 구성합니다.
D. Amazon API Gateway API 생성 API를 AWS Lambda와 통합하여 모바일 장치에서 결제
알림 수신 Lambda 함수를 호출하여 결제 알림을 검증하고 알림을 백엔드 애플리케이션에
보냅니다. Amazon Elastic Container Service에 백엔드 애플리케이션 배포 (아마존 ECS).
AWS Fargate 시작 유형으로 Amazon ECS를 구성합니다.
Answer: D
Explanation:
This option is the best solution because it allows the company to run its payment application
on AWS with minimal operational overhead and infrastructure management. By using
Amazon API Gateway, the company can create a secure and scalable API to receive
payment notifications from mobile devices. By using AWS Lambda, the company can run a
serverless function to validate the payment notifications and send them to the backend
application. Lambda handles the provisioning, scaling, and security of the function, reducing
the operational complexity and cost. By using Amazon ECS with AWS Fargate, the company
can run the backend application on a fully managed container service that scales the
compute resources automatically and does not require any EC2 instances to manage.
Fargate allocates the right amount of CPU and memory for each container and adjusts them
as needed.
A: Create an Amazon Simple Queue Service (Amazon SQS) queue Integrate the queue with
an Amazon EventBndge rule to receive payment notifications from mobile devices Configure
the rule to validate payment notifications and send the notifications to the backend
application Deploy the backend application on Amazon Elastic Kubernetes Service (Amazon
EKS) Anywhere Create a standalone cluster. This option is not optimal because it requires
224

IT Certification Guaranteed, The Easy Way!
the company to manage the Kubernetes cluster that runs the backend application. Amazon
EKS Anywhere is a deployment option that allows the company to create and operate
Kubernetes clusters on- premises or in other environments outside AWS. The company would
need to provision, configure, scale, patch, and monitor the cluster nodes, which can increase
the operational overhead and complexity. Moreover, the company would need to ensure the
connectivity and security between the AWS services and the EKS Anywhere cluster, which
can also add challenges and risks.
B: Create an Amazon API Gateway API Integrate the API with anAWS Step Functions state
ma-chine to receive payment notifications from mobile devices Invoke the state machine to
validate payment notifications and send the notifications to the backend application Deploy
the backend application on Amazon Elastic Kubernetes Sen/ice (Amazon EKS). Configure an
EKS cluster with self-managed nodes. This option is not ideal because it requires the
company to manage the EC2 instances that host the Kubernetes cluster that runs the
backend application. Amazon EKS is a fully managed service that runs Kubernetes on AWS,
but it still requires the company to manage the worker nodes that run the containers. The
company would need to provision, configure, scale, patch, and monitor the EC2 instances,
which can increase the operational overhead and infrastructure costs. Moreover, using AWS
Step Functions to validate the payment notifications may be unnecessary and complex, as
the validation logic can be implemented in a simpler way with Lambda or other services.
C: Create an Amazon Simple Queue Sen/ice (Amazon SQS) queue Integrate the queue with
an Amazon EventBridge rule to receive payment notifications from mobile devices Configure
the rule to validate payment notifications and send the notifications to the backend
application Deploy the backend application on Amazon EC2 Spot Instances Configure a Spot
Fleet with a default al-location strategy. This option is not cost-effective because it requires
the company to manage the EC2 instances that run the backend application. The company
would need to provision, configure, scale, patch, and monitor the EC2 instances, which can
increase the operational overhead and infrastructure costs. Moreover, using Spot Instances
can introduce the risk of interruptions, as Spot Instances are reclaimed by AWS when the
demand for On-Demand Instances increases.
The company would need to handle the interruptions gracefully and ensure the availability
and reliability of the backend application.
References:
* 1 Amazon API Gateway - Amazon Web Services
* 2 AWS Lambda - Amazon Web Services
* 3 Amazon Elastic Container Service - Amazon Web Services
* 4 AWS Fargate - Amazon Web Services
QUESTION NO: 327
회사에는 온프레미스에 여러 Windows 파일 서버가 있습니다. 이 회사는 파일을 Windows File
Server 파일 시스템용 Amazon FSx로 마이그레이션하고 통합하려고 합니다. 액세스 권한이
변경되지 않도록 하려면 파일 권한을 보존해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까? (2개를 선택하세요.)
A. 온프레미스에 AWS DataSync 에이전트를 배포합니다. 데이터를 FSx for Windows 파일
서버 파일 시스템으로 전송하도록 DataSync 작업을 예약합니다.
B. AWS CLI를 사용하여 각 파일 서버의 공유를 Amazon S3 버킷으로 복사합니다. AWS
225

IT Certification Guaranteed, The Easy Way!
DataSync 작업을 예약하여 데이터를 FSx for Windows File Server 파일 시스템으로
전송합니다.
C. 각 파일 서버에서 드라이브를 제거합니다. Amazon S3로 가져오기 위해 드라이브를
AWS로 배송합니다. 데이터를 FSx for Windows File Server 파일 시스템으로 전송하도록
AWS DataSync 작업 예약
D. AWS Snowcone 디바이스를 주문합니다. 장치를 온프레미스 네트워크에 연결합니다.
디바이스에서 AWS DataSync 에이전트를 시작합니다. 데이터를 FSx for Windows File Server
파일 시스템으로 전송하도록 DataSync 작업을 예약합니다.
E. AWS Snowball Edge Storage Optimized 장치를 주문하십시오. 장치를 온프레미스
네트워크에 연결합니다. AWS CLI를 사용하여 디바이스에 데이터를 복사합니다. Amazon
S3로 가져오기 위해 디바이스를 AWS로 반송합니다. 데이터를 FSx for Windows File Server
파일 시스템으로 전송하도록 AWS DataSync 작업을 예약합니다.
Answer: A D
Explanation:
A This option involves deploying DataSync agents on your on-premises file servers and using
DataSync to transfer the data directly to the FSx for Windows File Server. DataSync ensures
that file permissions are preserved during the migration process. D This option involves using
an AWS Snowcone device, a portable data transfer device. You would connect the
Snowcone device to your on-premises network, launch DataSync agents on the device, and
schedule DataSync tasks to transfer the data to FSx for Windows File Server.
DataSync handles the migration process while preserving file permissions.
QUESTION NO: 328
한 회사가 Amazon Elastic Container Service(Amazon ECS)에서 워크로드를 실행합니다.
ECS 작업 정의가 사용하는 컨테이너 이미지는 Common Vulnerabilities and
Exposures(CVE)를 검사해야 합니다.
새로 생성된 컨테이너 이미지도 스캔해야 합니다.
어떤 솔루션이 워크로드를 가장 적게 변경하면서 이러한 요구 사항을 충족할 수 있을까요?
A. Amazon Elastic Container Registry(Amazon ECR)를 개인 이미지 저장소로 사용하여
컨테이너 이미지를 저장합니다. ECR 기본 스캔에 대해 푸시 필터에서 스캔을 지정합니다.
B. 컨테이너 이미지를 Amazon S3 버킷에 저장합니다. Amazon Macie를 사용하여 이미지를
스캔합니다. S3 이벤트 알림을 사용하여 s3:ObjeclCreated:Put 이벤트 유형이 있는 모든
이벤트에 대해 Made 스캔을 시작합니다.
C. 워크로드를 Amazon Elastic Kubernetes Service(Amazon EKS)에 배포합니다. Amazon
Elastic Container Registry(Amazon ECR)를 개인 이미지 저장소로 사용합니다. ECR 강화
스캔에 대해 푸시 필터에서 스캔을 지정합니다.
D. 버전 관리가 활성화된 Amazon S3 버킷에 컨테이너 이미지를 저장합니다.
s3:ObjectCrealed:* 이벤트에 대한 S3 이벤트 알림을 구성하여 AWS Lambda 함수를
호출합니다. Lambda 함수를 구성하여 Amazon Inspector 스캔을 시작합니다.
Answer: A
QUESTION NO: 329
한 회사에서 새로운 동적 주문 웹사이트를 구축하고 있습니다. 회사는 서버 유지 관리 및
패치를 최소화하려고 합니다. 웹 사이트는 가용성이 높아야 하며 사용자 수요 변화에 맞춰
읽기 및 쓰기 용량을 최대한 빨리 확장해야 합니다.
226

IT Certification Guaranteed, The Easy Way!
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon S3에서 정적 콘텐츠 호스팅 Amazon API Gateway 및 AWS Lambda를 사용하여
동적 콘텐츠 호스팅 데이터베이스에 대한 온디맨드 용량으로 Amazon DynamoDB 사용 웹
사이트 콘텐츠를 제공하도록 Amazon CloudFront 구성
B. Amazon S3에서 정적 콘텐츠 호스팅 Amazon API Gateway 및 AWS Lambda를 사용하여
동적 콘텐츠 호스팅 데이터베이스에 대해 Aurora Auto Scaling과 함께 Amazon Aurora 사용
웹 사이트 콘텐츠를 제공하도록 Amazon CloudFront 구성
C. Amazon EC2 인스턴스에서 모든 웹 사이트 콘텐츠 호스팅 Auto Scaling 그룹을 생성하여
EC2 인스턴스 확장 Application Load Balancer를 사용하여 트래픽 분산 데이터베이스에 대해
프로비저닝된 쓰기 용량이 있는 Amazon DynamoDB 사용
D. Amazon EC2 인스턴스의 웹 사이트 콘텐츠 호스팅 Auto Scaling 그룹을 생성하여 EC2
인스턴스 크기 조정 Application Load Balancer를 사용하여 트래픽 분산 데이터베이스에
Amazon Aurora와 Aurora Auto Scaling 사용
Answer: A
Explanation:
Key phrase in the Question is must scale read and write capacity. Aurora is only for Read.
Amazon DynamoDB has two read/write capacity modes for processing reads and writes on
your tables: On-demand Provisioned (default, free-tier eligible)
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide
/HowItWorks.ReadWriteCapacityMode.html
QUESTION NO: 330
한 회사가 온프레미스 PostgreSQL 데이터베이스를 Amazon Aurora PostgreSQL로
마이그레이션하고 있습니다. 온프레미스 데이터베이스는 마이그레이션 중에 온라인 상태로
유지되고 액세스 가능해야 합니다. Aurora 데이터베이스는 온프레미스 데이터베이스와
동기화된 상태를 유지해야 합니다.
솔루션 설계자가 이러한 요구 사항을 충족하려면 어떤 조치 조합을 취해야 합니까? (2개를
선택하세요.)
A. 지속적인 복제 작업을 생성합니다.
B. 온프레미스 데이터베이스의 데이터베이스 백업을 생성합니다.
C. AWS Database Migration Service(AWS DMS) 복제 서버 생성
D. AWS Schema Conversion Tool(AWS SCT)을 사용하여 데이터베이스 스키마를
변환합니다.
E. Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성하여 데이터베이스
동기화를 모니터링합니다.
Answer: A C
Explanation:
AWS Database Migration Service supports homogeneous migrations such as Oracle to
Oracle, as well as heterogeneous migrations between different database platforms, such as
Oracle or Microsoft SQL Server to Amazon Aurora. With AWS Database Migration Service,
you can also continuously replicate data with low latency from any supported source to any
supported target. For example, you can replicate from multiple sources to Amazon Simple
Storage Service (Amazon S3) to build a highly available and scalable data lake solution. You
can also consolidate databases into a petabyte-scale data warehouse by streaming data to
227

IT Certification Guaranteed, The Easy Way!
Amazon Redshift. Learn more about the supported source and target databases.
https://aws.amazon.com/dms/
QUESTION NO: 331
한 회사가 AWS에서 새로운 애플리케이션을 위한 마이크로서비스 기반 아키텍처를 설계하고
있습니다. 각 마이크로서비스는 자체 Amazon EC2 인스턴스 세트에서 실행됩니다. 각
마이크로서비스는 Amazon S3 및 Amazon Simple Queue Service(Amazon SQS)와 같은 여러
AWS 서비스와 상호 작용해야 합니다.
회사에서는 최소 권한 원칙에 따라 각 EC2 인스턴스의 권한을 관리하려고 합니다.
어떤 솔루션이 이 요구 사항을 충족시킬까요?
A. 각 마이크로 서비스에 1AM 사용자를 할당합니다. 애플리케이션 코드에 저장된 액세스
키를 사용하여 AWS 서비스 요청을 인증합니다.
B. 모든 AWS 서비스에 액세스할 수 있는 권한이 있는 단일 1AM 역할을 만듭니다.
마이크로서비스를 실행하는 모든 EC2 인스턴스에 1AM 역할을 연결합니다.
C. AWS Organizations를 사용하여 각 마이크로서비스에 대해 별도의 계정을 만듭니다. 계정
수준에서 권한을 관리합니다.
D. 각 마이크로서비스의 특정 요구 사항에 따라 개별 1AM 역할을 만듭니다. 1AM 역할을
적절한 EC2 인스턴스와 연결합니다.
Answer: D
Explanation:
When designing a microservice architecture where each microservice interacts with different
AWS services, it's essential to follow the principle of least privilege. This means granting
each microservice only the permissions it needs to perform its tasks, reducing the risk of
unauthorized access or accidental actions.
The recommended approach is to create individual IAM roles with policies that grant each
microservice the specific permissions it requires. Then, these roles should be associated with
the EC2 instances that run the corresponding microservice. By doing so, each EC2 instance
will assume its specific IAM role, and permissions will be automatically managed by AWS.
IAM roles provide temporary credentials via the instance metadata service, eliminating the
need to hard-code credentials in your application code, which enhances security.
AWS References:
* IAM Roles for Amazon EC2 explains how EC2 instances can use IAM roles to securely
access AWS services without managing long-term credentials.
* Best Practices for IAM includes recommendations for implementing the least privilege
principle and using IAM roles effectively.
Why the other options are incorrect:
* A. Assign an IAM user to each microservice: This requires managing long-term credentials
(access keys), which should be avoided. Storing keys in application code is insecure and
creates a maintenance burden.
* B. Create a single IAM role: This violates the principle of least privilege, as a single role with
broad permissions across all services is less secure.
* C. Use AWS Organizations: This approach adds unnecessary complexity. Managing
permissions at the account level for each microservice is excessive for this use case and
doesn't adhere to the principle of least privilege.
228

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 332
한 회사가 AWS에 Amazon RDS를 백엔드 데이터베이스로 사용하는 서버리스
애플리케이션을 보유하고 있습니다. 애플리케이션에서 예상치 못한 갑작스러운 트래픽
증가가 발생하는 경우가 있습니다. 트래픽이 증가하는 동안 애플리케이션은 데이터베이스에
대한 연결을 자주 열고 닫으므로 애플리케이션이 데이터베이스에서 오류를 수신하거나
연결이 부족해집니다. 회사는 애플리케이션이 항상 확장 가능하고 가용성이 높은지 확인해야
합니다.
애플리케이션에 대한 코드 변경 없이 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 서버리스 애플리케이션의 RDS 데이터베이스 옵션 그룹에서 최대 연결 수를 늘립니다.
B. 최대 로드 트래픽을 충족하도록 RDS DB 인스턴스의 인스턴스 크기를 늘립니다.
C. 서버리스 애플리케이션과 Amazon RDS 간에 Amazon RDS 프록시를 배포합니다.
D. Amazon RDS용 예약 인스턴스를 구매하여 피크 로드 트래픽 중에 데이터베이스의
가용성을 높이세요.
Answer: C
Explanation:
Amazon RDS Proxy is a fully managed database proxy that makes applications more
scalable, more resilient to database failures, and more secure. RDS Proxy sits between your
application and your relational database to pool and share established database connections,
improving database efficiency and application scalability.
RDS Proxy also reduces the load on the database by handling connection management and
query retries for transient errors. By deploying RDS Proxy between your serverless
application and Amazon RDS, you can avoid opening and closing connections to the
database frequently, which can cause errors or run out of connections. This solution will also
reduce operational costs and improve availability of your application.
References: https://aws.amazon.com/rds/proxy/
QUESTION NO: 333
회사의 인프라는 단일 AWS 리전의 Amazon EC2 인스턴스와 Amazon RDS DB 인스턴스로
구성됩니다. 회사는 별도의 지역에 데이터를 백업하려고 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Backup을 사용하여 EC2 백업과 RDS 백업을 별도의 리전에 복사합니다.
B. Amazon Data Lifecycle Manager(Amazon DLM)를 사용하여 EC2 백업과 RDS 백업을
별도의 리전에 복사합니다.
C. EC2 인스턴스의 Amazon 머신 이미지(AMI)를 생성합니다. AMI를 별도의 리전에
복사합니다. 별도의 리전에서 RDS DB 인스턴스에 대한 읽기 전용 복제본을 생성합니다.
D. Amazon Elastic Block Store(Amazon EBS) 스냅샷을 생성합니다. EBS 스냅샷을 별도의
리전에 복사합니다. RDS 스냅샷을 생성합니다. RDS 스냅샷을 Amazon S3로 내보냅니다. S3
교차 리전 복제(CRR)를 별도의 리전으로 구성합니다.
Answer: A
Explanation:
To back up EC2 instances and RDS DB instances in a separate Region with the least
operational overhead, AWS Backup is a simple and cost-effective solution. AWS Backup can
copy EC2 backups and RDS backups to another Region automatically and securely. AWS
Backup also supports backup policies, retention rules, and monitoring features.
229

IT Certification Guaranteed, The Easy Way!
References:
* What Is AWS Backup?
* Cross-Region Backup
QUESTION NO: 334
회사에는 Java 및 PHP를 기반으로 하는 웹 애플리케이션이 있습니다. 회사는 애플리케이션을
온프레미스에서 AWS로 이동할 계획입니다. 회사에는 새로운 사이트 기능을 자주 테스트할
수 있는 능력이 필요합니다. 또한 회사에는 최소한의 운영 오버헤드가 필요한 고가용성
관리형 솔루션이 필요합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon S3 버킷 생성 S3 버킷에서 정적 웹 호스팅 활성화 S3 버킷에 정적 콘텐츠 업로드
AWS Lambda를 사용하여 모든 동적 콘텐츠 처리
B. AWS Elastic Beanstalk 환경에 웹 애플리케이션 배포 기능 테스트를 위해 URL 교환을
사용하여 여러 Elastic Beanstalk 환경 간 전환
C. Java 및 PHP로 구성된 Amazon EC2 인스턴스에 웹 애플리케이션 배포 Auto Scaling 그룹
및 Application Load Balancer를 사용하여 웹 사이트 가용성 관리
D. 웹 애플리케이션 컨테이너화 Amazon EC2 인스턴스에 웹 애플리케이션 배포 AWS 로드
밸런서 컨트롤러를 사용하여 테스트용 새 사이트 기능이 포함된 컨테이너 간에 트래픽을
동적으로 라우팅합니다.
Answer: B
Explanation:
Frequent feature testing -
- Multiple Elastic Beanstalk environments can be created easily for development, testing and
production use cases.
- Traffic can be routed between environments for A/B testing and feature iteration using
simple URL swapping techniques. No complex routing rules or infrastructure changes
required.
QUESTION NO: 335
회사에 TCP 및 UDP 멀티플레이어 게임 기능이 있는 온라인 게임 응용 프로그램이 있습니다.
이 회사는 Amazon Route 53을 사용하여 애플리케이션 트래픽이 서로 다른 AWS 리전에 있는
여러 NLB(Network Load Balancer)를 가리키도록 합니다. 회사는 사용자 증가에 대비하여
애플리케이션 성능을 개선하고 온라인 게임의 지연 시간을 줄여야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. NLB 앞에 Amazon CloudFront 배포를 추가합니다. Cache-Control: max-age 매개변수를
늘립니다.
B. NLB를 ALB(Application Load Balancer)로 교체합니다. 지연 시간 기반 라우팅을
사용하도록 Route 53을 구성합니다.
C. NLB 앞에 AWS Global Accelerator를 추가합니다. 올바른 수신기 포트를 사용하도록 Global
Accelerator 끝점을 구성합니다.
D. 'NLB 뒤에 Amazon API Gateway 엔드포인트를 추가합니다. API 캐싱을 활성화합니다.
다른 단계에 대한 메서드 캐싱을 재정의합니다.
Answer: C
Explanation:
This answer is correct because it improves the application performance and decreases
latency for the online game by using AWS Global Accelerator. AWS Global Accelerator is a
230

IT Certification Guaranteed, The Easy Way!
networking service that helps you improve the availability, performance, and security of your
public applications. Global Accelerator provides two global static public IPs that act as a fixed
entry point to your application endpoints, such as NLBs, in different AWS Regions. Global
Accelerator uses the AWS global network to route traffic to the optimal regional endpoint
based on health, client location, and policies that you configure. Global Accelerator also
terminates TCP and UDP traffic at the edge locations, which reduces the number of hops and
improves the network performance. By adding AWS Global Accelerator in front of the NLBs,
you can achieve up to 60% improvement in latency for your online game.
References:
* https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html
* https://aws.amazon.com/global-accelerator/
QUESTION NO: 336
한 회사는 AWS Key Management Service(AWS KMS) 키를 사용하여 AWS Lambda 환경
변수를 암호화하고 있습니다. 솔루션 설계자는 환경 변수를 해독하고 사용하는 데 필요한
권한이 있는지 확인해야 합니다.
솔루션 설계자는 올바른 권한을 구현하기 위해 어떤 단계를 수행해야 합니까? (2개를
선택하세요.)
A. Lambda 리소스 정책에 AWS KMS 권한을 추가합니다.
B. Lambda 실행 역할에 AWS KMS 권한을 추가합니다.
C. Lambda 함수 정책에 AWS KMS 권한을 추가합니다.
D. AWS KMS 키 정책에서 Lambda 실행 역할을 허용합니다.
E. AWS KMS 키 정책에서 Lambda 리소스 정책을 허용합니다.
Answer: B D
Explanation:
B and D are the correct answers because they ensure that the Lambda execution role has
the permissions to decrypt and use the environment variables, and that the AWS KMS key
policy allows the Lambda execution role to use the key. The Lambda execution role is an IAM
role that grants the Lambda function permission to access AWS resources, such as AWS
KMS. The AWS KMS key policy is a resource-based policy that controls access to the key.
By adding AWS KMS permissions in the Lambda execution role and allowing the Lambda
execution role in the AWS KMS key policy, the solutions architect can implement the correct
permissions for encrypting and decrypting environment variables. References:
* AWS Lambda Execution Role
* Using AWS KMS keys in AWS Lambda
QUESTION NO: 337
한 회사는 Amazon EC2 인스턴스 집합을 사용하여 온프레미스 데이터 소스에서 데이터를
수집하고 있습니다. 데이터는 JSON 형식이며 수집 속도는 최대 1MB/s입니다. EC2
인스턴스가 재부팅되면 전송 중인 데이터가 손실됩니다. 회사의 데이터 과학 팀은 수집된
데이터를 거의 실시간으로 쿼리하려고 합니다.
데이터 손실을 최소화하면서 확장이 가능한 거의 실시간 데이터 쿼리를 제공하는 솔루션은
무엇입니까?
A. Amazon Kinesis Data Streams에 데이터 게시 Kinesis 데이터 분석을 사용하여 데이터를
쿼리합니다.
231

IT Certification Guaranteed, The Easy Way!
B. Amazon Redshift를 대상으로 하여 Amazon Kinesis Data Firehose에 데이터 게시 Amazon
Redshift를 사용하여 데이터 쿼리
C. 수집된 데이터를 EC2 인스턴스 스토어에 저장합니다. Amazon S3를 대상으로 하여
Amazon Kinesis Data Firehose에 데이터를 게시합니다. Amazon Athena를 사용하여
데이터를 쿼리합니다.
D. 수집된 데이터를 Amazon Elastic Block Store(Amazon EBS) 볼륨에 저장 Amazon
ElastiCache에 데이터 게시 Redis 채널을 구독하여 데이터 쿼리
Answer: A
Explanation:
https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html
QUESTION NO: 338
솔루션 아키텍트는 VPC의 Amazon EC2 인스턴스에서 Amazon DynamoDB에 대한 API
호출이 인터넷을 통해 이동하지 않도록 해야 합니다.
솔루션 설계자는 이 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (2개를
선택하세요.)
A. 엔드포인트에 대한 라우팅 테이블 항목을 생성합니다.
B. DynamoDB용 게이트웨이 엔드포인트를 생성합니다.
C. Amazon EC2용 인터페이스 엔드포인트를 생성합니다.
D. VPC의 각 서브넷에 있는 엔드포인트에 대한 탄력적 네트워크 인터페이스를 생성합니다.
E. 엔드포인트의 보안 그룹에 보안 그룹 항목을 생성하여 액세스를 제공합니다.
Answer: B E
Explanation:
B and E are the correct answers because they allow the solutions architect to ensure that API
calls to Amazon DynamoDB from Amazon EC2 instances in a VPC do not travel across the
internet. By creating a gateway endpoint for DynamoDB, the solutions architect can enable
private connectivity between the VPC and DynamoDB. By creating a security group entry in
the endpoint's security group to provide access, the solutions architect can control which EC2
instances can communicate with DynamoDB through the endpoint.
References:
* Gateway Endpoints
* Controlling Access to Services with VPC Endpoints
QUESTION NO: 339
한 회사는 단일 가용 영역에서 Amazon EC2와 Amazon RDS 다중 AZ DB 인스턴스를
사용하는 상태 비저장 2계층 애플리케이션을 설계했습니다. 새로운 회사 경영진은
애플리케이션의 고가용성을 보장하려고 합니다.
이 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 다중 AZ EC2 Auto Scaling을 사용하도록 애플리케이션을 구성하고 Application Load
Balancer를 생성합니다.
B. EC2 인스턴스의 스냅샷을 찍어 다른 AWS 리전으로 보내도록 애플리케이션을 구성합니다.
C. Amazon Route 53 지연 시간 기반 라우팅을 사용하여 애플리케이션에 요청을 공급하도록
애플리케이션을 구성합니다.
D. 수신 요청을 처리하고 다중 AZ 애플리케이션 로드 밸런서를 생성하도록 Amazon Route 53
232

IT Certification Guaranteed, The Easy Way!
규칙을 구성합니다.
Answer: A
Explanation:
https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html
QUESTION NO: 340
회사는 웹사이트에서 검색 가능한 항목 저장소를 유지 관리합니다. 데이터는 1,000만 개가
넘는 행을 포함하는 MySQL용 Amazon RDS 데이터베이스 테이블에 저장됩니다.
데이터베이스에는 2TB의 범용 SSD 스토리지가 있습니다. 회사 웹사이트를 통해 매일 이
데이터에 대한 수백만 건의 업데이트가 있습니다. 회사는 일부 삽입이 있음을 발견했습니다.
작업에 10초 이상이 소요됩니다. 회사에서는 데이터베이스 스토리지 성능이 문제라고
판단했습니다. 이 성능 문제를 해결하는 솔루션은 무엇입니까?
A. 스토리지 유형을 프로비저닝된 IOPS SSD로 변경합니다.
B. DB 인스턴스를 메모리 최적화 인스턴스 클래스로 변경합니다.
C. DB 인스턴스를 성능 확장이 가능한 인스턴스 클래스로 변경합니다.
D. MySQL 기본 비동기식 복제를 통해 다중 AZ RDS 읽기 전용 복제본을 활성화합니다.
Answer: A
Explanation:
https://aws.amazon.com/ebs/features/
"Provisioned IOPS volumes are backed by solid-state drives (SSDs) and are the highest
performance EBS volumes designed for your critical, I/O intensive database applications.
These volumes are ideal for both IOPS-intensive and throughput-intensive workloads that
require extremely low latency."
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html
QUESTION NO: 341
한 회사에 매시간 수백 개의 .csv 파일을 Amazon S3 버킷에 저장하는 애플리케이션이
있습니다. 파일 크기는 1GB입니다. 파일이 업로드될 때마다 회사는 파일을 Apache Parquet
형식으로 변환하고 출력 파일을 S3 버킷에 저장해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Lambda 함수를 생성하여 .csv 파일을 다운로드하고, 파일을 Parquet 형식으로
변환하고, 출력 파일을 S3 버킷에 저장합니다. 각 S3 PUT 이벤트에 대해 Lambda 함수를
호출합니다.
B. Apache Spark 작업을 생성하여 .csv 파일을 읽고, 파일을 Parquet 형식으로 변환하고, 출력
파일을 S3 버킷에 배치합니다. 각 S3 PUT 이벤트에 대해 AWS Lambda 함수를 생성하여
Spark 작업을 호출합니다.
C. 애플리케이션이 .csv 파일을 배치하는 S3 버킷에 대한 AWS Glue 테이블과 AWS Glue
크롤러를 생성합니다. 주기적으로 Amazon Athena를 사용하여 AWS Glue 테이블을 쿼리하고,
쿼리 결과를 Parquet 형식으로 변환하고, 출력 파일을 S3 버킷에 저장하도록 AWS Lambda
함수를 예약합니다.
D. AWS Glue ETL(추출, 변환 및 로드) 작업을 생성하여 .csv 파일을 Parquet 형식으로
변환하고 출력 파일을 S3 버킷에 배치합니다. 각 S3 PUT 이벤트에 대해 AWS Lambda 함수를
생성하여 ETL 작업을 호출합니다.
Answer: D
233

IT Certification Guaranteed, The Easy Way!
Explanation:
https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-
types-for- converting-data-to-apache-parquet.html
QUESTION NO: 342
한 회사는 Amazon DynamoDB를 사용하여 고객 정보를 저장하는 쇼핑 애플리케이션을
실행합니다. 데이터 손상의 경우 솔루션 설계자는 15분의 RPO(복구 시점 목표)와 1시간의
RTO(복구 시간 목표)를 충족하는 솔루션을 설계해야 합니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까?
A. DynamoDB 전역 테이블을 구성합니다. RPO 복구의 경우 애플리케이션이 다른 AWS
리전을 가리키도록 합니다.
B. DynamoDB 지정 시간 복구를 구성합니다. RPO 복구의 경우 원하는 시점으로 복원합니다.
C. DynamoDB 데이터를 매일 Amazon S3 Glacier로 내보냅니다. RPO 복구의 경우 S3
Glacier에서 DynamoDB로 데이터를 가져옵니다.
D. 15분마다 DynamoDB 테이블에 대한 Amazon Elastic Block Store(Amazon EBS) 스냅샷을
예약합니다. RPO 복구의 경우 EBS 스냅샷을 사용하여 DynamoDB 테이블을 복원합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/PointInTimeRecover
y.html
QUESTION NO: 343
어떤 회사에서는 높은 IOPS가 필요한 HPC 워크로드를 실행합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (두 가지 선택)
A. Amazon EFS를 고성능 파일 시스템으로 사용하세요.
B. Amazon FSx for Lustre를 고성능 파일 시스템으로 사용하세요.
C. EC2 인스턴스의 자동 확장 그룹을 만듭니다. 예약된 인스턴스를 사용합니다. 스프레드
배치 그룹을 구성합니다. 분석을 위해 AWS Batch를 사용합니다.
D. Amazon S3에 Mountpoint를 고성능 파일 시스템으로 사용하세요.
E. EC2 인스턴스의 자동 확장 그룹을 만듭니다. 혼합 인스턴스 유형과 클러스터 배치 그룹을
사용합니다. 분석에는 Amazon EMR을 사용합니다.
Answer: B E
* Option B: FSx for Lustre is designed for HPC workloads with high IOPS.
* Option E: A cluster placement group ensures low-latency networking for HPC analytics
workloads.
* Option A: Amazon EFS is not optimized for HPC.
* Option D: Mountpoint for S3 does not meet high IOPS needs.
QUESTION NO: 344
한 회사가 사용자 디바이스에서 센서 데이터를 수집하는 AWS에 3계층 애플리케이션을
보유하고 있습니다. 트래픽은 NLB(Network Load Balancer)를 거쳐 웹 계층용 Amazon EC2
인스턴스로, 마지막으로 애플리케이션 계층용 EC2 인스턴스로 흐릅니다. 계층이
데이터베이스를 호출하는 경우 전송 중인 데이터의 보안을 향상하려면 솔루션 설계자가
무엇을 해야 합니까?
234

IT Certification Guaranteed, The Easy Way!
A. TLS 수신기 구성 NLB에 서버 인증서 배포
B. NLB에서 AWS Shield Advanced 활성화 AWS WAF 구성
C. 로드 밸런서를 ALB(Application Load Balancer)로 변경하고 ALB에서 AWS WAF를
활성화합니다.
D. AWS Key Management Service(AWS KMS)를 사용하여 EC2 인스턴스에서 Amazon
Elastic Block Store(Amazon EBS) 볼륨을 암호화합니다.
Answer: A
Explanation:
The best option to improve the security of the data in transit is to configure a TLS listener and
deploy the server certificate on the NLB. This will ensure that the data is encrypted and
secure as it travels through the network. Additionally, you could also configure AWS Shield
Advanced and enable AWS WAF on the NLB to further protect the network from malicious
attacks. Alternatively, you could also change the load balancer to an Application Load
Balancer (ALB) and enable AWS WAF on the ALB. Finally, you could also encrypt the
Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key
Management Service (AWS KMS).
You must specify an SSL certificate for a TLS listener. The load balancer uses the certificate
to terminate the connection and decrypt requests from clients before routing them to targets.
https://docs.aws.amazon.com
/elasticloadbalancing/latest/network/create-listener.html
QUESTION NO: 345
회사는 Microsoft SOL Server 데이터베이스를 사용합니다. 회사의 애플리케이션은
데이터베이스에 연결됩니다.
회사는 애플리케이션 코드를 최소한으로 변경하면서 Amazon Aurora PostgreSQL
데이터베이스로 마이그레이션하려고 합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.)
A. AWS Schema Conversion Tool <AWS SCT)를 사용하여 애플리케이션에서 SOL 쿼리를
다시 작성합니다.
B. Aurora PostgreSQL에서 Babelfish를 활성화하여 애플리케이션에서 SQL 대기열을
실행합니다.
C. AWS Schema Conversion Tool(AWS SCT) 및 AWS Database Migration Service(AWS
DMS)를 사용하여 데이터베이스 스키마와 데이터를 마이그레이션합니다.
D. Amazon RDS Proxy를 사용하여 애플리케이션을 Aurora PostgreSQL에 연결합니다.
E. AWS Database Migration Service(AWS DMS)를 사용하여 애플리케이션에서 SOI 쿼리를
다시 작성합니다.
Answer: B C
Explanation:
* Requirement Analysis: The goal is to migrate from Microsoft SQL Server to Amazon Aurora
PostgreSQL with minimal application code changes.
* Babelfish for Aurora PostgreSQL: Babelfish allows Aurora PostgreSQL to understand SQL
Server queries natively, reducing the need for application code changes.
* AWS Schema Conversion Tool (SCT): This tool helps in converting the database schema
from SQL Server to PostgreSQL.
235

IT Certification Guaranteed, The Easy Way!
* AWS Database Migration Service (DMS): DMS can be used to migrate data from SQL
Server to Aurora PostgreSQL seamlessly.
* Combined Approach: Enabling Babelfish addresses the SQL query compatibility, while SCT
and DMS handle the schema and data migration.
References
* Babelfish for Aurora PostgreSQL: Babelfish Documentation
* AWS SCT and DMS: AWS Database Migration Service
QUESTION NO: 346
회사는 단일 가용 영역에서 실행되는 Amazon EC2 인스턴스에 애플리케이션을
호스팅합니다. OSI(Open Systems Interconnection) 모델의 전송 계층을 사용하여
애플리케이션에 액세스할 수 있습니다. 회사는 고가용성을 갖기 위해 애플리케이션
아키텍처가 필요합니다. 어떤 단계 조합이 이러한 요구 사항을 가장 비용 효율적으로
충족합니까? (2개 선택_)
A. 다른 가용 영역에서 새 EC2 인스턴스를 구성합니다. Amazon Route 53을 사용하여
트래픽을 모든 인스턴스로 라우팅합니다.
B. EC2 인스턴스 앞에 Network Load Balancer를 구성합니다.
C. 인스턴스에 대한 TCP 트래픽을 전송하도록 Network Load Balancer를 구성합니다.
인스턴스에 대한 HTTP 및 HTTPS 트래픽을 위해 Application Load Balancer를 구성합니다.
D. EC2 인스턴스에 대한 Auto Scaling 그룹을 생성합니다. 여러 가용 영역을 사용하도록 Auto
Scaling 그룹을 구성합니다. 인스턴스에서 애플리케이션 상태 확인을 실행하도록 Auto
Scaling 그룹을 구성합니다.
E. Amazon CloudWatch 경보를 생성합니다. 중지된 상태로 전환되는 EC2 인스턴스를 다시
시작하도록 경보를 구성합니다.
Answer: A D
Explanation:
To achieve high availability for an application that runs on EC2 instances, the application
should be deployed across multiple Availability Zones and use a load balancer to distribute
traffic. An Auto Scaling group can be used to launch and manage EC2 instances in multiple
Availability Zones and perform health checks on them.
A Network Load Balancer can be used to handle transport layer traffic to the EC2 instances.
References:
* Auto Scaling Groups
* What Is a Network Load Balancer?
QUESTION NO: 347
로펌에서는 특정 미래 날짜까지 수정이나 삭제를 방지하면서 파일을 공개적으로 읽을 수 있게
하려면 어떻게 해야 할까요?
A. 정적 웹사이트 호스팅을 위해 구성된 Amazon S3 버킷에 파일을 업로드합니다. 모든 AWS
주체에게 읽기 전용 IAM 권한을 부여합니다.
B. S3 버킷을 만듭니다. S3 버전 관리를 활성화합니다. 보관 기간이 있는 S3 객체 잠금을
사용합니다. CloudFront 배포를 만듭니다. 버킷 정책을 사용하여 액세스를 제한합니다.
C. S3 버킷을 만듭니다. S3 버전 관리를 활성화합니다. AWS Lambda로 이벤트 트리거를
구성하여 개인 S3 버킷에서 수정된 객체를 복원합니다.
D. 정적 웹사이트 호스팅을 위해 S3 버킷에 파일을 업로드합니다. 보관 기간이 있는 S3 객체
236

IT Certification Guaranteed, The Easy Way!
잠금을 사용합니다. 읽기 전용 IAM 권한을 부여합니다.
Answer: B
* Option B ensures the use of S3 Object Lock and Versioning to meet compliance for
immutability.
CloudFront enhances performance while a bucket policy ensures secure access.
* Option A lacks immutability safeguards.
* Option C introduces unnecessary complexity.
* Option D misses out on additional security benefits offered by CloudFront.
QUESTION NO: 348
AWS에서 웹 애플리케이션을 호스팅하는 회사는 모든 Amazon EC2 인스턴스를 보장하려고
합니다. Amazon RDS DB 인스턴스. Amazon Redshift 클러스터는 태그로 구성됩니다. 회사는
이 검사를 구성하고 운영하는 노력을 최소화하려고 합니다.
이를 달성하려면 솔루션 아키텍트가 무엇을 해야 합니까?
A. AWS Config 규칙을 사용하여 적절하게 태그가 지정되지 않은 리소스를 정의하고
감지합니다.
B. Cost Explorer를 사용하여 태그가 제대로 지정되지 않은 리소스를 표시합니다. 해당
리소스에 수동으로 태그를 지정하세요.
C. 적절한 태그 할당을 위해 모든 리소스를 확인하는 API 호출을 작성합니다. EC2
인스턴스에서 코드를 주기적으로 실행합니다.
D. 적절한 태그 할당을 위해 모든 리소스를 확인하는 API 호출을 작성합니다. Amazon
CloudWatch를 통해 AWS Lambda 함수를 예약하여 코드를 주기적으로 실행합니다.
Answer: A
Explanation:
To ensure all Amazon EC2 instances, Amazon RDS DB instances, and Amazon Redshift
clusters are configured with tags, a solutions architect should use AWS Config rules to define
and detect resources that are not properly tagged. AWS Config rules are a set of
customizable rules that AWS Config uses to evaluate AWS resource configurations for
compliance with best practices and company policies. Using AWS Config rules can minimize
the effort of configuring and operating this check because it automates the process of
identifying non-compliant resources and notifying the responsible teams.
Reference:
AWS Config Developer Guide: AWS Config Rules
(https://docs.aws.amazon.com/config/latest
/developerguide/evaluate-config_use-managed-rules.html)
QUESTION NO: 349
회사에는 단일 AWS 지역에서 실행되는 지역 구독 기반 스트리밍 서비스가 있습니다.
아키텍처는 Amazon EC2 인스턴스의 웹 서버와 애플리케이션 서버로 구성됩니다. EC2
인스턴스는 Elastic Load Balancer 뒤의 Auto Scaling 그룹에 있습니다. 아키텍처에는 여러
가용 영역에 걸쳐 확장되는 Amazon Aurora 데이터베이스 클러스터가 포함되어 있습니다.
회사는 전 세계적으로 확장하고 애플리케이션 가동 중지 시간을 최소화하기를 원합니다.
A. 웹 계층 및 애플리케이션 계층에 대한 Auto Scaling 그룹을 확장하여 두 번째 지역의 가용
영역에 인스턴스를 배포합니다. Aurora 글로벌 데이터베이스를 사용하여 기본 지역과 두 번째
지역에 데이터베이스를 배포합니다. 두 번째 리전에 대한 장애 조치 라우팅 정책과 함께
237

IT Certification Guaranteed, The Easy Way!
Amazon Route 53 상태 확인을 사용하십시오.
B. 웹 계층과 애플리케이션 계층을 두 번째 지역에 배포합니다. 두 번째 리전에 Aurora
PostgreSQL 리전 간 Aurara 복제본을 추가합니다. 두 번째 리전에 대한 장애 조치 라우팅
정책과 함께 Amazon Route 53 상태 확인을 사용하고 필요에 따라 보조 리전을 기본 리전으로
승격합니다.
C. 웹 계층과 애플리케이션 계층을 두 번째 지역에 배포합니다. 두 번째 리전에 Aurora
PostSQL 데이터베이스를 생성합니다. AWS Database Migration Service(AWS DMS)를
사용하여 기본 데이터베이스를 두 번째 리전에 복제합니다. 두 번째 리전에 대한 장애 조치
라우팅 정책과 함께 Amazon Route 53 상태 확인을 사용하십시오.
D. 웹 계층과 애플리케이션 계층을 두 번째 지역에 배포합니다. Amazon Aurora 글로벌
데이터베이스를 사용하여 기본 지역과 두 번째 지역에 데이터베이스를 배포합니다. 두 번째
리전에 대한 장애 조치 라우팅 정책과 함께 Amazon Route 53 상태 확인을 사용하십시오.
필요에 따라 보조를 기본으로 승격합니다.
Answer: D
Explanation:
This option is the most efficient because it deploys the web tier and the application tier to a
second Region, which provides high availability and redundancy for the application. It also
uses an Amazon Aurora global database, which is a feature that allows a single Aurora
database to span multiple AWS Regions1. It also deploys the database in the primary Region
and the second Region, which provides low latency global reads and fast recovery from a
Regional outage. It also uses Amazon Route 53 health checks with a failover routing policy to
the second Region, which provides data protection by routing traffic to healthy endpoints in
different Regions2. It also promotes the secondary to primary as needed, which provides
data consistency by allowing write operations in one of the Regions at a time3. This solution
meets the requirement of expanding globally and ensuring that its application has minimal
downtime. Option A is less efficient because it extends the Auto Scaling groups for the web
tier and the application tier to deploy instances in Availability Zones in a second Region,
which could incur higher costs and complexity than deploying them separately. It also uses
an Aurora global database to deploy the database in the primary Region and the second
Region, which is correct.
However, it does not use Amazon Route 53 health checks with a failover routing policy to the
second Region, which could result in traffic being routed to unhealthy endpoints. Option B is
less efficient because it deploys the web tier and the application tier to a second Region,
which is correct. It also adds an Aurora PostgreSQL cross-Region Aurora Replica in the
second Region, which provides read scalability across Regions. However, it does not use an
Aurora global database, which provides faster replication and recovery than cross-Region
replicas. It also uses Amazon Route 53 health checks with a failover routing policy to the
second Region, which is correct. However, it does not promote the secondary to primary as
needed, which could result in data inconsistency or loss. Option C is less efficient because it
deploys the web tier and the application tier to a second Region, which is correct. It also
creates an Aurora PostgreSQL database in the second Region, which provides data
redundancy across Regions. However, it does not use an Aurora global database or cross-
Region replicas, which provide faster replication and recovery than creating separate
databases. It also uses AWS Database Migration Service (AWS DMS) to replicate the
primary database to the second Region, which provides data migration between different
238

IT Certification Guaranteed, The Easy Way!
sources and targets. However, it does not use an Aurora global database or cross-Region
replicas, which provide faster replication and recovery than using AWS DMS. It also uses
Amazon Route 53 health checks with a failover routing policy to the second Region, which is
correct.
QUESTION NO: 350
한 전자상거래 회사에서 Amazon RDS 기반 웹 애플리케이션의 성능 저하를 발견했습니다.
성능 저하의 원인은 비즈니스 분석가가 트리거하는 읽기 전용 SQL 쿼리 수가 증가했기
때문입니다. 솔루션 설계자는 기존 웹 애플리케이션을 최소한으로 변경하여 문제를 해결해야
합니다.
솔루션 설계자는 무엇을 추천해야 합니까?
A. 데이터를 Amazon DynamoDB로 내보내고 비즈니스 분석가가 쿼리를 실행하도록 합니다.
B. Amazon ElastiCache에 데이터를 로드하고 비즈니스 분석가가 쿼리를 실행하도록 합니다.
C. 기본 데이터베이스의 읽기 전용 복제본을 생성하고 비즈니스 분석가가 쿼리를 실행하도록
합니다.
D. 데이터를 Amazon Redshift 클러스터에 복사하고 비즈니스 분석가가 쿼리를 실행하도록
합니다.
Answer: C
Explanation:
Creating a read replica of the primary RDS database will offload the read-only SQL queries
from the primary database, which will help to improve the performance of the web
application. Read replicas are exact copies of the primary database that can be used to
handle read-only traffic, which will reduce the load on the primary database and improve the
performance of the web application. This solution can be implemented with minimal changes
to the existing web application, as the business analysts can continue to run their queries on
the read replica without modifying the code.
QUESTION NO: 351
한 전자 상거래 회사가 AWS에서 하루에 한 번 거래하는 웹 사이트를 시작하려고 합니다. 매일
24시간 동안 정확히 하나의 제품이 판매됩니다. 회사는 피크 시간 동안 밀리초의 대기
시간으로 매시간 수백만 건의 요청을 처리할 수 있기를 원합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon S3를 사용하여 다양한 S3 버킷에서 전체 웹 사이트 호스팅 Amazon CloudFront
배포 추가 S3 버킷을 배포 원본으로 설정 주문 데이터를 Amazon S3에 저장
B. 여러 가용 영역에 걸쳐 Auto Scaling 그룹에서 실행되는 Amazon EC2 인스턴스에 전체 웹
사이트 배포 ALB(Application Load Balancer)를 추가하여 웹 사이트 트래픽 분산 백엔드
API용으로 또 다른 ALB 추가 MySQL용 Amazon RDS에 데이터 저장
C. 컨테이너에서 실행할 전체 애플리케이션 마이그레이션 Amazon Elastic Kubernetes
Service(Amazon EKS)에서 컨테이너 호스팅 Kubernetes Cluster Autoscaler를 사용하여 포드
수를 늘리거나 줄여 트래픽 급증을 처리합니다. MySQL용 Amazon RDS에 데이터를
저장합니다.
D. Amazon S3 버킷을 사용하여 웹 사이트의 정적 콘텐츠를 호스팅합니다. Amazon
CloudFront 배포를 배포합니다. S3 버킷을 오리진으로 설정 백엔드 API에 Amazon API
Gateway 및 AWS Lambda 함수 사용 Amazon DynamoDB에 데이터 저장
Answer: D
239

IT Certification Guaranteed, The Easy Way!
Explanation:
To launch a one-deal-a-day website on AWS with millisecond latency during peak hours and
with the least operational overhead, the best option is to use an Amazon S3 bucket to host
the website's static content, deploy an Amazon CloudFront distribution, set the S3 bucket as
the origin, use Amazon API Gateway and AWS Lambda functions for the backend APIs, and
store the data in Amazon DynamoDB. This option requires minimal operational overhead and
can handle millions of requests each hour with millisecond latency during peak hours.
Therefore, option D is the correct answer.
Reference: https://aws.amazon.com/blogs/compute/building-a-serverless-multi-player-game-
with-aws-lambda- and-amazon-dynamodb/
QUESTION NO: 352
한 회사가 데이터를 Amazon S3 버킷으로 이동할 계획입니다. 데이터는 S3 버킷에 저장될 때
암호화되어야 합니다. 또한 암호화 키는 매년 자동으로 교체되어야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 데이터를 S3 버킷으로 이동합니다. Amazon S3 관리형 암호화 키(SSE-S3)로 서버 측
암호화를 사용합니다. SSE-S3 암호화 키의 내장 키 순환 동작을 사용합니다.
B. AWS Key Management Service(AWS KMS) 고객 관리형 키를 생성합니다. 자동 키 순환을
활성화합니다. 고객 관리형 KMS 키를 사용하도록 S3 버킷의 기본 암호화 동작을 설정합니다.
데이터를 S3 버킷으로 이동합니다.
C. AWS Key Management Service(AWS KMS) 고객 관리형 키를 생성합니다. 고객 관리형
KMS 키를 사용하도록 S3 버킷의 기본 암호화 동작을 설정합니다. 데이터를 S3 버킷으로
이동합니다. 매년 KMS 키를 수동으로 순환합니다.
D. 데이터를 S3 버킷으로 이동하기 전에 고객 키 자료로 데이터를 암호화합니다. 키 자료 없이
AWS Key Management Service(AWS KMS) 키를 생성합니다. 고객 키 자료를 KMS 키로
가져옵니다. 자동 키 순환을 활성화합니다.
Answer: B
Explanation:
SSE-S3 - is free and uses AWS owned CMKs (CMK = Customer Master Key). The encryption
key is owned and managed by AWS, and is shared among many accounts. Its rotation is
automatic with time that varies as shown in the table here. The time is not explicitly defined.
SSE-KMS - has two flavors:
AWS managed CMK. This is free CMK generated only for your account. You can only view it
policies and audit usage, but not manage it. Rotation is automatic - once per 1095 days (3
years), Customer managed CMK. This uses your own key that you create and can manage.
Rotation is not enabled by default. But if you enable it, it will be automatically rotated every 1
year. This variant can also use an imported key material by you. If you create such key with
an imported material, there is no automated rotation. Only manual rotation.
SSE-C - customer provided key. The encryption key is fully managed by you outside of AWS.
AWS will not rotate it.
This solution meets the requirements of moving data to an Amazon S3 bucket, encrypting the
data when it is stored in the S3 bucket, and automatically rotating the encryption key every
year with the least operational overhead. AWS Key Management Service (AWS KMS) is a
service that enables you to create and manage encryption keys for your data. A customer
managed key is a symmetric encryption key that you create and manage in AWS KMS. You
240

IT Certification Guaranteed, The Easy Way!
can enable automatic key rotation for a customer managed key, which means that AWS KMS
generates new cryptographic material for the key every year. You can set the S3 bucket's
default encryption behavior to use the customer managed KMS key, which means that any
object that is uploaded to the bucket without specifying an encryption method will be
encrypted with that key.
Option A is incorrect because using server-side encryption with Amazon S3 managed
encryption keys (SSE- S3) does not allow you to control or manage the encryption keys. SSE
-S3 uses a unique key for each object, and encrypts that key with a master key that is
regularly rotated by S3. However, you cannot enable or disable key rotation for SSE-S3 keys,
or specify the rotation interval. Option C is incorrect because manually rotating the KMS key
every year can increase the operational overhead and complexity, and it may not meet the
requirement of rotating the key every year if you forget or delay the rotation process. Option
D is incorrect because encrypting the data with customer key material before moving the data
to the S3 bucket can increase the operational overhead and complexity, and it may not
provide consistent encryption for all objects in the bucket. Creating a KMS key without key
material and importing the customer key material into the KMS key can enable you to use
your own source of random bits to generate your KMS keys, but it does not support
automatic key rotation.
References:
* https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html
* https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html
QUESTION NO: 353
회사는 보안 정책을 준수하는 방식으로 전 세계에 분산된 개발 팀에 회사의 AWS 리소스에
대한 안전한 액세스 권한을 제공해야 합니다.
이 회사는 현재 내부 인증을 위해 온프레미스 Active Directory를 사용합니다. 이 회사는 AWS
Organizations를 사용하여 여러 프로젝트를 지원하는 여러 AWS 계정을 관리합니다.
회사에는 기존 인프라와 통합되어 중앙 집중화된 ID 관리 및 액세스 제어를 제공하는
솔루션이 필요합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. AWS Directory Service를 설정하여 AWS에서 AWS 관리형 Microsoft Active Directory를
만듭니다.
온프레미스 Active Directory와 신뢰 관계를 구축합니다. Active Directory 그룹에 할당된 1AM
역할을 사용하여 회사의 AWS 계정 내 AWS 리소스에 액세스합니다.
B. 각 개발자에 대해 1AM 사용자를 만듭니다. 각 프로젝트에 대한 각 사용자의 참여에 따라 각
1AM 사용자에 대한 권한을 수동으로 관리합니다. 추가 보안 계층으로 다중 요소 인증(MFA)을
시행합니다.
C. AWS Directory Service에서 AD Connector를 사용하여 온프레미스 Active Directory에
연결합니다. AD Connector를 AWS 1AM Identity Center와 통합합니다. 각 AD 그룹에 특정
AWS 계정 및 리소스에 대한 액세스 권한을 부여하도록 권한 집합을 구성합니다.
D. Amazon Cognito를 사용하여 ID 페더레이션 솔루션을 배포합니다. ID 페더레이션 솔루션을
온프레미스 Active Directory와 통합합니다. Amazon Cognito를 사용하여 개발자가 AWS 계정
및 리소스에 액세스할 수 있도록 액세스 토큰을 제공합니다.
Answer: C
241

IT Certification Guaranteed, The Easy Way!
Explanation:
Using AD Connector with AWS IAM Identity Center (formerly AWS Single Sign-On) allows
the company to leverage its existing on-premises Active Directory for centralized identity
management and access control. AD Connector acts as a proxy to the on-premises AD
without requiring additional infrastructure or complex setup. This solution integrates
seamlessly with AWS, allowing the development team to use their existing AD credentials to
access AWS resources across multiple accounts managed by AWS Organizations.
The permissions for AWS resources can be managed centrally through IAM Identity Center
by configuring permission sets.
This solution provides:
* Least operational overhead: AD Connector is fully managed, and IAM Identity Center allows
centralized management of permissions across accounts.
* Secure access: The solution complies with security policies by using existing AD
authentication mechanisms.
* Option A (AWS Managed AD): Setting up a fully managed AWS AD and establishing a trust
is more complex and involves additional operational overhead.
* Option B (IAM Users): Manually managing IAM users and permissions is less scalable and
increases operational complexity.
* Option D (Cognito): Amazon Cognito is more suited for user-facing applications rather than
internal identity management for AWS resources.
AWS References:
* AD Connector with IAM Identity Center
* AWS IAM Identity Center
QUESTION NO: 354
한 회사가 Amazon RDS 데이터베이스를 사용하여 Amazon EC2 인스턴스에 애플리케이션을
배포했습니다. 회사는 최소 권한 원칙을 사용하여 데이터베이스 액세스 자격 증명을
구성했습니다. 회사의 보안 팀은 SQL 주입 및 기타 웹 기반 공격으로부터 애플리케이션과
데이터베이스를 보호하려고 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 보안 그룹과 네트워크 ACL을 사용하여 데이터베이스와 애플리케이션 서버를 보호합니다.
B. AWS WAF를 사용하여 애플리케이션을 보호합니다. RDS 매개변수 그룹을 사용하여 보안
설정을 구성합니다.
C. AWS 네트워크 방화벽을 사용하여 애플리케이션과 데이터베이스를 보호합니다.
D. 다양한 기능을 위해 애플리케이션 코드에서 다양한 데이터베이스 계정을 사용합니다.
데이터베이스 사용자에게 과도한 권한을 부여하지 마십시오.
Answer: B
Explanation:
AWS WAF is a web application firewall that helps protect web applications from common web
exploits that could affect application availability, compromise security, or consume excessive
resources. AWS WAF allows users to create rules that block, allow, or count web requests
based on customizable web security rules.
One of the types of rules that can be created is an SQL injection rule, which allows users to
specify a list of IP addresses or IP address ranges that they want to allow or block. By using
AWS WAF to protect the application, the company can prevent SQL injection and other web-
242

IT Certification Guaranteed, The Easy Way!
based attacks from reaching the application and the database.
RDS parameter groups are collections of parameters that define how a database instance
operates. Users can modify the parameters in a parameter group to change the behavior and
performance of the database. By using RDS parameter groups to configure the security
settings, the company can enforce best practices such as disabling remote root login,
requiring SSL connections, and limiting the maximum number of connections.
The other options are not correct because they do not effectively protect the application and
the database from SQL injection and other web-based attacks. Using security groups and
network ACLs to secure the database and application servers is not sufficient because they
only filter traffic at the network layer, not at the application layer. Using AWS Network Firewall
to protect the application and the database is not necessary because it is a stateful firewall
service that provides network protection for VPCs, not for individual applications or
databases. Using different database accounts in the application code for different functions is
a good practice, but it does not prevent SQL injection attacks from exploiting vulnerabilities in
the application code.
References:
* AWS WAF
* How AWS WAF works
* Working with IP match conditions
* Working with DB parameter groups
* Amazon RDS security best practices
QUESTION NO: 355
회사는 애플리케이션 로그를 Amazon CloudWatch Logs 로그 그룹에 저장합니다. 새로운
정책에 따라 회사는 모든 애플리케이션 로그를 Amazon OpenSearch Service(Amazon
Elasticsearch Service)에 거의 실시간으로 저장해야 합니다.
최소한의 운영 오버헤드로 이 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 로그를 Amazon OpenSearch Service(Amazon Elasticsearch Service)로 스트리밍하도록
CloudWatch Logs 구독을 구성합니다.
B. AWS Lambda 함수를 생성합니다. 로그 그룹을 사용하여 Amazon OpenSearch
Service(Amazon Elasticsearch Service)에 로그를 쓰는 함수를 호출합니다.
C. Amazon Kinesis Data Firehose 전송 스트림을 생성합니다. 로그 그룹을 전송 스트림의
소스로 구성합니다. Amazon OpenSearch Service(Amazon Elasticsearch Service)를 전송
스트림의 대상으로 구성합니다.
D. 로그를 Amazon Kinesis Data Streams에 전달하도록 각 애플리케이션 서버에 Amazon
Kinesis 에이전트를 설치하고 구성합니다. Amazon OpenSearch Service(Amazon
Elasticsearch Service)에 로그를 전달하도록 Kinesis Data Streams를 구성합니다.
Answer: B
Explanation:
https://computingforgeeks.com/stream-logs-in-aws-from-cloudwatch-to-elasticsearch/
QUESTION NO: 356
회사는 사용자를 비용 센터에 매핑하는 Amazon RDS 데이터베이스를 유지 관리합니다.
회사는 AWS Organizations의 조직에 계정을 가지고 있습니다. 회사에는 조직의 특정 AWS
계정에서 생성된 모든 리소스에 태그를 지정하는 솔루션이 필요합니다. 솔루션은 리소스를
243

IT Certification Guaranteed, The Easy Way!
생성한 사용자의 비용 센터 ID로 각 리소스에 태그를 지정해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 특정 AWS 계정을 마스터 계정에서 조직의 새 조직 단위(OU)로 이동합니다. 리소스가
생성되기 전에 모든 기존 리소스에 올바른 비용 센터 태그가 있어야 하는 서비스 제어
정책(SCP)을 생성합니다. 새 OU에 SCP를 적용합니다.
B. Lambda 함수가 RDS 데이터베이스에서 적절한 비용 센터를 조회한 후 리소스에 태그를
지정하는 AWS Lambda 함수를 생성합니다. AWS CloudTrail 이벤트에 반응하여 Lambda
함수를 호출하는 Amazon EventBridge 규칙을 구성합니다.
C. AWS CloudFormation 스택을 생성하여 AWS Lambda 함수를 배포합니다. RDS
데이터베이스에서 적절한 비용 센터를 조회하고 리소스에 태그를 지정하도록 Lambda 함수를
구성합니다. CloudFormation 스택을 호출하는 Amazon EventBridge 예약 규칙을 생성합니다.
D. 기본값으로 리소스에 태그를 지정하는 AWS Lambda 함수를 생성합니다. 리소스에 비용
센터 태그가 누락된 경우 AWS CloudTrail 이벤트에 반응하여 Lambda 함수를 호출하는
Amazon EventBridge 규칙을 구성합니다.
Answer: B
Explanation:
AWS Lambda is a serverless compute service that lets you run code without provisioning or
managing servers. Lambda can be used to tag resources with the cost center ID of the user
who created the resource, by querying the RDS database that maps users to cost centers.
Amazon EventBridge is a serverless event bus service that enables event-driven
architectures. EventBridge can be configured to react to AWS CloudTrail events, which are
recorded API calls made by or on behalf of the AWS account. EventBridge can invoke the
Lambda function when a resource is created in the specific AWS account, passing the user
identity and resource information as parameters. This solution will meet the requirements, as
it enables automatic tagging of resources based on the user and cost center mapping.
References:
* 1 provides an overview of AWS Lambda and its benefits.
* 2 provides an overview of Amazon EventBridge and its benefits.
* 3 explains the concept and benefits of AWS CloudTrail events.
QUESTION NO: 357
회사는 Amazon S3 Standard에 페타바이트 규모의 데이터를 저장하고 있습니다. 데이터는
여러 S3 버킷에 저장되어 있으며 다양한 빈도로 액세스됩니다. 회사는 모든 데이터에 대한
액세스 패턴을 알지 못합니다. 회사는 S3 사용 비용을 최적화하기 위해 각 S3 버킷에 대한
솔루션을 구현해야 합니다.
가장 효율적인 운영 효율성으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. S3 버킷의 객체를 S3 Intelligent-Tiering으로 전환하는 규칙을 사용하여 S3 수명 주기
구성을 생성합니다.
B. S3 스토리지 클래스 분석 도구를 사용하여 S3 버킷의 각 객체에 대한 올바른 계층을
결정합니다.
각 개체를 식별된 스토리지 계층으로 이동합니다.
C. S3 버킷의 객체를 S3 Glacier Instant Retrieval로 전환하는 규칙을 사용하여 S3 수명 주기
구성을 생성합니다.
D. S3 버킷의 객체를 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 전환하는 규칙을
244

IT Certification Guaranteed, The Easy Way!
사용하여 S3 수명 주기 구성을 생성합니다.
Answer: A
Explanation:
* Understanding the Requirement: The company has petabytes of data in S3 Standard
across multiple buckets with varying access frequencies. They do not know the access
patterns and need a cost- optimized storage solution with minimal operational effort.
* Analysis of Options:
* S3 Intelligent-Tiering: This storage class automatically moves data between two access
tiers (frequent and infrequent) based on changing access patterns. It incurs a small
monitoring and automation charge but eliminates the need to manually move data between
storage classes.
* S3 Storage Class Analysis Tool: While useful for determining access patterns, this tool
requires manual intervention to move objects to the appropriate storage class, which
increases operational overhead.
* S3 Glacier Instant Retrieval: This storage class is designed for data that is rarely accessed
but requires instant retrieval when needed. It may not be suitable for data with unknown and
varying access patterns.
* S3 One Zone-IA: This is a lower-cost option for infrequently accessed data stored in a
single availability zone. It does not provide the same level of durability and availability as
other options and requires knowledge of access patterns.
* Best Option for Operational Efficiency:
* S3 Intelligent-Tiering provides the best balance of cost savings and operational efficiency. It
dynamically adjusts to access patterns without manual intervention, ensuring the company is
only paying for what they need without the risk of incurring high costs for infrequent access
data.
References:
* Amazon S3 Intelligent-Tiering
* Managing your storage lifecycle
QUESTION NO: 358
회사에 중요한 데이터가 포함된 Amazon S3 버킷이 있습니다.
ㅏ. 회사는 실수로 데이터가 삭제되지 않도록 보호해야 합니다.
이러한 요구 사항을 충족하려면 솔루션 설계자가 수행해야 하는 단계 조합은 무엇입니까?
(2개를 선택하세요.)
A. S3 버킷에서 버전 관리를 활성화합니다.
B. S3 버킷에서 MFA 삭제를 활성화합니다.
C. S3 버킷에 버킷 정책을 생성합니다.
D. S3 버킷에서 기본 암호화를 활성화합니다.
E. S3 버킷의 객체에 대한 수명 주기 정책을 생성합니다.
Answer: A B
Explanation:
To protect data in an S3 bucket from accidental deletion, versioning should be enabled,
which enables you to preserve, retrieve, and restore every version of every object in an S3
bucket. Additionally, enabling MFA (multi-factor authentication) Delete on the S3 bucket adds
an extra layer of protection by requiring an authentication token in addition to the user's
245

IT Certification Guaranteed, The Easy Way!
access keys to delete objects in the bucket.
Reference:
AWS S3 Versioning documentation:
https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html AWS S3 MFA Delete
documentation: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMFADelete.
html
QUESTION NO: 359
회사에서는 특정 결제 ID에 대한 메시지가 전송된 순서대로 수신되도록 요구하는 결제 처리
시스템을 사용합니다. 그렇지 않으면 결제가 잘못 처리될 수 있습니다.
이 요구 사항을 충족하려면 솔루션 설계자가 어떤 조치를 취해야 합니까? (2개를 선택하세요.)
A. 결제 ID를 파티션 키로 사용하여 Amazon DynamoDB 테이블에 메시지를 씁니다.
B. 결제 ID를 파티션 키로 사용하여 Amazon Kinesis 데이터 스트림에 메시지를 씁니다.
C. 결제 ID를 키로 사용하여 Memcached용 Amazon ElastiCache 클러스터에 메시지를
씁니다.
D. Amazon Simple Queue Service(Amazon SQS) 대기열에 메시지를 씁니다. 결제 ID를
사용하도록 메시지 속성을 설정합니다.
E. Amazon Simple Queue Service(Amazon SQS) FIFO 대기열에 메시지를 씁니다. 결제ID를
사용할 메시지 그룹을 설정합니다.
Answer: B E
Explanation:
1) SQS FIFO queues guarantee that messages are received in the exact order they are sent.
Using the payment ID as the message group ensures all messages for a payment ID are
received sequentially. 2) Kinesis data streams can also enforce ordering on a per partition
key basis. Using the payment ID as the partition key will ensure strict ordering of messages
for each payment ID.
QUESTION NO: 360
한 회사가 다중 계층 온프레미스 애플리케이션을 AWS로 마이그레이션하고 있습니다.
애플리케이션은 단일 노드 MySQL 데이터베이스와 다중 노드 웹 계층으로 구성됩니다.
회사는 마이그레이션 중에 애플리케이션 변경을 최소화해야 합니다. 회사는 마이그레이션 후
애플리케이션 복원성을 개선하려고 합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.)
A. 웹 계층을 Application Load Balancer 뒤의 Auto Scaling 그룹에 있는 Amazon EC2
인스턴스로 마이그레이션합니다.
B. Network Load Balancer 뒤의 Auto Scaling 그룹에 있는 Amazon EC2 인스턴스로
데이터베이스를 마이그레이션합니다.
C. 데이터베이스를 Amazon RDS 다중 AZ 배포로 마이그레이션합니다.
D. 웹 계층을 AWS Lambda 함수로 마이그레이션합니다.
E. 데이터베이스를 Amazon DynamoDB 테이블로 마이그레이션합니다.
Answer: A C
Explanation:
An Auto Scaling group is a collection of EC2 instances that share similar characteristics and
can be scaled in or out automatically based on demand. An Auto Scaling group can be
246

IT Certification Guaranteed, The Easy Way!
placed behind an Application Load Balancer, which is a type of Elastic Load Balancing load
balancer that distributes incoming traffic across multiple targets in multiple Availability Zones.
This solution will improve the resiliency of the web tier by providing high availability,
scalability, and fault tolerance. An Amazon RDS Multi-AZ deployment is a configuration that
automatically creates a primary database instance and synchronously replicates the data to a
standby instance in a different Availability Zone. When a failure occurs, Amazon RDS
automatically fails over to the standby instance without manual intervention. This solution will
improve the resiliency of the database tier by providing data redundancy, backup support,
and availability. This combination of steps will meet the requirements with minimal changes
to the application during the migration.
References:
* 1 describes the concept and benefits of an Auto Scaling group.
* 2 provides an overview of Application Load Balancers and their benefits.
* 3 explains how Amazon RDS Multi-AZ deployments work and their benefits.
QUESTION NO: 361
한 대형 미디어 회사가 AWS에서 웹 애플리케이션을 호스팅하고 있습니다. 회사는 전 세계
사용자가 파일에 안정적으로 액세스할 수 있도록 기밀 미디어 파일 캐싱을 시작하려고
합니다. 콘텐츠는 Amazon S3 버킷에 저장됩니다. 회사는 요청이 지리적으로 어디서
발생하는지에 관계없이 콘텐츠를 신속하게 제공해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS DataSync를 사용하여 S3 버킷을 웹 애플리케이션에 연결합니다.
B. AWS Global Accelerator를 배포하여 S3 버킷을 웹 애플리케이션에 연결합니다.
C. Amazon CloudFront를 배포하여 S3 버킷을 CloudFront 엣지 서버에 연결합니다.
D. Amazon Simple Queue Service(Amazon SQS)를 사용하여 S3 버킷을 웹 애플리케이션에
연결합니다.
Answer: C
Explanation:
CloudFront uses a local cache to provide the response, AWS Global accelerator proxies
requests and connects to the application all the time for the response.
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-
restricting-access-to- s3.html#private-content-granting-permissions-to-oai
QUESTION NO: 362
회사는 거의 실시간으로 스트리밍 데이터를 처리하는 애플리케이션을 배포하고 있습니다.
회사는 워크로드에 Amazon EC2 인스턴스를 사용할 계획입니다. 네트워크 아키텍처는 노드
간 가능한 최저 지연 시간을 제공하도록 구성 가능해야 합니다. 이러한 요구 사항을 충족하는
네트워크 솔루션 조합은 무엇입니까? (2개 선택)
A. 각 EC2 인스턴스에서 향상된 네트워킹을 활성화하고 구성합니다.
B. EC2 인스턴스를 별도의 계정으로 그룹화합니다.
C. 클러스터 배치 그룹에서 EC2 인스턴스를 실행합니다.
D. 각 EC2 인스턴스에 여러 탄력적 네트워크 인터페이스를 연결합니다.
E. Amazon Elastic Block Store(Amazon EBS) 최적화 인스턴스 유형을 사용합니다.
Answer: A C
247

IT Certification Guaranteed, The Easy Way!
Explanation:
These options are the most suitable ways to configure the network architecture to provide the
lowest possible latency between nodes. Option A enables and configures enhanced
networking on each EC2 instance, which is a feature that improves the network performance
of the instance by providing higher bandwidth, lower latency, and lower jitter. Enhanced
networking uses single root I/O virtualization (SR-IOV) or Elastic Fabric Adapter (EFA) to
provide direct access to the network hardware. You can enable and configure enhanced
networking by choosing a supported instance type and a compatible operating system, and
installing the required drivers. Option C runs the EC2 instances in a cluster placement group,
which is a logical grouping of instances within a single Availability Zone that are placed close
together on the same underlying hardware.
Cluster placement groups provide the lowest network latency and the highest network
throughput among the placement group options. You can run the EC2 instances in a cluster
placement group by creating a placement group and launching the instances into it.
Option B is not suitable because grouping the EC2 instances in separate accounts does not
provide the lowest possible latency between nodes. Separate accounts are used to isolate
and organize resources for different purposes, such as security, billing, or compliance.
However, they do not affect the network performance or proximity of the instances. Moreover,
grouping the EC2 instances in separate accounts would incur additional costs and
complexity, and it would require setting up cross-account networking and permissions.
Option D is not suitable because attaching multiple elastic network interfaces to each EC2
instance does not provide the lowest possible latency between nodes. Elastic network
interfaces are virtual network interfaces that can be attached to EC2 instances to provide
additional network capabilities, such as multiple IP addresses, multiple subnets, or enhanced
security. However, they do not affect the network performance or proximity of the instances.
Moreover, attaching multiple elastic network interfaces to each EC2 instance would consume
additional resources and limit the instance type choices.
Option E is not suitable because using Amazon EBS optimized instance types does not
provide the lowest possible latency between nodes. Amazon EBS optimized instance types
are instances that provide dedicated bandwidth for Amazon EBS volumes, which are block
storage volumes that can be attached to EC2 instances.
EBS optimized instance types improve the performance and consistency of the EBS
volumes, but they do not affect the network performance or proximity of the instances.
Moreover, using EBS optimized instance types would incur additional costs and may not be
necessary for the streaming data workload. References:
* Enhanced networking on Linux
* Placement groups
* Elastic network interfaces
* Amazon EBS-optimized instances
QUESTION NO: 363
한 회사에서 컨테이너화된 애플리케이션 워크로드를 3개의 가용 영역에 걸쳐 VPC에
배포하려고 합니다. 회사에는 가용 영역 전반에 걸쳐 가용성이 높은 솔루션이 필요합니다.
솔루션을 사용하려면 애플리케이션을 최소한으로 변경해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
248

IT Certification Guaranteed, The Easy Way!
A. Amazon Elastic Container Service(Amazon ECS)를 사용하세요. 대상 추적 조정을
사용하도록 Amazon ECS 서비스 Auto Scaling을 구성합니다. 최소 용량을 3으로 설정합니다.
가용 영역 속성을 사용하여 분산되도록 작업 배치 전략 유형을 설정합니다.
B. Amazon Elastic Kubernetes Service(Amazon EKS) 자체 관리형 노드를 사용합니다. 대상
추적 조정을 사용하도록 Application Auto Scaling을 구성합니다. 최소 용량을 3으로
설정하세요.
C. Amazon EC2 예약 인스턴스를 사용합니다. 분산 배치 그룹에서 3개의 EC2 인스턴스를
시작합니다. 대상 추적 조정을 사용하도록 Auto Scaling 그룹을 구성합니다. 최소 용량을
3으로 설정하세요.
D. AWS Lambda 함수를 사용합니다. VPC에 연결하도록 Lambda 함수를 구성합니다.
Lambda를 확장 가능한 대상으로 사용하도록 Application Auto Scaling을 구성합니다. 최소
용량을 3으로 설정하세요.
Answer: A
Explanation:
The company wants to deploy its containerized application workloads to a VPC across three
Availability Zones, with high availability and minimal changes to the application. The solution
that will meet these requirements with the least operational overhead is:
* Use Amazon Elastic Container Service (Amazon ECS). Amazon ECS is a fully managed
container orchestration service that allows you to run and scale containerized applications on
AWS. Amazon ECS eliminates the need for you to install, operate, and scale your own
cluster management infrastructure.
Amazon ECS also integrates with other AWS services, such as VPC, ELB, CloudFormation,
CloudWatch, IAM, and more.
* Configure Amazon ECS Service Auto Scaling to use target tracking scaling. Amazon ECS
Service Auto Scaling allows you to automatically adjust the number of tasks in your service
based on the demand or custom metrics. Target tracking scaling is a policy type that adjusts
the number of tasks in your service to keep a specified metric at a target value. For example,
you can use target tracking scaling to maintain a target CPU utilization or request count per
task for your service.
* Set the minimum capacity to 3. This ensures that your service always has at least three
tasks running across three Availability Zones, providing high availability and fault tolerance
for your application.
* Set the task placement strategy type to spread with an Availability Zone attribute. This
ensures that your tasks are evenly distributed across the Availability Zones in your cluster,
maximizing the availability of your service.
This solution will provide high availability across Availability Zones, require minimal changes
to the application, and reduce the operational overhead of managing your own cluster
infrastructure.
References:
* Amazon Elastic Container Service
* Amazon ECS Service Auto Scaling
* Target Tracking Scaling Policies for Amazon ECS Services
* Amazon ECS Task Placement Strategies
QUESTION NO: 364
249

IT Certification Guaranteed, The Easy Way!
회사는 애플리케이션과 함께 Amazon Aurora PostgreSQL 프로비저닝 클러스터를
사용합니다. 애플리케이션의 최대 트래픽은 30분에서 몇 시간 동안 하루에 여러 번
발생합니다.
애플리케이션의 최대 트래픽을 처리하기 위해 데이터베이스 용량이 프로비저닝되었지만
피크가 아닌 시간 동안 데이터베이스에서 용량이 낭비되었습니다. 회사는 데이터베이스
비용을 절감하려고 합니다.
최소한의 운영 노력으로 이러한 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?
A. 데이터베이스 활용도를 모니터링하도록 Amazon CloudWatch 경보를 설정합니다. 트래픽
양에 따라 데이터베이스 용량을 확장하거나 축소합니다.
B. 데이터베이스를 Auto Scaling 그룹의 Amazon EC2 인스턴스로 마이그레이션합니다.
트래픽 양에 따라 인스턴스 수를 늘리거나 줄입니다.
C. 데이터베이스를 Amazon Aurora Serverless DB 클러스터로 마이그레이션하여 트래픽 양에
따라 용량을 확장하거나 축소합니다.
D. 하루 시작 시 필요한 데이터베이스 용량을 프로비저닝하도록 AWS Lambda 함수를
예약합니다. 하루가 끝날 때 용량을 줄이도록 다른 Lambda 함수를 예약합니다.
Answer: C
Explanation:
* Requirement Analysis: The database experiences peak traffic multiple times a day but has
wasted capacity during non-peak hours. The goal is to reduce costs with minimal operational
effort.
* Aurora Serverless Overview: Aurora Serverless automatically adjusts database capacity
based on current demand, scaling up during peak times and scaling down during non-peak
times.
* Cost Efficiency: Aurora Serverless charges only for the capacity used, which is more cost-
effective than provisioning for peak traffic.
* Operational Efficiency: Aurora Serverless eliminates the need for manual scaling or
scheduling Lambda functions for capacity management.
* Implementation: Migrate the database from the provisioned Aurora PostgreSQL cluster to
an Aurora Serverless cluster.
References
* Amazon Aurora Serverless: Aurora Serverless Documentation
QUESTION NO: 365
한 회사에서 Amazon DynamoDB를 데이터베이스 계층으로 사용하는 서버리스
애플리케이션을 배포했습니다. 애플리케이션 사용자가 크게 증가했습니다. 회사는
데이터베이스 응답 시간을 밀리초에서 마이크로초로 향상하고 데이터베이스에 대한 요청을
캐시하기를 원합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. DynamoDB Accelerator(DAX)를 사용합니다.
B. 데이터베이스를 Amazon Redshift로 마이그레이션합니다.
C. 데이터베이스를 Amazon RDS로 마이그레이션합니다.
D. Redis용 Amazon ElastiCache를 사용합니다.
Answer: A
Explanation:
250

IT Certification Guaranteed, The Easy Way!
DynamoDB Accelerator (DAX) is a fully managed, highly available caching service built for
Amazon DynamoDB. DAX delivers up to a 10 times performance improvement-from
milliseconds to microseconds- even at millions of requests per second. DAX does all the
heavy lifting required to add in-memory acceleration to your DynamoDB tables, without
requiring developers to manage cache invalidation, data population, or cluster management.
Now you can focus on building great applications for your customers without worrying about
performance at scale. You do not need to modify application logic because DAX is
compatible with existing DynamoDB API calls. This solution will meet the requirements with
the least operational overhead, as it does not require any code development or manual
intervention.
References:
* 1 provides an overview of Amazon DynamoDB Accelerator (DAX) and its benefits.
* 2 explains how to use DAX with DynamoDB for in-memory acceleration.
QUESTION NO: 366
회사는 fts 기업 데이터 센터의 대규모 NAS(Network-Attached Storage) 시스템에
700테라바이트의 데이터를 저장하고 있습니다. 이 회사는 10Gbps AWS Direct Connect
연결을 사용하는 하이브리드 환경을 보유하고 있습니다.
규제 기관의 감사 후 회사는 90일 이내에 데이터를 클라우드로 옮길 수 있습니다. 회사는
데이터를 중단 없이 효율적으로 이동해야 합니다. 회사는 여전히 이전 기간 동안 데이터에
액세스하고 데이터를 업데이트할 수 있어야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 회사 데이터 센터에서 AWS DataSync 에이전트를 생성합니다. 데이터 전송 작업을
만듭니다. Amazon S3 버킷으로 전송을 시작합니다.
B. AWS Snowball Edge Storage Optimized 디바이스에 데이터를 백업합니다. 디바이스를
AWS 데이터 센터로 배송합니다. 온프레미스 파일 시스템에 대상 Amazon S3 버킷을
탑재합니다.
C. rsync를 사용하여 Direct Connect 연결을 통해 로컬 스토리지에서 지정된 Amazon S3
버킷으로 직접 데이터를 복사합니다.
D. 테이프에 데이터를 백업합니다. 테이프를 AWS 데이터 센터로 배송합니다. 온프레미스
파일 시스템에 대상 Amazon S3 버킷을 탑재합니다.
Answer: A
Explanation:
This answer is correct because it meets the requirements of moving the data efficiently and
without disruption, and still being able to access and update the data during the transfer
window. AWS DataSync is an online data movement and discovery service that simplifies
and accelerates data migrations to AWS and helps you move data quickly and securely
between on-premises storage, edge locations, other clouds, and AWS Storage. You can
create an AWS DataSync agent in the corporate data center to connect your NAS system to
AWS over the Direct Connect connection. You can create a data transfer task to specify the
source location, destination location, and options for transferring the data. You can start the
transfer to an Amazon S3 bucket and monitor the progress of the task. DataSync
automatically encrypts data in transit and verifies data integrity during transfer. DataSync also
supports incremental transfers, which means that only files that have changed since the last
transfer are copied. This way, you can ensure that your data is synchronized between your
251

IT Certification Guaranteed, The Easy Way!
NAS system and S3 bucket, and you can access and update the data during the transfer
window.
References:
* https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html
* https://docs.aws.amazon.com/datasync/latest/userguide/how-datasync-works.html
QUESTION NO: 367
회사는 온프레미스 서버에서 Amazon EC2 인스턴스로 애플리케이션을 마이그레이션하고
있습니다. 마이그레이션 설계 요구 사항의 일부로 솔루션 설계자는 인프라 지표 경보를
구현해야 합니다. CPU 사용률이 짧은 시간 동안 50% 이상으로 증가하는 경우 회사에서는
조치를 취할 필요가 없습니다. 그러나 CPU 사용률이 50% 이상으로 증가하고 동시에
디스크의 읽기 IOPS가 높을 경우 회사는 가능한 한 빨리 조치를 취해야 합니다. 솔루션
설계자는 또한 허위 경보를 줄여야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 가능한 경우 Amazon CloudWatch 복합 경보를 생성합니다.
B. Amazon CloudWatch 대시보드를 생성하여 지표를 시각화하고 문제에 신속하게
대응합니다.
C. Amazon CloudWatch Synthetics 카나리아를 생성하여 애플리케이션을 모니터링하고
경보를 발생시킵니다.
D. 가능한 경우 여러 지표 임계값을 사용하여 단일 Amazon CloudWatch 지표 경보를
생성합니다.
Answer: A
Explanation:
Composite alarms determine their states by monitoring the states of other alarms. You can
**use composite alarms to reduce alarm noise**. For example, you can create a composite
alarm where the underlying metric alarms go into ALARM when they meet specific
conditions. You then can set up your composite alarm to go into ALARM and send you
notifications when the underlying metric alarms go into ALARM by configuring the underlying
metric alarms never to take actions. Currently, composite alarms can take the following
actions:
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_Composite_Alar
m.html
QUESTION NO: 368
회사에서는 회사의 기본 웹사이트를 사용할 수 없는 경우 사용자에게 백업 정적 오류
페이지를 표시하려고 합니다. 기본 웹 사이트의 DNS 레코드는 Amazon Route 53에서
호스팅됩니다. 도메인은 ALB(Application Load Balancer)를 가리키고 있습니다. 회사에는
변경 사항과 인프라 오버헤드를 최소화하는 솔루션이 필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 지연 시간 라우팅 정책을 사용하도록 Route 53 레코드를 업데이트합니다. 트래픽이 가장
반응이 빠른 엔드포인트로 전송되도록 Amazon S3 버킷에서 호스팅되는 정적 오류 페이지를
레코드에 추가합니다.
B. Route 53 액티브-패시브 장애 조치 구성을 설정합니다. Route 53 상태 확인에서 ALB
엔드포인트가 비정상이라고 판단되면 Amazon S3 버킷에 호스팅되는 정적 오류 페이지로
트래픽을 보냅니다.
252

IT Certification Guaranteed, The Easy Way!
C. 정적 오류 페이지를 엔드포인트로 호스팅하는 Amazon EC2 인스턴스와 ALB를 사용하여
Route 53 활성-활성 구성을 설정합니다. ALB에 대한 상태 확인이 실패한 경우에만
인스턴스에 요청을 보내도록 Route 53을 구성합니다.
D. 다중 응답 라우팅 정책을 사용하도록 Route 53 레코드를 업데이트합니다. 상태 확인을
만듭니다. 상태 확인을 통과하면 웹사이트로 트래픽을 보냅니다. 상태 확인을 통과하지 못한
경우 Amazon S3에서 호스팅되는 정적 오류 페이지로 트래픽을 보냅니다.
Answer: B
Explanation:
This solution meets the requirements of directing users to a backup static error page if the
primary website is unavailable, minimizing changes and infrastructure overhead. Route 53
active-passive failover configuration can route traffic to a primary resource when it is healthy
or to a secondary resource when the primary resource is unhealthy. Route 53 health checks
can monitor the health of the ALB endpoint and trigger the failover when needed. The static
error page can be hosted in an S3 bucket that is configured as a website, which is a simple
and cost-effective way to serve static content.
Option A is incorrect because using a latency routing policy can route traffic based on the
lowest network latency for users, but it does not provide failover functionality. Option C is
incorrect because using an active- active configuration with the ALB and an EC2 instance can
increase the infrastructure overhead and complexity, and it does not guarantee that the EC2
instance will always be healthy. Option D is incorrect because using a multivalue answer
routing policy can return multiple values for a query, but it does not provide failover
functionality.
References:
* https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-failover.html
* https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html
QUESTION NO: 369
A company needs a solution to enforce data encryption at rest on Amazon EC2 instances.
The solution must automatically identify noncompliant resources and enforce compliance
policies on findings.
Which solution will meet these requirements with the LEAST administrative overhead?
A. Use an 1AM policy that allows users to create only encrypted Amazon Elastic Block Store
(Amazon EBS) volumes. Use AWS Config and AWS Systems Manager to automate the
detection and remediation of unencrypted EBS volumes.
B. Use AWS Key Management Service (AWS KMS) to manage access to encrypted Amazon
Elastic Block Store (Amazon EBS) volumes. Use AWS Lambda and Amazon EventBridge to
automate the detection and remediation of unencrypted EBS volumes.
C. Use Amazon Macie to detect unencrypted Amazon Elastic Block Store (Amazon EBS)
volumes. Use AWS Systems Manager Automation rules to automatically encrypt existing and
new EBS volumes.
D. Amazon Inspector를 사용하여 암호화되지 않은 Amazon Elastic Block Store(Amazon
EBS) 볼륨을 감지합니다. AWS Systems Manager Automation 규칙을 사용하여 기존 및 새
EBS 볼륨을 자동으로 암호화합니다.
Answer: A
253

IT Certification Guaranteed, The Easy Way!
Explanation:
The best solution to enforce encryption at rest for Amazon EBS volumes is to use an IAM
policy to restrict the creation of unencrypted volumes. To automatically identify and remediate
unencrypted volumes, you can use AWS Config rules, which continuously monitor the
compliance of resources, and AWS Systems Manager to automate the remediation by
encrypting existing unencrypted volumes. This setup requires minimal administrative
overhead while ensuring compliance.
* Option B (KMS): KMS is for managing encryption keys, but Config and Systems Manager
provide a better solution for automatic detection and enforcement.
* Option C (Macie): Macie is for data classification and is not suitable for this use case.
* Option D (Inspector): Inspector is used for security vulnerabilities, not encryption
compliance.
AWS References:
* AWS Config Rules
* AWS Systems Manager
QUESTION NO: 370
한 회사가 여러 마이크로서비스로 구성된 애플리케이션을 구축하고 있습니다. 회사는
컨테이너 기술을 사용하여 AWS에 소프트웨어를 배포하기로 결정했습니다. 회사에는 유지
관리 및 확장을 위한 지속적인 노력을 최소화하는 솔루션이 필요합니다. 회사는 추가
인프라를 관리할 수 없습니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 조치 조합을 취해야 합니까? (2개를
선택하세요.)
A. Amazon Elastic Container Service(Amazon ECS) 클러스터를 배포합니다.
B. 여러 가용 영역에 걸쳐 있는 Amazon EC2 인스턴스에 Kubernetes 제어 플레인을
배포합니다.
C. Amazon EC2 시작 유형을 사용하여 Amazon Elastic Container Service(Amazon ECS)
서비스를 배포합니다. 2보다 크거나 같은 원하는 작업 번호 수준을 지정합니다.
D. Fargate 시작 유형을 사용하여 Amazon Elastic Container Service(Amazon ECS) 서비스를
배포합니다. 2보다 크거나 같은 원하는 작업 번호 수준을 지정합니다.
E. 여러 가용 영역에 걸쳐 있는 Amazon EC2 인스턴스에 Kubernetes 작업자 노드를
배포합니다. 각 마이크로서비스에 대해 두 개 이상의 복제본을 지정하는 배포를 만듭니다.
Answer: A D
Explanation:
AWS Fargate is a technology that you can use with Amazon ECS to run containers without
having to manage servers or clusters of Amazon EC2 instances. With Fargate, you no longer
have to provision, configure, or scale clusters of virtual machines to run containers.
https://docs.aws.amazon.com/AmazonECS/latest
/userguide/what-is-fargate.html
QUESTION NO: 371
회사는 많은 독립형 AWS 계정에서 통합된 다중 계정 아키텍처로 이동하려고 합니다. 회사는
다양한 사업부에 대해 많은 새로운 AWS 계정을 생성할 계획입니다. 회사는 중앙 집중식 회사
디렉터리 서비스를 사용하여 이러한 AWS 계정에 대한 액세스를 인증해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 작업 조합을 권장해야 합니까?
254

IT Certification Guaranteed, The Easy Way!
(2개를 선택하세요.)
A. 모든 기능이 활성화된 AWS Organizations에서 새 조직을 생성합니다. 조직에 새 AWS
계정을 생성합니다.
B. Amazon Cognito 자격 증명 풀을 설정합니다. Amazon Cognito 인증을 허용하도록 AWS
1AM Identity Center(AWS Single Sign-On)를 구성합니다.
C. AWS 계정을 관리하기 위한 서비스 제어 정책(SCP)을 구성합니다. AWS Directory
Service에 AWS 1AM Identity Center(AWS Single Sign-On)를 추가합니다.
D. AWS Organizations에서 새 조직을 생성합니다. AWS Directory Service를 직접 사용하도록
조직의 인증 메커니즘을 구성합니다.
E. 조직에 AWS 1AM Identity Center(AWS Single Sign-On)를 설정합니다. 1AM ID 센터를
구성하고 이를 회사의 회사 디렉터리 서비스와 통합합니다.
Answer: A E
Explanation:
AWS Organizations is a service that helps users centrally manage and govern multiple AWS
accounts. It allows users to create organizational units (OUs) to group accounts based on
business needs or other criteria. It also allows users to define and attach service control
policies (SCPs) to OUs or accounts to restrict the actions that can be performed by the
accounts1. By creating a new organization in AWS Organizations with all features turned on,
the solution can consolidate and manage the new AWS accounts for different business units.
AWS IAM Identity Center (formerly known as AWS Single Sign-On) is a service that provides
single sign-on access for all of your AWS accounts and cloud applications. It connects with
Microsoft Active Directory through AWS Directory Service to allow users in that directory to
sign in to a personalized AWS access portal using their existing Active Directory user names
and passwords. From the AWS access portal, users have access to all the AWS accounts
and cloud applications that they have permissions for2. By setting up IAM Identity Center in
the organization and integrating it with the company's corporate directory service, the solution
can authenticate access to these AWS accounts using a centralized corporate directory
service.
B: Set up an Amazon Cognito identity pool. Configure AWS 1AM Identity Center (AWS Single
Sign-On) to accept Amazon Cognito authentication. This solution will not meet the
requirement of authenticating access to these AWS accounts by using a centralized
corporate directory service, as Amazon Cognito is a service that provides user sign-up, sign-
in, and access control for web and mobile applications, not for corporate directory services3.
C: Configure a service control policy (SCP) to manage the AWS accounts. Add AWS 1AM
Identi-ty Center (AWS Single Sign-On) to AWS Directory Service. This solution will not work,
as SCPs are used to restrict the actions that can be performed by the accounts in an
organization, not to manage the accounts themselves1
. Also, IAM Identity Center cannot be added to AWS Directory Service, as it is a separate
service that connects with Microsoft Active Directory through AWS Directory Service2.
D: Create a new organization in AWS Organizations. Configure the organization's
authentication mechanism to use AWS Directory Service directly. This solution will not work,
as AWS Organizations does not have an authentication mechanism that can use AWS
Directory Service directly. AWS Organizations relies on IAM Identity Center to provide single
sign-on access for the accounts in an organization.
Reference URL:
255

IT Certification Guaranteed, The Easy Way!
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_integrate_services.html
QUESTION NO: 372
애플리케이션은 Amazon RDS MySQL DB 인스턴스를 사용합니다. RDS 데이터베이스의
디스크 공간이 부족해지고 있습니다.
솔루션 설계자는 다운타임 없이 디스크 공간을 늘리고 싶어합니다.
최소한의 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. RDS에서 스토리지 자동 확장을 활성화합니다.
B. RDS 데이터베이스 인스턴스 크기를 늘립니다.
C. RDS 데이터베이스 인스턴스 스토리지 유형을 프로비저닝된 IOPS로 변경합니다.
D. RDS 데이터베이스 백업, 저장 용량 증가, 데이터베이스 복원, 이전 인스턴스 중지
Answer: A
Explanation:
https://aws.amazon.com/about-aws/whats-new/2019/06/rds-storage-auto-scaling/
QUESTION NO: 373
솔루션 아키텍트는 초당 최대 5,000개의 메시지를 처리할 수 있는 솔루션을 구현해야 합니다.
솔루션은 메시지를 여러 소비자에게 이벤트로 게시해야 합니다. 메시지 크기는 최대
500KB입니다. 메시지 소비자는 여러 프로그래밍 언어를 사용하여 최소한의 지연 시간으로
메시지를 사용할 수 있어야 합니다. 솔루션은 게시된 메시지를 3개월 이상 보관해야 합니다.
솔루션은 메시지의 엄격한 순서를 적용해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon Kinesis Data Streams 데이터 스트림에 메시지를 게시합니다. 향상된 팬아웃을
활성화합니다. 전용 처리량을 사용하여 소비자가 데이터 스트림을 수집하도록 합니다.
B. Amazon Simple Notification Service(Amazon SNS) 토픽에 메시지를 게시합니다. 소비자가
Amazon Simple Queue Service(Amazon SQS) FIFO 큐를 사용하여 토픽을 구독하도록
합니다.
C. Amazon EventBridge에 메시지를 게시합니다. 각 소비자가 소비자의 대상에게 메시지를
전달하는 규칙을 만들 수 있도록 허용합니다.
D. Amazon Simple Notification Service(Amazon SNS) 토픽에 메시지를 게시합니다. 소비자가
Amazon Data Firehose를 사용하여 토픽을 구독하도록 합니다.
Answer: A
Explanation:
* A. Kinesis Data Streams: Supports high throughput, strict ordering, multiple consumers, and
data retention for 365 days.
* B. SNS + SQS FIFO: Can enforce ordering but lacks native support for 500 KB messages
and retention requirements.
* C. EventBridge: Lacks strict ordering and message size compatibility.
* D. SNS + Firehose: Not designed for strict ordering or large message sizes.
References: Amazon Kinesis Data Streams
QUESTION NO: 374
회사는 애플리케이션을 서버리스 솔루션으로 이동하려고 합니다. 서버리스 솔루션은 SL을
사용해 기존 데이터와 신규 데이터를 분석해야 합니다. 회사는 Amazon S3 버킷에 데이터를
저장합니다. 데이터는 암호화가 필요하며 다른 AWS 리전에 복제되어야 합니다.
256

IT Certification Guaranteed, The Easy Way!
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 새 S3 버킷을 생성합니다. 새 S3 버킷에 데이터를 로드합니다. S3 교차 리전 복제(CRR)를
사용하여 암호화된 객체를 다른 리전의 S3 버킷에 복제합니다. AWS KMS 다중 리전
시스템(SSE-KMS)과 함께 서버 측 암호화를 사용합니다. Amazon Athena를 사용하여
데이터를 쿼리합니다.
B. 새 S3 버킷을 생성합니다. 새 S3 버킷에 데이터를 로드합니다. S3 교차 리전 복제(CRR)를
사용하여 암호화된 객체를 다른 리전의 S3 버킷에 복제합니다. AWS KMS 다중 리전 키(SSE-
KMS)로 서버 측 암호화를 사용합니다. Amazon RDS를 사용하여 데이터를 쿼리합니다.
C. 기존 S3 버킷에 데이터를 로드합니다. S3 교차 리전 복제(CRR)를 사용하여 암호화된
객체를 다른 리전의 S3 버킷에 복제합니다. Amazon S3 관리형 암호화 키(SSE-S3)로 서버 측
암호화를 사용합니다. Amazon Athena를 사용하여 데이터를 쿼리합니다.
D. 기존 S3 버킷에 데이터를 로드합니다. S3 교차 리전 복제(CRR)를 사용하여 암호화된
객체를 다른 리전의 S3 버킷에 복제합니다. Amazon S3 관리형 암호화 키(SSE-S3)로 서버 측
암호화를 사용합니다. Amazon RDS를 사용하여 데이터를 쿼리합니다.
Answer: A
Explanation:
This solution meets the requirements of a serverless solution, encryption, replication, and
SQL analysis with the least operational overhead. Amazon Athena is a serverless interactive
query service that can analyze data in S3 using standard SQL. S3 Cross-Region Replication
(CRR) can replicate encrypted objects to an S3 bucket in another Region automatically.
Server-side encryption with AWS KMS multi-Region keys (SSE- KMS) can encrypt the data at
rest using keys that are replicated across multiple Regions. Creating a new S3 bucket can
avoid potential conflicts with existing data or configurations.
Option B is incorrect because Amazon RDS is not a serverless solution and it cannot query
data in S3 directly.
Option C is incorrect because server-side encryption with Amazon S3 managed encryption
keys (SSE-S3) does not use KMS keys and it does not support multi-Region replication.
Option D is incorrect because Amazon RDS is not a serverless solution and it cannot query
data in S3 directly. It is also incorrect for the same reason as option C.
References:
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-4.html
* https://aws.amazon.com/blogs/storage/considering-four-different-replication-options-for-
data-in- amazon-s3/
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html
* https://aws.amazon.com/athena/
QUESTION NO: 375
한 회사가 가장 최근 청구서에서 Amazon EC2 비용의 증가를 관찰했습니다. 청구 팀이 몇
개의 EC2 인스턴스에 대한 인스턴스 유형의 원치 않는 수직적 확장을 발견했습니다. 솔루션
설계자는 지난 2개월 동안의 EC2 비용을 비교하는 그래프를 생성하고 이를 수행해야 합니다.
-수직 확장의 근본 원인을 식별하기 위한 심층 분석 솔루션 설계자는 운영 오버헤드를
최소화하면서 어떻게 정보를 생성해야 합니까?
A. AWS 예산을 사용하여 예산 보고서를 생성하고 인스턴스 유형에 따라 EC2 비용을
비교합니다.
B. Cost Explorer의 세분화된 필터링 기능을 사용하여 인스턴스 유형에 따른 EC2 비용에 대한
257

IT Certification Guaranteed, The Easy Way!
심층 분석을 수행합니다.
C. AWS Billing and Cost Management 대시보드의 그래프를 사용하여 지난 2개월 동안
인스턴스 유형을 기준으로 EC2 비용을 비교합니다.
D. AWS 비용 및 사용 보고서를 사용하여 보고서를 생성하고 Amazon S3 버킷으로 보냅니다.
소스로 Amazon S3와 함께 Amazon QuickSight를 사용하여 인스턴스 유형을 기반으로 대화형
그래프를 생성합니다.
Answer: B
Explanation:
AWS Cost Explorer is a tool that enables you to view and analyze your costs and usage. You
can explore your usage and costs using the main graph, the Cost Explorer cost and usage
reports, or the Cost Explorer RI reports. You can view data for up to the last 12 months,
forecast how much you're likely to spend for the next
12 months, and get recommendations for what Reserved Instances to purchase. You can
use Cost Explorer to identify areas that need further inquiry and see trends that you can use
to understand your costs. https://docs.
aws.amazon.com/cost-management/latest/userguide/ce-what-is.html
QUESTION NO: 376
한 회사에서 Oracle 데이터베이스를 AWS로 마이그레이션하려고 합니다. 데이터베이스는
고해상도이고 지리 코드로 식별되는 수백만 개의 지리 정보 시스템(GIS) 이미지를 포함하는
단일 테이블로 구성됩니다.
자연재해가 발생하면 몇 분마다 수만 장의 이미지가 업데이트됩니다. 각 지리적 코드에는
이와 연결된 단일 이미지 또는 행이 있습니다. 회사는 이러한 이벤트 중에 가용성과 확장성이
뛰어난 솔루션을 원합니다. 어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로
충족합니까?
A. 데이터베이스 테이블에 이미지와 지리 코드를 저장합니다. Amazon RDS 다중 AZ DB
인스턴스에서 실행되는 Oracle을 사용합니다.
B. Amazon S3 버킷에 이미지를 저장합니다. 지리적 코드를 키로, 이미지 S3 URL을 값으로
사용하여 Amazon DynamoDB를 사용합니다.
C. Amazon DynamoDB 테이블에 이미지와 지리 코드를 저장합니다. 로드가 많은 시간 동안
DynamoDB Accelerator(DAX) 구성
D. Amazon S3 버킷에 이미지 저장 데이터베이스 테이블에 지리적 코드 및 이미지 S3 URL
저장 Amazon RDS 다중 AZ DB 인스턴스에서 실행되는 Oracle을 사용합니다.
Answer: B
Explanation:
Amazon S3 is a highly scalable, durable, and cost-effective object storage service that can
store millions of images1. Amazon DynamoDB is a fully managed NoSQL database that can
handle high throughput and low latency for key-value and document data2. By using S3 to
store the images and DynamoDB to store the geographic codes and image S3 URLs, the
solution can achieve high availability and scalability during natural disasters. It can also
leverage DynamoDB's features such as caching, auto-scaling, and global tables to improve
performance and reduce costs2.
A: Store the images and geographic codes in a database table Use Oracle running on an
Amazon RDS Multi- AZ DB instance. This solution will not meet the requirement of scalability
and cost-effectiveness, as Oracle is a relational database that may not handle large volumes
258

IT Certification Guaranteed, The Easy Way!
of unstructured data such as images efficiently3. It also involves higher licensing and
operational costs than S3 and DynamoDB12.
C: Store the images and geographic codes in an Amazon DynamoDB table Configure
DynamoDB Accelerator (DAX) during times of high load. This solution will not meet the
requirement of cost-effectiveness, as storing images in DynamoDB will consume more
storage space and incur higher charges than storing them in S312.
It will also require additional configuration and management of DAX clusters to handle high
load.
D: Store the images in Amazon S3 buckets Store geographic codes and image S3 URLs in a
database table Use Oracle running on an Amazon RDS Multi-AZ DB instance. This solution
will not meet the requirement of scalability and cost-effectiveness, as Oracle is a relational
database that may not handle high throughput and low latency for key-value data such as
geographic codes efficiently3. It also involves higher licensing and operational costs than
DynamoDB2.
Reference URL: https://dynobase.dev/dynamodb-vs-s3/
QUESTION NO: 377
한 회사에서 다중 계층 애플리케이션을 온프레미스에서 AWS 클라우드로 이동하여
애플리케이션의 성능을 개선하려고 합니다. 애플리케이션은 RESTful 서비스를 통해 서로
통신하는 애플리케이션 계층으로 구성됩니다. 한 계층이 과부하되면 트랜잭션이 삭제됩니다.
솔루션 설계자는 이러한 문제를 해결하고 애플리케이션을 현대화하는 솔루션을 설계해야
합니다.
어떤 솔루션이 이러한 요구 사항을 충족하고 운영상 가장 효율적입니까?
A. Amazon API Gateway를 사용하고 애플리케이션 계층으로 AWS Lambda 기능에 대한 직접
트랜잭션을 사용합니다. 애플리케이션 서비스 간의 통신 계층으로 Amazon Simple Queue
Service(Amazon SQS)를 사용합니다.
B. Amazon CloudWatch 메트릭을 사용하여 애플리케이션 성능 기록을 분석하여 성능 실패 시
서버의 최고 사용률을 확인합니다. 최대 요구 사항을 충족하도록 애플리케이션 서버의
Amazon EC2 인스턴스 크기를 늘립니다.
C. Amazon Simple Notification Service(Amazon SNS)를 사용하여 Auto Scaling 그룹의
Amazon EC2에서 실행되는 애플리케이션 서버 간의 메시징을 처리합니다. Amazon
CloudWatch를 사용하여 SNS 대기열 길이를 모니터링하고 필요에 따라 확장 및 축소합니다.
D. Amazon Simple Queue Service(Amazon SQS)를 사용하여 Auto Scaling 그룹의 Amazon
EC2에서 실행되는 애플리케이션 서버 간의 메시징을 처리합니다. Amazon CloudWatch를
사용하여 SQS 대기열 길이를 모니터링하고 통신 실패가 감지되면 확장합니다.
Answer: A
Explanation:
https://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-
apigateway-s3-dynamodb- cognito/module-4/ Build a Serverless Web Application with AWS
Lambda, Amazon API Gateway, AWS Amplify, Amazon DynamoDB, and Amazon Cognito.
This example showed similar setup as question: Build a Serverless Web Application with
AWS Lambda, Amazon API Gateway, AWS Amplify, Amazon DynamoDB, and Amazon
Cognito
QUESTION NO: 378
259

IT Certification Guaranteed, The Easy Way!
한 회사에 수천 명의 사용자가 있는 웹 애플리케이션이 있습니다. 이 애플리케이션은
사용자가 업로드한 이미지 8~10개를 사용하여 AI 이미지를 생성합니다. 사용자는 생성된 Al
이미지를 6시간마다 한 번씩 다운로드할 수 있습니다. 또한 회사는 생성된 AI 이미지를
사용자가 언제든지 다운로드할 수 있는 프리미엄 사용자 옵션도 제공합니다. 회사는 사용자가
업로드한 이미지를 사용하여 1년에 두 번 AI 모델 훈련을 실행합니다. 회사에는 이미지를
저장할 스토리지 솔루션이 필요합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 스토리지 솔루션은 무엇입니까?
A. 업로드된 이미지를 Amazon S3 Glacier Deep Archive로 이동합니다. 프리미엄 사용자 생성
AI 이미지를 S3 Standard로 이동합니다. 프리미엄이 아닌 사용자 생성 Al 이미지를 S3
Standard-Infrequent Access(S3 Standard-IA)로 이동합니다.
B. 업로드된 이미지를 Amazon S3 Glacier Deep Archive로 이동합니다. 생성된 모든 AI
이미지를 S3 Glacier 유연한 검색으로 이동합니다.
C. 업로드된 이미지를 Amazon S3 One Zone-Infrequent Access로 이동합니다. {S3 One
Zone-IA) 프리미엄 사용자 생성 Al 이미지를 S3 Standard로 이동합니다. 프리미엄이 아닌
사용자 생성 Al 이미지를 S3 Standard-Infrequent Access(S3 Standard-IA)로 이동합니다.
D. 업로드된 이미지를 Amazon S3 One Zone-Infrequent Access로 이동합니다. {S3 One
Zone-IA) 생성된 모든 Al 이미지를 S3 Glacier 유연한 검색으로 이동합니다.
Answer: C
Explanation:
* S3 One Zone-IA:
* Suitable for infrequently accessed data that doesn't require multiple Availability Zone
resilience.
* Cost-effective for storing user-uploaded images that are only used for AI model training
twice a year.
* S3 Standard:
* Ideal for frequently accessed data with high durability and availability.
* Store premium user-generated AI images here to ensure they are readily available for
download at any time.
* S3 Standard-IA:
* Cost-effective storage for data that is accessed less frequently but still requires rapid
retrieval.
* Store non-premium user-generated AI images here, as these images are only downloaded
once every 6 hours, making it a good balance between cost and accessibility.
* Cost-Effectiveness: This solution optimizes storage costs by categorizing data based on
access patterns and durability requirements, ensuring that each type of data is stored in the
most cost-effective manner.
References:
* Amazon S3 Storage Classes
* S3 One Zone-IA
QUESTION NO: 379
한 회사는 품목 가격을 기준으로 세금 계산에 대한 조회를 자동화하는 API를 사용자에게
제공합니다. 회사에서는 연휴 기간에만 더 많은 문의가 발생하여 응답 시간이 느려집니다.
솔루션 아키텍트는 확장 가능하고 탄력적인 솔루션을 설계해야 합니다.
이를 달성하기 위해 솔루션 설계자는 무엇을 해야 합니까?
260

IT Certification Guaranteed, The Easy Way!
A. Amazon EC2 인스턴스에 호스팅되는 API를 제공합니다. EC2 인스턴스는 API 요청이
이루어질 때 필요한 계산을 수행합니다.
B. 항목 이름을 허용하는 Amazon API Gateway를 사용하여 REST API를 설계합니다. API
Gateway는 세금 계산을 위해 항목 이름을 AWS Lambda에 전달합니다.
C. 뒤에 두 개의 Amazon EC2 인스턴스가 있는 Application Load Balancer를 생성합니다. EC2
인스턴스는 받은 항목 이름에 대한 세금을 계산합니다.
D. Amazon EC2 인스턴스에 호스팅된 API와 연결되는 Amazon API Gateway를 사용하여
REST API를 설계합니다. API Gateway는 세금 계산을 위해 항목 이름을 수락하고 EC2
인스턴스에 전달합니다.
Answer: B
Explanation:
Lambda server-less is scalable and elastic than EC2 api gateway solution
QUESTION NO: 380
한 회사가 AWS 클라우드를 사용하는 새로운 모바일 앱의 아키텍처를 설계하고 있습니다. 이
회사는 AWS Organizations의 조직 단위(OU)를 사용하여 계정을 관리합니다. 이 회사는
민감하고 민감하지 않은 1AM ID 값을 사용하여 Amazon EC2 인스턴스에 데이터 민감성
태그를 지정하려고 합니다. 태그를 삭제하거나 태그 없이 인스턴스를 만들 수 없어야 합니다.
어떤 단계 조합이 이러한 요구 사항을 충족할까요? (두 가지를 선택하세요.)
A. 조직에서 데이터 민감도 태그 키와 필요한 값을 지정하는 새 태그 정책을 만듭니다. EC2
인스턴스에 대한 태그 값을 적용합니다. 태그 정책을 적절한 OU에 연결합니다.
B. 조직에서 데이터 민감도 태그 키와 필요한 태그 값을 지정하는 새 서비스 제어 정책(SCP)을
만듭니다. EC2 인스턴스에 대한 태그 값을 적용합니다. SCP를 적절한 OU에 연결합니다.
C. 태그 키가 지정되지 않은 경우 실행 중인 인스턴스를 거부하는 태그 정책을 만듭니다. ID가
태그를 삭제하지 못하도록 하는 다른 태그 정책을 만듭니다. 태그 정책을 적절한 OU에
연결합니다.
D. 태그 키가 지정되지 않은 경우 인스턴스 생성을 거부하는 서비스 제어 정책(SCP)을
만듭니다. ID가 태그를 삭제하지 못하도록 하는 다른 SCP를 만듭니다. SCP를 적절한 OU에
연결합니다.
E. EC2 인스턴스가 데이터 민감도 태그와 지정된 값을 사용하는지 확인하기 위한 AWS Config
규칙을 만듭니다. 비준수 리소스가 발견되면 리소스를 삭제하도록 AWS Lambda 함수를
구성합니다.
Answer: A D
Explanation:
To meet the requirements for tagging and preventing instance creation or deletion without
proper tags, the company can use a combination of AWS Organizations tag policies and
service control policies (SCPs).
* Tag Policies: These enforce specific tag values across resources. Creating a tag policy with
required values (e.g., sensitive, non-sensitive) and attaching it to the appropriate
organizational unit (OU) ensures consistency in tagging.
* SCPs: SCPs can be used to enforce compliance by preventing instance creation without a
tag and preventing tag deletion. These policies control actions at the account level across the
organization.
Key AWS features:
261

IT Certification Guaranteed, The Easy Way!
* Tag Policies help standardize tags across accounts, and SCPs enforce governance by
restricting actions that violate the policies.
* AWS Documentation: AWS best practices recommend using tag policies and SCPs to
enforce compliance across multiple accounts within AWS Organizations.
QUESTION NO: 381
한 회사는 최근 단일 AWS 리전의 Amazon EC2 인스턴스에 애플리케이션을 다시 호스팅하여
웹 애플리케이션을 AWS로 마이그레이션했습니다. 회사는 가용성이 높고 내결함성을 갖도록
애플리케이션 아키텍처를 재설계하려고 합니다. 트래픽은 실행 중인 모든 EC2 인스턴스에
무작위로 도달해야 합니다.
이러한 요구 사항을 충족하기 위해 회사는 어떤 단계 조합을 취해야 합니까? (2개를
선택하세요.)
A. Amazon Route 53 장애 조치 라우팅 정책을 생성합니다.
B. Amazon Route 53 가중치 기반 라우팅 정책을 생성합니다.
C. Amazon Route 53 다중 응답 라우팅 정책을 생성합니다.
D. 3개의 EC2 인스턴스를 시작합니다. 즉, 하나의 가용 영역에 2개의 인스턴스가 있고 다른
가용 영역에 1개의 인스턴스가 있습니다.
E. 4개의 EC2 인스턴스를 시작합니다. 하나의 가용 영역에 2개의 인스턴스가 있고 다른 가용
영역에 2개의 인스턴스가 있습니다.
Answer: C E
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/multivalue-versus-simple-
policies/
QUESTION NO: 382
회사는 온프레미스 Oracle 관계형 데이터베이스에 데이터를 저장합니다. 회사는 분석을 위해
Amazon Aurora PostgreSQL에서 데이터를 제공해야 합니다. 회사는 AWS Site-to-Site VPN
연결을 사용하여 온프레미스 네트워크를 AWS에 연결합니다.
회사는 Aurora PostgreSQL로 마이그레이션하는 동안 소스 데이터베이스에서 발생하는 변경
사항을 포착해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. AWS Schema Conversion Tool(AWS SCT)을 사용하여 Oracle 스키마를 Aurora
PostgreSQL 스키마로 변환합니다. AWS Database Migration Service(AWS DMS) 전체 로드
마이그레이션 작업을 사용하여 데이터를 마이그레이션합니다.
B. AWS DataSync를 사용하여 데이터를 Amazon S3 버킷으로 마이그레이션합니다. Aurora
PostgreSQL aws_s3 확장을 사용하여 S3 데이터를 Aurora PostgreSQL로 가져옵니다.
C. AWS Schema Conversion Tool(AWS SCT)을 사용하여 Oracle 스키마를 Aurora
PostgreSQL 스키마로 변환합니다. AWS Database Migration Service(AWS DMS)를 사용하여
기존 데이터를 마이그레이션하고 진행 중인 변경 사항을 복제합니다.
D. AWS Snowball 장치를 사용하여 데이터를 Amazon S3 버킷으로 마이그레이션합니다.
Aurora PostgreSQL aws_s3 확장을 사용하여 S3 데이터를 Aurora PostgreSQL로 가져옵니다.
Answer: C
Explanation:
For the migration of data from an on-premises Oracle database to Amazon Aurora
262

IT Certification Guaranteed, The Easy Way!
PostgreSQL, this solution effectively handles schema conversion, data migration, and
ongoing data replication.
* AWS Schema Conversion Tool (SCT): SCT is used to convert the Oracle database schema
to a format compatible with Aurora PostgreSQL. This tool automatically converts the
database schema and code objects, like stored procedures, to the target database engine.
* AWS Database Migration Service (DMS): DMS is employed to perform the data migration. It
supports both full-load migrations (for initial data transfer) and continuous replication of
ongoing changes (Change Data Capture, or CDC). This ensures that any updates to the
Oracle database during the migration are captured and applied to the Aurora PostgreSQL
database, minimizing downtime.
* Why Not Other Options?:
* Option A (SCT + DMS full-load only): This option does not capture ongoing changes, which
is crucial for a live database migration to ensure data consistency.
* Option B (DataSync + S3): AWS DataSync is more suited for file transfers rather than
database migrations, and it doesn't support ongoing change replication.
* Option D (Snowball + S3): Snowball is typically used for large-scale data transfers that don't
require continuous synchronization, making it less suitable for this scenario where ongoing
changes must be captured.
AWS References:
* AWS Schema Conversion Tool - Guidance on using SCT for database schema conversions
.
* AWS Database Migration Service - Detailed documentation on using DMS for data
migrations and ongoing replication.
QUESTION NO: 383
DynamoDB의 거래 데이터를 거의 실시간 분석을 위해 S3 데이터 레이크로 수집하려면
어떻게 해야 합니까?
A. DynamoDB Streams를 사용하여 S3에 쓰는 Lambda 함수를 호출합니다.
B. DynamoDB Streams를 사용하여 Data Firehose에 쓰는 Lambda 함수를 호출하고, Data
Firehose는 S3에 씁니다.
C. DynamoDB에서 Kinesis Data Streams를 활성화합니다. S3에 쓰는 Lambda 함수를
호출하도록 구성합니다.
D. DynamoDB에서 Kinesis Data Streams를 활성화합니다. Data Firehose를 사용하여 S3에
씁니다.
Answer: A
* Option A is the simplest solution, using DynamoDB Streams and Lambda for real-time
ingestion into S3.
* Options B, C, and D add unnecessary complexity with Data Firehose or Kinesis.
QUESTION NO: 384
한 회사는 동일한 AWS 리전의 테스트 환경에 대량의 프로덕션 데이터를 복제하는 기능을
개선하려고 합니다. 데이터는 Amazon Elastic Block Store(Amazon EBS) 볼륨의 Amazon
EC2 인스턴스에 저장됩니다. 복제된 데이터를 수정해도 프로덕션 환경에 영향을 주어서는 안
됩니다. 이 데이터에 액세스하는 소프트웨어에는 지속적으로 높은 I/O 성능이 필요합니다.
솔루션 설계자는 프로덕션 데이터를 테스트 환경으로 복제하는 데 필요한 시간을 최소화해야
263

IT Certification Guaranteed, The Easy Way!
합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 프로덕션 EBS 볼륨의 EBS 스냅샷을 찍습니다. 테스트 환경에서 스냅샷을 EC2 인스턴스
스토어 볼륨으로 복원합니다.
B. EBS 다중 연결 기능을 사용하도록 프로덕션 EBS 볼륨을 구성합니다. 프로덕션 EBS
볼륨의 EBS 스냅샷을 찍습니다. 테스트 환경의 EC2 인스턴스에 프로덕션 EBS 볼륨을
연결합니다.
C. 프로덕션 EBS 볼륨의 EBS 스냅샷을 찍습니다. 새 EBS 볼륨을 생성하고 초기화합니다.
프로덕션 EBS 스냅샷에서 볼륨을 복원하기 전에 테스트 환경의 EC2 인스턴스에 새 EBS
볼륨을 연결합니다.
D. 프로덕션 EBS 볼륨의 EBS 스냅샷을 찍습니다. EBS 스냅샷에서 EBS 빠른 스냅샷 복원
기능을 활성화합니다. 스냅샷을 새 EBS 볼륨으로 복원합니다. 테스트 환경의 EC2 인스턴스에
새 EBS 볼륨을 연결합니다.
Answer: C
Explanation:
To clone the production data into the test environment with high I/O performance and without
affecting the production environment, the best option is to take EBS snapshots of the
production EBS volumes and restore them onto new EBS volumes in the test environment.
Then, attach the new EBS volumes to EC2 instances in the test environment. This option
minimizes the time required to clone the data and ensures that modifications to the cloned
data do not affect the production environment. Therefore, option C is the correct answer.
Reference: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-
volume.html
QUESTION NO: 385
소셜 미디어 회사는 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스에서
애플리케이션을 실행합니다. ALB는 Amazon CloudFront 배포의 오리진입니다. 이
애플리케이션은 Amazon S3 버킷에 10억 개 이상의 이미지가 저장되어 있으며 초당 수천
개의 이미지를 처리합니다. 회사는 이미지 크기를 동적으로 조정하고 고객에게 적절한 형식을
제공하기를 원합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EC2 인스턴스에 외부 이미지 관리 라이브러리를 설치합니다. 이미지 관리 라이브러리를
사용하여 이미지를 처리합니다.
B. CloudFront 오리진 요청 정책을 생성합니다. 정책을 사용하여 자동으로 이미지 크기를
조정하고 요청의 User-Agent HTTP 헤더를 기반으로 적절한 형식을 제공합니다.
C. 외부 이미지 관리 라이브러리와 함께 Lambda@Edge 함수를 사용합니다. Lambda@Edge
함수를 이미지를 제공하는 CloudFront 동작과 연결합니다.
D. CloudFront 응답 헤더 정책을 생성합니다. 정책을 사용하여 자동으로 이미지 크기를
조정하고 요청의 User-Agent HTTP 헤더를 기반으로 적절한 형식을 제공합니다.
Answer: C
Explanation:
To resize images dynamically and serve appropriate formats to clients, a Lambda@Edge
function with an external image management library can be used. Lambda@Edge allows
running custom code at the edge locations of CloudFront, which can process the images on
264

IT Certification Guaranteed, The Easy Way!
the fly and optimize them for different devices and browsers. An external image management
library can provide various image manipulation and optimization features.
References:
* Lambda@Edge
* Resizing Images with Amazon CloudFront & Lambda@Edge
QUESTION NO: 386
한 회사가 AWS Organizations에 10개의 AWS 계정이 포함된 조직을 설정합니다. 솔루션
아키텍트는 수천 명의 직원에게 계정에 대한 액세스를 제공하는 솔루션을 설계해야 합니다.
회사에는 기존 ID 공급자(IdP)가 있습니다. 회사는 기존 IdP를 사용하여 AWS에 인증하려고
합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 필수 AWS 계정의 직원에 대한 1AM 사용자를 만듭니다. 1AM 사용자를 기존 IdP에
연결합니다. 1AM 사용자에 대한 페더레이션 인증을 구성합니다.
B. 기존 IdP에서 동기화된 사용자 이메일 주소와 비밀번호로 AWS 계정 루트 사용자를
설정합니다.
C. AWS 1AM Identity Center 구성 1AM Identity Center를 기존 IdP에 연결 기존 IdP에서
사용자 및 그룹 프로비저닝
D. AWS Resource Access Manager(AWS RAM)를 사용하여 기존 IdP의 사용자와 AWS
계정에 대한 액세스를 공유합니다.
Answer: C
Explanation:
* AWS IAM Identity Center:
* IAM Identity Center provides centralized access management for multiple AWS accounts
within an organization and integrates seamlessly with existing identity providers (IdPs)
through SAML 2.0 federation.
* It allows users to authenticate using their existing IdP credentials and gain access to AWS
resources without the need to create and manage separate IAM users in each account.
* IAM Identity Center also simplifies provisioning and de-provisioning users, as it can
automatically synchronize users and groups from the external IdP to AWS, ensuring secure
and managed access.
* Integration with Existing IdP:
* The solution involves configuring IAM Identity Center to connect to the company's IdP using
SAML. This setup allows employees to log in with their existing credentials, reducing the
complexity of managing separate AWS credentials.
* Once connected, IAM Identity Center handles authentication and authorization, granting
users access to the AWS accounts based on their assigned roles and permissions.
Why the Other Options Are Incorrect:
* Option A: Creating separate IAM users for each employee is not scalable or efficient.
Managing thousands of IAM users across multiple AWS accounts introduces unnecessary
complexity and operational overhead.
* Option B: Using AWS root users with synchronized passwords is a security risk and goes
against AWS best practices. Root accounts should never be used for day-to-day operations.
* Option D: AWS Resource Access Manager (RAM) is used for sharing AWS resources
between accounts, not for federating access for users across accounts. It doesn't provide a
265

IT Certification Guaranteed, The Easy Way!
solution for authentication via an external IdP.
AWS References:
* AWS IAM Identity Center
* SAML 2.0 Integration with AWS IAM Identity Center
By setting up IAM Identity Center and connecting it to the existing IdP, the company can
efficiently manage access for thousands of employees across multiple AWS accounts with a
high degree of operational efficiency and security. Therefore, Option C is the best solution.
QUESTION NO: 387
한 회사가 AWS에서 민감한 고객 데이터를 처리할 클라우드 기반 애플리케이션을 구축하고
있습니다. 애플리케이션은 데이터베이스로 Amazon RDS를 사용합니다. 객체 스토리지로
Amazon S3, 서버리스 처리를 위해 AWS Lambda를 호출하는 S3 이벤트 알림.
이 회사는 AWS 1AM Identity Center를 사용하여 사용자 자격 증명을 관리합니다. 개발,
테스트 및 운영 팀은 민감한 고객 데이터의 기밀성을 보장하는 동시에 Amazon RDS 및
Amazon S3에 안전하게 액세스할 수 있어야 합니다. 솔루션은 최소 권한 원칙을 준수해야
합니다.
어떤 솔루션이 운영 오버헤드를 가장 적게 차지하면서 이러한 요구 사항을 충족시킬 수
있을까요?
A. 최소 권한이 있는 1AM 역할을 사용하여 모든 팀에 액세스 권한을 부여합니다. 팀 책임에
따라 Amazon RDS 및 S3 객체 액세스에 대한 특정 권한을 정의하는 사용자 지정 1AM
정책으로 각 팀에 1AM 역할을 할당합니다.
B. Identity Center 디렉토리로 1AM Identity Center를 활성화합니다. Amazon RDS 및 Amazon
S3에 대한 세부적인 액세스 권한이 있는 권한 세트를 만들고 구성합니다. 모든 팀을 권한
세트로 특정 액세스 권한이 있는 그룹에 할당합니다.
C. 역할 기반 권한이 있는 모든 팀의 각 멤버에 대해 개별 1AM 사용자를 만듭니다. 사용자
요구 사항에 따라 각 사용자에게 RDS 및 S3 액세스에 대한 사전 정의된 정책이 있는 1AM
역할을 할당합니다. 주기적 자격 증명 평가를 위해 1AM Access Analyzer를 구현합니다.
D. AWS Organizations를 사용하여 각 팀에 대해 별도의 계정을 만듭니다. 최소 권한으로 교차
계정 1AM 역할을 구현합니다. 팀 역할 및 책임에 따라 RDS 및 S3 액세스에 대한 특정 권한을
부여합니다.
Answer: B
Explanation:
This solution allows for secure and least-privilege access with minimal operational overhead.
* IAM Identity Center: AWS IAM Identity Center (formerly AWS SSO) enables you to centrally
manage access to multiple AWS accounts and applications. By using IAM Identity Center,
you can assign permission sets that define what users or groups can access, ensuring that
only necessary permissions are granted.
* Permission Sets: Permission sets in IAM Identity Center allow you to define granular access
controls for specific services, such as Amazon RDS and S3. You can tailor these permissions
to meet the needs of different teams, adhering to the principle of least privilege.
* Group Management: By assigning users to groups and associating those groups with
specific permission sets, you reduce the complexity and overhead of managing individual
IAM roles and policies. This method also simplifies compliance and audit processes.
* Why Not Other Options?:
* Option A (IAM roles): While IAM roles can provide least-privilege access, managing multiple
266

IT Certification Guaranteed, The Easy Way!
roles and policies across teams increases operational overhead compared to using IAM
Identity Center.
* Option C (Individual IAM users): Managing individual IAM users and roles can be
cumbersome and does not scale well compared to group-based management in IAM Identity
Center.
* Option D (AWS Organizations with cross-account roles): Creating separate accounts and
cross-account roles adds unnecessary complexity and overhead for this use case, where
IAM Identity Center provides a more straightforward solution.
AWS References:
* AWS IAM Identity Center - Overview and best practices for using IAM Identity Center.
* Managing Access Permissions Using IAM Identity Center - Guide on creating and managing
permission sets for secure access.
QUESTION NO: 388
다음 IAM 정책은 IAM 그룹에 첨부됩니다. 이것은 그룹에 적용되는 유일한 정책입니다.
A. 그룹 멤버는 us-east-1 지역 내에서 모든 Amazon EC2 작업을 허용합니다. 허용 권한
이후의 명령문은 적용되지 않습니다.
B. 그룹 구성원은 다중 인증(MFA)을 사용하여 로그인하지 않는 한 us-east-1 지역의 Amazon
EC2 권한이 거부됩니다.
C. 그룹 멤버는 다중 인증(MFA)으로 로그인한 경우 모든 리전에 대한 ec2:Stoplnstances 및
ec2:Terminatelnstances 권한이 허용됩니다. 그룹 멤버는 다른 모든 Amazon EC2 작업이
허용됩니다.
D. 그룹 멤버는 멀티팩터 인증(MFA)으로 로그인한 경우에만 us-east-1 지역에 대한
ec2:Stoplnstances 및 ec2:Terminatelnstances 권한이 허용됩니다. 그룹 멤버는 us-east-1
지역 내에서 다른 모든 Amazon EC2 작업이 허용됩니다.
Answer: D
Explanation:
This answer is correct because it reflects the effect of the IAM policy on the group members.
The policy has two statements: one with an Allow effect and one with a Deny effect. The
Allow statement grants permission to perform any EC2 action on any resource within the us-
east-1 Region. The Deny statement overrides the Allow statement and denies permission to
perform the ec2:StopInstances and ec2:TerminateInstances actions on any resource within
the us-east-1 Region, unless the group member is logged in with MFA. Therefore, the group
members can perform any EC2 action except stopping or terminating instances in the us-
east-1 Region, unless they use MFA.
QUESTION NO: 389
한 회사에서 UDP 연결을 사용하는 VoIP(Voice over Internet Protocol) 서비스를 제공합니다.
이 서비스는 Auto Scaling 그룹에서 실행되는 Amazon EC2 인스턴스로 구성됩니다. 이 회사는
여러 AWS 지역에 배포되어 있습니다.
회사는 사용자를 지연 시간이 가장 짧은 지역으로 라우팅해야 합니다. 회사에는 지역 간
자동화된 장애 조치도 필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. NLB(Network Load Balancer) 및 관련 대상 그룹을 배포합니다. 대상 그룹을 Auto Scaling
그룹과 연결합니다. 각 리전에서 NLB를 AWS Global Accelerator 엔드포인트로 사용합니다.
267

IT Certification Guaranteed, The Easy Way!
B. ALB(Application Load Balancer) 및 관련 대상 그룹을 배포합니다. 대상 그룹을 Auto
Scaling 그룹과 연결합니다. 각 리전에서 ALB를 AWS Global Accelerator 엔드포인트로
사용합니다.
C. NLB(Network Load Balancer) 및 관련 대상 그룹을 배포합니다. 대상 그룹을 Auto Scaling
그룹과 연결합니다. 각 NLB의 별칭을 가리키는 Amazon Route 53 지연 시간 레코드를
생성합니다. 지연 시간 레코드를 오리진으로 사용하는 Amazon CloudFront 배포를
생성합니다.
D. ALB(Application Load Balancer) 및 관련 대상 그룹을 배포합니다. 대상 그룹을 Auto
Scaling 그룹과 연결합니다. 각 ALB의 별칭을 가리키는 Amazon Route 53 가중치 기반
레코드를 생성합니다. 가중치 기반 레코드를 오리진으로 사용하는 Amazon CloudFront
배포를 배포합니다.
Answer: D
Explanation:
https://aws.amazon.com/global-accelerator/faqs/
HTTP /HTTPS - ALB ; TCP and UDP - NLB; Lowest latency routing and more throughput. Als
o supports failover, uses Anycast Ip addressing - Global Accelerator Caching at Egde
Locations - Cloutfront WS Global Accelerator automatically checks the health of your
applications and routes user traffic only to healthy application endpoints. If the health status
changes or you make configuration updates, AWS Global Accelerator reacts instantaneously
to route your users to the next available endpoint..
QUESTION NO: 390
한 회사가 Amazon API Gateway 및 AWS Lambd a를 사용하는 공개적으로 액세스 가능한
서버리스 애플리케이션을 실행하고 있습니다. 최근 봇넷의 사기성 요청으로 인해
애플리케이션 트래픽이 급증했습니다.
승인되지 않은 사용자의 요청을 차단하기 위해 솔루션 설계자는 어떤 단계를 수행해야
합니까? (2개를 선택하세요.)
A. 정품 사용자에게만 공유되는 API Key로 사용 계획을 수립합니다.
B. 사기성 IP 주소의 요청을 무시하도록 Lambda 함수 내에 로직을 통합합니다.
C. 악의적인 요청을 대상으로 하고 이를 필터링하는 작업을 트리거하는 AWS WAF 규칙을
구현합니다.
D. 기존 퍼블릭 API를 프라이빗 API로 변환합니다. 사용자를 새 API 엔드포인트로
리디렉션하려면 DNS 레코드를 업데이트하세요.
E. API에 액세스를 시도하는 각 사용자에 대해 IAM 역할을 생성합니다. 사용자는 API 호출을
수행할 때 역할을 맡게 됩니다.
Answer: A C
Explanation:
https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-
plans.html#:~:
text=Don%27t%20rely%20on%20API%20keys%20as%20your%20only%20means%20of%
20authentication%20and%20authorization%20for%20your%20APIs
https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-
plans.html
QUESTION NO: 391
268

IT Certification Guaranteed, The Easy Way!
한 회사는 높은 동시성 AWS Lambda 함수를 사용하여 마케팅 이벤트 중에 메시지 대기열에서
지속적으로 증가하는 메시지 수를 처리합니다. Lambda 함수는 CPU 집약적인 코드를
사용하여 메시지를 처리합니다. 회사는 컴퓨팅 비용을 줄이고 고객의 서비스 대기 시간을
유지하기를 원합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Lambda 함수에 대해 예약된 동시성을 구성합니다. Lambda 함수에 할당된 메모리를
줄입니다.
B. Lambda 함수에 대해 예약된 동시성을 구성합니다. AWS Compute Optimizer 권장 사항에
따라 메모리를 늘리십시오.
C. Lambda 함수에 대해 프로비저닝된 동시성을 구성합니다. Lambda 함수에 할당된 메모리를
줄입니다.
D. Lambda 함수에 대해 프로비저닝된 동시성을 구성합니다. AWS Compute Optimizer 권장
사항에 따라 메모리를 늘리십시오.
Answer: D
Explanation:
The company wants to reduce the compute costs and maintain service latency for its Lambda
functions that process a constantly increasing number of messages in a message queue.
The Lambda functions use CPU intensive code to process the messages. To meet these
requirements, a solutions architect should recommend the following solution:
* Configure provisioned concurrency for the Lambda functions. Provisioned concurrency is
the number of pre-initialized execution environments that are allocated to the Lambda
functions. These execution environments are prepared to respond immediately to incoming
function requests, reducing the cold start latency. Configuring provisioned concurrency also
helps to avoid throttling errors due to reaching the concurrency limit of the Lambda service.
* Increase the memory according to AWS Compute Optimizer recommendations. AWS
Compute Optimizer is a service that provides recommendations for optimal AWS resource
configurations based on your utilization data. By increasing the memory allocated to the
Lambda functions, you can also increase the CPU power and improve the performance of
your CPU intensive code. AWS Compute Optimizer can help you find the optimal memory
size for your Lambda functions based on your workload characteristics and performance
goals.
This solution will reduce the compute costs by avoiding unnecessary over-provisioning of
memory and CPU resources, and maintain service latency by using provisioned concurrency
and optimal memory size for the Lambda functions.
References:
* Provisioned Concurrency
* AWS Compute Optimizer
QUESTION NO: 392
한 회사가 AWS 클라우드에서 공개 웹 애플리케이션 출시를 준비하고 있습니다. 아키텍처는
Elastic Load Balancer(ELB) 뒤의 VPC 내의 Amazon EC2 인스턴스로 구성됩니다. DNS에는
타사 서비스가 사용됩니다. 회사의 솔루션 설계자는 대규모 DDoS 공격을 탐지하고 방어할 수
있는 솔루션을 권장해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 계정에서 Amazon GuardDuty를 활성화합니다.
269

IT Certification Guaranteed, The Easy Way!
B. EC2 인스턴스에서 Amazon Inspector를 활성화합니다.
C. AWS Shield를 활성화하고 Amazon Route 53을 할당합니다.
D. AWS Shield Advanced를 활성화하고 여기에 ELB를 할당합니다.
Answer: D
Explanation:
https://aws.amazon.com/shield/faqs/
QUESTION NO: 393
한 대규모 국제 대학이 모든 컴퓨팅 서비스를 AWS 클라우드에 배포했습니다. 이러한
서비스에는 Amazon EC2가 포함됩니다. 아마존 RDS. 그리고 Amazon DynamoDB. 이 대학은
현재 인프라를 백업하기 위해 많은 사용자 정의 스크립트를 사용하고 있습니다. 그러나
대학에서는 AWS 기본 옵션을 사용하여 관리를 중앙 집중화하고 데이터 백업을 최대한
자동화하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Storage Gateway 테이프 게이트웨이 가상 테이프 라이브러리와 함께 타사 백업
소프트웨어를 사용합니다.
B. AWS Backup을 사용하여 사용 중인 서비스에 대한 모든 백업을 구성하고 모니터링합니다.
C. AWS Config를 사용하여 일정에 따라 모든 데이터 소스의 스냅샷을 찍도록 수명 주기
관리를 설정합니다.
D. AWS Systems Manager State Manager를 사용하여 백업 작업의 구성 및 모니터링을
관리합니다.
Answer: B
Explanation:
* Understanding the Requirement: The university wants to centralize management and
automate backups for its AWS services (EC2, RDS, and DynamoDB), reducing reliance on
custom scripts.
* Analysis of Options:
* Third-party backup software with AWS Storage Gateway: This solution introduces external
dependencies and adds complexity compared to using native AWS services.
* AWS Backup: Provides a centralized, fully managed service to automate and manage
backups across various AWS services, including EC2, RDS, and DynamoDB.
* AWS Config: Primarily used for compliance and configuration monitoring, not for backup
management.
* AWS Systems Manager State Manager: Useful for configuration management but not
specifically designed for managing backups.
* Best Solution:
* AWS Backup: This service offers the necessary functionality to centralize and automate
backups, providing a streamlined and integrated solution with minimal effort.
References:
* AWS Backup
QUESTION NO: 394
소셜 미디어 회사에는 데이터를 수집하고 처리하는 워크로드가 있습니다. 워크로드는
온프레미스 NFS 스토리지에 데이터를 저장합니다. 데이터 저장소는 회사의 확장되는
비즈니스 요구 사항을 충족할 만큼 빠르게 확장할 수 없습니다. 회사는 현재 데이터 저장소를
270

IT Certification Guaranteed, The Easy Way!
AWS로 마이그레이션하려고 합니다. 어떤 솔루션이 충족할까요? 이러한 요구 사항이 가장
비용 효율적입니까?
A. AWS Storage Gateway 볼륨 게이트웨이 설정 Amazon S3 수명 주기 정책을 사용하여
데이터를 적절한 스토리지 클래스로 전환합니다.
B. AWS Storage Gateway Amazon S3 파일 게이트웨이 설정 Amazon S3 수명 주기 정책을
사용하여 데이터를 적절한 스토리지 클래스로 전환합니다.
C. Amazon Elastic File System(Amazon EFS) Standard-Infrequent Access(Standard-IA)
스토리지 클래스 사용 간헐적 액세스 수명 주기 정책 활성화
D. Amazon Elastic File System(Amazon EFS) One Zone-Infrequent Access(One Zone-IA)
스토리지 클래스 사용 간헐적 액세스 수명 주기 정책 활성화
Answer: B
Explanation:
This solution meets the requirements most cost-effectively because it enables the company
to migrate its on- premises NFS data store to AWS without changing the existing applications
or workflows. AWS Storage Gateway is a hybrid cloud storage service that provides
seamless and secure integration between on-premises and AWS storage. Amazon S3 File
Gateway is a type of AWS Storage Gateway that provides a file interface to Amazon S3, with
local caching for low-latency access. By setting up an Amazon S3 File Gateway, the
company can store and retrieve files as objects in Amazon S3 using standard file protocols
such as NFS. The company can also use an Amazon S3 Lifecycle policy to automatically
transition the data to the appropriate storage class based on the frequency of access and the
cost of storage. For example, the company can use S3 Standard for frequently accessed
data, S3 Standard-Infrequent Access (S3 Standard-IA) or S3 One Zone- Infrequent Access
(S3 One Zone-IA) for less frequently accessed data, and S3 Glacier or S3 Glacier Deep
Archive for long-term archival data.
Option A is not a valid solution because AWS Storage Gateway Volume Gateway is a type of
AWS Storage Gateway that provides a block interface to Amazon S3, with local caching for
low-latency access. Volume Gateway is not suitable for migrating an NFS data store, as it
requires attaching the volumes to EC2 instances or on-premises servers using the iSCSI
protocol. Option C is not a valid solution because Amazon Elastic File System (Amazon EFS)
is a fully managed elastic NFS file system that is designed for workloads that require high
availability, scalability, and performance. Amazon EFS Standard-Infrequent Access
(Standard-IA) is a storage class within Amazon EFS that is optimized for infrequently
accessed files, with a lower price per GB and a higher price per access. Using Amazon EFS
Standard-IA for migrating an NFS data store would not be cost-effective, as it would incur
higher access charges and require additional configuration to enable lifecycle management.
Option D is not a valid solution because Amazon EFS One Zone-Infrequent Access (One
Zone- IA) is a storage class within Amazon EFS that is optimized for infrequently accessed
files that do not require the availability and durability of Amazon EFS Standard or Standard-
IA. Amazon EFS One Zone-IA stores data in a single Availability Zone, which reduces the
cost by 47% compared to Amazon EFS Standard-IA, but also increases the risk of data loss
in the event of an Availability Zone failure. Using Amazon EFS One Zone- IA for migrating an
NFS data store would not be cost-effective, as it would incur higher access charges and
require additional configuration to enable lifecycle management. It would also compromise
the availability and durability of the data.
271

IT Certification Guaranteed, The Easy Way!
References:
* AWS Storage Gateway - Amazon Web Services
* Amazon S3 File Gateway - AWS Storage Gateway
* Object Lifecycle Management - Amazon Simple Storage Service
* [AWS Storage Gateway Volume Gateway - AWS Storage Gateway]
* [Amazon Elastic File System - Amazon Web Services]
* [Using EFS storage classes - Amazon Elastic File System]
QUESTION NO: 395
한 회사가 AWS에서 회사 고객 포털의 백엔드인 데이터베이스 워크로드를 실행합니다. 이
회사는 Amazon RDS for PostgreSQL에서 Multi-AZ 데이터베이스 클러스터를 실행합니다.
회사는 30일 백업 보존 정책을 구현해야 합니다. 회사는 현재 자동화된 RDS 백업과 수동 RDS
백업을 모두 보유하고 있습니다. 회사는 30일 이내에 생성된 두 가지 유형의 기존 RDS 백업을
모두 유지하려고 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. AWS Backup을 사용하여 자동 백업에 대한 RDS 백업 보존 정책을 30일로 구성합니다.
30일이 지난 수동 백업을 수동으로 삭제합니다.
B. RDS 자동 백업을 비활성화합니다. 30일 이상 된 자동 백업 및 수동 백업을 삭제합니다.
RDS 백업 보존 정책을 자동 백업에 대해 30일로 구성합니다.
C. 자동 백업에 대해 RDS 백업 보존 정책을 30일로 구성합니다. 30일 이상 된 수동 백업을
수동으로 삭제합니다.
D. RDS 자동 백업을 비활성화합니다. AWS CloudFormation을 사용하여 30일 이상 된 자동
백업 및 수동 백업을 자동으로 삭제합니다. 자동 백업의 경우 RDS 백업 보존 정책을 30일로
구성합니다.
Answer: A
Explanation:
Setting the RDS backup retention policy to 30 days for automated backups through AWS
Backup allows the company to retain backups cost-effectively. Manual backups, however,
are not automatically managed by RDS's retention policy, so they need to be manually
deleted if they are older than 30 days to avoid unnecessary storage costs.
Key AWS features:
* Automated Backups: Can be configured with a retention policy of up to 35 days, ensuring
that older automated backups are deleted automatically.
* Manual Backups: These are not subject to the automated retention policy and must be
manually managed to avoid extra costs.
* AWS Documentation: AWS recommends using backup retention policies for automated
backups while manually managing manual backups.
QUESTION NO: 396
한 회사가 AWS에서 서버리스 애플리케이션을 호스팅하고 있습니다. 애플리케이션은
Amazon API Gateway, AWS Lambda 및 PostgreSQL용 Amazon RDS 데이터베이스를
사용합니다. 회사에서는 트래픽이 최고조에 달하거나 트래픽이 예측할 수 없는 시간 동안
데이터베이스 연결 시간 초과로 인해 발생하는 애플리케이션 오류가 증가하고 있음을
확인했습니다. 회사에는 최소한의 코드 변경으로 애플리케이션 오류를 줄이는 솔루션이
필요합니다.
272

IT Certification Guaranteed, The Easy Way!
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. Lambda 동시성 비율을 줄입니다.
B. RDS DB 인스턴스에서 RDS Proxy를 활성화합니다.
C. 더 많은 연결을 허용하도록 RDS DB 인스턴스 클래스의 크기를 조정합니다.
D. 온디맨드 확장을 통해 데이터베이스를 Amazon DynamoDB로 마이그레이션합니다.
Answer: B
Explanation:
Using RDS Proxy, you can handle unpredictable surges in database traffic. Otherwise, these
surges might cause issues due to oversubscribing connections or creating new connections
at a fast rate. RDS Proxy establishes a database connection pool and reuses connections in
this pool. This approach avoids the memory and CPU overhead of opening a new database
connection each time. To protect the database against oversubscription, you can control the
number of database connections that are created. https://docs.aws.
amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html
QUESTION NO: 397
한 회사가 Amazon RDS DB 인스턴스에 데이터를 저장할 계획입니다. 회사는 저장 데이터를
암호화해야 합니다.
이 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 암호화 키를 생성하고 AWS Secrets Manager에 키를 저장합니다. 키를 사용하여 DB
인스턴스를 암호화합니다.
B. AWS Certificate Manager(ACM)에서 인증서를 생성합니다. 인증서를 사용하여 DB
인스턴스에서 SSL/TLS를 활성화합니다.
C. AWS Key Management Service(AWS KMS)에서 고객 마스터 키(CMK)를 생성합니다. DB
인스턴스에 대한 암호화를 활성화합니다.
D. AWS Identity and Access Management {IAM)에서 인증서를 생성합니다. 인증서를
사용하여 DB 인스턴스에서 SSUTLS를 활성화합니다.
Answer: A
Explanation:
To encrypt data at rest in Amazon RDS, you can use the encryption feature of Amazon RDS,
which uses AWS Key Management Service (AWS KMS). With this feature, Amazon RDS
encrypts each database instance with a unique key. This key is stored securely by AWS
KMS. You can manage your own keys or use the default AWS-managed keys. When you
enable encryption for a DB instance, Amazon RDS encrypts the underlying storage, including
the automated backups, read replicas, and snapshots.
QUESTION NO: 398
솔루션 설계자는 2계층 웹 애플리케이션을 설계하고 있습니다. 애플리케이션은 퍼블릭
서브넷의 Amazon EC2에서 호스팅되는 공개 웹 계층으로 구성됩니다. 데이터베이스 계층은
프라이빗 서브넷의 Amazon EC2에서 실행되는 Microsoft SQL Server로 구성됩니다. 회사 이
상황에서 보안 그룹을 어떻게 구성해야 합니까? (2개 선택)
A. 0.0.0.0/0에서 포트 443의 인바운드 트래픽을 허용하도록 웹 계층에 대한 보안 그룹을
구성합니다.
B. 0.0.0.0/0에서 포트 443의 아웃바운드 트래픽을 허용하도록 웹 계층에 대한 보안 그룹을
273

IT Certification Guaranteed, The Easy Way!
구성합니다.
C. 웹 계층의 보안 그룹에서 포트 1433의 인바운드 트래픽을 허용하도록 데이터베이스 계층의
보안 그룹을 구성합니다.
D. 데이터베이스 계층에 대한 보안 그룹을 구성하여 포트 443 및 1433에서 웹 계층에 대한
보안 그룹에 대한 아웃바운드 트래픽을 허용합니다.
E. 웹 계층의 보안 그룹에서 포트 443 및 1433의 인바운드 트래픽을 허용하도록 데이터베이스
계층에 대한 보안 그룹을 구성합니다.
Answer: A C
Explanation:
"Security groups create an outbound rule for every inbound rule." Not completely right.
Statefull does NOT mean that if you create an inbound (or outbound) rule, it will create an
outbound (or inbound) rule. What it does mean is: suppose you create an inbound rule on
port 443 for the X ip. When a request enters on port 443 from X ip, it will allow traffic out for
that request in the port 443. However, if you look at the outbound rules, there will not be any
outbound rule on port 443 unless explicitly create it. In ACLs, which are stateless, you would
have to create an inbound rule to allow incoming requests and an outbound rule to allow your
application responds to those incoming requests.
https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html#SecurityGroup
Rules
QUESTION NO: 399
한 회사가 온프레미스 데이터 센터의 여러 애플리케이션에 영향을 미치는 위반을
경험했습니다. 공격자는 서버에서 실행 중인 사용자 지정 애플리케이션의 취약점을
이용했습니다. 회사는 현재 Amazon EC2 인스턴스에서 실행되도록 애플리케이션을
마이그레이션하고 있습니다. 회사는 구현을 원합니다. EC2 인스턴스의 취약성을 적극적으로
검사하고 결과를 자세히 설명하는 보고서를 보내는 솔루션 어떤 솔루션이 이러한 요구 사항을
충족합니까?
A. AWS Shield를 배포하여 EC2 인스턴스의 취약점을 검사합니다. AWS Lambda 함수를
생성하여 모든 결과를 AWS CloudTrail에 기록합니다.
B. Amazon Macie 및 AWS Lambda 함수를 배포하여 EC2 인스턴스의 취약성을 검사합니다.
결과를 AWS CloudTrail에 기록합니다.
C. Amazon GuardDuty 켜기 GuardDuty 에이전트를 EC2 인스턴스에 배포 AWS Lambda
함수를 구성하여 결과를 자세히 설명하는 보고서 생성 및 배포를 자동화합니다.
D. Amazon Inspector 켜기 Amazon Inspector 에이전트를 EC2 인스턴스에 배포 AWS
Lambda 함수를 구성하여 결과를 자세히 설명하는 보고서 생성 및 배포를 자동화합니다.
Answer: D
Explanation:
Amazon Inspector:
* Performs active vulnerability scans of EC2 instances. It looks for software vulnerabilities,
unintended network accessibility, and other security issues.
* Requires installing an agent on EC2 instances to perform scans. The agent must be
deployed to each instance.
* Provides scheduled scan reports detailing any findings of security risks or vulnerabilities.
These reports can be used to patch or remediate issues.
* Is best suited for proactively detecting security weaknesses and misconfigurations in your
274

IT Certification Guaranteed, The Easy Way!
AWS environment.
QUESTION NO: 400
한 회사는 여러 AWS 지역에 Amazon EC2 인스턴스를 보유하고 있습니다. 인스턴스는 모두
동일한 Amazon S3 버킷에서 기밀 데이터를 저장하고 검색합니다. 이 회사는 현재 아키텍처의
보안을 개선하고자 합니다.
회사에서는 자사 VPC 내의 Amazon EC2 인스턴스만 S3 버킷에 액세스할 수 있도록 하려고
합니다.
회사는 버킷에 대한 다른 모든 액세스를 차단해야 합니다.
어떤 솔루션이 이 요구 사항을 충족시킬까요?
A. 오전 1시 정책을 사용하여 S3 버킷에 대한 액세스를 제한합니다.
B. 서버 측 암호화(SSE)를 사용하여 S3 버킷의 데이터를 휴면 상태로 암호화합니다. 암호화
키를 EC2 인스턴스에 저장합니다.
C. Amazon S3에 대한 VPC 엔드포인트를 만듭니다. 엔드포인트에서만 연결을 허용하도록 S3
버킷 정책을 구성합니다.
D. AWS Key Management Service(AWS KMS)를 고객 관리 키와 함께 사용하여 데이터를 S3
버킷으로 보내기 전에 암호화합니다.
Answer: C
Explanation:
Creating a VPC endpoint for S3 and configuring a bucket policy to allow access only from the
endpoint ensures that only EC2 instances within the VPC can access the S3 bucket. This
solution improves security by restricting access at the network level without the need for
public internet access.
* Option A (IAM policies): IAM policies alone cannot restrict access based on the network
location.
* Option B and D (Encryption): Encryption secures data at rest but does not restrict network
access to the bucket.
AWS References:
* Amazon S3 VPC Endpoints
QUESTION NO: 401
회사는 민감한 데이터를 Amazon S3에 저장합니다. 솔루션 설계자는 암호화 솔루션을
생성해야 합니다. 회사는 암호화해야 하는 모든 데이터에 대해 최소한의 노력으로 암호화
키를 생성, 교체 및 비활성화할 수 있는 사용자의 능력을 완전히 제어해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon S3 관리형 암호화 키(SSE-S3)와 함께 기본 서버 측 암호화를 사용하여 민감한
데이터를 저장합니다.
B. AWS Key Management Service(AWS KMS)를 사용하여 고객 관리형 키를 생성합니다.
AWS KMS 키(SSE-KMS)로 서버 측 암호화를 사용하여 S3 객체를 암호화하려면 새 키를
사용합니다.
C. AWS Key Management Service를 사용하여 AWS 관리형 키를 생성합니다. {AWS KMS) 새
키를 사용하여 AWS KMS 키(SSE-KMS)로 서버 측 암호화를 사용하여 S3 객체를
암호화합니다.
D. S3 객체를 Amazon EC2 인스턴스로 다운로드합니다. 고객 관리형 키를 사용하여 객체를
암호화합니다. 암호화된 객체를 Amazon S3에 다시 업로드합니다.
275

IT Certification Guaranteed, The Easy Way!
Answer: B
Explanation:
* Understanding the Requirement: The company needs to control the creation, rotation, and
disabling of encryption keys for data stored in S3 with minimal effort.
* Analysis of Options:
* SSE-S3: Provides server-side encryption using S3 managed keys but does not offer full
control over key management.
* Customer managed key with AWS KMS (SSE-KMS): Allows the company to fully control
key creation, rotation, and disabling, providing a high level of security and compliance.
* AWS managed key with AWS KMS (SSE-KMS): While it provides some control, it does not
offer the same level of granularity as customer-managed keys.
* EC2 instance encryption and re-upload: This approach is operationally intensive and does
not leverage AWS managed services for efficient key management.
* Best Solution:
* Customer managed key with AWS KMS (SSE-KMS): This solution meets the requirement
for full control over encryption keys with minimal operational overhead, leveraging AWS
managed services for secure key management.
References:
* AWS Key Management Service (KMS)
* Amazon S3 Encryption
QUESTION NO: 402
회사에서는 SSL/TLS 인증서를 사용하도록 Amazon CloudFront 배포를 구성하려고 합니다.
회사는 배포에 기본 도메인 이름을 사용하기를 원하지 않습니다. 대신 회사는 배포에 다른
도메인 이름을 사용하기를 원합니다.
추가 비용이 발생하면서 인증서를 배포하는 솔루션은 무엇입니까?
A. us-east-1 리전의 AWS Certificate Manager(ACM)에서 Amazon 발급 개인 인증서를
요청합니다.
B. us-west-1 리전의 AWS Certificate Manager(ACM)에서 Amazon 발급 개인 인증서를
요청합니다.
C. us-east-1 리전의 AWS Certificate Manager(ACU)에서 Amazon 발급 공인 인증서를
요청합니다.
D. us-west-1 Regon의 AWS Certificate Manager(ACU)에서 Amazon 발급 공인 인증서를
요청합니다.
Answer: C
Explanation:
This option is the most efficient because it requests an Amazon issued public certificate from
AWS Certificate Manager (ACM), which is a service that lets you easily provision, manage,
and deploy public and private SSL
/TLS certificates for use with AWS services and your internal connected resources1. It also
requests the certificate in the us-east-1 Region, which is required for using an ACM certificate
with CloudFront2. It also meets the requirement of deploying the certificate without incurring
any additional costs, as ACM does not charge for certificates that are used with supported
AWS services3. This solution meets the requirement of configuring its CloudFront distribution
to use SSL/TLS certificates and using a different domain name for the distribution. Option A
276

IT Certification Guaranteed, The Easy Way!
is less efficient because it requests an Amazon issued private certificate from ACM, which is
a type of certificate that can be used only within your organization or virtual private cloud
(VPC).
However, this does not meet the requirement of configuring its CloudFront distribution to use
SSL/TLS certificates, as CloudFront requires a public certificate. It also requests the
certificate in the us-east-1 Region, which is correct. Option B is less efficient because it
requests an Amazon issued private certificate from ACM, which is incorrect for the same
reason as option A. It also requests the certificate in the us-west-1 Region, which is incorrect
as CloudFront requires a certificate in the us-east-1 Region. Option D is less efficient
because it requests an Amazon issued public certificate from ACM, which is correct.
However, it requests the certificate in the us-west-1 Region, which is incorrect as CloudFront
requires a certificate in the us-east-1 Region.
QUESTION NO: 403
전자 상거래 회사는 AWS Organizations 조직의 일부인 AWS 계정에서 애플리케이션을
실행합니다. 애플리케이션은 모든 계정에 걸쳐 Amazon Aurora PostgreSQL
데이터베이스에서 실행됩니다. 회사는 악의적인 활동을 방지하고 데이터베이스에 대한
비정상적으로 실패하거나 불완전한 로그인 시도를 식별해야 합니다. 어느 솔루션 가장 운영상
효율적인 방식으로 이러한 요구 사항을 충족할 수 있습니까?
A. 서비스 제어 정책(SCP)을 조직의 루트에 연결하여 실패한 로그인 시도를 식별합니다.
B. 조직의 멤버 계정에 대해 Amazon GuardDuty에서 Amazon RDS 보호 기능을
활성화합니다.
C. Aurora 일반 로그를 Amazon CloudWatch Logs의 로그 그룹에 게시합니다. 로그 데이터를
중앙 Amazon S3 버킷으로 내보냅니다.
D. AWS CloudTrail의 모든 Aurora PostgreSQL 데이터베이스 이벤트를 중앙 Amazon S3
버킷에 게시합니다.
Answer: C
Explanation:
This option is the most operationally efficient way to meet the requirements because it allows
the company to monitor and analyze the database login activity across all the accounts in the
organization. By publishing the Aurora general logs to a log group in Amazon CloudWatch
Logs, the company can enable the logging of the database connections, disconnections, and
failed authentication attempts. By exporting the log data to a central Amazon S3 bucket, the
company can store the log data in a durable and cost-effective way and use other AWS
services or tools to perform further analysis or alerting on the log data. For example, the
company can use Amazon Athena to query the log data in Amazon S3, or use Amazon SNS
to send notifications based on the log data.
A: Attach service control policies (SCPs) to the root of the organization to identify the failed
login attempts.
This option is not effective because SCPs are not designed to identify the failed login
attempts, but to restrict the actions that the users and roles can perform in the member
accounts of the organization. SCPs are applied to the AWS API calls, not to the database
login attempts. Moreover, SCPs do not provide any logging or analysis capabilities for the
database activity.
B: Enable the Amazon RDS Protection feature in Amazon GuardDuty for the member
277

IT Certification Guaranteed, The Easy Way!
accounts of the organization. This option is not optimal because the Amazon RDS Protection
feature in Amazon GuardDuty is not available for Aurora PostgreSQL databases, but only for
Amazon RDS for MySQL and Amazon RDS for MariaDB databases. Moreover, the Amazon
RDS Protection feature does not monitor the database login attempts, but the network and
API activity related to the RDS instances.
D: Publish all the Aurora PostgreSQL database events in AWS CloudTrail to a central
Amazon S3 bucket.
This option is not sufficient because AWS CloudTrail does not capture the database login
attempts, but only the AWS API calls made by or on behalf of the Aurora PostgreSQL
database. For example, AWS CloudTrail can record the events such as creating, modifying,
or deleting the database instances, clusters, or snapshots, but not the events such as
connecting, disconnecting, or failing to authenticate to the database.
References:
* 1 Working with Amazon Aurora PostgreSQL - Amazon Aurora
* 2 Working with log groups and log streams - Amazon CloudWatch Logs
* 3 Exporting Log Data to Amazon S3 - Amazon CloudWatch Logs
* 4 Amazon GuardDuty FAQs
* 5 Logging Amazon RDS API Calls with AWS CloudTrail - Amazon Relational Database
Service
QUESTION NO: 404
한 회사가 Amazon Elastic Block Store(Amazon EBS) 볼륨 암호화 전략을 표준화하려고
합니다. 또한 볼륨 암호화 검사를 운영하는 데 필요한 비용과 구성 노력을 최소화하려고
합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. EBS 볼륨을 설명하고 EBS 볼륨이 암호화되었는지 확인하기 위한 API 호출을 작성합니다.
Amazon EventBridge를 사용하여 API 호출을 실행하기 위한 AWS Lambda 함수를
예약합니다.
B. EBS 볼륨을 설명하고 EBS 볼륨이 암호화되었는지 확인하기 위해 API 호출을 작성합니다.
AWS Fargate 작업에서 API 호출을 실행합니다.
C. EBS 볼륨에서 태그를 사용해야 하는 AWS Identity and Access Management(1AM) 정책을
만듭니다. AWS Cost Explorer를 사용하여 적절하게 태그가 지정되지 않은 리소스를
표시합니다. 태그가 지정되지 않은 리소스를 수동으로 암호화합니다.
D. Amazon EBS에 대한 AWS Config 규칙을 생성하여 볼륨이 암호화되었는지 평가하고
암호화되지 않은 경우 볼륨에 플래그를 지정합니다.
Answer: D
Explanation:
AWS Config is a service that enables you to assess, audit, and evaluate the configurations of
your AWS resources. By creating a Config rule, you can automatically check whether your
Amazon EBS volumes are encrypted and flag those that are not, with minimal cost and
configuration effort.
* AWS Config Rule: AWS Config provides managed rules that you can use to automatically
check the compliance of your resources against predefined or custom criteria. In this case,
you would create a rule to evaluate EBS volumes and determine if they are encrypted. If a
volume is not encrypted, the rule will flag it, allowing you to take corrective action.
278

IT Certification Guaranteed, The Easy Way!
* Operational Overhead: This approach significantly reduces operational overhead because
once the rule is in place, it continuously monitors your EBS volumes for compliance, and
there's no need for manual checks or custom scripting.
* Why Not Other Options?:
* Option A (Lambda with API calls and EventBridge): While this can work, it involves writing
and maintaining custom code, which increases operational overhead compared to using a
managed AWS Config rule.
* Option B (API calls on Fargate): Running API calls on Fargate is more complex and costly
compared to using AWS Config, which provides a simpler, managed solution.
* Option C (IAM policy with Cost Explorer): This option does not directly enforce encryption
compliance and involves manual intervention, making it less efficient and more prone to
errors.
AWS References:
* AWS Config Rules - Overview of AWS Config rules and how they can be used to evaluate
resource configurations.
* Amazon EBS Encryption - Information on how to manage and enforce encryption for EBS
volumes.
QUESTION NO: 405
온라인 사진 공유 회사는 us-west-1 리전에 존재하는 Amazon S3 버킷에 Hs 사진을
저장합니다. 회사는 us-east-1 지역에 모든 새 사진의 사본을 저장해야 합니다.
최소한의 운영 노력으로 이 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?
A. us-east-1에 두 번째 S3 버킷을 생성합니다. S3 교차 리전 복제를 사용하여 기존 S3 버킷의
사진을 두 번째 S3 버킷으로 복사합니다.
B. 기존 S3 버킷의 CORS(교차 원본 리소스 공유) 구성을 생성합니다. CORS 규칙의
AllowedOngm 요소에 us-east-1을 지정합니다.
C. 여러 가용 영역에 걸쳐 us-east-1에 두 번째 S3 버킷을 생성합니다. S3 수명 주기 규칙을
생성하여 두 번째 S3 버킷에 사진을 저장합니다.
D. us-east-1에 두 번째 S3 버킷을 생성합니다. 객체 생성 및 업데이트 이벤트에 대한 S3
이벤트 알림을 구성하여 AWS Lambda 함수를 호출하여 기존 S3 버킷의 사진을 두 번째 S3
버킷으로 복사합니다.
Answer: A
Explanation:
* Understanding the Requirement: The company needs to store a copy of all new photos in
the us-east-1 Region from an S3 bucket in the us-west-1 Region.
* Analysis of Options:
* Cross-Region Replication: Automatically replicates objects across regions with minimal
operational effort once configured.
* CORS Configuration: Used for allowing resources on a web page to be requested from
another domain, not for replication.
* S3 Lifecycle Rule: Manages the transition of objects between storage classes within the
same bucket, not for cross-region replication.
* S3 Event Notifications with Lambda: Requires additional configuration and management
compared to Cross-Region Replication.
* Best Solution:
279

IT Certification Guaranteed, The Easy Way!
* S3 Cross-Region Replication: This solution provides an automated and efficient way to
replicate objects to another region, meeting the requirement with the least operational effort.
References:
* Amazon S3 Cross-Region Replication
QUESTION NO: 406
애플리케이션은 VPC A에 탄력적 IP 주소가 있는 Amazon EC2 인스턴스에서 실행됩니다.
애플리케이션은 VPC B의 데이터베이스에 액세스해야 합니다. 두 VPC 모두 동일한 AWS
계정에 있습니다.
필요한 액세스를 가장 안전하게 제공하는 솔루션은 무엇입니까?
A. VPC A에 있는 애플리케이션 서버의 퍼블릭 IP 주소에서 들어오는 모든 트래픽을 허용하는
DB 인스턴스 보안 그룹을 생성합니다.
B. VPC A와 VPC B 간의 VPC 피어링 연결을 구성합니다.
C. DB 인스턴스에 공개적으로 액세스할 수 있도록 합니다. DB 인스턴스에 퍼블릭 IP 주소를
할당합니다.
D. 탄력적 IP 주소가 있는 EC2 인스턴스를 VPC B로 시작합니다. 새 EC2 인스턴스를 통해
모든 요청을 프록시합니다.
Answer: B
Explanation:
A VPC peering connection is a networking connection between two VPCs that enables users
to route traffic between them using private IP addresses. Instances in either VPC can
communicate with each other as if they are within the same network. A VPC peering
connection can be created between VPCs in the same or different AWS accounts and
Regions1. By configuring a VPC peering connection between VPC A and VPC B, the solution
can provide the required access most securely.
A: Create a DB instance security group that allows all traffic from the public IP address of the
application server in VPC A. This solution will not provide the required access most securely,
as it involves exposing the DB instance to the public internet and relying on a single IP
address for access control2.
C: Make the DB instance publicly accessible. Assign a public IP address to the DB instance.
This solution will not provide the required access most securely, as it involves exposing the
DB instance to the public internet and allowing any source to connect to it2.
D: Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through
the new EC2 instance. This solution will not provide the required access most securely, as it
involves creating an additional resource and configuring a proxy server that may introduce
latency and complexity3.
Reference URL: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html
QUESTION NO: 407
한 회사에서 새로운 모바일 앱을 개발 중입니다. 회사는 크로스 사이트 스크립팅이나 SQL
주입과 같은 일반적인 애플리케이션 수준 공격으로부터 ALB(Application Load Balancer)를
보호하기 위해 적절한 트래픽 필터링을 구현해야 합니다. 회사는 최소한의 인프라와 운영
직원을 보유하고 있습니다. 회사는 AWS 환경에 대한 서버 관리, 업데이트 및 보안에 대한
책임을 줄여야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 권장해야 합니까?
280

IT Certification Guaranteed, The Easy Way!
A. AWS WAF 규칙을 구성하고 이를 ALB와 연결합니다.
B. 공개 호스팅이 활성화된 Amazon S3를 사용하여 애플리케이션을 배포합니다.
C. AWS Shield Advanced를 배포하고 ALB를 보호되는 리소스로 추가합니다.
D. 타사 방화벽을 실행하는 Amazon EC2 인스턴스로 트래픽을 전달하는 새 ALB를 생성한
다음 트래픽을 현재 ALB로 전달합니다.
Answer: A
Explanation:
A solutions architect should recommend option A, which is to configure AWS WAF rules and
associate them with the ALB. This will allow the company to apply traffic filtering at the
application layer, which is necessary for protecting the ALB against common application-level
attacks such as cross-site scripting or SQL injection. AWS WAF is a managed service that
makes it easy to protect web applications from common web exploits that could affect
application availability, compromise security, or consume excessive resources.
The company can easily manage and update the rules to ensure the security of its
application.
QUESTION NO: 408
회사에는 자사 독점 애플리케이션의 로그 파일을 분석할 수 있는 기능이 필요합니다. 로그는
Amazon S3 버킷에 JSON 형식으로 저장됩니다. 쿼리는 간단하고 온디맨드로 실행됩니다.
솔루션 아키텍트는 기존 아키텍처를 최소한으로 변경하면서 분석을 수행해야 합니다. 솔루션
아키텍트는 최소한의 비용으로 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까? 운영
오버헤드의 양은 얼마입니까?
A. Amazon Redshift를 사용하여 모든 콘텐츠를 한 곳에 로드하고 필요에 따라 SQL 쿼리를
실행합니다.
B. Amazon CloudWatch Logs를 사용하여 로그 저장 필요에 따라 Amazon CloudWatch
콘솔에서 SQL 쿼리 실행
C. Amazon S3와 함께 Amazon Athena를 직접 사용하여 필요에 따라 쿼리를 실행합니다.
D. AWS Glue를 사용하여 로그 카탈로그 작성 Amazon EMR에서 임시 Apache Spark
클러스터를 사용하여 필요에 따라 SQL 쿼리 실행
Answer: C
Explanation:
Amazon Athena can be used to query JSON in S3
QUESTION NO: 409
한 전자상거래 회사에서 사용자 트래픽이 증가하고 있습니다. 회사의 스토어는 웹 계층과
별도의 데이터베이스 계층으로 구성된 2계층 웹 애플리케이션으로 Amazon EC2 인스턴스에
배포됩니다. 트래픽이 증가함에 따라 회사는 아키텍처로 인해 사용자에게 적시에 마케팅 및
주문 확인 이메일을 보내는 데 상당한 지연이 발생하고 있음을 발견했습니다. 회사는 복잡한
이메일 전달 문제를 해결하는 데 소요되는 시간을 줄이고 운영 오버헤드를 최소화하기를
원합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 이메일 처리 전용 EC2 인스턴스를 사용하여 별도의 애플리케이션 계층을 생성합니다.
B. Amazon Simple Email Service(Amazon SES)를 통해 이메일을 보내도록 웹 인스턴스를
구성합니다.
281

IT Certification Guaranteed, The Easy Way!
C. Amazon Simple Notification Service(Amazon SNS)를 통해 이메일을 보내도록 웹
인스턴스를 구성합니다.
D. 이메일 처리 전용 EC2 인스턴스를 사용하여 별도의 애플리케이션 계층을 생성합니다.
Auto Scaling 그룹에 인스턴스를 배치합니다.
Answer: B
Explanation:
Amazon SES is a cost-effective and scalable email service that enables businesses to send
and receive email using their own email addresses and domains. Configuring the web
instance to send email through Amazon SES is a simple and effective solution that can
reduce the time spent resolving complex email delivery issues and minimize operational
overhead.
QUESTION NO: 410
한 회사가 Amazon S3 버킷에서 웹 애플리케이션을 호스팅하고 있습니다. 애플리케이션은
Amazon Cognito를 자격 증명 공급자로 사용하여 사용자를 인증하고 다른 S3 버킷에 복원된
보호된 리소스에 대한 액세스를 제공하는 JSON 웹 토큰(JWT)을 반환합니다.
애플리케이션 배포 시 사용자는 오류를 보고하고 보호된 콘텐츠에 액세스할 수 없습니다.
솔루션 설계자는 사용자가 보호된 콘텐츠에 액세스할 수 있도록 적절한 권한을 제공하여 이
문제를 해결해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 보호된 동의에 액세스하기 위한 적절한 IAM 역할을 맡도록 Amazon Cognito 자격 증명
풀을 업데이트합니다.
B. 애플리케이션이 보호된 콘텐츠에 액세스할 수 있도록 S3 ACL을 업데이트합니다.
C. 애플리케이션을 Amazon 33에 재배포하여 S3 버킷의 최종적 일관된 읽기가 사용자가
보호된 콘텐츠에 액세스하는 능력에 영향을 미치지 않도록 합니다.
D. Amazon Cognito 풀을 업데이트하여 타이 자격 증명 풀 내에서 사용자 지정 속성 매핑을
사용하고 사용자에게 보호된 콘텐츠에 액세스할 수 있는 적절한 권한을 부여합니다.
Answer: A
Explanation:
Amazon Cognito identity pools assign your authenticated users a set of temporary, limited-
privilege credentials to access your AWS resources. The permissions for each user are
controlled through IAM roles that you create.
https://docs.aws.amazon.com/cognito/latest/developerguide/role-based-access-control.html
QUESTION NO: 411
주로 온프레미스에서 애플리케이션 서버를 실행하는 회사가 AWS로 마이그레이션하기로
결정했습니다. 회사는 온프레미스에서 iSCSI(인터넷 소형 컴퓨터 시스템 인터페이스)
스토리지를 확장할 필요성을 최소화하려고 합니다. 회사는 최근에 액세스한 데이터만 로컬에
저장되기를 원합니다.
이러한 요구 사항을 충족하려면 회사에서 어떤 AWS 솔루션을 사용해야 합니까?
A. Amazon S3 파일 게이트웨이
B. AWS Storage Gateway 테이프 게이트웨이
C. AWS Storage Gateway 볼륨 게이트웨이 저장 볼륨
D. AWS Storage Gateway 볼륨 게이트웨이 캐시 볼륨
282

IT Certification Guaranteed, The Easy Way!
Answer: D
Explanation:
AWS Storage Gateway Volume Gateway provides two configurations for connecting to iSCSI
storage, namely, stored volumes and cached volumes. The stored volume configuration
stores the entire data set on- premises and asynchronously backs up the data to AWS. The
cached volume configuration stores recently accessed data on-premises, and the remaining
data is stored in Amazon S3. Since the company wants only its recently accessed data to
remain stored locally, the cached volume configuration would be the most appropriate. It
allows the company to keep frequently accessed data on-premises and reduce the need for
scaling its iSCSI storage while still providing access to all data through the AWS cloud. This
configuration also provides low-latency access to frequently accessed data and cost-effective
off-site backups for less frequently accessed data.
https://docs.amazonaws.cn/en_us/storagegateway/latest/vgw/StorageGatewayConcepts.html
#storage-gateway- cached-concepts
QUESTION NO: 412
한 엔터테인먼트 회사는 Amazon DynamoDB를 사용하여 미디어 메타데이터를 저장하고
있습니다. 애플리케이션이 읽기 집중적이어서 지연이 발생하고 있습니다. 회사에는 추가 운영
오버헤드를 처리할 직원이 없으며 애플리케이션을 재구성하지 않고 DynamoDB의 성능
효율성을 개선해야 합니다.
이 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까?
A. Redis용 Amazon ElastiCache를 사용합니다.
B. Amazon DynamoDB Accelerator(DAX)를 사용합니다.
C. DynamoDB 전역 테이블을 사용하여 데이터를 복제합니다.
D. 자동 검색이 활성화된 Memcached용 Amazon ElastiCache를 사용합니다.
Answer: B
Explanation:
https://aws.amazon.com/dynamodb/dax/
QUESTION NO: 413
회사에는 고객이 Amazon S3 버킷에 이미지를 업로드하는 데 사용하는 애플리케이션이
있습니다. 매일 밤 회사는 회사가 그날 받은 모든 이미지를 처리하는 Amazon EC2 스팟
집합을 시작합니다. 각 이미지를 처리하는 데는 2분이 걸리며 512MB의 메모리가 필요합니다.
솔루션 설계자는 이미지가 업로드될 때 이미지를 처리하도록 애플리케이션을 변경해야
합니다. 어떤 변경 사항이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?
A. S3 이벤트 알림을 사용하여 Amazon Simple Queue Service(Amazon SQS) 대기열에
이미지 세부 정보가 포함된 메시지를 씁니다. 대기열에서 메시지를 읽고 이미지를 처리하도록
AWS Lambda 함수를 구성합니다.
B. S3 이벤트 알림을 사용하여 이미지 세부 정보가 포함된 메시지를 Amazon Simple Queue
Service(Amazon SQS) 대기열에 씁니다. 대기열에서 메시지를 읽고 이미지를 처리하도록
EC2 예약 인스턴스를 구성합니다.
C. S3 이벤트 알림을 사용하여 이미지 세부 정보가 포함된 메시지를 Amazon SNS(Amazon
SNS) 주제에 게시합니다. 주제를 구독하고 이미지를 처리하도록 Amazon Elastic Container
Service(Amazon ECS)에서 컨테이너 인스턴스를 구성합니다.
D. S3 이벤트 알림을 사용하여 이미지 세부 정보가 포함된 메시지를 Amazon SNS(Amazon
283

IT Certification Guaranteed, The Easy Way!
SNS) 주제에 게시합니다. 주제를 구독하고 이미지를 처리합니다.
Answer: A
Explanation:
* Understanding the Requirement: The company needs to process images as they are
uploaded to S3 in a cost-effective manner, currently using an EC2 Spot Fleet for nightly
processing.
* Analysis of Options:
* S3 Event Notifications to SQS and Lambda: This setup allows for event-driven processing
with Lambda, which scales automatically based on the number of messages in the queue. It
is cost- effective as Lambda charges are based on the compute time used.
* S3 Event Notifications to SQS and EC2 Reserved Instance: Involves managing EC2
instances, which adds operational overhead and is less cost-effective.
* S3 Event Notifications to SNS and ECS: More complex and potentially less cost-effective
compared to using Lambda for simple processing tasks.
* S3 Event Notifications to SNS: Requires additional configuration and management to
process messages.
* Best Solution:
* S3 Event Notifications to SQS and Lambda: This option is the most cost-effective and
scalable, leveraging AWS managed services with minimal operational overhead.
References:
* Amazon S3 Event Notifications
* Amazon SQS
* AWS Lambda
QUESTION NO: 414
한 회사가 AWS에서 확장 가능한 웹 애플리케이션을 호스팅하려고 합니다. 이
애플리케이션은 전 세계 다양한 지역의 사용자가 액세스할 수 있습니다. 애플리케이션
사용자는 최대 기가바이트 크기의 고유한 데이터를 다운로드하고 업로드할 수 있습니다.
개발팀은 업로드 및 다운로드 대기 시간을 최소화하고 성능을 최대화할 수 있는 비용
효율적인 솔루션을 원합니다.
이를 달성하려면 솔루션 아키텍트가 무엇을 해야 합니까?
A. Transfer Acceleration이 포함된 Amazon S3를 사용하여 애플리케이션을 호스팅합니다.
B. CacheControl 헤더와 함께 Amazon S3를 사용하여 애플리케이션을 호스팅합니다.
C. Auto Scaling 및 Amazon CloudFront와 함께 Amazon EC2를 사용하여 애플리케이션을
호스팅합니다.
D. Auto Scaling 및 Amazon ElastiCache와 함께 Amazon EC2를 사용하여 애플리케이션을
호스팅합니다.
Answer: C
Explanation:
This answer is correct because it meets the requirements of hosting a scalable web
application that can handle large data transfers from different geographic regions. Amazon
EC2 provides scalable compute capacity for hosting web applications. Auto Scaling can
automatically adjust the number of EC2 instances based on the demand and traffic patterns.
Amazon CloudFront is a content delivery network (CDN) that can cache static and dynamic
content at edge locations closer to the users, reducing latency and improving performance.
284

IT Certification Guaranteed, The Easy Way!
CloudFront can also use S3 Transfer Acceleration to speed up the transfers between S3
buckets and CloudFront edge locations.
References:
* https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-
scaling.html
* https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html
* https://aws.amazon.com/s3/transfer-acceleration/
QUESTION NO: 415
회사는 AWS Lambda와 통합된 Amazon API Gateway API 백엔드를 사용하는 프런트엔드
애플리케이션을 호스팅합니다. API가 요청을 수신하면 Lambda 함수는 많은 librane을
로드합니다. 그런 다음 Lambda 함수는 Amazon RDS 데이터베이스에 연결하여 데이터를
처리하고 해당 데이터를 프론트엔드 애플리케이션. 회사는 회사 운영에 대한 변경 사항을
최소화하면서 모든 사용자에 대한 응답 대기 시간을 최대한 낮추고자 합니다. 어떤 솔루션이
이러한 요구 사항을 충족합니까?
A. API를 우회하여 쿼리 속도를 높이기 위해 프런트엔드 애플리케이션과 데이터베이스 간의
연결을 설정합니다.
B. 요청을 처리하는 Lambda 함수에 대해 프로비저닝된 동시성을 구성합니다.
C. 유사한 데이터 세트를 더 빠르게 검색하기 위해 Amazon S3에서 쿼리 결과를 캐시합니다.
D. 데이터베이스 크기를 늘려 Lambda가 한 번에 설정할 수 있는 연결 수를 늘립니다.
Answer: B
Explanation:
Configure provisioned concurrency for the Lambda function that handles the requests.
Provisioned concurrency allows you to set the amount of compute resources that are
available to the Lambda function, so that it can handle more requests at once and reduce
latency. Caching the results of the queries in Amazon S3 could also help to reduce latency,
but it would not be as effective as setting up provisioned concurrency.
Increasing the size of the database would not help to reduce latency, as this would not
increase the number of connections the Lambda function could establish, and establishing a
direct connection between the frontend application and the database would bypass the API,
which would not be the best solution either.
Using AWS Lambda with Amazon API Gateway - AWS Lambda
https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html
AWS Lambda FAQs
https://aws.amazon.com/lambda/faqs/
QUESTION NO: 416
한 회사는 중앙 집중식 AWS 계정을 사용하여 다양한 Amazon S3 버킷에 로그 데이터를
저장하고 있습니다. 솔루션 설계자는 데이터가 S3 버킷에 업로드되기 전에 데이터가
암호화되어 있는지 확인해야 합니다. 또한 데이터는 전송 중에 암호화되어야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 클라이언트 측 암호화를 사용하여 S3 버킷에 업로드되는 데이터를 암호화합니다.
B. 서버 측 암호화를 사용하여 S3 버킷에 업로드되는 데이터를 암호화합니다.
C. S3 업로드를 위해 S3 관리형 암호화 키(SSE-S3)와 함께 서버 측 암호화를 사용해야 하는
버킷 정책을 생성합니다.
285

IT Certification Guaranteed, The Easy Way!
D. 기본 AWS Key Management Service(AWS KMS) 키를 사용하여 S3 버킷을 암호화하는
보안 옵션을 활성화합니다.
Answer: A
Explanation:
Client-side encryption is a method of encrypting data before uploading it to Amazon S3. It
allows users to manage the encryption process, encryption keys, and related tools1. By using
client-side encryption, the solution can ensure that the data is encrypted at rest and in transit,
as Amazon S3 will not have access to the encryption keys or the unencrypted data2.
QUESTION NO: 417
회사에 사용자가 웹 인터레이스 또는 모바일 앱을 통해 문서를 업로드하는 프로덕션 웹
애플리케이션이 있습니다. 새로운 규정 요구 사항에 따라 새 문서는 저장된 후에 수정하거나
삭제할 수 없습니다.
솔루션 설계자는 이 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 업로드된 문서를 S3 버전 관리 및 S3 객체 잠금이 활성화된 Amazon S3 버킷에 저장
B. 업로드된 문서를 Amazon S3 버킷에 저장합니다. 문서를 주기적으로 보관하도록 S3 수명
주기 정책을 구성합니다.
C. 업로드된 문서를 S3 버전 관리가 활성화된 Amazon S3 버킷에 저장합니다. 모든 액세스를
읽기 전용으로 제한하도록 ACL을 구성합니다.
D. 업로드된 문서를 Amazon Elastic File System(Amazon EFS) 볼륨에 저장합니다. 읽기 전용
모드에서 볼륨을 마운트하여 데이터에 액세스합니다.
Answer: A
Explanation:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html
QUESTION NO: 418
회사는 두 개의 AWS 리전에서 3티어 애플리케이션을 실행합니다. 웹 계층, 애플리케이션
계층 및 데이터베이스 계층은 Amazon EC2 인스턴스에서 실행됩니다. 회사는 데이터베이스
계층에 Microsoft SQL Server Enterprise용 Amazon RDS를 사용합니다. 주간 및 월간
보고서를 실행할 때 데이터베이스 계층에 높은 로드가 발생합니다. 회사는 데이터베이스
계층의 로드를 줄이고 싶어합니다.
최소한의 관리 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 읽기 전용 복제본을 생성합니다. 새로운 읽기 복제본을 사용하도록 보고서를 구성합니다.
B. RDS 데이터베이스를 Amazon DynamoDB로 변환_ DynamoDB를 사용하도록 보고서 구성
C. 더 큰 인스턴스 크기를 선택하여 기존 RDS DB 인스턴스를 수정합니다.
D. 기존 ROS DB 인스턴스를 수정하고 해당 인스턴스를 Auto Scaling 그룹에 넣습니다.
Answer: A
Explanation:
it allows the company to create read replicas of its RDS database and reduce the load on the
database tier. By creating read replicas, the company can offload read traffic from the
primary database instance to one or more replicas. By configuring the reports to use the new
read replicas, the company can improve performance and availability of its database tier.
References:
* Working with Read Replicas
286

IT Certification Guaranteed, The Easy Way!
* Read Replicas for Amazon RDS for SQL Server
QUESTION NO: 419
회사에는 Auto Scaling 그룹의 여러 Amazon EC2 인스턴스에 다중 계층 애플리케이션이
배포되어 있습니다. Oracle 인스턴스용 Amazon RDS는 Oracle 관련 PL/SQL 기능을 사용하는
애플리케이션의 데이터 계층입니다. 애플리케이션 트래픽이 꾸준히 증가하고 있습니다. 이로
인해 EC2 인스턴스가 과부하되고 RDS 인스턴스의 스토리지가 부족해집니다. Auto Scaling
그룹에는 조정 지표가 없으며 최소 정상 인스턴스 수만 정의합니다. 회사는 트래픽이
정체되기 전에 꾸준하지만 예측할 수 없는 속도로 계속해서 증가할 것이라고 예측합니다.
시스템이 증가된 트래픽에 맞춰 자동으로 확장될 수 있도록 솔루션 설계자는 무엇을 해야
합니까? (2개를 선택하세요.)
A. Oracle 인스턴스용 RDS에서 스토리지 Auto Scaling을 구성합니다.
B. Auto Scaling 스토리지를 사용하려면 데이터베이스를 Amazon Aurora로
마이그레이션합니다.
C. 사용 가능한 저장 공간이 부족할 경우 Oracle 인스턴스용 RDS에 대한 경보를 구성합니다.
D. 평균 CPU를 조정 지표로 사용하도록 Auto Scaling 그룹을 구성합니다.
E. 평균 여유 메모리를 확인 지표로 사용하도록 Auto Scaling 그룹을 구성합니다.
Answer: A D
Explanation:
Auto scaling storage RDS will ease storage issues and migrating Oracle Pl/Sql to Aurora is
cumbersome. Also Aurora has auto storage scaling by default.
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide
/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling
QUESTION NO: 420
한 회사가 Amazon RDS for PostgreSQL에서 데이터베이스를 실행합니다. 이 회사는
30일마다 암호를 순환하여 마스터 사용자 암호를 관리하는 안전한 솔루션을 원합니다. 어떤
솔루션이 가장 적은 운영 오버헤드로 이러한 요구 사항을 충족할까요?
A. Amazon EventBridge를 사용하여 사용자 지정 AWS Lambda 함수를 예약하여 30일마다
비밀번호를 교체합니다.
B. AWS CLI에서 modlfy-db-instance 명령을 사용하여 비밀번호를 변경합니다.
C. AWS Secrets Manager를 Amazon RDS for PostgreSQL과 통합하여 비밀번호 교체를
자동화합니다.
D. AWS Systems Manager Parameter Store를 Amazon RDS for PostgreSQL과 통합하여
비밀번호 교체를 자동화합니다.
Answer: C
Explanation:
AWS Secrets Manager can integrate directly with Amazon RDS for automatic and seamless
password rotation. Secrets Manager handles the complexity of password management,
including generating strong passwords and rotating them at a defined interval (e.g., every 30
days). It also automatically updates the connection information for RDS, minimizing
operational overhead.
* Option A (Lambda with EventBridge): While possible, this requires custom coding and
operational management of Lambda, which introduces additional complexity.
287

IT Certification Guaranteed, The Easy Way!
* Option B (Manual password change): Using the modify-db-instance command requires
manual intervention and is not automated, leading to more operational effort.
* Option D (Parameter Store): Systems Manager Parameter Store is less specialized for
password management than Secrets Manager and does not have built-in automated rotation
for RDS credentials.
AWS References:
* AWS Secrets Manager Rotation for RDS
QUESTION NO: 421
회사는 단일 AWS 지역에서 Amazon EC2 인스턴스의 Auto Scaling 그룹을 사용하여 웹
사이트를 제공합니다. 웹 사이트에는 데이터베이스가 필요하지 않습니다. 회사는 확장 중이며
회사의 엔지니어링 팀은 웹 사이트를 두 번째 지역에 배포합니다. 회사는 성장을 수용하고
재해 복구 목적을 위해 두 지역에 트래픽을 분산하려고 합니다. 솔루션은 웹 사이트가
비정상인 지역의 트래픽을 처리해서는 안 됩니다.
이러한 요구 사항을 충족하기 위해 회사는 어떤 정책이나 리소스를 사용해야 합니까?
A. Amazon Route 53 단순 라우팅 정책
B. Amazon Route 53 다중값 응답 라우팅 정책
C. 두 지역의 EC2 인스턴스 ID를 지정하는 대상 그룹이 있는 한 지역의 Application Load
Balancer
D. 두 지역의 EC2 인스턴스 IP 주소를 지정하는 대상 그룹이 있는 한 지역의 Application Load
Balancer
Answer: B
Explanation:
* Amazon Route 53 Multivalue Answer Routing: This routing policy allows Route 53 to return
multiple values, such as IP addresses, in response to DNS queries. This can distribute traffic
across multiple resources and includes health checks to ensure traffic is only routed to
healthy instances.
* Health Checks:
* Configure health checks for each Region to monitor the health of the website instances.
* Route 53 will only include healthy instances in the DNS responses, ensuring that traffic is
not routed to an unhealthy Region.
* Load Distribution and Disaster Recovery:
* Multivalue answer routing helps balance the load between instances in different Regions.
* If instances in one Region become unhealthy, Route 53 will route traffic to the healthy
instances in the other Region.
* Operational Simplicity: This solution does not require complex configurations or additional
resources, making it a simple yet effective way to distribute traffic and ensure high
availability.
References:
* Amazon Route 53 Routing Policies
* Multivalue Answer Routing
QUESTION NO: 422
회사는 Amazon EC2 인스턴스에서 애플리케이션을 실행합니다. 회사는 애플리케이션에 대한
재해 복구(DR) 솔루션을 구현해야 합니다. DR 솔루션의 RTO(복구 시간 목표)는 4시간
288

IT Certification Guaranteed, The Easy Way!
미만이어야 합니다. 또한 DR 솔루션은 일반 작업 중에 가능한 최소한의 AWS 리소스를
사용해야 합니다.
어떤 솔루션이 운영상 가장 효율적인 방식으로 이러한 요구 사항을 충족합니까?
A. Amazon 머신 이미지(AMI)를 생성하여 EC2 인스턴스를 백업합니다. AMI를 보조 AWS
리전에 복사합니다. AWS Lambda 및 사용자 지정 스크립트를 사용하여 보조 리전에서 인프라
배포를 자동화합니다.
B. Amazon 머신 이미지(AMI)를 생성하여 EC2 인스턴스를 백업합니다. AMI를 보조 AWS
리전에 복사합니다. AWS CloudFormation을 사용하여 보조 리전에서 인프라 배포를
자동화합니다.
C. 보조 AWS 지역에서 EC2 인스턴스를 시작합니다. 보조 리전의 EC2 인스턴스를 항상 활성
상태로 유지하세요.
D. 보조 가용 영역에서 EC2 인스턴스를 시작합니다. 보조 가용 영역의 EC2 인스턴스를 항상
활성 상태로 유지합니다.
Answer: B
Explanation:
it allows the company to implement a disaster recovery (DR) solution for the application that
has a recovery time objective (RTO) of less than 4 hours and uses the fewest possible AWS
resources during normal operations. By creating Amazon Machine Images (AMIs) to back up
the EC2 instances and copying the AMIs to a secondary AWS Region, the company can
create point-in-time snapshots of the application and store them in a different geographical
location. By automating infrastructure deployment in the secondary Region by using AWS
CloudFormation, the company can quickly launch a stack of resources from a template in
case of a disaster. This is a cost-effective and operationally efficient way to implement a DR
solution for EC2 instances. References:
* Amazon Machine Images (AMI)
* Copying an AMI
* AWS CloudFormation
* Working with Stacks
QUESTION NO: 423
한 회사에서 새로운 웹 기반 고객 관계 관리 애플리케이션을 구축하고 있습니다.
애플리케이션은 ALB(Application Load Balancer) 뒤에 있는 Amazon Elastic Block
Store(Amazon EBS) 볼륨이 지원하는 여러 Amazon EC2 인스턴스를 사용합니다.
애플리케이션은 Amazon Aurora 데이터베이스도 사용합니다. 애플리케이션의 모든 데이터는
저장 및 전송 중에 암호화되어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. ALB에서 AWS Key Management Service(AWS KMS) 인증서를 사용하여 전송 중인
데이터를 암호화합니다. AWS Certificate Manager(ACM)를 사용하여 미사용 EBS 볼륨과
Aurora 데이터베이스 스토리지를 암호화합니다.
B. AWS 루트 계정을 사용하여 AWS Management Console에 로그인합니다. 회사의 암호화
인증서를 업로드하세요. 루트 계정에 있는 동안 계정의 저장 및 전송 중인 모든 데이터에 대해
암호화를 켜는 옵션을 선택합니다.
C. AWS KMS(AWS Key Management Service)를 사용하여 EBS 볼륨과 Aurora 데이터베이스
스토리지를 암호화합니다. ACM(AWS Certificate Manager) 인증서를 ALB에 연결하여 전송
중인 데이터를 암호화합니다.
289

IT Certification Guaranteed, The Easy Way!
D. BitLocker를 사용하여 모든 저장 데이터를 암호화합니다. 회사의 TLS 인증서 키를 AWS
Key Management Service(AWS KMS)로 가져옵니다. 전송 중인 데이터를 암호화하려면 KMS
키를 ALB에 연결하세요.
Answer: C
Explanation:
This option is the most efficient because it uses AWS Key Management Service (AWS KMS),
which is a service that makes it easy for you to create and manage cryptographic keys and
control their use across a wide range of AWS services and with your applications running on
AWS1. It also uses AWS KMS to encrypt the EBS volumes and Aurora database storage at
rest, which provides data protection by encrypting your data with encryption keys that you
manage23. It also uses AWS Certificate Manager (ACM), which is a service that lets you
easily provision, manage, and deploy public and private Secure Sockets Layer/Transport
Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected
resources. It also attaches an ACM certificate to the ALB to encrypt data in transit, which
provides data protection by enabling SSL/TLS encryption for connections between clients
and the load balancer. This solution meets the requirement of encrypting all data for the
application at rest and in transit. Option A is less efficient because it uses AWS KMS
certificates on the ALB to encrypt data in transit, which is not possible as AWS KMS does not
provide certificates but only keys. It also uses AWS Certificate Manager (ACM) to encrypt the
EBS volumes and Aurora database storage at rest, which is not possible as ACM does not
provide encryption but only certificates. Option B is less efficient because it uses the AWS
root account to log in to the AWS Management Console, which is not recommended as it has
unrestricted access to all resources in your account. It also uploads the company's
encryption certificates, which is not necessary as ACM can provide certificates for free. It also
selects the option to turn on encryption for all data at rest and in transit for the account, which
is not possible as encryption settings are specific to each service and resource. Option D is
less efficient because it uses BitLocker to encrypt all data at rest, which is a Windows feature
that provides encryption for volumes on Windows servers. However, this does not provide
encryption for Aurora database storage at rest, as Aurora runs on Linux servers. It also
imports the company's TLS certificate keys to AWS KMS, which is not necessary as ACM
can provide certificates for free. It also attaches the KMS keys to the ALB to encrypt data in
transit, which is not possible as ALB requires certificates and not keys.
QUESTION NO: 424
솔루션 설계자가 애플리케이션을 위한 새로운 Amazon CloudFront 배포를 생성하고
있습니다. 사용자가 제출한 정보 중 일부는 민감한 정보입니다. 애플리케이션은 HTTPS를
사용하지만 다른 보안 계층이 필요합니다. 민감한 정보는 전체 애플리케이션 스택에서
보호되어야 하며 정보에 대한 액세스는 특정 애플리케이션으로 제한되어야 합니다.
솔루션 아키텍트는 어떤 조치를 취해야 합니까?
A. CloudFront 서명된 URL을 구성합니다.
B. CloudFront 서명 쿠키를 구성합니다.
C. CloudFront 필드 수준 암호화 프로필을 구성합니다.
D. CloudFront를 구성하고 뷰어 프로토콜 정책에 대해 오리진 프로토콜 정책 설정을 HTTPS
전용으로 설정합니다.
Answer: C
290

IT Certification Guaranteed, The Easy Way!
Explanation:
it allows the company to protect sensitive information submitted by users throughout the
entire application stack and restrict access to certain applications. By configuring a
CloudFront field-level encryption profile, the company can encrypt specific fields of user data
at the edge locations before sending it to the origin servers. By using public-private key pairs,
the company can ensure that only authorized applications can decrypt and access the
sensitive information. References:
* Field-Level Encryption
* Encrypting and Decrypting Data
QUESTION NO: 425
회사는 AWS에 새로운 공개 웹 애플리케이션을 배포하려고 합니다. 애플리케이션에 Amazon
EC2 인스턴스를 사용하는 웹 서버 계층이 포함되어 있습니다. 애플리케이션에 MySQL DB
인스턴스용 Amazon RDS를 사용하는 데이터베이스 계층도 포함되어 있습니다.
애플리케이션은 안전해야 하며 글로벌 고객이 액세스할 수 있어야 합니다. 동적 IP 주소가
있는 솔루션 설계자는 이러한 요구 사항을 충족하도록 보안 그룹을 어떻게 구성해야 합니까?
A. 웹 서버에 대한 보안 그룹을 구성하여 0.0.0에서 포트 443의 인바운드 트래픽을
허용합니다. 0/0) 웹 서버의 보안 그룹에서 포트 3306의 인바운드 트래픽을 허용하도록 DB
인스턴스의 보안 그룹을 구성합니다.
B. 고객의 IP 주소에서 포트 443의 인바운드 트래픽을 허용하도록 웹 서버에 대한 보안 그룹을
구성합니다. DB 인스턴스에 대한 보안 그룹을 구성하여 웹 서버의 보안 그룹에서 포트 3306의
인바운드 트래픽을 허용합니다.
C. 고객의 IP 주소에서 포트 443의 인바운드 트래픽을 허용하도록 웹 서버에 대한 보안 그룹을
구성합니다. 고객의 IP 주소에서 포트 3306의 인바운드 트래픽을 허용하도록 DB 인스턴스에
대한 보안 그룹을 구성합니다.
D. 0.0.0.0.0부터 포트 443의 인바운드 트래픽을 허용하도록 웹 서버의 보안 그룹을
구성합니다. 0.0.0.0/0부터 포트 3306의 인바운드 트래픽을 허용하도록 DB 인스턴스의 보안
그룹을 구성합니다.
Answer: A
Explanation:
* Restricting inbound access to the web servers to only port 443, which is used for HTTPS
traffic, and allowing access from any IP address (0.0.0.0/0), since the application is public
and accessible for global customers.
* Restricting inbound access to the DB instance to only port 3306, which is used for MySQL
traffic, and allowing access only from the security group of the web servers, which creates a
secure connection between the two tiers and prevents unauthorized access to the database.
* Restricting outbound access to the minimum required for both tiers, which is not specified in
the question but can be assumed to be similar to the inbound rules.
References:
* Security groups - Amazon Virtual Private Cloud
* 5 Best Practices for AWS Security Groups - DZone
QUESTION NO: 426
한 데이터 분석 회사가 배치 처리 시스템을 AWS로 마이그레이션하려고 합니다. 회사는
FTP를 통해 하루 동안 정기적으로 수천 개의 작은 데이터 파일을 받습니다. 온프레미스 일괄
291

IT Certification Guaranteed, The Easy Way!
작업은 밤새 데이터 파일을 처리합니다. 그러나 일괄 작업 실행을 완료하는 데 몇 시간이
걸립니다.
회사는 파일을 보내는 FTP 클라이언트를 최소한으로 변경하여 수신 데이터 파일을 처리할 수
있는 AWS 솔루션을 원합니다. 솔루션은 파일이 성공적으로 처리된 수신 데이터 파일을
삭제해야 합니다. 각 파일을 처리하는 데는 3~8분이 소요됩니다.
어떤 솔루션이 운영상 가장 효율적인 방식으로 이러한 요구 사항을 충족합니까?
A. FTP 서버를 실행하는 Amazon EC2 인스턴스를 사용하여 수신 파일을 Amazon S3 Glacier
유연한 검색의 객체로 저장합니다. AWS Batch에서 작업 대기열을 구성합니다. Amazon
EventBridge 규칙을 사용하여 S3 Glacier 유연한 검색에서 야간에 객체를 처리하는 작업을
호출합니다. 작업에서 개체를 처리한 후 개체를 삭제합니다.
B. FTP 서버를 실행하는 Amazon EC2 인스턴스를 사용하여 Amazon Elastic Block
Store(Amazon EBS) 볼륨에 수신 파일을 저장합니다. AWS Batch에서 작업 대기열을
구성합니다. Amazon EventBridge 규칙을 사용하여 EBS 볼륨에서 매일 밤 파일 프로세스를
호출합니다. 작업에서 파일을 처리한 후 파일을 삭제합니다.
C. AWS Transfer Family를 사용하여 Amazon Elastic Block Store(Amazon EBS) 볼륨에 수신
파일을 저장하기 위한 FTP 서버를 생성합니다. AWS Batch에서 작업 대기열을 구성합니다.
각 파일이 도착하면 Amazon S3 이벤트 알림을 사용하여 AWS Batch에서 작업을 호출합니다.
작업에서 파일을 처리한 후 파일을 삭제합니다.
D. AWS Transfer Family를 사용하여 Amazon S3 Standard에 수신 파일을 저장할 FTP 서버를
생성합니다. 파일을 처리하고 처리된 후 파일을 삭제하는 AWS Lambda 함수를 생성합니다.
파일이 도착할 때 람다 함수를 호출하는 S3 이벤트 알림
Answer: D
Explanation:
This option is the most operationally efficient because it uses AWS Transfer Family to create
an FTP server that can store incoming files in Amazon S3 Standard12, which is a low-cost
and highly available storage service. It also uses AWS Lambda to process the files and
delete them after they are processed, which is a serverless and scalable solution that does
not require any batch scheduling or infrastructure management. It also uses S3 event
notifications to invoke the Lambda function when the files arrive, which enables near real-
time processing of the incoming data files3. Option A is less efficient because it uses Amazon
S3 Glacier Flexible Retrieval, which is a cold storage class that has higher retrieval costs and
longer retrieval times than Amazon S3 Standard. It also uses EventBridge rules to invoke the
job nightly, which does not meet the requirement of processing incoming data files as soon
as possible. Option B is less efficient because it uses an EBS volume to store incoming files,
which is a block storage service that has higher costs and lower durability than Amazon S3. It
also uses EventBridge rules to invoke the job nightly, which does not meet the requirement of
processing incoming data files as soon as possible. Option C is less efficient because it uses
an EBS volume to store incoming files, which is a block storage service that has higher costs
and lower durability than Amazon S3. It also uses AWS Batch to process the files, which
requires managing compute resources and job queues.
QUESTION NO: 427
한 회사는 AWS를 사용하여 공개 전자상거래 웹사이트를 호스팅합니다. 웹 사이트는 인터넷
트래픽에 AWS Global Accelerator 액셀러레이터를 사용합니다. Global Accelerator
Accelerator는 Auto Scaling 그룹의 진입점인 ALB(Application Load Balancer)로 트래픽을
292

IT Certification Guaranteed, The Easy Way!
전달합니다.
회사는 최근 웹사이트에서 ODoS 공격을 확인했습니다. 회사에는 향후 공격을 완화할 수 있는
솔루션이 필요합니다.
최소한의 구현 노력으로 이러한 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?
A. 속도 기반 규칙을 사용하여 Global Accelerator Accelerator가 트래픽을 차단하도록 AWS
WAF 웹 ACL을 구성합니다.
B. VPC 네트워크 ACL을 업데이트하여 공격을 차단하기 위해 ALB 지표를 읽도록 AWS
Lambda 함수를 구성합니다.
C. 비율 기반 규칙을 사용하여 트래픽을 차단하도록 ALB에서 AWS WAF 웹 ACL을
구성합니다.
D. Global Accelerator 액셀러레이터 앞에 Ama7on CloudFront 배포를 구성합니다.
Answer: A
Explanation:
* Understanding the Requirement: The company needs to mitigate DDoS attacks on its
website, which uses AWS Global Accelerator to route traffic to an Application Load Balancer
(ALB).
* Analysis of Options:
* AWS WAF on Global Accelerator: Allows for centralized protection and can block traffic
based on rate-based rules, effectively mitigating DDoS attacks with minimal implementation
effort.
* Lambda Function and VPC Network ACL: Requires custom implementation and ongoing
management, increasing complexity and effort.
* AWS WAF on ALB: Provides protection but involves additional configuration and
management at the ALB level.
* CloudFront Distribution in front of Global Accelerator: Adds unnecessary complexity and
changes the current traffic flow setup.
* Best Solution:
* AWS WAF on Global Accelerator: This provides the required protection with the least
implementation effort, ensuring effective DDoS mitigation and maintaining the existing
architecture.
References:
* AWS WAF
* Using AWS WAF with AWS Global Accelerator
QUESTION NO: 428
한 글로벌 기업이 AWS에서 워크로드를 실행하고 있습니다. 이 기업의 애플리케이션은
중요한 데이터 저장 및 분석을 위해 AWS 리전 전체에서 Amazon S3 버킷을 사용합니다.
회사는 매일 수백만 개의 객체를 여러 S3 버킷에 저장합니다. 회사는 버전 관리가 활성화되지
않은 모든 S3 버킷을 식별하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 여러 지역에서 버전 관리가 활성화되지 않은 모든 S3 버킷을 식별하는 규칙이 있는 AWS
CloudTrail 이벤트를 설정합니다.
B. Amazon S3 Storage Lens를 사용하여 여러 지역에서 버전 관리가 활성화되지 않은 모든
S3 버킷을 식별합니다.
C. S3용 IAM Access Analyser를 활성화하여 여러 지역에서 버전 관리가 활성화되지 않은
293

IT Certification Guaranteed, The Easy Way!
모든 S3 버킷을 식별합니다.
D. S3 다중 지역 액세스 포인트를 생성하여 지역 전체에서 버전 관리가 활성화되지 않은 모든
S3 버킷을 식별합니다.
Answer: B
Explanation:
* Amazon S3 Storage Lens:
* S3 Storage Lens provides organization-wide visibility into object storage usage and activity
trends.
* It can generate metrics and insights about your S3 buckets, including versioning status.
* Configuration:
* Enable S3 Storage Lens at the organization level.
* Configure the dashboard to include the versioning status metric.
* Identify Non-Versioned Buckets:
* Use the S3 Storage Lens dashboard to filter and identify buckets that do not have
versioning enabled.
* Storage Lens provides detailed insights and reports which can be used to enforce
compliance and manage storage effectively.
* Operational Efficiency: Using S3 Storage Lens provides a centralized, easy-to-use interface
for monitoring bucket configurations across multiple Regions and accounts, reducing the
need for custom scripts or manual checks.
References:
* Amazon S3 Storage Lens
* S3 Storage Lens Metrics
QUESTION NO: 429
회사는 각 워크로드에 대해 AWS 계정을 생성하여 워크로드를 격리하려고 합니다. 회사에는
워크로드에 대한 네트워킹 구성 요소를 중앙에서 관리하는 솔루션이 필요합니다. 또한
솔루션은 자동 보안 제어(가드레일)가 포함된 계정을 생성해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Control Tower를 사용하여 계정을 배포합니다. 프라이빗 서브넷과 퍼블릭 서브넷이
있는 VPC가 있는 네트워킹 계정을 생성합니다. AWS Resource Access Manager(AWS
RAM)를 사용하여 워크로드 계정과 서브넷을 공유합니다.
B. AWS Organizations를 사용하여 계정을 배포합니다. 프라이빗 서브넷과 퍼블릭 서브넷이
있는 VPC가 있는 네트워킹 계정을 생성합니다. AWS Resource Access Manager(AWS
RAM)를 사용하여 워크로드 계정과 서브넷을 공유합니다.
C. AWS Control Tower를 사용하여 계정을 배포합니다. 각 워크로드 계정에 VPC를
배포합니다. 전송 게이트웨이 연결을 사용하여 검사 VPC를 통해 라우팅하도록 각 VPC를
구성합니다.
D. AWS Organizations를 사용하여 계정을 배포합니다. 각 워크로드 계정에 VPC를
배포합니다. 전송 게이트웨이 연결을 사용하여 검사 VPC를 통해 라우팅하도록 각 VPC를
구성합니다.
Answer: A
Explanation:
* AWS Control Tower: Provides a managed service to set up and govern a secure, multi-
account AWS environment based on AWS best practices. It automates the setup of AWS
294

IT Certification Guaranteed, The Easy Way!
Organizations and applies security controls (guardrails).
* Networking Account:
* Create a centralized networking account that includes a VPC with both private and public
subnets.
* This centralized VPC will manage and control the networking resources.
* AWS Resource Access Manager (AWS RAM):
* Use AWS RAM to share the subnets from the networking account with the other workload
accounts.
* This allows different workload accounts to utilize the shared networking resources without
the need to manage their own VPCs.
* Operational Efficiency: Using AWS Control Tower simplifies the setup and governance of
multiple AWS accounts, while AWS RAM facilitates centralized management of networking
resources, reducing operational overhead and ensuring consistent security and compliance.
References:
* AWS Control Tower
* AWS Resource Access Manager
QUESTION NO: 430
회사는 300개 이상의 글로벌 웹사이트 및 애플리케이션을 호스팅합니다. 이 회사는 매일
30TB 이상의 클릭스트림 데이터를 분석할 플랫폼이 필요합니다.
솔루션 설계자는 클릭스트림 데이터를 전송하고 처리하기 위해 무엇을 해야 합니까?
A. 데이터를 Amazon S3 버킷에 보관하고 데이터로 Amazon EMR 더스터를 실행하여 분석을
생성하도록 AWS Data Pipeline을 설계합니다.
B. Amazon EC2 인스턴스의 Auto Scaling 그룹을 생성하여 데이터를 처리하고 Amazon
Redshift가 Tor 분석을 사용할 수 있도록 Amazon S3 데이터 레이크로 보냅니다.
C. Amazon CloudFron에 데이터 캐시: Amazon S3 버킷에 데이터 저장 S3 버킷에 객체가
추가되면 AWS Lambda 함수를 실행하여 데이터 tor 분석을 처리합니다.
D. Amazon Kinesis Data Streams에서 데이터를 수집합니다. Amazon Kinesis Data
Firehose를 사용하여 Amazon S3 데이터 레이크로 데이터 전송 분석을 위해 Amazon
Redshift에 데이터 로드
Answer: D
Explanation:
https://aws.amazon.com/es/blogs/big-data/real-time-analytics-with-amazon-redshift-
streaming-ingestion/
QUESTION NO: 431
회사에 AWS로 마이그레이션해야 하는 Windows 기반 애플리케이션이 있습니다.
애플리케이션을 사용하려면 여러 가용 영역에 배포된 여러 Amazon EC2 Windows
인스턴스에 연결된 공유 Windows 파일 시스템을 사용해야 합니다.
이 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 볼륨 게이트웨이 모드에서 AWS Storage Gateway를 구성합니다. 각 Windows 인스턴스에
볼륨을 마운트합니다.
B. Windows 파일 서버용 Amazon FSx를 구성합니다. Amazon FSx 파일 시스템을 각
Windows 인스턴스에 탑재합니다.
C. Amazon Elastic File System(Amazon EFS)을 사용하여 파일 시스템을 구성합니다. 각
295

IT Certification Guaranteed, The Easy Way!
Windows 인스턴스에 EFS 파일 시스템을 탑재합니다.
D. 필요한 크기로 Amazon Elastic Block Store(Amazon EBS) 볼륨을 구성합니다. 각 EC2
인스턴스를 볼륨에 연결합니다. 볼륨 내의 파일 시스템을 각 Windows 인스턴스에
마운트합니다.
Answer: B
Explanation:
This solution meets the requirement of migrating a Windows-based application that requires
the use of a shared Windows file system attached to multiple Amazon EC2 Windows
instances that are deployed across multiple Availability Zones. Amazon FSx for Windows File
Server provides fully managed shared storage built on Windows Server, and delivers a wide
range of data access, data management, and administrative capabilities. It supports the
Server Message Block (SMB) protocol and can be mounted to EC2 Windows instances
across multiple Availability Zones.
Option A is incorrect because AWS Storage Gateway in volume gateway mode provides
cloud-backed storage volumes that can be mounted as iSCSI devices from on-premises
application servers, but it does not support SMB protocol or EC2 Windows instances. Option
C is incorrect because Amazon Elastic File System (Amazon EFS) provides a scalable and
elastic NFS file system for Linux-based workloads, but it does not support SMB protocol or
EC2 Windows instances. Option D is incorrect because Amazon Elastic Block Store (Amazon
EBS) provides persistent block storage volumes for use with EC2 instances, but it does not
support SMB protocol or attaching multiple instances to the same volume.
References:
* https://aws.amazon.com/fsx/windows/
* https://docs.aws.amazon.com/fsx/latest/WindowsGuide/using-file-shares.html
QUESTION NO: 432
솔루션 아키텍트는 AWS CloudFormation 템플릿을 사용하여 3티어 웹 애플리케이션을
배포하고 있습니다. 웹 애플리케이션은 웹 계층과 Amazon DynamoDB 테이블에 사용자
데이터를 저장하고 검색하는 애플리케이션 계층으로 구성됩니다. 웹 및 애플리케이션 계층은
Amazon EC2 인스턴스에서 호스팅되며 데이터베이스 계층은 공개적으로 액세스할 수
없습니다. 애플리케이션 EC2 인스턴스는 템플릿에 API 자격 증명을 노출하지 않고
DynamoDB 테이블에 액세스해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. DynamoDB 테이블을 읽기 위한 IAM 역할을 생성합니다. 인스턴스 프로필을 참조하여
역할을 애플리케이션 인스턴스와 연결합니다.
B. DynamoDB 테이블에서 읽고 쓰는 데 필요한 권한이 있는 IAM 역할을 생성합니다. EC2
인스턴스 프로필에 역할을 추가하고 인스턴스 프로필을 애플리케이션 인스턴스와
연결합니다.
C. AWS CloudFormation 템플릿의 매개변수 섹션을 사용하여 DynamoDB 테이블에서 읽고
쓰는 데 필요한 권한이 있는 이미 생성된 IAM 사용자로부터 사용자 입력 액세스 및 보안 키를
얻습니다.
D. AWS CloudFormation 템플릿에서 DynamoDB 테이블을 읽고 쓰는 데 필요한 권한이 있는
IAM 사용자를 생성합니다. GetAtt 함수를 사용하여 액세스 키와 비밀 키를 검색하고 이를
사용자 데이터를 통해 애플리케이션 인스턴스에 전달합니다.
Answer: B
296

IT Certification Guaranteed, The Easy Way!
Explanation:
it allows the application EC2 instances to access the DynamoDB tables without exposing API
credentials in the template. By creating an IAM role that has the required permissions to read
and write from the DynamoDB tables and adding it to the EC2 instance profile, the
application instances can use temporary security credentials that are automatically rotated by
AWS. This is a secure and best practice way to grant access to AWS resources from EC2
instances. References:
* IAM Roles for Amazon EC2
* Using Instance Profiles
QUESTION NO: 433
한 회사에서 Amazon Elastic Container Service(Amazon ECS) 클러스터에 배포된 새
애플리케이션을 시작하고 ECS 작업에 Fargate 시작 유형을 사용하고 있습니다. 회사는 시작
시 애플리케이션에 대한 높은 트래픽이 예상되므로 CPU 및 메모리 사용량을 모니터링하고
있습니다. 회사는 활용도가 감소하면 비용을 절감하려고 합니다. 솔루션 설계자는 무엇을
권장해야 합니까?
A. Amazon EC2 Auto Scaling을 사용하여 이전 트래픽 패턴을 기반으로 특정 기간에 규모를
조정합니다.
B. AWS Lambda 함수를 사용하여 Amazon CloudWatch 경보를 트리거하는 지표 위반에 따라
Amazon ECS를 확장합니다.
C. ECS 지표 위반이 Amazon CloudWatch 경보를 트리거할 때 간단한 조정 정책과 함께
Amazon EC2 Auto Scaling을 사용하여 조정합니다.
D. ECS 지표 위반이 Amazon CloudWatch 경보를 트리거할 때 대상 추적 정책과 함께 AWS
Application Auto Scaling을 사용하여 규모를 조정합니다.
Answer: D
Explanation:
https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-
scaling.html
QUESTION NO: 434
ALB(Application Load Balancer) 뒤에 있는 Amazon EC2 인스턴스에서 웹 사이트를
호스팅하는 회사가 있습니다. 웹 사이트는 정적 콘텐츠를 제공합니다. 웹 사이트 트래픽이
증가하고 있으며 회사는 잠재적인 비용 증가를 우려하고 있습니다.
웹 사이트 비용을 줄이려면 솔루션 아키텍트가 무엇을 해야 합니까?
A. Amazon CloudFront 배포를 생성하여 엣지 위치에 정적 파일을 캐시합니다.
B. Amazon ElastiCache 클러스터 생성 ALB를 ElastiCache 클러스터에 연결하여 캐시된
파일을 제공합니다.
C. AWS WAF 웹 ACL을 생성하고 이를 ALB와 연결합니다. 웹 ACL에 규칙을 추가하여 정적
파일을 캐시합니다.
D. 대체 AWS 지역에 두 번째 ALB 생성 사용자 트래픽을 가장 가까운 지역으로 라우팅하여
데이터 전송 비용을 최소화합니다.
Answer: A
Explanation:
Amazon CloudFront is a content delivery network (CDN) that can improve the performance
and reduce the cost of serving static content from a website. CloudFront can cache static
297

IT Certification Guaranteed, The Easy Way!
files at edge locations closer to the users, reducing the latency and data transfer costs.
CloudFront can also integrate with Amazon S3 as the origin for the static content, eliminating
the need for EC2 instances to host the website. CloudFront meets all the requirements of the
question, while the other options do not. References:
* https://aws.amazon.com/blogs/architecture/architecting-a-low-cost-web-content-publishing-
system/
* https://nodeployfriday.com/posts/static-website-hosting/
* https://aws.amazon.com/cloudfront/
QUESTION NO: 435
회사에서 온프레미스 데이터 센터를 AWS로 마이그레이션하려고 합니다. 회사의 규정 준수
요구 사항에 따라 회사는 ap-northeast-3 리전만 사용할 수 있습니다. 회사 관리자는 VPC를
인터넷에 연결할 수 없습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까? (2개를 선택하세요.)
A. AWS Control Tower를 사용하여 데이터 상주 가드레일을 구현하여 인터넷 액세스를
거부하고 ap-northeast-3을 제외한 모든 AWS 리전에 대한 액세스를 거부합니다.
B. AWS WAF의 규칙을 사용하여 인터넷 액세스를 방지합니다. AWS 계정 설정에서 ap-
northeast-3을 제외한 모든 AWS 리전에 대한 액세스를 거부합니다.
C. AWS Organizations를 사용하여 VPC가 인터넷에 액세스하지 못하도록 방지하는 서비스
제어 정책(SCPS)을 구성합니다. ap-northeast-3을 제외한 모든 AWS 리전에 대한 액세스를
거부합니다.
D. 각 VPC의 네트워크 ACL에 대한 아웃바운드 규칙을 생성하여 0.0.0.0/0의 모든 트래픽을
거부합니다. ap-northeast-3 이외의 AWS 리전 사용을 방지하려면 각 사용자에 대한 IAM
정책을 생성합니다.
E. AWS Config를 사용하여 관리형 규칙을 활성화하여 인터넷 게이트웨이를 탐지 및 경고하고
ap-northeast-3 외부에 배포된 새 리소스를 탐지 및 경고합니다.
Answer: A C
Explanation:
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_ex
amples_vpc.
html#example_vpc_2
QUESTION NO: 436
회사는 Auto Scaling 그룹의 Amazon EC2 온디맨드 인스턴스에서 애플리케이션을
호스팅합니다. 신청 피크 시간은 매일 같은 시간에 발생합니다. 애플리케이션 사용자는 피크
시간이 시작될 때 애플리케이션 성능이 느려진다고 보고합니다. 애플리케이션은 일반적으로
피크 시간이 시작된 후 2~3시간 후에 실행됩니다. 회사는 애플리케이션이 피크 시간대 시작
시 제대로 작동하는지 확인하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 트래픽을 인스턴스에 적절하게 분산하도록 Application Load Balancer를 구성합니다.
B. 메모리 사용률에 따라 새 인스턴스를 시작하도록 Auto Scaling 그룹에 대한 동적 조정
정책을 구성합니다.
C. CPU 사용률을 기반으로 새 인스턴스를 시작하도록 Auto Scaling 그룹에 대한 동적 조정
정책을 구성합니다.
D. 피크 시간 전에 새 인스턴스를 시작하도록 Auto Scaling 그룹에 대한 예약된 조정 정책을
298

IT Certification Guaranteed, The Easy Way!
구성합니다.
Answer: D
Explanation:
* Understanding the Requirement: The application experiences slow performance at the start
of peak hours, but normalizes after a few hours. The goal is to ensure proper performance at
the beginning of peak hours.
* Analysis of Options:
* Application Load Balancer: Ensures proper traffic distribution but does not address the need
to have sufficient instances running at the start of peak hours.
* Dynamic Scaling Policy Based on Memory or CPU Utilization: While dynamic scaling reacts
to usage metrics, it may not preemptively scale in anticipation of peak hours, leading to
delays as new instances are launched and become available.
* Scheduled Scaling Policy: This allows the Auto Scaling group to launch instances ahead of
time, ensuring that enough instances are available and ready to handle the increased load
right at the start of peak hours.
* Best Solution:
* Scheduled Scaling Policy: This approach ensures that new instances are launched and
ready before peak hours begin, addressing the slow performance issue at the start of peak
periods.
References:
* Scheduled Scaling for Amazon EC2 Auto Scaling
QUESTION NO: 437
한 회사가 AWS에서 온라인 트랜잭션 처리(OLTP) 워크로드를 실행하고 있습니다. 이
워크로드는 다중 AZ 배포에서 암호화되지 않은 Amazon RDS DB 인스턴스를 사용합니다. 이
인스턴스에서 일일 데이터베이스 스냅샷이 생성됩니다.
앞으로 데이터베이스와 스냅샷이 항상 암호화되도록 솔루션 설계자는 무엇을 해야 합니까?
A. 최신 DB 스냅샷의 복사본을 암호화합니다. 암호화된 스냅샷을 복원하여 기존 DB 인스턴스
교체
B. 새로운 암호화된 Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성하고 여기에
스냅샷을 복사합니다. DB 인스턴스에서 암호화를 활성화합니다.
C. AWS Key Management Service(AWS KMS)를 사용하여 스냅샷을 복사하고 암호화를
활성화합니다. 암호화된 스냅샷을 기존 DB 인스턴스로 복원합니다.
D. AWS Key Management Service(AWS KMS) 관리형 키(SSE-KMS)를 사용하여 서버 측
암호화를 사용하여 암호화된 Amazon S3 버킷에 스냅샷을 복사합니다.
Answer: A
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.
html#USER_RestoreFromSnapshot.CON
Under "Encrypt unencrypted resources" -
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide
/EBSEncryption.html
QUESTION NO: 438
한 회사가 AWS 클라우드의 컨테이너에서 애플리케이션을 실행하려고 합니다. 이러한
299

IT Certification Guaranteed, The Easy Way!
애플리케이션은 상태 비저장이며 기본 인프라 내의 중단을 허용할 수 있습니다. 회사에는
비용과 운영 오버헤드를 최소화하는 솔루션이 필요합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. Amazon EC2 Auto Scaling 그룹의 스팟 인스턴스를 사용하여 애플리케이션 컨테이너를
실행합니다.
B. Amazon Elastic Kubernetes Service(Amazon EKS) 관리형 노드 그룹에서 스팟 인스턴스를
사용합니다.
C. Amazon EC2 Auto Scaling 그룹의 온디맨드 인스턴스를 사용하여 애플리케이션
컨테이너를 실행합니다.
D. Amazon Elastic Kubernetes Service(Amazon EKS) 관리형 노드 그룹에서 온디맨드
인스턴스를 사용합니다.
Answer: A
Explanation:
https://aws.amazon.com/cn/blogs/compute/cost-optimization-and-resilience-eks-with-spot-
instances/
QUESTION NO: 439
회사에는 전 세계 20,000개 이상의 소매점 위치에 배포된 클라이언트에게 서비스를 제공하는
애플리케이션이 있습니다. 애플리케이션은 포트 443에서 HTTPS를 통해 노출되는 백엔드 웹
서비스로 구성됩니다. 애플리케이션은 ALB(Application Load Balancer) 뒤의 Amazon EC2
인스턴스에서 호스팅됩니다.
소매점은 공용 인터넷을 통해 웹 애플리케이션과 통신합니다. 회사는 각 소매점에서 현지
ISP가 할당한 IP 주소를 등록할 수 있도록 허용합니다.
회사 보안팀에서는 소매점에서 등록한 IP 주소로만 접속을 제한하여 애플리케이션
엔드포인트의 보안을 강화할 것을 권장합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. AWS WAF 웹 ACL을 ALB와 연결합니다. ALB의 IP 규칙 세트를 사용하여 트래픽을
필터링합니다. 등록된 IP 주소를 포함하도록 규칙의 IP 주소를 업데이트합니다.
B. ALB를 관리하기 위해 AWS Firewall Manager를 배포합니다. ALB에 대한 트래픽을
제한하도록 방화벽 규칙을 구성합니다. 등록된 IP 주소를 포함하도록 방화벽 규칙을
수정합니다.
C. Amazon DynamoDB 테이블에 IP 주소를 저장합니다. ALB에서 AWS Lambda 인증 기능을
구성하여 수신 요청이 등록된 IP 주소에서 오는지 확인합니다.
D. ALB의 공용 인터페이스가 포함된 서브넷에서 네트워크 ACL을 구성합니다. 등록된 각 IP
주소에 대한 항목으로 네트워크 ACL의 수신 규칙을 업데이트합니다.
Answer: A
Explanation:
* AWS WAF (Web Application Firewall): AWS WAF allows you to create custom rules to
block or allow web requests based on conditions that you specify.
* Web ACL (Access Control List):
* Create a web ACL and associate it with the ALB.
* Use IP rule sets to specify the IP addresses of the retail locations that are allowed to access
the application.
* Security and Flexibility:
300

IT Certification Guaranteed, The Easy Way!
* AWS WAF provides a scalable way to manage access control, ensuring that only traffic
from registered IP addresses is allowed.
* You can dynamically update the IP rule sets to add or remove IP addresses as needed.
* Operational Simplicity: Using AWS WAF with a web ACL is straightforward and integrates
seamlessly with the ALB, providing an efficient solution for managing access control based
on IP addresses.
References:
* AWS WAF
* How AWS WAF Works
QUESTION NO: 440
한 회사가 온프레미스에 컨테이너화된 애플리케이션을 구축하고 있으며 해당 애플리케이션을
AWS로 이전하기로 결정했습니다. li이 배포된 직후 애플리케이션에는 수천 명의 사용자가
있게 됩니다. 회사는 대규모 컨테이너 배포를 관리하는 방법을 확신하지 못합니다. 회사는
운영 오버헤드를 최소화하는 고가용성 아키텍처에 컨테이너화된 애플리케이션을 배포해야
합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon Elastic Container Registry(Amazon ECR) 리포지토리에 컨테이너 이미지를
저장합니다. AWS Fargate 시작 유형과 함께 Amazon Elastic Container Service(Amazon
ECS) 클러스터를 사용하여 컨테이너를 실행합니다. 수요에 따라 자동으로 규모를 조정하려면
대상 추적을 사용하세요.
B. Amazon Elastic Container Registry(Amazon ECR) 리포지토리에 컨테이너 이미지를
저장합니다. Amazon EC2 시작 유형과 함께 Amazon Elastic Container Service(Amazon
ECS) 클러스터를 사용하여 컨테이너를 실행합니다. 수요에 따라 자동으로 규모를 조정하려면
대상 추적을 사용하세요.
C. Amazon EC2 인스턴스에서 실행되는 리포지토리에 컨테이너 이미지를 저장합니다. 여러
가용 영역에 분산되어 있는 EC2 인스턴스에서 컨테이너를 실행하세요. Amazon
CloudWatch에서 평균 CPU 사용률을 모니터링합니다. 필요에 따라 새 EC2 인스턴스 시작
D. 컨테이너 이미지가 포함된 Amazon EC2 Amazon 머신 이미지(AMI)를 생성합니다. 여러
가용 영역에 걸쳐 Auto Scaling 그룹에서 EC2 인스턴스를 시작합니다. 평균 CPU 사용률
임계값이 위반되면 Amazon CloudWatch 경보를 사용하여 EC2 인스턴스를 확장합니다.
Answer: A
Explanation:
AWS Fargate is a serverless experience for user applications, allowing the user to
concentrate on building applications instead of configuring and managing servers. Fargate
also automates resource management, allowing users to easily scale their applications in
response to demand.
QUESTION NO: 441
회사는 단일 가용 영역의 Amazon EC2 인스턴스에서 3개의 애플리케이션을 호스팅합니다. 웹
애플리케이션은 EC2 인스턴스에서 호스팅되는 자체 관리형 MySQL 데이터베이스를
사용하여 Amazon Elastic Block Store(Amazon EBS) 볼륨에 데이터를 저장합니다. MySQL
데이터베이스는 현재 1TB 프로비저닝된 IOPS SSD(io2) EBS 볼륨을 사용합니다. 회사는
최대 트래픽 시 읽기 및 쓰기 모두에 대해 1,000 IOPS의 트래픽을 예상합니다.
회사는 두 배의 IOPS 용량을 유지하면서 중단을 최소화하고 성능을 안정화하며 비용을
301

IT Certification Guaranteed, The Easy Way!
절감하기를 원합니다. 회사는 데이터베이스 계층을 가용성이 높고 내결함성이 뛰어난 완전
관리형 솔루션으로 확장하기를 원합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. io2 Block Express EBS 볼륨이 있는 MySQL DB 인스턴스용 Amazon RDS의 다중 AZ
배포를 사용합니다.
B. 범용 SSD(gp2) EBS 볼륨이 있는 MySQL DB 인스턴스용 Amazon RDS의 다중 AZ 배포를
사용합니다.
C. Amazon S3 Intelligent-Tiering 액세스 계층을 사용합니다.
D. 두 개의 대형 EC2 인스턴스를 사용하여 활성-수동 모드로 데이터베이스를 호스팅합니다.
Answer: B
Explanation:
RDS supported Storage >
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html GP2 max
IOPS > https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/general-purpose.html#gp2-
performance Amazon RDS provides three storage types: General Purpose SSD (also known
as gp2 and gp3), Provisioned IOPS SSD (also known as io1), and magnetic (also known as
standard). They differ in performance characteristics and price, which means that you can
tailor your storage performance and cost to the needs of your database workload. You can
create MySQL, MariaDB, Oracle, and PostgreSQL RDS DB instances with up to 64 tebibytes
(TiB) of storage. You can create SQL Server RDS DB instances with up to 16 TiB of storage.
For this amount of storage, use the Provisioned IOPS SSD and General Purpose SSD
storage types.
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html
QUESTION NO: 442
Amazon EC2 인스턴스에 호스팅된 회사의 웹 사이트는 Amazon S3에 저장된 분류된
데이터를 처리합니다. 보안 문제로 인해 회사에서는 EC2 리소스와 Amazon S3 간의 안전한
연결이 필요합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. VPC 엔드포인트에서의 액세스를 허용하도록 S3 버킷 정책을 설정합니다.
B. S3 버킷에 대한 읽기-쓰기 액세스 권한을 부여하도록 오전 1시 정책을 설정합니다.
C. 프라이빗 서브넷 외부의 리소스에 액세스하기 위해 NAT 게이트웨이를 설정합니다.
D. S3 버킷에 액세스하기 위한 액세스 키 ID와 보안 액세스 키를 설정합니다.
Answer: A
Explanation:
This solution meets the following requirements:
* It is private and secure, as it allows the EC2 instances to access the S3 bucket without
using the public internet. A VPC endpoint is a gateway that enables you to create a private
connection between your VPC and another AWS service, such as S3, within the same
Region. A VPC endpoint for S3 provides secure and direct access to S3 buckets and objects
using private IP addresses from your VPC. You can also use VPC endpoint policies and S3
bucket policies to control the access to the S3 resources based on the endpoint, the IAM
user, the IAM role, or the source IP address.
* It is simple and scalable, as it does not require any additional AWS services, gateways, or
302

IT Certification Guaranteed, The Easy Way!
NAT devices.
A VPC endpoint for S3 is a fully managed service that scales automatically with the network
traffic.
You can create a VPC endpoint for S3 with a few clicks in the VPC console or with a simple
API call.
You can also use the same VPC endpoint to access multiple S3 buckets in the same Region.
References:
* VPC Endpoints - Amazon Virtual Private Cloud
* Gateway VPC endpoints - Amazon Virtual Private Cloud
* Using Amazon S3 with interface VPC endpoints - Amazon Simple Storage Service
* Using Amazon S3 with gateway VPC endpoints - Amazon Simple Storage Service
QUESTION NO: 443
한 회사가 여러 가용 영역의 Amazon EC2 인스턴스에서 실행되는 웹 기반 애플리케이션을
구축하고 있습니다. 웹 애플리케이션은 총 900TB 크기의 텍스트 문서 저장소에 대한 액세스를
제공합니다. 회사는 웹 애플리케이션에 대한 수요가 높은 기간을 경험할 것으로 예상합니다.
솔루션 설계자는 텍스트 문서의 스토리지 구성 요소가 항상 애플리케이션의 요구 사항을
충족하도록 확장될 수 있는지 확인해야 합니다. 회사는 솔루션의 전체 비용에 대해 우려하고
있습니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 스토리지 솔루션은 무엇입니까?
A. Amazon Elastic Block Store(Amazon EBS)
B. Amazon Elastic File System(Amazon EFS)
C. Amazon Elasticsearch Service(Amazon ES)
D. 아마존 S3
Answer: D
Explanation:
Amazon S3 is cheapest and can be accessed from anywhere.
QUESTION NO: 444
전자 상거래 회사가 AWS에서 다중 계층 애플리케이션을 실행하고 있습니다. 프런트엔드 및
백엔드 계층은 Amazon EC2에서 실행되고, 데이터베이스는 MYSQL용 Amazon RDS에서
실행됩니다. RDS 인스턴스가 있는 백엔드 계층 커뮤니티. 성능 저하를 유발하는
데이터베이스에서 동일한 데이터베이스를 반환하라는 호출이 자주 발생합니다.
백엔드 성능을 향상하려면 어떤 조치를 취해야 합니까?
A. Amazon SNS를 구현하여 데이터베이스 호출을 저장합니다.
B. Amazon ElasticCache를 구현하여 대규모 데이터베이스를 캐시합니다.
C. 데이터베이스 호출을 캐시하기 위해 MySQL용 RDS 읽기 전용 복제본을 구현합니다.
D. Amazon Kinesis Data Firehose를 구현하여 데이터베이스에 대한 호출을 스트리밍합니다.
Answer: B
Explanation:
the best solution is to implement Amazon ElastiCache to cache the large datasets, which will
store the frequently accessed data in memory, allowing for faster retrieval times. This can
help to alleviate the frequent calls to the database, reduce latency, and improve the overall
performance of the backend tier.
303

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 445
회사는 인공 지능 및 기계 학습(Al/ML)을 연구하는 고객에게 데이터 세트를 판매합니다.
데이터 세트는 us-east-1 지역의 Amazon S3 버킷에 저장되는 형식이 지정된 대용량
파일입니다. 회사는 다음을 수행하는 웹 애플리케이션을 호스팅합니다. 고객이 특정 데이터
세트에 대한 액세스를 구매하는 데 사용합니다. 웹 애플리케이션은 Application Load Balancer
뒤의 여러 Amazon EC2 인스턴스에 배포됩니다. 구매가 이루어진 후 고객은 파일에 액세스할
수 있는 S3 서명 URL을 받습니다.
고객은 북미와 유럽에 분산되어 있습니다. 회사는 데이터 전송과 관련된 비용을 줄이고
성능을 유지하거나 향상시키기를 원합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 기존 S3 버킷에 S3 Transfer Acceleration 구성 고객 요청을 S3 Transfer Acceleration
엔드포인트로 전달 액세스 제어를 위해 S3 서명 URL을 계속 사용
B. 기존 S3 버킷을 원본으로 사용하여 Amazon CloudFront 배포 배포 CloudFront URL에 대한
직접 고객 요청 액세스 제어를 위해 CloudFront 서명 URL로 전환
C. 버킷 간 S3 교차 리전 복제를 사용하여 eu-central-1 리전에 두 번째 S3 버킷을 설정합니다.
고객 요청을 가장 가까운 리전에 전달합니다. 액세스 제어를 위해 S3 서명 URL을 계속
사용합니다.
D. 최종 사용자에게 데이터세트를 스트리밍할 수 있도록 웹 애플리케이션을 수정합니다. 기존
S3 버킷에서 데이터를 읽도록 웹 애플리케이션을 구성합니다. 애플리케이션에서 직접 액세스
제어를 구현합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.htm
l
QUESTION NO: 446
회사에서 고성능 컴퓨팅 및 인공 지능을 사용하여 사기 방지 및 감지 기술을 개선하려고
합니다. 회사는 가능한 한 빨리 단일 워크로드를 완료하기 위해 분산 처리가 필요합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon Elastic Kubernetes Service(Amazon EKS) 및 여러 컨테이너를 사용합니다.
B. AWS ParallelCluster 및 메시지 전달 인터페이스(MPI) 라이브러리를 사용합니다.
C. Application Load Balancer 및 Amazon EC2 인스턴스를 사용합니다.
D. AWS Lambda 함수를 사용합니다.
Answer: B
Explanation:
AWS ParallelCluster is a service that allows you to create and manage high-performance
computing (HPC) clusters on AWS. It supports multiple schedulers, including AWS Batch,
which can run distributed workloads across multiple EC2 instances1.
MPI is a standard for message passing between processes in parallel computing. It provides
functions for sending and receiving data, synchronizing processes, and managing
communication groups2.
By using AWS ParallelCluster and MPI libraries, you can take advantage of the following
benefits:
* You can easily create and configure HPC clusters that meet your specific requirements,
304

IT Certification Guaranteed, The Easy Way!
such as instance type, number of nodes, network configuration, and storage options1.
* You can leverage the scalability and elasticity of AWS to run large-scale parallel workloads
without worrying about provisioning or managing servers1.
* You can use MPI libraries to optimize the performance and efficiency of your parallel
applications by enabling inter-process communication and data exchange2.
* You can choose from a variety of MPI implementations that are compatible with AWS
ParallelCluster, such as Open MPI, Intel MPI, and MPICH3.
QUESTION NO: 447
소매 회사에는 여러 사업체가 있습니다. 각 비즈니스의 IT 팀은 자체 AWS 계정을 관리합니다.
각 팀 계정은 AWS Organizations 조직의 일부입니다. 각 팀은 팀 자체 AWS 계정의 Amazon
DynamoDB 테이블에서 제품 재고 수준을 모니터링합니다.
회사는 중앙 재고 보고 애플리케이션을 공유 AWS 계정에 배포하고 있습니다.
애플리케이션은 모든 팀의 DynamoDB 테이블에서 항목을 읽을 수 있어야 합니다.
어떤 인증 옵션이 이러한 요구 사항을 가장 안전하게 충족합니까?
A. 인벤토리 애플리케이션 계정에서 DynamoDB를 AWS Secrets Manager와 통합합니다.
Secrets Manager의 올바른 암호를 사용하여 DynamoDB 테이블을 인증하고 읽도록
애플리케이션을 구성합니다. 30일마다 비밀 교체를 예약합니다.
B. 모든 비즈니스 계정에서 프로그래밍 방식으로 액세스할 수 있는 1AM 사용자를 만듭니다.
올바른 오전 1시 사용자 액세스 키 ID와 보안 액세스 키를 사용하여 DynamoDB 테이블을
인증하고 읽도록 애플리케이션을 구성합니다. 30일마다 오전 1시 액세스 키를 수동으로
교체합니다.
C. 모든 비즈니스 계정에서 DynamoDB 테이블에 대한 액세스 권한을 부여하는 정책과
인벤토리 애플리케이션 계정의 특정 역할을 신뢰하는 신뢰 정책을 사용하여 BU_ROLE이라는
1AM 역할을 생성합니다. 인벤토리 계정에서 STS AssumeRole API 작업에 대한 액세스를
허용하는 APP_ROLE이라는 역할을 생성합니다. APP_ROLE을 사용하도록 애플리케이션을
구성하고 DynamoDB 테이블을 읽으려면 교차 계정 역할 BU_ROLE을 가정합니다.
D. DynamoDB를 AWS Certificate Manager(ACM)와 통합합니다. DynamoDB를 인증하기
위해 ID 인증서를 생성합니다. 올바른 인증서를 사용하여 DynamoDB 테이블을 인증하고
읽도록 애플리케이션을 구성합니다.
Answer: C
Explanation:
This solution meets the requirements most securely because it uses IAM roles and the STS
AssumeRole API operation to authenticate and authorize the inventory application to access
the DynamoDB tables in different accounts. IAM roles are more secure than IAM users or
certificates because they do not require long-term credentials or passwords. Instead, IAM
roles provide temporary security credentials that are automatically rotated and can be
configured with a limited duration. The STS AssumeRole API operation enables you to
request temporary credentials for a role that you are allowed to assume. By using this
operation, you can delegate access to resources that are in different AWS accounts that you
own or that are owned by third parties. The trust policy of the role defines which entities can
assume the role, and the permissions policy of the role defines which actions can be
performed on the resources. By using this solution, you can avoid hard- coding credentials or
certificates in the inventory application, and you can also avoid storing them in Secrets
Manager or ACM. You can also leverage the built-in security features of IAM and STS, such
305

IT Certification Guaranteed, The Easy Way!
as MFA, access logging, and policy conditions.
References:
* IAM Roles
* STS AssumeRole
* Tutorial: Delegate Access Across AWS Accounts Using IAM Roles
QUESTION NO: 448
한 회사가 온프레미스 Oracle 데이터베이스를 Amazon Aurora PostgreSQL로 이전하고
있습니다. 데이터베이스에는 동일한 테이블에 쓰는 여러 애플리케이션이 있습니다.
애플리케이션은 한 달 간격으로 하나씩 마이그레이션해야 합니다. 경영진은 데이터베이스의
읽기 및 쓰기 횟수가 많다는 우려를 표명했습니다. 마이그레이션하는 동안 두 데이터베이스
모두에서 데이터가 동기화 상태로 유지되어야 합니다.
솔루션 설계자는 무엇을 추천해야 합니까?
A. 초기 마이그레이션에는 AWS DataSync를 사용합니다. AWS Database Migration
Service(AWS DMS)를 사용하여 변경 데이터 캡처(CDC) 복제 작업과 테이블 매핑을 생성하여
모든 테이블을 선택합니다.
B. 초기 마이그레이션에는 AWS DataSync를 사용합니다. AWS Database Migration
Service(AWS DMS)를 사용하여 전체 로드 및 변경 데이터 캡처(CDC) 복제 작업과 테이블
매핑을 생성하여 모든 테이블을 선택합니다.
C. 메모리 최적화 복제 인스턴스를 사용하여 AWS Database Migration Service(AWS DMS)와
함께 AWS Schema Conversion Tool을 사용합니다. 전체 로드와 변경 데이터 캡처(CDC) 복제
작업 및 테이블 매핑을 생성하여 모든 테이블을 선택합니다.
D. 컴퓨팅 최적화 복제 인스턴스를 사용하여 AWS Database Migration Service(AWS DMS)와
함께 AWS Schema Conversion Tool을 사용합니다. 전체 로드와 CDC(변경 데이터 캡처) 복제
작업 및 테이블 매핑을 생성하여 가장 큰 테이블을 선택합니다.
Answer: C
Explanation:
https://aws.amazon.com/ko/premiumsupport/knowledge-center/dms-memory-optimization/
QUESTION NO: 449
회사는 회사의 Amazon RDS 데이터베이스에 연결되는 애플리케이션을 AWS에서
실행합니다. 애플리케이션은 주말과 연중 피크 시간대에 확장됩니다. 회사는 데이터베이스에
연결하는 애플리케이션에 대해 데이터베이스를 보다 효과적으로 확장하려고 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 데이터베이스에 대한 대상 그룹 구성과 함께 연결 풀링과 함께 Amazon DynamoDB를
사용합니다. DynamoDB 엔드포인트를 사용하도록 애플리케이션을 변경합니다.
B. 데이터베이스의 대상 그룹과 함께 Amazon RDS Proxy를 사용합니다. RDS Proxy
엔드포인트를 사용하도록 애플리케이션을 변경합니다.
C. Amazon EC2에서 실행되는 사용자 지정 프록시를 데이터베이스의 중개자로 사용합니다.
사용자 정의 프록시 엔드포인트를 사용하도록 애플리케이션을 변경하십시오.
D. AWS Lambda 함수를 사용하여 데이터베이스에 대한 대상 그룹 구성과 함께 연결 풀링을
제공합니다. Lambda 함수를 사용하도록 애플리케이션을 변경합니다.
Answer: B
Explanation:
Amazon RDS Proxy is a fully managed, highly available database proxy for Amazon
306

IT Certification Guaranteed, The Easy Way!
Relational Database Service (RDS) that makes applications more scalable, more resilient to
database failures, and more secure1. RDS Proxy allows applications to pool and share
connections established with the database, improving database efficiency and application
scalability2. RDS Proxy also reduces failover times for Aurora and RDS databases by up to
66% and enables IAM authentication and Secrets Manager integration for database access1.
RDS Proxy can be enabled for most applications with no code changes2.
QUESTION NO: 450
회사는 AWS에 웹 애플리케이션을 배포할 것입니다. 이 회사는 확장 요구 사항을 지원하기
위해 기본 DB 인스턴스와 5개의 읽기 전용 복제본을 사용하여 MySQL용 Amazon RDS에서
백엔드 데이터베이스를 호스팅합니다. 읽기 전용 복제본은 기본 DB 인스턴스 뒤에서 1초 이상
기록해서는 안 됩니다. 데이터베이스는 정기적으로 예약된 저장 프로시저를 실행합니다.
웹 사이트의 트래픽이 증가함에 따라 리드가 가장 많은 기간 동안 복제본에 추가 지연이
발생합니다. 솔루션 설계자는 복제 지연을 최대한 줄여야 합니다. 솔루션 설계자는
애플리케이션 코드 변경을 최소화하고 지속적인 오버헤드를 최소화해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
데이터베이스를 Amazon Aurora MySQL로 마이그레이션합니다. 읽기 전용 복제본을 Aurora
복제본으로 교체하고 Aurora Auto Scaling을 구성합니다. 저장 프로시저를 Aurora MySQL
기본 함수로 바꿉니다.
데이터베이스 앞에 Redis 클러스터용 Amazon ElasticCache를 배포합니다. 애플리케이션이
데이터베이스를 쿼리하기 전에 캐시를 확인하도록 애플리케이션을 수정합니다. 저장
프로시저를 AWS Lambda 기능으로 바꿉니다.
A. Amazon EC2 인스턴스에서 실행되는 MYSQL 데이터베이스로 데이터베이스를
마이그레이션합니다. 모든 복제본 노드에 최적화된 대규모 컴퓨팅을 선택하세요. EC2
인스턴스에서 저장 프로시저를 유지 관리합니다.
B. 데이터베이스 형식으로 Redis용 Amazon ElastiCache 클러스터를 배포합니다.
애플리케이션이 데이터베이스를 쿼리하기 전에 캐시를 확인하도록 애플리케이션을
수정합니다. 저장 프로시저를 AWS Lambda 함수로 바꿉니다.
C. Amazon EC2 인스턴스에서 실행되는 MySQL 데이터베이스로 데이터베이스를
마이그레이션합니다. 모든 복제본 노드에 대해 컴퓨팅 최적화된 대규모 EC2 인스턴스를
선택하고, EC2 인스턴스에서 저장 프로시저를 유지합니다.
D. 데이터베이스를 Amazon DynamoDB로 마이그레이션하고, 필요한 처리량을 지원하기 위해
읽기 용량 단위(RCU) 수를 프로비저닝하고, 온디맨드 용량 확장을 구성합니다. 저장
프로시저를 DynamoDB 스트림으로 바꿉니다.
Answer: A
Explanation:
Option A is the most appropriate solution for reducing replication lag without significant
changes to the application code and minimizing ongoing operational overhead. Migrating the
database to Amazon Aurora MySQL allows for improved replication performance and higher
scalability compared to Amazon RDS for MySQL. Aurora Replicas provide faster replication,
reducing the replication lag, and Aurora Auto Scaling ensures that there are enough Aurora
Replicas to handle the incoming traffic. Additionally, Aurora MySQL native functions can
replace the stored procedures, reducing the load on the database and improving
performance.
307

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 451
한 온라인 소매 회사는 5천만 명 이상의 활성 고객을 보유하고 있으며 매일 25,000건 이상의
주문을 받습니다. 회사는 고객의 구매 데이터를 수집하고 이 데이터를 Amazon S3에
저장합니다. 추가 고객 데이터는 Amazon RDS에 저장됩니다.
회사는 다양한 팀이 분석을 수행할 수 있도록 모든 데이터를 다양한 팀에서 사용할 수 있도록
하려고 합니다. 솔루션은 데이터에 대한 세분화된 권한을 관리하는 기능을 제공해야 하며
운영 오버헤드를 최소화해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 구매 데이터를 마이그레이션하여 Amazon RDS에 직접 씁니다. RDS 액세스 제어를
사용하여 액세스를 제한합니다.
B. Amazon RDS에서 Amazon S3로 데이터를 주기적으로 복사하도록 AWS Lambda 함수를
예약합니다. AWS Glue 크롤러를 생성합니다. Amazon Athena를 사용하여 데이터를
쿼리합니다. S3 정책을 사용하여 액세스를 제한합니다.
C. AWS Lake Formation을 사용하여 데이터 레이크를 생성합니다. Amazon RDS에 대한 AWS
Glue JDBC 연결을 생성합니다. Lake Formation의 S3 버킷을 등록하세요. Lake Formation
액세스 제어를 사용하여 액세스를 제한하세요.
D. Amazon Redshift 클러스터를 생성합니다. Amazon S3 및 Amazon RDS에서 Amazon
Redshift로 데이터를 주기적으로 복사하도록 AWS Lambda 함수를 예약합니다. Amazon
Redshift 액세스 제어를 사용하여 액세스를 제한합니다.
Answer: C
Explanation:
To make all the data available to various teams and minimize operational overhead, the
company can create a data lake by using AWS Lake Formation. This will allow the company
to centralize all the data in one place and use fine-grained access controls to manage access
to the data. To meet the requirements of the company, the solutions architect can create a
data lake by using AWS Lake Formation, create an AWS Glue JDBC connection to Amazon
RDS, and register the S3 bucket in Lake Formation. The solutions architect can then use
Lake Formation access controls to limit access to the data. This solution will provide the
ability to manage fine-grained permissions for the data and minimize operational overhead.
QUESTION NO: 452
한 전자상거래 회사에는 Amazon API Gateway와 AWS Lambda 함수를 사용하는 주문 처리
애플리케이션이 있습니다. 애플리케이션은 Amazon Aurora PostgreSQL 데이터베이스에
데이터를 저장합니다. 최근 판매 이벤트 중에 고객 주문이 갑자기 급증했습니다. 일부 고객이
시간 초과를 경험했고 애플리케이션이 해당 고객의 주문을 처리하지 못했습니다. 솔루션
설계자는 열려 있는 연결 수가 많아 데이터베이스에서 CPU 사용률과 메모리 사용률이 높다고
판단했습니다. 솔루션 설계자는 작업을 수행하는 동안 시간 초과 오류를 방지해야 합니다.
응용 프로그램에 대한 가능한 최소한의 변경.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Lambda 함수에 대해 프로비저닝된 동시성 구성 여러 AWS 지역에서 글로벌
데이터베이스가 되도록 데이터베이스 수정
B. Amazon RDS Proxy를 사용하여 데이터베이스용 프록시 생성 데이터베이스 엔드포인트
대신 RDS Proxy 엔드포인트를 사용하도록 Lambda 함수 수정
C. 다른 AWS 리전에 있는 데이터베이스에 대한 읽기 전용 복제본을 생성합니다. API
Gateway의 쿼리 문자열 매개변수를 사용하여 트래픽을 읽기 전용 복제본으로 라우팅합니다.
308

IT Certification Guaranteed, The Easy Way!
D. AWS Database Migration Service(AWS DMS| OynamoDB 테이블을 사용하도록 Lambda
함수 수정)를 사용하여 Aurora PostgreSQL에서 Amazon DynamoDB로 데이터 마이그레이션
Answer: B
Explanation:
Many applications, including those built on modern serverless architectures, can have a large
number of open connections to the database server and may open and close database
connections at a high rate, exhausting database memory and compute resources. Amazon
RDS Proxy allows applications to pool and share connections established with the database,
improving database efficiency and application scalability.
https://aws.amazon.com/id/rds/proxy/
QUESTION NO: 453
회사의 애플리케이션이 Elastic Load Balancing(ELB) 로드 밸런서 뒤에 있는 Auto Scaling
그룹 내의 Amazon EC2 인스턴스에서 실행되고 있습니다. 애플리케이션 기록을 기준으로
회사는 매년 휴일 동안 트래픽이 급증할 것으로 예상합니다. 솔루션 아키텍트는 Auto Scaling
그룹이 애플리케이션 사용자에 대한 성능 영향을 최소화하기 위해 용량을 사전에 늘리도록
하는 전략을 설계해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. CPU 사용률이 90%를 초과하는 경우 EC2 인스턴스를 확장하기 위해 Amazon CloudWatch
경보를 생성합니다.
B. 최대 수요가 예상되는 기간 전에 Auto Scaling 그룹을 확장하기 위해 반복 예약 작업을
생성합니다.
C. 최대 수요 기간 동안 Auto Scaling 그룹의 최소 및 최대 EC2 인스턴스 수를 늘립니다.
D. Auto Scaling:EC2_INSTANCE_LAUNCH 이벤트가 있을 때 알림을 보내도록 Amazon
Simple Notification Service(Amazon SNS) 알림을 구성합니다.
Answer: B
Explanation:
* Understanding the Requirement: The company anticipates a spike in traffic during a holiday
and wants to ensure the Auto Scaling group can handle the increased load without impacting
performance.
* Analysis of Options:
* CloudWatch Alarm: This reacts to spikes based on metrics like CPU utilization but does not
proactively scale before the anticipated demand.
* Recurring Scheduled Action: This allows the Auto Scaling group to scale up based on a
known schedule, ensuring additional capacity is available before the expected spike.
* Increase Min/Max Instances: This could result in unnecessary costs by maintaining higher
capacity even when not needed.
* SNS Notification: Alerts on scaling events but does not proactively manage scaling to
prevent performance issues.
* Best Solution for Proactive Scaling:
* Create a recurring scheduled action: This approach ensures that the Auto Scaling group
scales up before the peak demand, providing the necessary capacity proactively without
manual intervention.
References:
* Scheduled Scaling for Auto Scaling
309

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 454
한 회사에는 Amazon EC2 인스턴스의 Amazon Elastic Kubernetes Service(Amazon EKS)
클러스터에서 실행되는 애플리케이션이 있습니다. 이 애플리케이션에는 Amazon
DynamoDB를 사용하는 U1과 애플리케이션 배포의 일부로 Amazon S3를 사용하는 데이터
서비스가 있습니다.
회사는 U1용 EKS Pod가 Amazon DynamoDB에만 액세스할 수 있고 데이터 서비스용 EKS
Pod가 Amazon S3에만 액세스할 수 있도록 해야 합니다. 회사는 AWS Identity and Access
Management |IAM을 사용합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 별도의 1AM 정책(또는 필요한 권한으로 Amazon S3 및 DynamoDB 액세스)을 만듭니다.
두 1AM 정책을 EC2 인스턴스 프로필에 연결합니다. 역할 기반 액세스 제어(RBAC)를
사용하여 Amazon S3 또는 DynamoDB(또는 해당 EKS Pod)에 대한 액세스를 제어합니다.
B. 별도의 1AM 정책(또는 필요한 권한으로 Amazon S3 및 DynamoDB 액세스)을 만듭니다.
Amazon S3 1AM 정책을 EKS Pod에 직접 연결합니다(또는 U1의 경우 데이터 서비스와
DynamoDB 정책을 EKS Pod에 연결합니다).
C. U1 및 데이터 서비스에 대한 별도의 Kubernetes 서비스 계정을 만들어 1AM 역할을
맡습니다.
Amazon S3 전체 액세스 정책을 데이터 서비스 계정에 연결하고
AmazonDynamoDBFullAccess 정책을 U1 서비스 계정에 연결합니다.
D. U1 및 데이터 서비스에 대해 별도의 Kubernetes 서비스 계정을 만들어 1AM 역할을
맡습니다. 사용
1AM 역할은 U1에서 Amazon S3로 가는 EKS Pod와 DynamoDB로 가는 데이터 서비스를
위한 EKS Pod에 대한 액세스를 제공하는 서비스 계정(IRSA)입니다.
Answer: A
QUESTION NO: 455
회사는 온프레미스 데이터 센터에서 마케팅 웹사이트를 호스팅합니다. 웹사이트는 정적
문서로 구성되어 있으며 단일 서버에서 실행됩니다. 관리자는 웹사이트 콘텐츠를 자주
업데이트하지 않으며 SFTP 클라이언트를 사용하여 새 문서를 업로드합니다.
회사는 AWS에서 웹 사이트를 호스팅하고 Amazon CloudFront를 사용하기로 결정했습니다.
회사의 솔루션 아키텍트는 CloudFront 배포판을 생성합니다. 솔루션 설계자는 CloudFront
오리진 역할을 하는 웹 사이트 호스팅을 위해 가장 비용 효율적이고 복원력이 뛰어난
아키텍처를 설계해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon Lightsail을 사용하여 가상 서버를 생성합니다. Lightsail 인스턴스에서 웹 서버를
구성합니다. SFTP 클라이언트를 사용하여 웹사이트 콘텐츠를 업로드합니다.
B. Amazon EC2 인스턴스에 대한 AWS Auto Scaling 그룹을 생성합니다. Application Load
Balancer를 사용하세요. SFTP 클라이언트를 사용하여 웹사이트 콘텐츠를 업로드합니다.
C. 프라이빗 Amazon S3 버킷을 생성합니다. S3 버킷 정책을 사용하여 CloudFront 원본
액세스 ID(OAI)의 액세스를 허용합니다. AWSCLI를 사용하여 웹 사이트 콘텐츠를
업로드합니다.
D. 퍼블릭 Amazon S3 버킷을 생성합니다. SFTP용 AWS 전송을 구성합니다. 웹사이트
호스팅을 위해 S3 버킷을 구성합니다. SFTP 클라이언트를 사용하여 웹사이트 콘텐츠를
업로드합니다.
310

IT Certification Guaranteed, The Easy Way!
Answer: C
Explanation:
https://docs.aws.amazon.com/cli/latest/reference/transfer/describe-server.html
QUESTION NO: 456
한 회사가 프라이빗 서브넷에 애플리케이션을 호스팅합니다. 이 회사는 이미 애플리케이션을
Amazon Cognito와 통합했습니다. 이 회사는 Amazon Cognito 사용자 풀을 사용하여 사용자를
인증합니다.
회사에서는 애플리케이션이 Amazon S3 버킷에 사용자 문서를 안전하게 저장할 수 있도록
애플리케이션을 수정해야 합니다.
Amazon S3를 애플리케이션과 안전하게 통합하려면 어떤 단계 조합이 필요할까요? (2가지를
선택하세요.)
A. 사용자가 성공적으로 로그인할 때 보안 Amazon S3 액세스 토큰을 생성하는 Amazon
Cognito 자격 증명 풀을 생성합니다.
B. 기존 Amazon Cognito 사용자 풀을 사용하여 사용자가 성공적으로 로그인하면 Amazon S3
액세스 토큰을 생성합니다.
C. 회사에서 애플리케이션을 호스팅하는 동일한 VPC에 Amazon S3 VPC 엔드포인트를
생성합니다.
D. 회사가 애플리케이션을 호스팅하는 VPC에 NAT 게이트웨이를 만듭니다. Amazon
Cognito에서 시작되지 않은 모든 요청을 거부하기 위해 S3 버킷에 정책을 할당합니다.
E. 사용자의 IP 주소에서만 액세스할 수 있도록 허용하는 정책을 S3 버킷에 연결합니다.
Answer: A C
Explanation:
To securely integrate Amazon S3 with an application that uses Amazon Cognito for user
authentication, the following two steps are essential:
* Step 1: Create an Amazon Cognito Identity Pool (Option A)
* Amazon Cognito Identity Pools allow users to obtain temporary AWS credentials to access
AWS resources, such as Amazon S3, after successfully authenticating with the Cognito user
pool.
The identity pool bridges the gap between user authentication and AWS service access by
generating temporary credentials using AWS Identity and Access Management (IAM).
* Once a user logs in using the Cognito User Pool, the identity pool provides IAM roles with
specific permissions that the application can use to access S3 securely. This ensures that
each user has appropriate access controls while accessing the S3 bucket.
* This is a secure way to ensure that users only have temporary and least-privilege access to
the S3 bucket for their documents.
* Step 2: Create an Amazon S3 VPC Endpoint (Option C)
* By creating an Amazon S3 VPC endpoint, the company ensures that communication
between the application (which is hosted in a private subnet) and the S3 bucket occurs over
the AWS private network, without the need to traverse the internet. This enhances security
and prevents exposure of data to public networks.
* The VPC endpoint allows the application to access the S3 bucket privately and securely
within the VPC. It also ensures that traffic stays within the AWS network, reducing attack
surface and improving overall security.
Why the Other Options Are Incorrect:
311

IT Certification Guaranteed, The Easy Way!
* Option B: This is incorrect because Amazon Cognito User Pools are used for user
authentication, not for generating S3 access tokens. To provide S3 access, you need to use
Amazon Cognito Identity Pools, which offer AWS credentials.
* Option D: A NAT gateway is unnecessary in this scenario. Using a VPC endpoint for S3
access provides a more secure and cost-effective solution by keeping traffic within AWS.
* Option E: Attaching a policy to restrict access based on IP addresses is not scalable or
efficient. It would require managing users' dynamic IP addresses, which is not an effective
security measure for this use case.
AWS References:
* Amazon Cognito Identity Pools
* Amazon VPC Endpoints for S3
QUESTION NO: 457
최근 회사의 IT 비용을 분석한 결과 백업 비용을 줄여야 할 필요성이 강조되었습니다. 회사의
최고 정보 책임자(CIO)는 물리적 백업 테이프의 사용을 제거하여 온프레미스 백업 인프라를
단순화하고 비용을 절감하려고 합니다. 회사는 온프레미스 백업 애플리케이션 및
워크플로우에 대한 기존 투자를 보존해야 합니다.
솔루션 설계자는 무엇을 추천해야 합니까?
A. NFS 인터페이스를 사용하여 백업 애플리케이션과 연결하도록 AWS Storage Gateway를
설정합니다.
B. NFS 인터페이스를 사용하여 백업 애플리케이션과 연결하는 Amazon EFS 파일 시스템을
설정합니다.
C. iSCSI 인터페이스를 사용하여 백업 애플리케이션과 연결하는 Amazon EFS 파일 시스템을
설정합니다.
D. iSCSI-VTL(가상 테이프 라이브러리) 인터페이스를 사용하여 백업 애플리케이션과
연결하도록 AWS Storage Gateway를 설정합니다.
Answer: D
Explanation:
it allows the company to simplify the on-premises backup infrastructure and reduce costs by
eliminating the use of physical backup tapes. By setting up AWS Storage Gateway to
connect with the backup applications using the iSCSI-virtual tape library (VTL) interface, the
company can store backup data on virtual tapes in S3 or Glacier. This preserves the existing
investment in the on-premises backup applications and workflows while leveraging AWS
storage services. References:
* AWS Storage Gateway
* Tape Gateway
QUESTION NO: 458
한 회사는 점수 업데이트를 백엔드 프로세서로 스트리밍한 다음 결과를 리더보드에 게시하는
모바일 게임을 개발하고 있습니다. 솔루션 설계자는 대규모 트래픽 급증을 처리할 수 있는
솔루션을 설계해야 합니다. 모바일 게임 업데이트를 수신 순서대로 처리하고 처리된 내용을
저장해야 합니다. 고가용성 데이터베이스 업데이트 회사는 또한 솔루션을 유지하는 데 필요한
관리 오버헤드를 최소화하려고 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해
무엇을 해야 합니까?
A. Amazon Kinesis Data Streams에 점수 업데이트 푸시 AWS Lambda를 사용하여 Kinesis
312

IT Certification Guaranteed, The Easy Way!
Data Streams의 업데이트를 처리합니다. 처리된 업데이트를 Amazon DynamoDB에
저장합니다.
B. Amazon Kinesis Data Streams에 점수 업데이트를 푸시합니다. Auto Scaling을 위해
설정된 Amazon EC2 인스턴스 집합으로 업데이트를 처리합니다. 처리된 업데이트를 Amazon
Redshift에 저장합니다.
C. Amazon Simple 알림 서비스(Amazon SNS) 주제에 점수 업데이트 푸시 업데이트를
처리하려면 SNS 주제에 대한 AWS Lambda 함수를 구독하세요. Amazon EC2에서 실행되는
SQL 데이터베이스에 처리된 업데이트를 저장합니다.
D. Amazon Simple Queue Service(Amazon SQS) 대기열에 점수 업데이트를 푸시합니다.
Auto Scaling이 포함된 Amazon EC2 인스턴스 집합을 사용하여 SQS 대기열의 업데이트를
처리합니다. 처리된 업데이트를 Amazon RDS 다중 AZ DB 인스턴스에 저장합니다.
Answer: A
Explanation:
Amazon Kinesis Data Streams is a scalable and reliable service that can ingest, buffer, and
process streaming data in real-time. It can handle large traffic spikes and preserve the order
of the incoming data records. AWS Lambda is a serverless compute service that can process
the data streams from Kinesis Data Streams without requiring any infrastructure
management. It can also scale automatically to match the throughput of the data stream.
Amazon DynamoDB is a fully managed, highly available, and fast NoSQL database that can
store the processed updates from Lambda. It can also handle high write throughput and
provide consistent performance. By using these services, the solutions architect can design a
solution that meets the requirements of the company with the least operational overhead.
QUESTION NO: 459
한 회사에서 1PB 온프레미스 이미지 리포지토리를 AWS로 마이그레이션하려고 합니다.
이미지는 서버리스 웹 애플리케이션에서 사용됩니다. 저장소에 저장된 이미지는 거의
액세스되지 않지만 즉시 사용할 수 있어야 합니다. 또한 이미지는 저장 시 암호화되고 실수로
삭제되지 않도록 보호해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 클라이언트 측 암호화를 구현하고 Amazon S3 Glacier 저장소에 이미지를 저장합니다.
실수로 삭제되는 것을 방지하기 위해 저장소 잠금을 설정합니다.
B. S3 Standard-Infrequent Access(S3 Standard-IA) 스토리지 클래스의 Amazon S3 버킷에
이미지를 저장합니다. S3 버킷에서 버전 관리 기본 암호화 및 MFA 삭제를 활성화합니다.
C. Amazon FSx for Windows File Server 파일 공유에 이미지를 저장합니다. AWS Key
Management Service(AWS KMS) 고객 마스터 키(CMK)를 사용하여 파일 공유의 이미지를
암호화하도록 Amazon FSx 파일 공유를 구성합니다. NTFS 사용 실수로 삭제되는 것을
방지하기 위해 이미지에 대한 권한 집합
D. Infrequent Access 스토리지 클래스의 Amazon Elastic File System(Amazon EFS) 파일
공유에 이미지를 저장합니다. AWS Key Management Service(AWS KMS) 고객 마스터
키(CMK)를 사용하여 EFS 파일 공유를 구성하여 파일 공유의 이미지. 실수로 삭제되는 것을
방지하려면 이미지에 NFS 권한 집합을 사용하세요.
Answer: B
Explanation:
This answer is correct because it provides a resilient and durable replacement for the on-
premises file share that is compatible with a serverless web application. Amazon S3 is a fully
313

IT Certification Guaranteed, The Easy Way!
managed object storage service that can store any amount of data and serve it over the
internet. It supports the following features:
* Resilience: Amazon S3 stores data across multiple Availability Zones within a Region, and
offers
99.999999999% (11 9's) of durability. It also supports cross-region replication, which enables
automatic and asynchronous copying of objects across buckets in different AWS Regions.
* Durability: Amazon S3 encrypts data at rest using server-side encryption with either
Amazon S3- managed keys (SSE-S3), AWS KMS keys (SSE-KMS), or customer-provided
keys (SSE-C). It also supports encryption in transit using SSL/TLS. Amazon S3 also provides
data protection features such as versioning, which keeps multiple versions of an object in the
same bucket, and MFA Delete, which requires additional authentication for deleting an object
version or changing the versioning state of a bucket.
* Performance: Amazon S3 delivers high performance and scalability for serving static and
dynamic web content. It also supports features such as S3 Transfer Acceleration, which
speeds up data transfers by routing requests to AWS edge locations, and S3 Select, which
enables retrieving only a subset of data from an object by using simple SQL expressions.
The S3 Standard-Infrequent Access (S3 Standard-IA) storage class is suitable for storing
images that are rarely accessed, but must be immediately available when needed. It offers
the same high durability, throughput, and low latency as S3 Standard, but with a lower
storage cost per GB and a higher per-request cost.
References:
* Amazon Simple Storage Service
* Storage classes - Amazon Simple Storage Service
QUESTION NO: 460
한 회사가 워크로드를 AWS로 마이그레이션하고 있습니다. 회사는 SQL Server 인스턴스에서
실행되는 온프레미스 관계형 데이터베이스에 민감하고 중요한 데이터를 보유하고 있습니다.
회사는 AWS 클라우드를 사용하여 보안을 강화하고 데이터베이스의 운영 오버헤드를 줄이고
싶어합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 데이터베이스를 Amazon EC2 인스턴스로 마이그레이션합니다. 암호화를 위해 AWS Key
Management Service(AWS KMS) AWS 관리형 키를 사용합니다.
B. 데이터베이스를 SQL Server DB 인스턴스용 다중 AZ Amazon RDS로
마이그레이션합니다. 암호화를 위해 AWS Key Management Service(AWS KMS) AWS
관리형 키를 사용합니다.
C. 데이터를 Amazon S3 버킷으로 마이그레이션 Amazon Macie를 사용하여 데이터 보안 보장
D. 데이터베이스를 Amazon DynamoDB 테이블로 마이그레이션합니다. Amazon CloudWatch
Logs를 사용하여 데이터 보안 보장
Answer: B
Explanation:
* Understanding the Requirement: The company needs to migrate sensitive and critical data
from on- premises SQL Server databases to AWS, aiming to increase security and reduce
operational overhead.
* Analysis of Options:
* EC2 Instances with KMS: Running SQL Server on EC2 provides control but requires
significant operational overhead for management, backups, patching, and high availability.
314

IT Certification Guaranteed, The Easy Way!
* Multi-AZ Amazon RDS for SQL Server with KMS: Amazon RDS for SQL Server offers
managed database services, reducing operational overhead. Multi-AZ deployment provides
high availability, and KMS encryption ensures data security.
* Amazon S3 and Macie: S3 is not a suitable replacement for relational databases, and
Macie is used for data security and compliance but not for database operations.
* Amazon DynamoDB and CloudWatch Logs: DynamoDB is a NoSQL database and does
not support SQL Server workloads directly. CloudWatch Logs are used for monitoring, not for
ensuring database security.
* Best Solution:
* Multi-AZ Amazon RDS for SQL Server with KMS: This solution meets the requirements for
security, high availability, and reduced operational overhead by using a managed database
service with encryption.
References:
* Amazon RDS for SQL Server
* AWS Key Management Service (KMS)
QUESTION NO: 461
회사는 회사 데이터 센터의 Kubernetes 클러스터에서 컨테이너 애플리케이션을 실행합니다.
애플리케이션은 AMQP(Advanced Message Queuing Protocol)를 사용하여 메시지 큐와
통신합니다. 데이터 센터는 회사의 확장되는 비즈니스 요구 사항을 충족할 만큼 빠르게
확장할 수 없습니다. 회사는 마이그레이션을 원합니다. 최소한의 운영 오버헤드로 이러한
요구 사항을 충족하는 솔루션은 무엇입니까? \
A. 컨테이너 애플리케이션을 Amazon Elastic Container Service(Amazon ECS)로
마이그레이션합니다. Amazon Simple Queue Service(Amazon SQS)를 사용하여 메시지를
검색합니다.
B. 컨테이너 애플리케이션을 Amazon Elastic Kubernetes Service(Amazon EKS)로
마이그레이션합니다. Amazon MQ를 사용하여 메시지를 검색합니다.
C. 고가용성 Amazon EC2 인스턴스를 사용하여 애플리케이션을 실행합니다. Amazon MQ를
사용하여 메시지를 검색합니다.
D. AWS Lambda 함수를 사용하여 애플리케이션을 실행합니다. Amazon Simple Queue
Service(Amazon SQS)를 사용하여 메시지를 검색합니다.
Answer: B
Explanation:
This option is the best solution because it allows the company to migrate the container
application to AWS with minimal changes and leverage a managed service to run the
Kubernetes cluster and the message queue.
By using Amazon EKS, the company can run the container application on a fully managed
Kubernetes control plane that is compatible with the existing Kubernetes tools and plugins.
Amazon EKS handles the provisioning, scaling, patching, and security of the Kubernetes
cluster, reducing the operational overhead and complexity. By using Amazon MQ, the
company can use a fully managed message broker service that supports AMQP and other
popular messaging protocols. Amazon MQ handles the administration, maintenance, and
scaling of the message broker, ensuring high availability, durability, and security of the
messages.
A: Migrate the container application to Amazon Elastic Container Service (Amazon ECS) Use
315

IT Certification Guaranteed, The Easy Way!
Amazon Simple Queue Service (Amazon SQS) to retrieve the messages. This option is not
optimal because it requires the company to change the container orchestration platform from
Kubernetes to ECS, which can introduce additional complexity and risk. Moreover, it requires
the company to change the messaging protocol from AMQP to SQS, which can also affect
the application logic and performance. Amazon ECS and Amazon SQS are both fully
managed services that simplify the deployment and management of containers and
messages, but they may not be compatible with the existing application architecture and
requirements.
C: Use highly available Amazon EC2 instances to run the application Use Amazon MQ to
retrieve the messages. This option is not ideal because it requires the company to manage
the EC2 instances that host the container application. The company would need to provision,
configure, scale, patch, and monitor the EC2 instances, which can increase the operational
overhead and infrastructure costs. Moreover, the company would need to install and maintain
the Kubernetes software on the EC2 instances, which can also add complexity and risk.
Amazon MQ is a fully managed message broker service that supports AMQP and other
popular messaging protocols, but it cannot compensate for the lack of a managed
Kubernetes service.
D: Use AWS Lambda functions to run the application Use Amazon Simple Queue Service
(Amazon SQS) to retrieve the messages. This option is not feasible because AWS Lambda
does not support running container applications directly. Lambda functions are executed in a
sandboxed environment that is isolated from other functions and resources. To run container
applications on Lambda, the company would need to use a custom runtime or a wrapper
library that emulates the container API, which can introduce additional complexity and
overhead. Moreover, Lambda functions have limitations in terms of available CPU, memory,
and runtime, which may not suit the application needs. Amazon SQS is a fully managed
message queue service that supports asynchronous communication, but it does not support
AMQP or other messaging protocols.
References:
* 1 Amazon Elastic Kubernetes Service - Amazon Web Services
* 2 Amazon MQ - Amazon Web Services
* 3 Amazon Elastic Container Service - Amazon Web Services
* 4 AWS Lambda FAQs - Amazon Web Services
QUESTION NO: 462
한 회사에서 프라이빗 서브넷의 Amazon Elastic Kubernetes Service(Amazon EKS)에서
실행되는 포드로 Java Spring Boot 애플리케이션을 배포했습니다. 애플리케이션은 Amazon
DynamoDB 테이블에 데이터를 써야 합니다. 솔루션 아키텍트는 트래픽을 인터넷에 노출하지
않고도 애플리케이션이 DynamoDB 테이블과 상호 작용할 수 있는지 확인해야 합니다.
이 목표를 달성하기 위해 솔루션 설계자는 어떤 단계 조합을 수행해야 합니까? (2개를
선택하세요.)
A. EKS 포드에 충분한 권한이 있는 IAM 역할을 연결합니다.
B. EKS 포드에 충분한 권한이 있는 IAM 사용자를 연결합니다.
C. 프라이빗 서브넷의 네트워크 ACL을 통해 DynamoDB 테이블에 대한 아웃바운드 연결을
허용합니다.
D. DynamoDB용 VPC 엔드포인트를 생성합니다.
316

IT Certification Guaranteed, The Easy Way!
E. Java Spring Boot 코드에 액세스 키를 포함합니다.
Answer: A D
Explanation:
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/vpc-endpoints-
dynamodb.html
https://aws.amazon.com/about-aws/whats-new/2019/09/amazon-eks-adds-support-to-assign-
iam-permissions- to-kubernetes-service-accounts/
QUESTION NO: 463
솔루션 설계자는 Windows Internet Information Services(IIS) 웹 애플리케이션을 AWS로
마이그레이션해야 합니다. 애플리케이션은 현재 사용자의 온프레미스 NAS(Network Attached
Storage)에서 호스팅되는 파일 공유에 의존합니다. 솔루션 설계자는 MS 웹 서버를 AWS로
마이그레이션할 것을 제안했습니다. 스토리지 솔루션에 연결된 여러 가용 영역의 Amazon
EC2 인스턴스 및 인스턴스에 연결된 Elastic Load Balancer 구성 온프레미스 파일 공유에
대한 복원력과 내구성이 가장 뛰어난 것은 무엇입니까?
A. 파일 공유를 Amazon RDS로 마이그레이션합니다.
B. 파일 공유를 AWS Storage Gateway로 마이그레이션
C. 파일 공유를 Amazon FSx for Windows File Server로 마이그레이션합니다.
D. 파일 공유를 Amazon Elastic File System(Amazon EFS)으로 마이그레이션합니다.
Answer: C
Explanation:
This answer is correct because it provides a resilient and durable replacement for the on-
premises file share that is compatible with Windows IIS web servers. Amazon FSx for
Windows File Server is a fully managed service that provides shared file storage built on
Windows Server. It supports the SMB protocol and integrates with Microsoft Active Directory,
which enables seamless access and authentication for Windows-based applications. Amazon
FSx for Windows File Server also offers the following benefits:
* Resilience: Amazon FSx for Windows File Server can be deployed in multiple Availability
Zones, which provides high availability and failover protection. It also supports automatic
backups and restores, as well as self-healing features that detect and correct issues.
* Durability: Amazon FSx for Windows File Server replicates data within and across
Availability Zones, and stores data on highly durable storage devices. It also supports
encryption at rest and in transit, as well as file access auditing and data deduplication.
* Performance: Amazon FSx for Windows File Server delivers consistent sub-millisecond
latencies and high throughput for file operations. It also supports SSD storage, native
Windows features such as Distributed File System (DFS) Namespaces and Replication, and
user-driven performance scaling.
References:
* Amazon FSx for Windows File Server
* Using Microsoft Windows file shares
QUESTION NO: 464
한 회사가 AWS에서 2계층 웹 애플리케이션을 개발하고 있습니다. 회사 개발자는 백엔드
Amazon RDS 데이터베이스에 직접 연결되는 Amazon EC2 인스턴스에 애플리케이션을
배포했습니다. 회사는 애플리케이션에 데이터베이스 자격 증명을 하드코딩해서는 안 됩니다.
317

IT Certification Guaranteed, The Easy Way!
또한 회사는 정기적으로 데이터베이스 자격 증명을 자동으로 교체하는 솔루션을 구현해야
합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 인스턴스 메타데이터에 데이터베이스 자격 증명을 저장합니다. Amazon
EventBridge(Amazon CloudWatch Events) 규칙을 사용하여 RDS 자격 증명과 인스턴스
메타데이터를 동시에 업데이트하는 예약된 AWS Lambda 함수를 실행합니다.
B. 암호화된 Amazon S3 버킷의 구성 파일에 데이터베이스 자격 증명을 저장합니다. Amazon
EventBridge(Amazon CloudWatch Events) 규칙을 사용하여 RDS 자격 증명과 구성 파일의
자격 증명을 동시에 업데이트하는 예약된 AWS Lambda 함수를 실행합니다. S3 버전 관리를
사용하여 이전 값으로 폴백하는 기능을 보장합니다.
C. 데이터베이스 자격 증명을 AWS Secrets Manager에 암호로 저장합니다. 보안 비밀에 대한
자동 순환을 켭니다. EC2 역할에 필요한 권한을 연결하여 보안 암호에 대한 액세스 권한을
부여합니다.
D. 데이터베이스 자격 증명을 AWS Systems Manager Parameter Store에 암호화된
파라미터로 저장합니다. 암호화된 매개변수에 대해 자동 회전을 켭니다. EC2 역할에 필요한
권한을 연결하여 암호화된 파라미터에 대한 액세스 권한을 부여합니다.
Answer: C
Explanation:
https://docs.aws.amazon.com/secretsmanager/latest/userguide/create_database_secret.html
QUESTION NO: 465
한 회사는 최근 전체 IT 환경을 AWS 클라우드로 마이그레이션했습니다. 회사는 사용자가
적절한 변경 제어 프로세스를 사용하지 않고 대규모 Amazon EC2 인스턴스를
프로비저닝하고 보안 그룹 규칙을 수정하고 있음을 발견했습니다. 솔루션 설계자는 이러한
인벤토리 및 구성 변경 사항을 추적하고 감사하기 위한 전략을 고안해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 조치를 취해야 합니까? (2개 선택)
A. AWS CloudTrail을 활성화하고 감사에 사용합니다.
B. Amazon EC2 인스턴스에 대한 데이터 수명주기 정책 사용
C. AWS Trusted Advisor를 활성화하고 보안 대시보드를 참조합니다.
D. AWS Config를 활성화하고 감사 및 규정 준수 목적으로 규칙을 생성합니다.
E. AWS CloudFormation 템플릿을 사용하여 이전 리소스 구성 복원
Answer: A D
Explanation:
A) Enable AWS CloudTrail and use it for auditing. AWS CloudTrail provides a record of API
calls and can be used to audit changes made to EC2 instances and security groups. By
analyzing CloudTrail logs, the solutions architect can track who provisioned oversized
instances or modified security groups without proper approval. D) Enable AWS Config and
create rules for auditing and compliance purposes. AWS Config can record the configuration
changes made to resources like EC2 instances and security groups. The solutions architect
can create AWS Config rules to monitor for non-compliant changes, like launching certain
instance types or opening security group ports without permission. AWS Config would alert
on any violations of these rules.
QUESTION NO: 466
318

IT Certification Guaranteed, The Easy Way!
한 회사가 온프레미스 Oracle 데이터베이스를 Amazon RDS for Oracle 데이터베이스로
마이그레이션하고 있습니다. 이 회사는 규제 요건을 충족하기 위해 90일 동안 데이터를
보관해야 합니다. 또한 이 회사는 최대 14일 동안 특정 시점으로 데이터베이스를 복원할 수
있어야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. Amazon RDS 자동 백업을 만듭니다. 보관 기간을 90일로 설정합니다.
B. 매일 Amazon RDS 수동 스냅샷을 만듭니다. 90일 이상 된 수동 스냅샷을 삭제합니다.
C. Oracle용 Amazon Aurora Clone 기능을 사용하여 특정 시점 복원을 만듭니다. 90일 이상 된
복제본을 삭제합니다.
D. Amazon RDS용 AWS Backup을 사용하여 보존 기간이 90일인 백업 계획을 만듭니다.
Answer: D
Explanation:
AWS Backup is the most appropriate solution for managing backups with minimal operational
overhead while meeting the regulatory requirement to retain data for 90 days and enabling
point-in-time restore for up to 14 days.
* AWS Backup: AWS Backup provides a centralized backup management solution that
supports automated backup scheduling, retention management, and compliance reporting
across AWS services, including Amazon RDS. By creating a backup plan, you can define a
retention period (in this case, 90 days) and automate the backup process.
* Point-in-Time Restore (PITR): Amazon RDS supports point-in-time restore for up to 35 days
with automated backups. By using AWS Backup in conjunction with RDS, you ensure that
your backup strategy meets the requirement for restoring data to a specific point in time
within the last 14 days.
* Why Not Other Options?:
* Option A (RDS Automated Backups): While RDS automated backups support PITR, they do
not directly support retention beyond 35 days without manual intervention.
* Option B (Manual Snapshots): Manually creating and managing snapshots is operationally
intensive and less automated compared to AWS Backup.
* Option C (Aurora Clones): Aurora Clone is a feature specific to Amazon Aurora and is not
applicable to Amazon RDS for Oracle.
AWS References:
* AWS Backup - Overview of AWS Backup and its capabilities.
* Amazon RDS Automated Backups - Information on how RDS automated backups work and
their limitations.
QUESTION NO: 467
회사에서는 사용 중인 두 개의 NAT 인스턴스가 회사 애플리케이션에 필요한 트래픽을 더
이상 지원할 수 없게 되는 것을 우려하고 있습니다. 솔루션 설계자는 가용성이 높고
내결함성이 있으며 자동으로 확장 가능한 솔루션을 구현하려고 합니다.
솔루션 설계자는 무엇을 추천해야 합니까?
A. 두 개의 NAT 인스턴스를 제거하고 동일한 가용 영역에 있는 두 개의 NAT 게이트웨이로
교체합니다.
B. 다양한 가용 영역의 NAT 인스턴스에 대해 Network Load Balancer와 함께 Auto Scaling
그룹을 사용합니다.
C. 두 개의 NAT 인스턴스를 제거하고 이를 다른 가용 영역에 있는 두 개의 NAT 게이트웨이로
319

IT Certification Guaranteed, The Easy Way!
교체합니다.
D. 두 개의 NAT 인스턴스를 서로 다른 가용 영역의 스팟 인스턴스로 교체하고 Network Load
Balancer를 배포합니다.
Answer: C
Explanation:
If you have resources in multiple Availability Zones and they share one NAT gateway, and if
the NAT gateway's Availability Zone is down, resources in the other Availability Zones lose
internet access. To create an Availability Zone-independent architecture, create a NAT
gateway in each Availability Zone and configure your routing to ensure that resources use the
NAT gateway in the same Availability Zone. https://docs.aws.
amazon.com/vpc/latest/userguide/vpc-nat-gateway.html#nat-gateway-basics
QUESTION NO: 468
회사에서 애플리케이션을 설계하고 있습니다. 애플리케이션은 AWS Lambda 함수를
사용하여 Amazon API Gateway를 통해 정보를 수신하고 해당 정보를 Amazon Aurora
PostgreSQL 데이터베이스에 저장합니다.
개념 증명 단계에서 회사는 데이터베이스에 로드해야 하는 대량의 데이터를 처리하기 위해
Lambda 할당량을 크게 늘려야 합니다. 솔루션 설계자는 확장성을 향상하고 구성 노력을
최소화하기 위해 새로운 디자인을 권장해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Lambda 함수 코드를 Amazon EC2 인스턴스에서 실행되는 Apache Tomcat 코드로
리팩터링합니다. 기본 JDBC(Java Database Connectivity) 드라이버를 사용하여
데이터베이스를 연결합니다.
B. 플랫폼을 Aurora에서 Amazon DynamoDB로 변경합니다. DynamoDB Accelerator(DAX)
클러스터를 프로비저닝합니다. DAX 클라이언트 SDK를 사용하여 DAX 클러스터에서 기존
DynamoDB API 호출을 가리킵니다.
C. 두 개의 Lambda 함수를 설정합니다. 정보를 수신하는 하나의 기능을 구성하십시오. 정보를
데이터베이스에 로드하도록 다른 기능을 구성하십시오. Amazon Simple 알림 서비스(Amazon
SNS)를 사용하여 Lambda 함수를 통합합니다.
D. 두 개의 Lambda 함수를 설정합니다. 정보를 수신하는 하나의 기능을 구성하십시오. 정보를
데이터베이스에 로드하도록 다른 기능을 구성하십시오. Amazon Simple Queue
Service(Amazon SQS) 대기열을 사용하여 Lambda 함수를 통합합니다.
Answer: B
Explanation:
bottlenecks can be avoided with queues (SQS).
QUESTION NO: 469
한 회사가 5개의 온프레미스 애플리케이션을 AWS 클라우드의 VPC로 마이그레이션하고
있습니다. 각 애플리케이션은 현재 온프레미스의 격리된 가상 네트워크에 배포되어 있으며
AWS 클라우드에도 유사하게 배포되어야 합니다. 애플리케이션은 공유 서비스 VPC에
연결되어야 합니다. 모든 애플리케이션은 서로 통신할 수 있어야 합니다.
마이그레이션이 성공하면 회사는 100개 이상의 애플리케이션에 대해 마이그레이션
프로세스를 반복합니다.
최소한의 관리 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 애플리케이션 VPC와 공유 서비스 VPC 사이에 소프트웨어 VPN 터널을 배포합니다. 해당
320

IT Certification Guaranteed, The Easy Way!
서브넷의 애플리케이션 VPC 사이의 경로를 공유 서비스 VPC에 추가합니다.
B. 애플리케이션 VPC와 공유 서비스 VPC 간에 VPC 피어링 연결을 배포합니다. 피어링
연결을 통해 해당 서브넷의 애플리케이션 VPC 간 경로를 공유 서비스 VPC에 추가합니다.
C. 애플리케이션 VPC와 공유 서비스 VPC 간에 AWS Direct Connect 연결을 배포합니다.
해당 서브넷의 애플리케이션 VPC에서 공유 서비스 VPC 및 애플리케이션 VPC로 경로를
추가합니다. 공유 서비스 VPC 서브넷의 경로를 애플리케이션 VPC에 추가합니다.
D. 전송 게이트웨이와 애플리케이션 VPC 및 공유 서비스 VPC 간의 연결을 사용하여 전송
게이트웨이를 배포합니다. 전송 게이트웨이를 통해 해당 서브넷의 애플리케이션 VPC와
애플리케이션 VPC 간의 경로를 공유 서비스 VPC에 추가합니다.
Answer: D
Explanation:
* Understanding the Requirement: The company needs to migrate applications to AWS,
maintaining isolated networks while allowing communication with a shared services VPC and
among the applications.
* Analysis of Options:
* Software VPN Tunnels: This approach involves high administrative overhead and
complexity in managing multiple VPN connections.
* VPC Peering: While suitable for smaller numbers of VPCs, it becomes complex and hard to
manage at scale with over 100 applications.
* Direct Connect: Primarily used for high-bandwidth, low-latency connections to on-premises
networks, not inter-VPC communication.
* Transit Gateway: Simplifies network management by acting as a central hub, allowing easy
routing and scalability as more applications are migrated.
* Best Solution:
* Transit Gateway: This provides a scalable, efficient solution with minimal administrative
overhead for managing network connections between multiple VPCs and the shared services
VPC.
References:
* AWS Transit Gateway
* Building a Transit Gateway
QUESTION NO: 470
한 회사가 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용하여 컨테이너
애플리케이션을 실행합니다. EKS 클러스터는 Kubernetes secrets 객체에 민감한 정보를
저장합니다. 이 회사는 정보가 암호화되었는지 확인하고자 합니다. 어떤 솔루션이 가장 적은
운영 오버헤드로 이러한 요구 사항을 충족할까요?
A. AWS Key Management Service(AWS KMS)를 사용하여 컨테이너 애플리케이션을
사용하여 정보를 암호화합니다.
B. AWS Key Management Service(AWS KMS)를 사용하여 EKS 클러스터에서 비밀 암호화를
활성화합니다.
C. AWS Key Management Service(AWS KMS)를 사용하여 정보를 암호화하는 AWS Lambda
구문을 구현합니다.
D. AWS Systems Manager Parameter Store를 사용하여 AWS Key Management
Service(AWS KMS)를 통해 정보를 암호화합니다.
Answer: B
321

IT Certification Guaranteed, The Easy Way!
Explanation:
it allows the company to encrypt the Kubernetes secrets object in the EKS cluster with the
least operational overhead. By enabling secrets encryption in the EKS cluster, the company
can use AWS Key Management Service (AWS KMS) to generate and manage encryption
keys for encrypting and decrypting secrets at rest.
This is a simple and secure way to protect sensitive information in EKS clusters. References:
* Encrypting Kubernetes secrets with AWS KMS
* Kubernetes Secrets
QUESTION NO: 471
한 회사에 야간 배치 작업을 실행하여 데이터를 처리하는 Amazon EC2 인스턴스가 있습니다.
EC2 인스턴스는 온디맨드 결제를 사용하는 Auto Scaling 그룹에서 실행됩니다. 한
인스턴스에서 작업이 실패하면 다른 인스턴스에서 작업을 다시 처리합니다. 일괄 작업은 매일
오전 12시부터 오전 6시(현지 시간) 사이에 실행됩니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 EC2 인스턴스를 제공하는 솔루션은
무엇입니까?
A. 배치 작업에서 사용하는 Auto Scaling 그룹의 인스턴스 제품군을 포함하는 Amazon EC2용
1년 Savings Plan을 구매합니다.
B. 일괄 작업에서 사용하는 Auto Scaling 그룹에 있는 인스턴스의 특정 인스턴스 유형 및 운영
체제에 대해 1년 예약 인스턴스를 구매합니다.
C. Auto Scaling 그룹에 대한 새 시작 템플릿을 생성합니다. 인스턴스를 스팟 인스턴스로
설정합니다. CPU 사용량에 따라 확장하도록 정책을 설정합니다.
D. Auto Scaling 그룹을 위한 새 시작 템플릿을 생성합니다. 인스턴스 크기를 늘립니다. CPU
사용량에 따라 확장하도록 정책을 설정합니다.
Answer: C
Explanation:
This option is the most cost-effective solution because it leverages the Spot Instances, which
are unused EC2 instances that are available at up to 90% discount compared to On-Demand
prices. Spot Instances can be interrupted by AWS when the demand for On-Demand
instances increases, but since the batch jobs are fault- tolerant and can be reprocessed by
another instance, this is not a major issue. By using a launch template, the company can
specify the configuration of the Spot Instances, such as the instance type, the operating
system, and the user data. By using an Auto Scaling group, the company can automatically
scale the number of Spot Instances based on the CPU usage, which reflects the load of the
batch jobs. This way, the company can optimize the performance and the cost of the EC2
instances for the nightly batch jobs.
A: Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the
Auto Scaling group that the batch job uses. This option is not optimal because it requires a
commitment to a consistent amount of compute usage per hour for a one-year term,
regardless of the instance type, size, region, or operating system.
This can limit the flexibility and scalability of the Auto Scaling group and result in overpaying
for unused compute capacity. Moreover, Savings Plans do not provide a capacity
reservation, which means the company still needs to reserve capacity with On-Demand
Capacity Reservations and pay lower prices with Savings Plans.
B: Purchase a 1-year Reserved Instance for the specific instance type and operating system
322

IT Certification Guaranteed, The Easy Way!
of the instances in the Auto Scaling group that the batch job uses. This option is not ideal
because it requires a commitment to a specific instance configuration for a one-year term,
which can reduce the flexibility and scalability of the Auto Scaling group and result in
overpaying for unused compute capacity. Moreover, Reserved Instances do not provide a
capacity reservation, which means the company still needs to reserve capacity with On-
Demand Capacity Reservations and pay lower prices with Reserved Instances.
D: Create a new launch template for the Auto Scaling group Increase the instance size Set a
policy to scale out based on CPU usage. This option is not cost-effective because it does not
take advantage of the lower prices of Spot Instances. Increasing the instance size can
improve the performance of the batch jobs, but it can also increase the cost of the On-
Demand instances. Moreover, scaling out based on CPU usage can result in launching more
instances than needed, which can also increase the cost of the system.
References:
* 1 Spot Instances - Amazon Elastic Compute Cloud
* 2 Launch templates - Amazon Elastic Compute Cloud
* 3 Auto Scaling groups - Amazon EC2 Auto Scaling
* 4 Savings Plans - Amazon EC2 Reserved Instances and Other AWS Reservation Models
QUESTION NO: 472
회사에는 JSON 문서를 처리하고 그 결과를 온프레미스 SQL 데이터베이스에 출력하는 작은
Python 애플리케이션이 있습니다. 이 애플리케이션은 매일 수천 번 실행됩니다. 회사는
애플리케이션을 AWS 클라우드로 이동하려고 합니다. 이 회사는 확장성을 최대화하고 운영
오버헤드를 최소화하는 고가용성 솔루션이 필요합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. JSON 문서를 Amazon S3 버킷에 넣습니다. 여러 Amazon EC2 인스턴스에서 Python
코드를 실행하여 문서를 처리합니다. Amazon Aurora DB 클러스터에 결과 저장
B. JSON 문서를 Amazon S3 버킷에 넣습니다. 문서가 S3 버킷에 도착하면 이를 처리하기
위해 Python 코드를 실행하는 AWS Lambda 함수를 생성합니다. 결과를 Amazon Aurora DB
클러스터에 저장합니다.
C. JSON 문서를 Amazon Elastic Block Store(Amazon EBS) 볼륨에 넣습니다. EBS 다중 연결
기능을 사용하여 볼륨을 여러 Amazon EC2 인스턴스에 연결합니다. EC2 인스턴스에서
Python 코드를 실행하여 문서를 처리합니다. Amazon RDS DB 인스턴스에 결과를
저장합니다.
D. JSON 문서를 Amazon Simple Queue Service(Amazon SQS) 대기열에 메시지로
배치합니다. Python 코드를 Amazon EC2 시작 유형으로 구성된 Amazon Elastic Container
Service(Amazon ECS) 클러스터에 컨테이너로 배포합니다. 컨테이너를 사용하여 SQS
메시지를 처리합니다. Amazon RDS DB 인스턴스에 결과를 저장합니다.
Answer: B
Explanation:
By placing the JSON documents in an S3 bucket, the documents will be stored in a highly
durable and scalable object storage service. The use of AWS Lambda allows the company to
run their Python code to process the documents as they arrive in the S3 bucket without
having to worry about the underlying infrastructure. This also allows for horizontal scalability,
as AWS Lambda will automatically scale the number of instances of the function based on
the incoming rate of requests. The results can be stored in an Amazon Aurora DB cluster,
323

IT Certification Guaranteed, The Easy Way!
which is a fully-managed, high-performance database service that is compatible with MySQL
and PostgreSQL. This will provide the necessary durability and scalability for the results of
the processing.
https://aws.amazon.com/rds/aurora/
QUESTION NO: 473
한 회사가 AWS에서 2계층 전자상거래 웹사이트를 운영하고 있습니다. 웹 계층은 트래픽을
Amazon EC2 인스턴스로 보내는 로드 밸런서로 구성됩니다. 데이터베이스 계층은 Amazon
RDS DB 인스턴스를 사용합니다. EC2 인스턴스와 RDS DB 인스턴스는 공용 인터넷에
노출되어서는 안 됩니다. EC2 인스턴스에서는 타사 웹 서비스를 통해 주문 결제 처리를
완료하려면 인터넷 액세스가 필요합니다. 애플리케이션의 가용성이 높아야 합니다.
이러한 요구 사항을 충족하는 구성 옵션 조합은 무엇입니까? (2개를 선택하세요.)
A. Auto Scaling 그룹을 사용하여 프라이빗 서브넷에서 EC2 인스턴스를 시작합니다.
프라이빗 서브넷에 RDS 다중 AZ DB 인스턴스를 배포합니다.
B. 2개의 가용 영역에 걸쳐 2개의 프라이빗 서브넷과 2개의 NAT 게이트웨이로 VPC를
구성합니다. 프라이빗 서브넷에 Application Load Balancer를 배포합니다.
C. Auto Scaling 그룹을 사용하여 두 가용 영역의 퍼블릭 서브넷에서 EC2 인스턴스를
시작합니다. 프라이빗 서브넷에 RDS 다중 AZ DB 인스턴스를 배포합니다.
D. 2개의 가용 영역에 걸쳐 퍼블릭 서브넷 1개, 프라이빗 서브넷 1개, NAT 게이트웨이 2개로
VPC를 구성합니다. 퍼블릭 서브넷에 Application Load Balancer를 배포합니다.
E. 2개의 가용 영역에 걸쳐 2개의 퍼블릭 서브넷, 2개의 프라이빗 서브넷, 2개의 NAT
게이트웨이로 VPC를 구성합니다. 퍼블릭 서브넷에 Application Load Balancer를 배포합니다.
Answer: A E
Explanation:
Before you begin: Decide which two Availability Zones you will use for your EC2 instances.
Configure your virtual private cloud (VPC) with at least one public subnet in each of these
Availability Zones. These public subnets are used to configure the load balancer. You can
launch your EC2 instances in other subnets of these Availability Zones instead.
QUESTION NO: 474
회사의 도메인 이름 레코드를 호스팅하는 DNS 공급자가 AWS에서 실행되는 웹 사이트의
서비스 중단을 초래하는 중단을 겪고 있습니다. 회사는 보다 탄력적인 관리형 DNS 서비스로
마이그레이션해야 하며 해당 서비스가 AWS에서 실행되기를 원합니다.
DNS 호스팅 서비스를 신속하게 마이그레이션하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 도메인 이름에 대한 Amazon Route 53 퍼블릭 호스팅 영역을 생성합니다. 이전 공급자가
호스팅하는 도메인 레코드가 포함된 영역 파일을 가져옵니다.
B. 도메인 이름에 대한 Amazon Route 53 프라이빗 호스팅 영역을 생성합니다. 이전 공급자가
호스팅한 도메인 레코드가 포함된 영역 파일을 가져옵니다.
C. AWS에서 Simple AD 디렉터리를 생성합니다. 도메인 레코드에 대해 DNS 공급자와
Microsoft Active Directory용 AWS Directory Service 간의 영역 전송을 활성화합니다.
D. VPC에서 Amazon Route 53 Resolver 인바운드 엔드포인트를 생성합니다. 공급자의
DNS가 DNS 쿼리를 전달할 IP 주소를 지정합니다. 도메인에 대한 DNS 쿼리를 인바운드
엔드포인트에 지정된 IP 주소로 전달하도록 공급자의 DNS를 구성합니다.
Answer: A
324

IT Certification Guaranteed, The Easy Way!
Explanation:
To migrate the DNS hosting service to a more resilient managed DNS service on AWS, the
company should use Amazon Route 53, which is a highly available and scalable cloud DNS
web service. Route 53 can host public DNS records for the company's domain name and
provide reliable and secure DNS resolution. To rapidly migrate the DNS hosting service, the
company should create a public hosted zone for the domain name in Route 53, which is a
container for the domain's DNS records. Then, the company should import the zone file
containing the domain records hosted by the previous provider, which is a text file that
defines the DNS records for the domain. This way, the company can quickly transfer the
existing DNS records to Route
53 without manually creating them. After importing the zone file, the company should update
the domain registrar to use the name servers that Route 53 assigns to the hosted zone. This
will ensure that DNS queries for the domain name are routed to Route 53 and resolved by
the imported records.
QUESTION NO: 475
한 회사가 분산 애플리케이션을 AWS로 마이그레이션하고 있습니다. 애플리케이션이 다양한
워크로드를 처리합니다. 레거시 플랫폼은 기본 서버 평가판으로 구성되어 여러 컴퓨팅
노드에서 작업을 조정합니다. 회사는 탄력성과 확장성을 극대화하는 솔루션으로
애플리케이션을 현대화하려고 합니다. 솔루션 설계자는 어떻게 설계해야 할까요? 이러한
요구 사항을 충족하는 아키텍처는 무엇입니까?
A. Amazon Simple Queue Service(Amazon SQS) 대기열을 작업의 대상으로 구성합니다.
Auto Scaling 그룹에서 관리되는 Amazon EC2 인스턴스로 컴퓨팅 노드를 구현합니다.
예약된 조정을 사용하도록 EC2 Auto Scaling 구성
B. Amazon Simple Queue Service(Amazon SQS) 대기열을 작업 대상으로 구성 Auto Scaling
그룹에서 관리되는 Amazon EC2 인스턴스로 컴퓨팅 노드 구현 대기열 크기에 따라 EC2 Auto
Scaling 구성
C. Auto Scaling 그룹에서 관리되는 Amazon EC2 인스턴스로 기본 서버 및 컴퓨팅 노드를
구현합니다. fobs의 대상으로 AWS CloudTrail 구성 기본 서버의 로드를 기반으로 EC2 Auto
Scaling 구성
D. Auto Scaling 그룹에서 관리되는 Amazon EC2 인스턴스로 기본 서버 및 컴퓨팅 노드 구현
Amazon EventBridge(Amazon CloudWatch Events)를 작업의 대상으로 구성 컴퓨팅 노드의
부하를 기반으로 EC2 Auto Scaling 구성
Answer: B
Explanation:
To maximize resiliency and scalability, the best solution is to use an Amazon SQS queue as
a destination for the jobs. This decouples the primary server from the compute nodes,
allowing them to scale independently.
This also helps to prevent job loss in the event of a failure. Using an Auto Scaling group of
Amazon EC2 instances for the compute nodes allows for automatic scaling based on the
workload. In this case, it's recommended to configure the Auto Scaling group based on the
size of the Amazon SQS queue, which is a better indicator of the actual workload than the
load on the primary server or compute nodes. This approach ensures that the application can
handle variable workloads, while also minimizing costs by automatically scaling up or down
the compute nodes as needed.
325

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 476
회사는 제3자 데이터 피드와 통합해야 합니다. 데이터 피드는 새 데이터를 사용할 준비가 되면
외부 서비스에 알리기 위해 웹후크를 보냅니다. 개발자는 회사가 웹후크 콜백을 수신할 때
데이터를 검색하는 AWS Lambfe 함수를 작성했습니다. 개발자는 타사가 호출할 수 있도록
Lambda 함수를 제공해야 합니다.
가장 효율적인 운영 효율성으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Lambda 함수에 대한 함수 URL을 생성합니다. Webhook에 대한 Lambda 함수 URL을
타사에 제공합니다.
B. Lambda 함수 앞에 Application Load Balancer(ALB)를 배포합니다. 웹훅에 대한 ALB
URL을 제3자에게 제공하세요.
C. Amazon Simple 알림 서비스(Amazon SNS) 주제를 생성합니다. 주제를 Lambda 함수에
연결합니다. 웹훅을 위해 SNS 주제의 공개 호스트 이름을 제3자에게 제공합니다.
D. Amazon Simple Queue Service(Amazon SQS) 대기열을 생성합니다. Lambda 함수에
대기열을 연결합니다. 웹후크에 대해 SQS 대기열의 공개 호스트 이름을 제3자에게
제공합니다.
Answer: A
Explanation:
A function URL is a unique identifier for a Lambda function that can be used to invoke the
function over HTTPS. It is composed of the API endpoint of the AWS Region where the
function is deployed, and the name or ARN of the function1. By creating a function URL for
the Lambda function, the solution can make the Lambda function available for the third party
to call with the most operational efficiency.
B: Deploy an Application Load Balancer (ALB) in front of the Lambda function. Provide the
ALB URL to the third party for the webhook. This solution will not meet the requirement of the
most operational efficiency, as it involves creating and managing an additional resource
(ALB) that is not necessary for invoking a Lambda function over HTTPS2.
C: Create an Amazon Simple Notification Service (Amazon SNS) topic. Attach the topic to
the Lambda function. Provide the public hostname of the SNS topic to the third party for the
webhook. This solution will not work, as Amazon SNS topics do not have public hostnames
that can be used as webhooks. SNS topics are used to publish messages to subscribers, not
to receive messages from external sources3.
D: Create an Amazon Simple Queue Service (Amazon SQS) queue. Attach the queue to the
Lamb-da function. Provide the public hostname of the SQS queue to the third party for the
webhook. This solution will not work, as Amazon SQS queues do not have public hostnames
that can be used as webhooks. SQS queues are used to send, store, and receive messages
between AWS services, not to receive messages from external sources.
Reference URL: https://docs.aws.amazon.com/lambda/latest/dg/lambda-api-permissions-
ref.html
QUESTION NO: 477
솔루션 설계자는 웹, 애플리케이션 및 데이터베이스 계층으로 구성된 고가용성
애플리케이션을 설계해야 합니다. HTTPS 콘텐츠 전달은 전달 시간을 최소화하면서 최대한
에지에 가까워야 합니다.
이러한 요구 사항을 충족하고 가장 안전한 솔루션은 무엇입니까?
326

IT Certification Guaranteed, The Easy Way!
A. 퍼블릭 서브넷의 여러 중복 Amazon EC2 인스턴스로 퍼블릭 Application Load
Balancer(ALB)를 구성합니다. 퍼블릭 ALB를 원본으로 사용하여 HTTPS 콘텐츠를 제공하도록
Amazon CloudFront를 구성합니다.
B. 프라이빗 서브넷의 여러 중복 Amazon EC2 인스턴스로 퍼블릭 Application Load
Balancer를 구성합니다. EC2 인스턴스를 원본으로 사용하여 HTTPS 콘텐츠를 제공하도록
Amazon CloudFront를 구성합니다.
C. 프라이빗 서브넷의 여러 중복 Amazon EC2 인스턴스로 퍼블릭 Application Load
Balancer(ALB)를 구성합니다. 퍼블릭 ALB를 원본으로 사용하여 HTTPS 콘텐츠를 제공하도록
Amazon CloudFront를 구성합니다.
D. 퍼블릭 서브넷의 여러 중복 Amazon EC2 인스턴스로 퍼블릭 Application Load Balancer를
구성합니다. EC2 인스턴스를 원본으로 사용하여 HTTPS 콘텐츠를 제공하도록 Amazon
CloudFront를 구성합니다.
Answer: C
Explanation:
This solution meets the requirements for a highly available application with web, application,
and database tiers, as well as providing edge-based content delivery. Additionally, it
maximizes security by having the ALB in a private subnet, which limits direct access to the
web servers, while still being able to serve traffic over the Internet via the public ALB. This
will ensure that the web servers are not exposed to the public Internet, which reduces the
attack surface and provides a secure way to access the application.
QUESTION NO: 478
회사에는 3계층 상태 비저장 웹 애플리케이션에 대한 백업 전략이 필요합니다. 웹
애플리케이션은 조정 이벤트에 응답하도록 구성된 동적 조정 정책을 사용하여 Auto Scaling
그룹의 Amazon EC2 인스턴스에서 실행됩니다. 데이터베이스 계층은 PostgreSQL용 Amazon
RDS에서 실행됩니다. 웹 애플리케이션에는 EC2 인스턴스에 임시 로컬 스토리지가 필요하지
않습니다. 회사의 RPO(복구 지점 목표)는 2시간입니다. 백업 전략은 이 환경에 맞게 확장성을
극대화하고 리소스 활용도를 최적화해야 합니다. 이러한 요구 사항을 충족하는 솔루션은
무엇입니까?
A. RPO를 충족하기 위해 2시간마다 EC2 인스턴스 및 데이터베이스의 Amazon Elastic Block
Store(Amazon EBS) 볼륨 스냅샷을 찍습니다.
B. Amazon Elastic Block Store(Amazon EBS) 스냅샷을 생성하도록 스냅샷 수명 주기 정책을
구성합니다. Amazon RDS에서 자동 백업을 활성화하여 RPO를 충족합니다.
C. 웹 및 애플리케이션 계층의 최신 Amazon 머신 이미지(AMI)를 유지합니다. Amazon
RDS에서 자동 백업을 활성화하고 특정 시점 복구를 사용하여 RPO를 충족합니다.
D. 2시간마다 EC2 인스턴스의 Amazon Elastic Block Store(Amazon EBS) 볼륨 스냅샷을
찍습니다. Amazon RDS에서 자동 백업을 활성화하고 특정 시점 복구를 사용하여 RPO를
충족합니다.
Answer: C
Explanation:
Since the application has no local data on instances, AMIs alone can meet the RPO by
restoring instances from the most recent AMI backup. When combined with automated RDS
backups for the database, this provides a complete backup solution for this environment. The
other options involving EBS snapshots would be unnecessary given the stateless nature of
327

IT Certification Guaranteed, The Easy Way!
the instances. AMIs provide all the backup needed for the app tier. This uses native,
automated AWS backup features that require minimal ongoing management: - AMI
automated backups provide point-in-time recovery for the stateless app tier. - RDS automated
backups provide point-in-time recovery for the database.
QUESTION NO: 479
회사는 보고를 위해 50TB의 데이터를 사용합니다. 회사는 이 데이터를 온프레미스에서
AWS로 이동하려고 합니다. 회사 데이터 센터의 사용자 지정 애플리케이션이 매주 데이터
변환 작업을 실행합니다. 회사는 데이터 전송이 완료되어 필요할 때까지 애플리케이션을 일시
중지할 계획입니다. 가능한 한 빨리 전송 프로세스를 시작하려면 데이터 센터에 추가 작업
부하에 사용할 수 있는 네트워크 대역폭이 없습니다. 솔루션 아키텍트는 데이터를 전송해야
하며 AWS 클라우드에서 계속 실행되도록 변환 작업을 구성해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS DataSync를 사용하여 데이터 이동 AWS Glue를 사용하여 사용자 지정 변환 작업을
생성합니다.
B. AWS Snowcone 디바이스를 주문하여 데이터를 이동합니다. 변환 애플리케이션을
디바이스에 배포합니다.
C. AWS Snowball Edge Storage Optimized 디바이스를 주문합니다. 데이터를 장치에
복사합니다. AWS Glue를 사용하여 사용자 지정 변환 작업을 생성합니다.
D. Amazon EC2 컴퓨팅이 포함된 AWS Snowball Edge Storage Optimized 디바이스를
주문합니다. 데이터를 디바이스에 복사합니다. AWS에서 새 EC2 인스턴스를 생성하여 변환
애플리케이션을 실행합니다.
Answer: C
Explanation:
* Understanding the Requirement: The company needs to transfer 50 TB of data to AWS with
minimal operational overhead and no available network bandwidth for the transfer. The
transformation job must continue running in the AWS Cloud.
* Analysis of Options:
* AWS DataSync and AWS Glue: DataSync is suitable for online data transfer, but there is no
available network bandwidth. AWS Glue can be used for data transformation but does not
solve the bandwidth issue.
* AWS Snowcone: Snowcone is a smaller device suitable for smaller data transfers, and
deploying the transformation application on it may not be feasible for 50 TB of data.
* AWS Snowball Edge Storage Optimized with Glue: This device is designed for large data
transfers. Copying the data to the device is straightforward, and AWS Glue can handle data
transformation in the cloud.
* AWS Snowball Edge Storage Optimized with EC2: This involves setting up EC2 instances
for transformation, adding operational complexity compared to using AWS Glue.
* Best Solution:
* AWS Snowball Edge Storage Optimized with AWS Glue: This provides the least operational
overhead for transferring large amounts of data and setting up the transformation job in the
cloud.
References:
* AWS Snowball Edge
* AWS Glue
328

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 480
회사는 기존 온프레미스 모놀리식 애플리케이션을 AWS로 마이그레이션하려고 합니다.
회사는 프런트엔드 코드와 백엔드 코드를 최대한 많이 유지하려고 합니다. 그러나 회사는
애플리케이션을 더 작은 애플리케이션으로 분할하기를 원합니다. 다른 팀이 각
애플리케이션을 관리합니다. 회사에는 운영 오버헤드를 최소화하는 확장성이 뛰어난
솔루션이 필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Lambda에서 애플리케이션을 호스팅합니다. 애플리케이션을 Amazon API
Gateway와 통합합니다.
B. AWS Amplify를 사용하여 애플리케이션을 호스팅합니다. AWS Lambda와 통합된 Amazon
API Gateway API에 애플리케이션을 연결합니다.
C. Amazon EC2 인스턴스에서 애플리케이션을 호스팅합니다. Auto Scaling 그룹의 EC2
인스턴스를 대상으로 하여 Application Load Balancer를 설정합니다.
D. Amazon Elastic Container Service(Amazon ECS)에서 애플리케이션을 호스팅합니다.
Amazon ECS를 대상으로 하여 Application Load Balancer를 설정합니다.
Answer: D
Explanation:
https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-
application-load- balancers/
QUESTION NO: 481
한 회사에 직원이 15명 있습니다. 이 회사는 직원의 근무 시작일을 Amazon DynamoDB
테이블에 저장합니다. 이 회사는 직원의 근무 기념일에 각 직원에게 이메일 메시지를
보내려고 합니다.
이러한 요구 사항을 충족하면서 가장 운영 효율적인 솔루션은 무엇일까요?
A. DynamoDB 테이블을 스캔하고 필요할 때 Amazon Simple Notification Service(Amazon
SNS)를 사용하여 직원에게 이메일 메시지를 보내는 스크립트를 만듭니다. cron 작업을
사용하여 Amazon EC2 인스턴스에서 매일 이 스크립트를 실행합니다.
B. DynamoDB 테이블을 스캔하고 Amazon Simple Queue Service {Amazon SQS}를
사용하여 필요할 때 직원에게 이메일 메시지를 보내는 스크립트를 만듭니다. cron 작업을
사용하여 이 스크립트를 Amazon EC2 인스턴스에서 매일 실행합니다.
C. DynamoDB 테이블을 스캔하고 Amazon Simple Notification Service(Amazon SNS)를
사용하여 필요할 때 직원에게 이메일 메시지를 보내는 AWS Lambda 함수를 만듭니다. 이
Lambda 함수를 매일 실행되도록 예약합니다.
D. DynamoDB 테이블을 스캔하고 필요할 때 Amazon Simple Queue Service(Amazon
SQS)를 사용하여 직원에게 이메일 메시지를 전송하는 AWS Lambda 함수를 만듭니다. 이
Lambda 함수가 매일 실행되도록 예약합니다.
Answer: C
Explanation:
* AWS Lambda for Operational Efficiency:
* AWS Lambda is a serverless compute service that allows you to run code without
provisioning or managing servers. It automatically scales based on the number of invocations
and eliminates the need to maintain and monitor EC2 instances, making it far more
operationally efficient compared to running a cron job on EC2.
329

IT Certification Guaranteed, The Easy Way!
* By using Lambda, you pay only for the compute time that your function uses. This is
especially beneficial when dealing with lightweight tasks, such as scanning a DynamoDB
table and sending email messages once a day.
* Amazon DynamoDB:
* DynamoDB is a highly scalable, fully managed NoSQL database. The table stores
employee start dates, and scanning the table to find the employees who have a work
anniversary on the current day is a lightweight operation. Lambda can easily perform this
operation using the DynamoDB Scan API or queries, depending on how the data is
structured.
* Amazon SNS for Email Notifications:
* Amazon Simple Notification Service (SNS) is a fully managed messaging service that
supports sending notifications to a variety of endpoints, including email. SNS is well-suited for
sending out email messages to employees, as it can handle the fan-out messaging pattern
(sending the same message to multiple recipients).
* In this scenario, once Lambda identifies employees who have their work anniversaries, it
can use SNS to send the email notifications efficiently. SNS integrates seamlessly with
Lambda, and sending emails via SNS is a common pattern for this type of use case.
* Event Scheduling:
* To automate this daily task, you can schedule the Lambda function using Amazon
EventBridge (formerly CloudWatch Events). EventBridge can trigger the Lambda function on
a daily schedule (cron-like scheduling). This avoids the complexity and operational overhead
of manually setting up cron jobs on EC2 instances.
* Why Not EC2 or SQS?:
* Option A & B suggest running a cron job on an Amazon EC2 instance. This approach
requires you to manage, scale, and patch the EC2 instance, which increases operational
overhead. Lambda is a better choice because it automatically scales and doesn't require
server management.
* Amazon Simple Queue Service (SQS) is ideal for decoupling distributed systems but isn't
necessary in this context because the goal is to send notifications to employees on their work
anniversaries. SQS adds unnecessary complexity for this straightforward use case, where
SNS is the simpler and more efficient solution.
AWS References:
* AWS Lambda
* Amazon SNS
* Amazon DynamoDB
* Amazon EventBridge
Summary:
Using AWS Lambda combined with Amazon SNS to send notifications, and scheduling the
function with Amazon EventBridge to run daily, is the most operationally efficient solution. It
leverages AWS serverless technologies, which reduce the need for infrastructure
management and provide automatic scaling. Therefore, Option C is the correct and optimal
choice.
QUESTION NO: 482
한 회사가 AWS에서 HPC(고성능 컴퓨팅) 워크로드를 실행하고 있습니다. 워크로드에는
긴밀하게 결합된 노드 간 통신을 통해 대기 시간이 짧은 네트워크 성능과 높은 네트워크
330

IT Certification Guaranteed, The Easy Way!
처리량이 필요했습니다. Amazon EC2 인스턴스는 컴퓨팅 및 스토리지 용량에 맞게 적절한
크기를 가지며 기본 옵션을 사용하여 시작됩니다.
워크로드 성능을 향상시키기 위해 솔루션 아키텍트는 무엇을 제안해야 합니까?
A. Amazon EC2 인스턴스를 시작하는 동안 클러스터 배치 그룹을 선택합니다.
B. Amazon EC2 인스턴스를 시작하는 동안 전용 인스턴스 테넌시를 선택하세요.
C. Amazon EC2 인스턴스를 시작하는 동안 Elastic Inference 액셀러레이터를 선택합니다.
D. Amazon EC2 인스턴스를 시작하는 동안 필요한 용량 예약을 선택합니다.
Answer: A
Explanation:
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2-
placementgroup.html
"A cluster placement group is a logical grouping of instances within a single Availability Zone
that benefit from low network latency, high network throughput"
QUESTION NO: 483
게임 회사에는 점수를 표시하는 웹 애플리케이션이 있습니다. 애플리케이션은 Application
Load Balancer 뒤의 Amazon EC2 인스턴스에서 실행됩니다. 애플리케이션은 MySQL용
Amazon RDS 데이터베이스에 데이터를 저장합니다. 사용자는 데이터베이스 읽기 성능으로
인해 오랜 지연과 중단을 경험하기 시작했습니다. 회사는 애플리케이션 아키텍처 변경을
최소화하면서 사용자 경험을 개선하기를 원합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 데이터베이스 앞에 Amazon ElastiCache를 사용합니다.
B. 애플리케이션과 데이터베이스 간에 RDS Proxy를 사용합니다.
C. 애플리케이션을 EC2 인스턴스에서 AWS Lambda로 마이그레이션합니다.
D. Amazon RDS for MySQL에서 Amazon DynamoDB로 데이터베이스를
마이그레이션합니다.
Answer: A
Explanation:
ElastiCache can help speed up the read performance of the database by caching frequently
accessed data, reducing latency and allowing the application to access the data more
quickly. This solution requires minimal modifications to the current architecture, as
ElastiCache can be used in conjunction with the existing Amazon RDS for MySQL database.
QUESTION NO: 484
한 회사가 온프레미스 데이터 센터의 가상 머신(VM)에서 여러 워크로드를 실행합니다. 회사는
빠르게 확장하고 있습니다. 온프레미스 데이터 센터는 비즈니스 요구 사항을 충족할 만큼
빠르게 확장할 수 없습니다. 회사는 워크로드를 AWS로 마이그레이션하려고 합니다.
마이그레이션은 시간에 민감합니다. 회사는 비중요한 워크로드에 대해 리프트 앤 시프트
전략을 사용하고 싶어합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (세 가지를 선택하십시오.)
A. AWS Schema Conversion Tool(AWS SCT)을 사용하여 VM에 대한 데이터를 수집합니다.
B. AWS 애플리케이션 마이그레이션 서비스를 사용합니다. VM에 AWS 복제 에이전트를
설치합니다.
C. VM의 초기 복제를 완료합니다. 테스트 인스턴스를 시작하여 VM에서 수용 테스트를
331

IT Certification Guaranteed, The Easy Way!
수행합니다.
D. VM의 모든 작업을 중지하고 전환 인스턴스를 시작합니다.
E. AWS App2Container(A2C)를 사용하여 VM에 대한 데이터를 수집합니다.
F. AWS Database Migration Service(AWS DMS)를 사용하여 VM을 마이그레이션합니다.
Answer: B C D
Explanation:
AWS Application Migration Service (AWS MGN) is the recommended tool for a lift-and-shift
strategy, especially for time-sensitive migrations. It automates the replication of on-premises
VMs to AWS, minimizing the effort required for migration and testing.
Key steps:
* Replication with AWS MGN: The AWS Replication Agent is installed on the VMs to
continuously replicate data to AWS, allowing you to manage migration easily.
* Testing and Cutover: Initial replication allows for testing in AWS before performing the final
cutover, ensuring that the migration process is smooth and data integrity is maintained.
* AWS Documentation: AWS MGN is recommended for migrating virtual machines to the
cloud with minimal downtime and disruption.
QUESTION NO: 485
회사는 해당 애플리케이션을 위한 스토리지 솔루션을 찾고 있습니다. 솔루션은 가용성과
확장성이 높아야 합니다. 또한 솔루션은 파일 시스템으로 작동하고, 기본 프로토콜을 통해
AWS 및 온프레미스의 여러 Linux 인스턴스로 탑재 가능해야 하며, 최소 크기 요구 사항이
없어야 합니다. 회사는 온프레미스 네트워크에서 VPC로 액세스하기 위해 Site-to-Site VPN을
설정했습니다.
이러한 요구 사항을 충족하는 스토리지 솔루션은 무엇입니까?
A. Amazon FSx 다중 AZ 배포
B. Amazon Elastic Block Store(Amazon EBS) 다중 연결 볼륨
C. 탑재 대상이 여러 개인 Amazon Elastic File System(Amazon EFS)
D. 단일 탑재 대상과 여러 액세스 포인트가 있는 Amazon Elastic File System(Amazon EFS)
Answer: C
Explanation:
Amazon EFS is a fully managed file system that can be mounted by multiple Linux instances
in AWS and on premises through native protocols such as NFS and SMB. Amazon EFS has
no minimum size requirements and can scale up and down automatically as files are added
and removed. Amazon EFS also supports high availability and durability by allowing multiple
mount targets in different Availability Zones within a region.
Amazon EFS meets all the requirements of the question, while the other options do not.
References:
* https://aws.amazon.com/efs/
* https://docs.aws.amazon.com/wellarchitected/latest/performance-efficiency-pillar/storage-
architecture- selection.html
* https://aws.amazon.com/blogs/storage/from-on-premises-to-aws-hybrid-cloud-architecture-
for-network- file-shares/
QUESTION NO: 486
한 회사가 Linux 기반 웹 서버 그룹을 AWS로 마이그레이션하고 있습니다. 웹 서버는 일부
332

IT Certification Guaranteed, The Easy Way!
콘텐츠에 대해 공유 파일 저장소의 파일에 액세스해야 합니다. 회사는 신청서를 변경해서는
안 됩니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 웹 서버에 액세스할 수 있는 Amazon S3 Standard 버킷을 생성합니다.
B. Amazon S3 버킷을 원본으로 사용하여 Amazon CloudFront 배포를 구성합니다.
C. Amazon Elastic File System(Amazon EFS) 파일 시스템을 생성합니다. 모든 웹 서버에 EFS
파일 시스템을 탑재합니다.
D. 범용 SSD(gp3) Amazon Elastic Block Store(Amazon EBS) 볼륨을 구성합니다. 모든 웹
서버에 EBS 볼륨을 마운트합니다.
Answer: C
Explanation:
Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system
on all web servers. To meet the requirements of providing a shared file store for Linux-based
web servers without making changes to the application, using an Amazon EFS file system is
the best solution. Amazon EFS is a managed NFS file system service that provides shared
access to files across multiple Linux-based instances, which makes it suitable for this use
case. Amazon S3 is not ideal for this scenario since it is an object storage service and not a
file system, and it requires additional tools or libraries to mount the S3 bucket as a file
system. Amazon CloudFront can be used to improve content delivery performance but is not
necessary for this requirement. Additionally, Amazon EBS volumes can only be mounted to
one instance at a time, so it is not suitable for sharing files across multiple instances.
QUESTION NO: 487
회사에 보고서를 생성하는 금융 애플리케이션이 있습니다. 보고서의 크기는 평균 50KB이며
Amazon S3에 저장됩니다. 보고서는 생산 후 첫 주 동안 자주 액세스되며 수년간 보관해야
합니다. 보고서는 6시간 이내에 검색 가능해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. S3 표준을 사용합니다. S3 수명 주기 규칙을 사용하여 7일 후에 보고서를 S3 Glacier로
전환합니다.
B. S3 표준을 사용합니다. S3 수명 주기 규칙을 사용하여 7일 후에 보고서를 S3 Standard-
Infrequent Access(S3 Standard-IA)로 전환합니다.
C. S3 지능형 계층화를 사용합니다. 보고서를 S3 Standard-Infrequent Access(S3 Standard-
IA) 및 S3 Glacier로 전환하도록 S3 Intelligent-Tiering을 구성합니다.
D. S3 표준을 사용합니다. S3 수명 주기 규칙을 사용하여 7일 후에 보고서를 S3 Glacier Deep
Archive로 전환합니다.
Answer: A
Explanation:
To store and retrieve reports that are frequently accessed during the first week and must be
stored for several years, S3 Standard and S3 Glacier are suitable solutions. S3 Standard
offers high durability, availability, and performance for frequently accessed data. S3 Glacier
offers secure and durable storage for long-term data archiving at a low cost. S3 Lifecycle
rules can be used to transition the reports from S3 Standard to S3 Glacier after 7 days, which
can reduce storage costs. S3 Glacier also supports retrieval within 6 hours.
References:
333

IT Certification Guaranteed, The Easy Way!
* Storage Classes
* Object Lifecycle Management
* Retrieving Archived Objects from Amazon S3 Glacier
QUESTION NO: 488
회사에는 두 개의 Amazon EC2 인스턴스에서 호스팅되는 동적 웹 애플리케이션이 있습니다.
회사에는 SSL 종료를 수행하기 위해 각 인스턴스에 자체 SSL 인증서가 있습니다.
최근 트래픽이 증가했으며, 운영팀에서는 SSL 암호화 및 복호화로 인해 웹 서버의 컴퓨팅
용량이 최대 한도에 도달했다고 판단했습니다.
애플리케이션 성능을 높이려면 솔루션 아키텍트가 무엇을 해야 합니까?
A. ACM(AWS Certificate Manager)을 사용하여 새 SSL 인증서를 생성하고 각 인스턴스에
ACM 인증서를 설치합니다.
B. Amazon S3 버킷 생성 SSL 인증서를 S3 버킷으로 마이그레이션 SSL 종료를 위해 버킷을
참조하도록 EC2 인스턴스 구성
C. 프록시 서버로 다른 EC2 인스턴스 생성 SSL 인증서를 새 인스턴스로 마이그레이션하고
기존 EC2 인스턴스에 직접 연결하도록 구성
D. SSL 인증서를 AWS Certificate Manager(ACM)로 가져오기 ACM의 SSL 인증서를
사용하는 HTTPS 리스너로 Application Load Balancer를 생성합니다.
Answer: D
Explanation:
https://aws.amazon.com/certificate-manager/:
"With AWS Certificate Manager, you can quickly request a certificate, deploy it on ACM-
integrated AWS resources, such as Elastic Load Balancers, Amazon CloudFront
distributions, and APIs on API Gateway, and let AWS Certificate Manager handle certificate
renewals. It also enables you to create private certificates for your internal resources and
manage the certificate lifecycle centrally."
QUESTION NO: 489
회사는 AWS에 사용자 데이터를 저장합니다. 데이터는 영업 시간 동안 피크 사용량으로
지속적으로 사용됩니다.
액세스 패턴은 다양하며, 일부 데이터는 몇 달 동안 사용되지 않습니다. 솔루션 아키텍트는
높은 가용성을 유지하면서도 최고 수준의 내구성을 유지하는 비용 효율적인 솔루션을
선택해야 합니다.
이러한 요구 사항을 충족하는 스토리지 솔루션은 무엇입니까?
A. Amazon S3 표준
B. Amazon S3 지능형 계층화
C. Amazon S3 Glacier Deep Archive
D. Amazon S3 One Zone-Infrequent Access(S3 One Zone-IA)
Answer: B
Explanation:
Amazon S3 Intelligent-Tiering is the most cost-effective solution for this scenario, providing
both high availability and durability while adjusting automatically to changing access patterns.
It moves data across two access tiers: one optimized for frequent access and another for
infrequent access, based on usage patterns.
This tiering ensures that the company avoids paying for unused storage while also keeping
334

IT Certification Guaranteed, The Easy Way!
frequently accessed data in a more accessible tier.
Key AWS references and benefits of S3 Intelligent-Tiering:
* High Durability and Availability: Amazon S3 offers 99.999999999% durability and 99.9%
availability for objects stored, ensuring data is always protected.
* Automatic Tiering: Data is automatically moved between tiers based on access patterns,
making it ideal for workloads with unpredictable or variable access patterns.
* No Retrieval Fees: Unlike S3 One Zone-IA or Glacier, there are no retrieval fees, making
this more cost-effective in scenarios where access patterns vary over time.
* AWS Documentation: According to the AWS Well-Architected Framework under the Cost
Optimization Pillar, S3 Intelligent-Tiering is recommended for storage when access patterns
change over time, as it minimizes costs while maintaining availability.
QUESTION NO: 490
회사는 온프레미스에서 실행되는 Windows 파일 서버에 5TB 이상의 파일 데이터를 보유하고
있습니다. 사용자와 애플리케이션은 매일 데이터와 상호 작용합니다. 회사는 Windows
워크로드를 AWS로 이동하고 있습니다. 회사가 이 프로세스를 계속함에 따라 회사는 최소
지연 시간으로 AWS 및 온프레미스 파일 스토리지에 액세스할 수 있어야 하며 운영
오버헤드를 최소화하고 기존 파일 액세스 패턴을 크게 변경할 필요가 없는 솔루션이
필요합니다. 회사는 AWS 연결을 위해 AWS Site-to-Site VPN 연결을 사용합니다. 솔루션
설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. AWS에서 Windows 파일 서버용 Amazon FSx를 배포 및 구성합니다. 온-프레미스 파일
데이터를 Windows 파일 서버용 FSx로 이동합니다. AWS에서 Windows 파일 서버용 FSx를
사용하도록 워크로드를 재구성합니다.
B. Amazon S3 파일 게이트웨이 온프레미스 배포 및 구성 온프레미스 파일 데이터를 S3 파일
게이트웨이로 이동 S3 파일 게이트웨이를 사용하도록 온프레미스 워크로드 및 클라우드
워크로드 재구성
C. Amazon S3 파일 게이트웨이 온프레미스 배포 및 구성 온프레미스 파일 데이터를 Amazon
S3로 이동 각 워크로드의 위치에 따라 Amazon S3 직접 또는 S3 파일 게이트웨이를
사용하도록 워크로드 재구성
D. AWS에서 Windows 파일 서버용 Amazon FSx 배포 및 구성 Amazon FSx 파일 게이트웨이
온프레미스 배포 및 구성 온프레미스 파일 데이터를 FSx 파일 게이트웨이로 이동 AWS에서
Windows 파일 서버용 FSx를 사용하도록 클라우드 워크로드 구성 FSx 파일 게이트웨이를
사용하도록 온프레미스 워크로드 구성
Answer: D
Explanation:
https://docs.aws.amazon.com/filegateway/latest/filefsxw/what-is-file-fsxw.html To meet the
requirements of the company to have access to both AWS and on-premises file storage with
minimum latency, a hybrid cloud architecture can be used. One solution is to deploy and
configure Amazon FSx for Windows File Server on AWS, which provides fully managed
Windows file servers. The on-premises file data can be moved to the FSx File Gateway,
which can act as a bridge between on-premises and AWS file storage. The cloud workloads
can be configured to use FSx for Windows File Server on AWS, while the on- premises
workloads can be configured to use the FSx File Gateway. This solution minimizes
operational overhead and requires no significant changes to the existing file access patterns.
The connectivity between on- premises and AWS can be established using an AWS Site-to
335

IT Certification Guaranteed, The Easy Way!
-Site VPN connection.
Reference:
AWS FSx for Windows File Server: https://aws.amazon.com/fsx/windows/
AWS FSx File Gateway: https://aws.amazon.com/fsx/file-gateway/
AWS Site-to-Site VPN: https://aws.amazon.com/vpn/site-to-site-vpn/
QUESTION NO: 491
한 회사에서 가용성이 높은 SFTP 서비스를 운영하고 있습니다. SFTP 서비스는 탄력적 IP
주소로 실행되는 두 개의 Amazon EC2 Linux 인스턴스를 사용하여 인터넷에서 신뢰할 수
있는 IP 소스의 트래픽을 허용합니다. SFTP 서비스는 인스턴스에 연결된 공유 스토리지를
통해 지원됩니다. 사용자 계정은 SFTP 서버에서 Linux 사용자로 생성 및 관리됩니다.
회사는 높은 IOPS 성능과 고도로 구성 가능한 보안을 제공하는 서버리스 옵션을 원합니다.
회사는 또한 사용자 권한에 대한 제어를 유지하기를 원합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 암호화된 Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성합니다. 신뢰할 수 있는
IP 주소만 허용하는 퍼블릭 엔드포인트를 사용하여 AWS Transfer Family SFTP 서비스를
생성합니다. SFTP 서비스 엔드포인트에 EBS 볼륨을 연결합니다. 사용자에게 SFTP 서비스에
대한 액세스 권한을 부여합니다.
B. 암호화된 Amazon Elastic File System(Amazon EFS) 볼륨을 생성합니다. 탄력적 IP 주소와
인터넷 연결 액세스가 가능한 VPC 엔드포인트를 사용하여 AWS Transfer Family SFTP
서비스를 생성합니다. 신뢰할 수 있는 IP 주소만 허용하는 엔드포인트에 보안 그룹을
연결합니다. EFS 볼륨을 SFTP 서비스 엔드포인트에 연결합니다. 사용자에게 SFTP 서비스에
대한 액세스 권한을 부여합니다.
C. 기본 암호화가 활성화된 Amazon S3 버킷을 생성합니다. 신뢰할 수 있는 IP 주소만
허용하는 퍼블릭 엔드포인트를 사용하여 AWS Transfer Family SFTP 서비스를 생성합니다.
S3 버킷을 SFTP 서비스 엔드포인트에 연결합니다. 사용자에게 SFTP 서비스에 대한 액세스
권한을 부여합니다.
D. 기본 암호화가 활성화된 Amazon S3 버킷을 생성합니다. 프라이빗 서브넷에서 내부 액세스
권한이 있는 VPC 엔드포인트를 사용하여 AWS Transfer Family SFTP 서비스를 생성합니다.
신뢰할 수 있는 IP 주소만 허용하는 보안 그룹을 연결합니다. S3 버킷을 SFTP 서비스
엔드포인트에 연결합니다. 사용자에게 SFTP 서비스에 대한 액세스 권한을 부여합니다.
Answer: C
Explanation:
AWS Transfer Family is a secure transfer service that enables you to transfer files into and
out of AWS storage services using SFTP, FTPS, FTP, and AS2 protocols. You can use AWS
Transfer Family to create an SFTP-enabled server with a public endpoint that allows only
trusted IP addresses. You can also attach an Amazon S3 bucket with default encryption
enabled to the SFTP service endpoint, which will provide high IOPS performance and highly
configurable security for your data at rest. You can also maintain control over user
permissions by granting users access to the SFTP service using IAM roles or service-
managed identities.
References: https://docs.aws.amazon.com/transfer/latest/userguide/what-is-aws-transfer-
family.html
https://docs.aws.amazon.com/transfer/latest/userguide/create-server-s3.html
336

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 492
한 회사는 데이터가 Amazon S3 버킷에 저장된 환경을 운영합니다. 객체는 하루 종일 자주
액세스됩니다. 이 회사는 S3 버킷에 저장된 데이터에 대해 엄격한 데이터 암호화 요구 사항을
가지고 있습니다. 이 회사는 현재 암호화를 위해 AWS Key Management Service(AWS
KMS)를 사용합니다.
이 회사는 AWS KMS에 대한 추가 호출 없이 S3 객체 암호화와 관련된 비용을 최적화하려고
합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon S3 관리 키(SSE-S3)를 사용하여 서버 측 암호화를 사용합니다.
B. AWS KMS 키(SSE-KMS)를 사용하여 새 객체의 서버 측 암호화에 S3 버킷 키를
사용합니다.
C. AWS KMS 고객 관리 키로 클라이언트 측 암호화를 사용합니다.
D. AWS KMS에 저장된 고객 제공 키(SSE-C)를 사용하여 서버 측 암호화를 사용합니다.
Answer: B
Explanation:
An S3 Bucket Key reduces the cost of using AWS KMS for server-side encryption by
decreasing the number of requests made to KMS. By enabling S3 Bucket Key, the company
can meet its encryption requirements with KMS keys while optimizing costs by reducing the
number of KMS API requests.
Key AWS features:
* Cost Optimization: S3 Bucket Keys reduce the frequency of KMS calls, optimizing the cost
associated with encryption while still using AWS KMS for key management.
* Compliance with KMS Encryption: This solution continues to meet the strict encryption
requirements of the company by using KMS managed keys.
* AWS Documentation: Using an S3 Bucket Key is recommended for organizations looking to
optimize encryption costs without compromising security.
QUESTION NO: 493
회사에 통합 결제를 사용하는 여러 AWS 계정이 있습니다. 이 회사는 Oracle On-Demand DB
인스턴스용 활성 고성능 Amazon RDS를 90일 동안 여러 개 실행합니다. 회사의 재무팀은
통합 결제 계정 및 기타 모든 AWS 계정에서 AWS Trusted Advisor에 액세스할 수 있습니다.
재무팀은 RDS에 대한 Trusted Advisor 확인 권장 사항에 액세스하려면 적절한 AWS 계정을
사용해야 합니다. 재무팀은 RDS 비용을 줄이기 위해 적절한 Trusted Advisor 점검을 검토해야
합니다.
이러한 요구 사항을 충족하기 위해 재무팀은 어떤 단계 조합을 취해야 합니까? (2개를
선택하세요.)
A. RDS 인스턴스가 실행 중인 계정의 Trusted Advisor 권장 사항을 사용합니다.
B. 통합 결제 계정의 Trusted Advisor 권장 사항을 사용하여 모든 RDS 인스턴스 확인을
동시에 확인하세요.
C. Amazon RDS 예약 인스턴스 최적화에 대한 Trusted Advisor 점검을 검토합니다.
D. Amazon RDS 유휴 DB 인스턴스에 대한 Trusted Advisor 점검을 검토합니다.
E. Amazon Redshift 예약 노드 최적화에 대한 Trusted Advisor 점검을 검토합니다.
Answer: B C
Explanation:
337

IT Certification Guaranteed, The Easy Way!
B: Use the Trusted Advisor recommendations from the consolidated billing account to see all
RDS instance checks at the same time.
The consolidated billing account has access to all the other AWS accounts that use
consolidated billing. Using the Trusted Advisor recommendations from the consolidated
billing account will allow the finance team to see all RDS instance checks for all accounts at
the same time.
C: Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.
The Trusted Advisor check for Amazon RDS Reserved Instance Optimization provides
recommendations for purchasing reserved instances to reduce RDS costs. By reviewing this
check, the finance team can identify which RDS instances can be converted to reserved
instances to save costs.
QUESTION NO: 494
솔루션 설계자는 회사의 응용 프로그램에 새로운 Microsoft를 할당해야 합니다. 클라이언트는
micoservice에 도달하기 위해 HTTPS 엔드포인트를 호출할 수 있어야 합니다. 또한
마이크로서비스는 인증 호출에 AWS ID 및 액세스 관리(IAM)를 사용해야 합니다. 솔루션
설계자는 Go 1.x로 작성된 단일 AWS Lambda 함수를 사용하여 이 마이크로서비스에 대한
논리를 작성합니다.
가장 운영상 효율적인 방식으로 기능을 배포하는 솔루션은 무엇입니까?
A. Amazon API Gateway REST API를 생성합니다. Lambda 함수를 사용하도록 메서드를
구성합니다. API에서 IAM 인증을 활성화합니다.
B. 함수에 대한 Lambda 함수 URL을 생성합니다. 인증 유형으로 AWS_IAM을 지정합니다.
C. Amazon CloudFront 배포를 생성합니다. Lambda@Edge에 함수를 배포합니다. IAM 인증
로직을 Lambda@Edge 함수에 통합합니다.
D. Amazon CloudFront 배포판을 생성합니다. CloudFront Functions에 함수를 배포합니다.
인증 유형으로 AWS_IAM을 지정합니다.
Answer: A
Explanation:
A: Create an Amazon API Gateway REST API. Configure the method to use the Lambda
function. Enable IAM authentication on the API. This option is the most operationally efficient
as it allows you to use API Gateway to handle the HTTPS endpoint and also allows you to
use IAM to authenticate the calls to the microservice. API Gateway also provides many
additional features such as caching, throttling, and monitoring, which can be useful for a
microservice.
QUESTION NO: 495
어떤 회사가 웹 애플리케이션의 콘텐츠에 대한 액세스를 제한하려고 합니다. 이 회사는
AWS에서 사용 가능한 권한 부여 기술을 사용하여 콘텐츠를 보호해야 합니다. 이 회사는 또한
로그인 지연 시간이 짧은 권한 부여 및 인증을 위한 서버리스 아키텍처를 구현하려고 합니다.
솔루션은 웹 애플리케이션과 통합되어야 하며 웹 콘텐츠를 전 세계적으로 제공해야 합니다.
애플리케이션은 현재 사용자 기반이 작지만, 회사는 애플리케이션의 사용자 기반이 증가할
것으로 예상합니다. 어떤 솔루션이 이러한 요구 사항을 충족할까요?
A. 인증을 위해 Amazon Cognito를 구성합니다. 권한 부여를 위해 Lambda@Edge를
구현합니다. 웹 애플리케이션을 전 세계적으로 제공하도록 Amazon CloudFront를
구성합니다.
338

IT Certification Guaranteed, The Easy Way!
B. 인증을 위해 Microsoft Active Directory에 대한 AWS Directory Service를 구성합니다. 권한
부여를 위해 AWS Lambda를 구현합니다. Application Load Balancer를 사용하여 웹
애플리케이션을 전역적으로 제공합니다.
C. 인증을 위해 Amazon Cognito를 구성합니다. 권한 부여를 위해 AWS Lambda를
구현합니다. Amazon S3 Transfer Acceleration을 사용하여 웹 애플리케이션을 전 세계적으로
제공합니다.
D. 인증을 위해 Microsoft Active Directory에 대한 AWS Directory Service를 구성합니다. 권한
부여를 위해 Lambda@Edge를 구현합니다. AWS Elastic Beanstalk를 사용하여 웹
애플리케이션을 전 세계적으로 제공합니다.
Answer: A
Explanation:
Amazon Cognito provides scalable, serverless authentication, and Lambda@Edge is used
for authorization, providing low-latency access control at the edge. Amazon CloudFront
serves the web application globally with reduced latency and ensures secure access for
users around the world. This solution minimizes operational overhead while providing
scalability and security.
* Option B (Directory Service): Directory Service is more suitable for enterprise use cases
involving Active Directory, not for web-based applications.
* Option C (S3 Transfer Acceleration): S3 Transfer Acceleration helps with file transfers but
does not provide authorization features.
* Option D (Elastic Beanstalk): Elastic Beanstalk adds unnecessary overhead when
CloudFront can handle global delivery efficiently.
AWS References:
* Amazon Cognito
* Lambda@Edge
QUESTION NO: 496
한 회사는 PostgreSQL DB 인스턴스용 Amazon RDS를 사용하여 여러 웹 서버를 실행하고
있습니다. 정기적인 규정 준수 확인 후 회사는 모든 프로덕션 데이터베이스에 대해 1초 미만의
RPO(복구 목표)를 요구하는 표준을 설정했습니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. DB 인스턴스에 대한 다중 AZ 배포 활성화
B. 하나의 가용 영역에서 OB 인스턴스에 대한 Auto Scaling을 활성화합니다.
C. 하나의 가용 영역에 06 인스턴스를 구성하고 별도의 가용 영역에 여러 읽기 전용 복제본을
생성합니다.
D. 하나의 가용 영역에 06 인스턴스를 구성하고 AWS Database Migration Service(AWS
DMS) 변경 데이터 캡처(CDC) 작업을 구성합니다.
Answer: A
Explanation:
This option is the most efficient because it uses a Multi-AZ deployment for the DB instance,
which provides enhanced availability and durability for RDS database instances by
automatically replicating the data to a standby instance in a different Availability Zone1. It
also provides a recovery point objective (RPO) of less than 1 second for all its production
databases, as the standby instance is kept in sync with the primary instance using
synchronous physical replication2. This solution meets the requirement of requiring a RPO of
339

IT Certification Guaranteed, The Easy Way!
less than
1 second for all its production databases. Option B is less efficient because it uses auto
scaling for the DB instance in one Availability Zone, which is a way to automatically adjust the
compute capacity of your DB instance based on load or a schedule3. However, this does not
provide a RPO of less than 1 second for all its production databases, as it does not replicate
the data to another Availability Zone. Option C is less efficient because it uses read replicas
in a separate Availability Zone, which are read-only copies of your primary database that can
serve read traffic and support scaling. However, this does not provide a RPO of less than 1
second for all its production databases, as read replicas use asynchronous replication and
can lag behind the primary database. Option D is less efficient because it uses AWS
Database Migration Service (AWS DMS) change data capture (CDC) tasks, which are tasks
that capture changes made to source data and apply them to target data. However, this does
not provide a RPO of less than 1 second for all its production databases, as AWS DMS uses
asynchronous replication and can lag behind the source database.
QUESTION NO: 497
한 회사의 전자상거래 웹사이트에 예측할 수 없는 트래픽이 있으며 AWS Lambda 기능을
사용하여 PostgreSQL용 프라이빗 Amazon RDS DB 인스턴스에 직접 액세스합니다. 회사는
예측 가능한 데이터베이스 성능을 유지하고 Lambda 호출이 너무 많은 연결로 인해
데이터베이스에 과부하가 걸리지 않도록 하려고 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. RDS 사용자 지정 엔드포인트에서 클라이언트 드라이버를 가리킵니다. VPC 내부에
Lambda 함수를 배포합니다.
B. RDS 프록시 엔드포인트에서 클라이언트 드라이버를 가리킵니다. VPC 내부에 Lambda
함수를 배포합니다.
C. RDS 사용자 지정 엔드포인트에서 클라이언트 드라이버를 가리킵니다. VPC 외부에
Lambda 함수를 배포합니다.
D. RDS 프록시 엔드포인트에서 클라이언트 드라이버를 가리킵니다. VPC 외부에 Lambda
함수를 배포합니다.
Answer: B
Explanation:
To maintain predictable database performance and ensure that the Lambda invocations do
not overload the database with too many connections, a solutions architect should point the
client driver at an RDS proxy endpoint and deploy the Lambda functions inside a VPC. An
RDS proxy is a fully managed database proxy that allows applications to share connections
to a database, improving database availability and scalability. By using an RDS proxy, the
Lambda functions can reuse existing connections, rather than creating new ones for every
invocation, reducing the connection overhead and latency. Deploying the Lambda functions
inside a VPC allows them to access the private RDS DB instance securely and efficiently,
without exposing it to the public internet. References:
* Using Amazon RDS Proxy with AWS Lambda
* Configuring a Lambda function to access resources in a VPC
QUESTION NO: 498
AWS에서 애플리케이션을 호스팅하는 회사 Lambda 함수 매트는 Amazon API Gateway
340

IT Certification Guaranteed, The Easy Way!
API에 의해 호출됨 Lambda 함수는 고객 데이터를 Amazon Aurora MySQL 데이터베이스에
저장 회사가 데이터베이스를 업그레이드할 때마다 Lambda 함수는 업그레이드가 완료될
때까지 데이터베이스 연결을 설정하지 못합니다 그 결과 일부 이벤트에 대해 고객 데이터가
기록되지 않습니다. 솔루션 설계자는 데이터베이스 업그레이드 중에 생성되는 고객 데이터를
저장하는 솔루션을 설계해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Lambda 함수와 데이터베이스 사이에 위치하도록 Amazon RDS 프록시 프로비저닝 RDS
프록시에 연결하도록 Lambda 함수 구성
B. 나 Lambda 함수의 실행 시간을 최대로 늘림 고객 데이터를 데이터베이스에 저장하는
코드에 재시도 메커니즘 생성
C. 고객 데이터를 Lambda 로컬 스토리지에 유지합니다. 고객 데이터를 데이터베이스에
저장하기 위해 로컬 스토리지를 스캔하도록 새로운 Lambda 함수를 구성합니다.
D. Amazon Simple Queue Service(Amazon SOS) FIFO 대기열에 고객 데이터 저장 대기열을
폴링하고 고객 데이터를 데이터베이스에 저장하는 새 Lambda 함수 생성
Answer: D
Explanation:
https://www.learnaws.org/2020/12/13/aws-rds-proxy-deep-dive/
RDS proxy can improve application availability in such a situation by waiting for the new
database instance to be functional and maintaining any requests received from the
application during this time. The end result is that the application is more resilient to issues
with the underlying database.
This will enable solution to hold data till the time DB comes back to normal. RDS proxy is to
optimally utilize the connection between Lambda and DB. Lambda can open multiple
connection concurrently which can be taxing on DB compute resources, hence RDS proxy
was introduced to manage and leverage these connections efficiently.
QUESTION NO: 499
회사는 AWS Organizations의 조직을 사용하여 애플리케이션이 포함된 AWS 계정을
관리합니다. 회사는 조직 내에 전용 모니터링 회원 계정을 설정합니다. 회사는 Amazon
CloudWatch를 사용하여 계정 전체의 관측 가능성 데이터를 쿼리하고 시각화하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 모니터링 계정에 대해 CloudWatch 교차 계정 관찰 기능을 활성화합니다. 모니터링
계정에서 제공하는 AWS CloudFormation 템플릿을 각 AWS 계정에 배포하여 모니터링
계정과 데이터를 공유합니다.
B. 조직 루트 조직 단위(OU) 아래의 모니터링 계정에서 CloudWatch에 대한 액세스를
제공하도록 서비스 제어 정책(SCP)을 설정합니다.
C. 모니터링 계정에 새 IAM 사용자를 구성합니다. 각 AWS 계정에서 계정의 CloudWatch
데이터를 쿼리하고 시각화할 수 있도록 오전 1시 정책을 구성합니다. 새로운 1AM 정책을
새로운 1AM 사용자에게 연결합니다.
D. 모니터링 계정에 새 IAM 사용자를 생성합니다. 각 AWS 계정에서 교차 계정 1AM 정책을
생성합니다. 새 IAM 사용자에게 오전 1시 정책을 연결합니다.
Answer: A
Explanation:
This solution meets the requirements because it allows the monitoring account to query and
visualize observability data across the accounts by using CloudWatch. CloudWatch cross-
341

IT Certification Guaranteed, The Easy Way!
account observability is a feature that enables a central monitoring account to view and
interact with observability data shared by other accounts. To enable cross-account
observability, the monitoring account needs to configure the types of data to be shared
(metrics, logs, and traces) and the source accounts to be linked. The source accounts can be
specified by account IDs, organization IDs, or organization paths. To share the data with the
monitoring account, the source accounts need to deploy an AWS CloudFormation template
provided by the monitoring account. This template creates an observability link resource that
represents the link between the source account and the monitoring account. The template
also creates a sink resource that represents an attachment point in the monitoring account.
The source accounts can share their observability data with the sink in the monitoring
account. The monitoring account can then use the CloudWatch console, API, or CLI to
search, analyze, and correlate the observability data across the accounts. References:
CloudWatch cross-account observability, Setting up CloudWatch cross-account observability,
[Observability Access Manager API Reference]
QUESTION NO: 500
한 회사가 온프레미스 PostgreSQL 데이터베이스를 Amazon Aurora PostgreSQL로
마이그레이션하고 있습니다. 온프레미스 데이터베이스는 마이그레이션 중에 온라인 상태를
유지하고 액세스 가능해야 합니다. Aurora 데이터베이스는 온프레미스 데이터베이스와
동기화 상태를 유지해야 합니다.
솔루션 아키텍트는 이러한 요구 사항을 충족하기 위해 어떤 조치 조합을 취해야 합니까? (두
가지를 선택하세요.)
A. AWS Schema Conversion Tool(AWS SCT)을 사용하여 데이터베이스 스키마를
변환합니다.
B. 데이터베이스 동기화를 모니터링하기 위한 Amazon EventBridge(Amazon CloudWatch
Events) 규칙을 생성합니다.
C. 온프레미스 데이터베이스의 데이터베이스 백업을 만듭니다.
D. AWS Database Migration Service(AWS DMS) 복제 서버 생성
E. 지속적인 복제 작업을 생성합니다.
Answer: D,E
Explanation:
AWS Database Migration Service supports homogeneous migrations such as Oracle to
Oracle, as well as heterogeneous migrations between different database platforms, such as
Oracle or Microsoft SQL Server to Amazon Aurora. With AWS Database Migration Service,
you can also continuously replicate data with low latency from any supported source to any
supported target. For example, you can replicate from multiple sources to Amazon Simple
Storage Service (Amazon S3) to build a highly available and scalable data lake solution. You
can also consolidate databases into a petabyte-scale data warehouse by streaming data to
Amazon Redshift. Learn more about the supported source and target databases.
https://aws.amazon.com/dms/
QUESTION NO: 501
한 회사가 AWS DataSync를 사용하여 온프레미스 시스템에서 AWS로 수백만 개의 파일을
마이그레이션하고 있습니다. 파일 크기는 평균 10KB입니다.
이 회사는 파일 저장을 위해 Amazon S3를 사용하려고 합니다. 마이그레이션 후 첫 1년
342

IT Certification Guaranteed, The Easy Way!
동안은 파일에 한두 번 액세스해야 하며 즉시 사용할 수 있어야 합니다. 1년 후에는 파일을
최소한 보관해야 합니다.
7년.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. 보관 도구를 사용하여 파일을 큰 객체로 그룹화합니다. DataSync를 사용하여 객체를
마이그레이션합니다. 첫해 동안 S3 Glacier Instant Retrieval에 객체를 저장합니다.
라이프사이클 구성을 사용하여 1년 후 7년의 보존 기간으로 파일을 S3 Glacier Deep
Archive로 전환합니다.
B. 보관 도구를 사용하여 파일을 큰 개체로 그룹화합니다. DataSync를 사용하여 개체를 S3
Standard-Infrequent Access(S3 Standard-IA)로 복사합니다. 라이프사이클 구성을 사용하여
1년 후 7년의 보존 기간으로 파일을 S3 Glacier Instant Retrieval로 전환합니다.
C. 파일의 대상 스토리지 클래스를 S3 Glacier Instant로 구성합니다. 검색 수명 주기 정책을
사용하여 1년 후 7년의 보존 기간으로 파일을 S3 Glacier Flexible Retrieval로 전환합니다.
D. DataSync 작업을 구성하여 파일을 S3 Standard-Infrequent Access(S3 Standard-IA)로
전송합니다. 라이프사이클 구성을 사용하여 파일을 S3로 전환합니다. 1년 후 7년의 보존
기간으로 Deep Archive합니다.
Answer: A
QUESTION NO: 502
법률 회사는 대중과 정보를 공유해야 합니다. 정보에는 공개적으로 읽을 수 있어야 하는 수백
개의 파일이 포함되어 있습니다. 지정된 미래 날짜 이전에 누구든지 파일을 수정하거나
삭제하는 것은 금지되어 있습니다.
가장 안전한 방식으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 정적 웹 사이트 호스팅용으로 구성된 Amazon S3 버킷에 모든 파일을 업로드합니다.
지정된 날짜까지 S3 버킷에 액세스하는 모든 AWS 보안 주체에게 읽기 전용 오전 1시 권한을
부여합니다.
B. S3 버전 관리가 활성화된 새 Amazon S3 버킷을 생성합니다. 지정된 날짜에 따라 보존
기간이 있는 S3 객체 잠금을 사용합니다. 정적 웹 사이트 호스팅을 위한 S3 버킷을
구성합니다. objrcts에 대한 읽기 전용 액세스를 허용하도록 S3 버킷 정책을 설정합니다.
C. S3 버전 관리가 활성화된 새 Amazon S3 버킷을 생성합니다. 객체 수정 또는 삭제 시 AWS
Lambda 함수를 실행하도록 이벤트 트리거를 구성합니다. 객체를 프라이빗 S3 버킷의 원래
버전으로 바꾸도록 Lambda 함수를 구성합니다.
D. 정적 웹 사이트 호스팅용으로 구성된 Amazon S3 버킷에 모든 파일을 업로드합니다.
파일이 포함된 폴더를 선택합니다. 지정된 날짜에 따라 보존 기간이 있는 S3 객체 잠금을
사용하세요. S3 버킷에 액세스하는 모든 AWS 보안 주체에게 읽기 전용 오전 1시 권한을
부여합니다.
Answer: B
Explanation:
Amazon S3 is a service that provides object storage in the cloud. It can be used to store and
serve static web content, such as HTML, CSS, JavaScript, images, and videos1. By creating
a new Amazon S3 bucket and configuring it for static website hosting, the solution can share
information with the public.
Amazon S3 Versioning is a feature that keeps multiple versions of an object in the same
bucket. It helps protect objects from accidental deletion or overwriting by preserving,
343

IT Certification Guaranteed, The Easy Way!
retrieving, and restoring every version of every object stored in an S3 bucket2. By enabling
S3 Versioning on the new bucket, the solution can prevent modifications or deletions of the
files by anyone.
Amazon S3 Object Lock is a feature that allows users to store objects using a write-once-
read-many (WORM) model. It can help prevent objects from being deleted or overwritten for
a fixed amount of time or indefinitely. It requires S3 Versioning to be enabled on the bucket3.
By using S3 Object Lock with a retention period in accordance with the designated date, the
solution can prohibit modifications or deletions of the files by anyone before that date.
Amazon S3 bucket policies are JSON documents that define access permissions for a bucket
and its objects.
They can be used to grant or deny access to specific users or groups based on conditions
such as IP address, time of day, or source bucket. By setting an S3 bucket policy to allow
read-only access to the objects, the solution can ensure that the files are publicly readable.
A: Upload all files to an Amazon S3 bucket that is configured for static website hosting. Grant
read-only 1AM permissions to any AWS principals that access the S3 bucket until the
designated date. This solution will not meet the requirement of prohibiting modifications or
deletions of the files by anyone before a designated future date, as IAM permissions only
apply to AWS principals, not to public users. It also does not use any feature to prevent
accidental or intentional deletion or overwriting of the files.
C: Create a new Amazon S3 bucket with S3 Versioning enabled Configure an event trigger to
run an AWS Lambda function in case of object modification or deletion. Configure the
Lambda func-tion to replace the objects with the original versions from a private S3 bucket.
This solution will not meet the requirement of prohibiting modifications or deletions of the files
by anyone before a designated future date, as it only reacts to object modification or deletion
events after they occur. It also involves creating and managing an additional resource
(Lambda function) and a private S3 bucket.
D: Upload all files to an Amazon S3 bucket that is configured for static website hosting.
Select the folder that contains the files. Use S3 Object Lock with a retention period in
accordance with the designated date. Grant read-only 1AM permissions to any AWS
principals that access the S3 bucket. This solution will not meet the requirement of prohibiting
modifications or deletions of the files by anyone before a designated future date, as it does
not enable S3 Versioning on the bucket, which is required for using S3 Object Lock. It also
does not allow read-only access to public users.
Reference URL:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html
QUESTION NO: 503
전자상거래 회사가 온프레미스 워크로드를 AWS 클라우드로 마이그레이션하고 있습니다.
워크로드는 현재 웹 애플리케이션과 스토리지를 위한 백엔드 Microsoft SQL 데이터베이스로
구성되어 있습니다.
이 회사는 프로모션 이벤트 동안 많은 고객을 기대합니다. AWS 클라우드의 새로운 인프라는
가용성이 높고 확장 가능해야 합니다.
이러한 요구 사항을 가장 적은 관리 비용으로 충족할 수 있는 솔루션은 무엇일까요?
A. 웹 애플리케이션을 애플리케이션 로드 밸런서 뒤의 두 가용성 영역에 걸쳐 두 개의 Amazon
EC2 인스턴스로 마이그레이션합니다. 데이터베이스를 두 가용성 영역에 읽기 복제본이 있는
344

IT Certification Guaranteed, The Easy Way!
Microsoft SQL Server용 Amazon RDS로 마이그레이션합니다.
B. 웹 애플리케이션을 애플리케이션 로드 밸런서 뒤의 두 가용성 영역에 걸쳐 자동 확장
그룹에서 실행되는 Amazon EC2 인스턴스로 마이그레이션합니다. 데이터베이스 복제를
사용하여 별도의 AWS 지역에 걸쳐 두 개의 EC2 인스턴스로 데이터베이스를
마이그레이션합니다.
C. 웹 애플리케이션을 애플리케이션 로드 밸런서 뒤의 두 가용성 영역에 걸쳐 자동 확장
그룹에서 실행되는 Amazon EC2 인스턴스로 마이그레이션합니다. 다중 AZ 배포를 사용하여
데이터베이스를 Amazon RDS로 마이그레이션합니다.
D. 웹 애플리케이션을 Application Load Balancer 뒤의 3개 가용성 영역에 걸쳐 3개의 Amazon
EC2 인스턴스로 마이그레이션합니다. 데이터베이스를 3개 가용성 영역에 걸쳐 3개의 EC2
인스턴스로 마이그레이션합니다.
Answer: C
Explanation:
To ensure high availability and scalability, the web application should run in an Auto Scaling
group across two Availability Zones behind an Application Load Balancer (ALB). The
database should be migrated to Amazon RDS with Multi-AZ deployment, which ensures fault
tolerance and automatic failover in case of an AZ failure. This setup minimizes administrative
overhead while meeting the company's requirements for high availability and scalability.
* Option A: Read replicas are typically used for scaling read operations, and Multi-AZ
provides better availability for a transactional database.
* Option B: Replicating across AWS Regions adds unnecessary complexity for a single web
application.
* Option D: EC2 instances across three Availability Zones add unnecessary complexity for
this scenario.
AWS References:
* Auto Scaling Groups
* Amazon RDS Multi-AZ
QUESTION NO: 504
회사는 퍼블릭 및 프라이빗 서브넷이 있는 VPC에서 애플리케이션을 실행합니다. VPC는 여러
가용 영역으로 확장됩니다. 애플리케이션은 프라이빗 서브넷의 Amazon EC2 인스턴스에서
실행됩니다. 애플리케이션은 Amazon Simple Queue Service(Amazon SOS) 대기열을
사용합니다.
솔루션 아키텍트는 EC2 인스턴스와 SOS 대기열 간의 연결을 설정하기 위해 보안 솔루션을
설계해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon SOS에 대한 인터페이스 VPC 엔드포인트를 구현합니다. 프라이빗 서브넷을
사용하도록 엔드포인트를 구성합니다. 프라이빗 서브넷에 있는 EC2 인스턴스의 트래픽을
허용하는 인바운드 액세스 규칙이 있는 보안 그룹을 엔드포인트에 추가합니다.
B. Amazon SOS에 대한 인터페이스 VPC 엔드포인트를 구현합니다. 퍼블릭 서브넷을
사용하도록 엔드포인트를 구성합니다. 프라이빗 서브넷에 있는 EC2 인스턴스로부터의
액세스를 허용하는 VPC 엔드포인트 정책을 인터페이스 엔드포인트에 연결합니다.
C. Ama7on SOS에 대한 인터페이스 VPC 엔드포인트를 구현합니다. 퍼블릭 서브넷을
사용하도록 엔드포인트를 구성합니다. 지정된 VPC 엔드포인트의 요청만 허용하는
인터페이스 VPC 엔드포인트에 Amazon SOS 액세스 정책을 연결합니다.
D. Amazon SOS에 대한 게이트웨이 엔드포인트를 구현합니다. 프라이빗 서브넷에 NAT
345

IT Certification Guaranteed, The Easy Way!
게이트웨이를 추가합니다. 첨부
SOS 대기열에 대한 액세스를 허용하는 EC2 인스턴스에 대한 1AM 역할.
Answer: A
Explanation:
* Understanding the Requirement: The application running on EC2 instances in private
subnets needs to securely connect to an Amazon SQS queue without exposing traffic to the
public internet.
* Analysis of Options:
* Interface VPC Endpoint in Private Subnets: Allows private, secure connectivity to SQS
without using the public internet. Configuring security groups ensures controlled access from
EC2 instances.
* Interface VPC Endpoint in Public Subnets: Not necessary for private EC2 instances and
exposes additional security risks.
* Gateway Endpoint: Gateway endpoints are not supported for SQS; they are used for
services like S3 and DynamoDB.
* NAT Gateway with IAM Role: Increases costs and complexity compared to using an
interface VPC endpoint directly.
* Best Solution:
* Interface VPC Endpoint in Private Subnets: This option ensures secure, private connectivity
to SQS, meeting the requirement with minimal complexity and optimal security.
References:
* VPC Endpoints
* Amazon SQS and VPC Endpoints
QUESTION NO: 505
한 회사가 새로운 가구 재고 애플리케이션을 구축하고 있습니다. 이 회사는 여러 가용성
영역에 걸쳐 Amazon EC2 인스턴스 플릿에 애플리케이션을 배포했습니다. EC2 인스턴스는
VPC의 애플리케이션 로드 밸런서(ALB) 뒤에서 실행됩니다.
솔루션 아키텍트는 수신 트래픽이 하나의 EC2 인스턴스를 선호하는 것으로 나타나 일부
요청에 지연이 발생하는 것을 발견했습니다.
이 문제를 해결하기 위해 솔루션 아키텍트는 무엇을 해야 할까요?
A. ALB에서 세션 친화성(스티키 세션)을 비활성화합니다.
B. ALB를 네트워크 로드 밸런서로 교체합니다.
C. 각 가용성 영역의 EC2 인스턴스 수를 늘립니다.
D. ALB 대상 그룹의 상태 점검 빈도를 조정합니다.
Answer: A
Explanation:
The issue described in the question, where incoming traffic seems to favor one EC2 instance,
is often caused by session affinity (also known as sticky sessions) being enabled on the
Application Load Balancer (ALB).
When session affinity is enabled, the ALB routes requests from the same client to the same
EC2 instance.
This can cause an imbalance in traffic distribution, leading to performance bottlenecks on
certain instances while others remain underutilized.
To resolve this issue, disabling session affinity ensures that the ALB distributes incoming
346

IT Certification Guaranteed, The Easy Way!
traffic evenly across all EC2 instances, allowing better load distribution and reducing latency.
The ALB will rely on its round- robin or least outstanding requests algorithm (depending on
the configuration) to distribute traffic more evenly across instances.
* Option B (Network Load Balancer): The NLB is designed for Layer 4 (TCP) traffic and low
latency use cases, but it is not needed here as the problem is with load balancing logic at the
application layer (Layer 7). The ALB is more appropriate for HTTP/HTTPS traffic.
* Option C (Increase EC2 Instances): Adding more EC2 instances does not solve the root
issue of uneven traffic distribution.
* Option D (Health Check Frequency): Adjusting health check frequency won't address the
imbalance caused by session affinity.
AWS References:
* Application Load Balancer Sticky Sessions
QUESTION NO: 506
회사는 Amazon S3에 회계 기록을 저장해야 합니다. 기록은 1년 동안 즉시 액세스할 수
있어야 하며 그 후 추가로 9년 동안 보관해야 합니다. 관리자 및 루트 사용자를 포함하여
회사의 그 누구도 전체 10년 동안 기록을 삭제할 수 없습니다. 기록은 최대한의 복원력으로
저장해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 전체 10년 동안 S3 Glacier에 기록을 저장합니다. 접근통제 정책을 사용하여 10년 동안
기록 삭제를 거부합니다.
B. S3 Intelligent-Tiering을 이용하여 기록을 저장합니다. IAM 정책을 사용하여 레코드 삭제를
거부합니다.
10년 후 삭제를 허용하도록 IAM 정책을 변경합니다.
C. S3 수명 주기 정책을 사용하여 기록을 S3 Standard에서 S3 Glacier Deep Archive로
전환합니다.
일년. 10년 동안 규정 준수 모드에서 S3 Object Lock을 사용합니다.
D. S3 수명 주기 정책을 사용하여 1년 후 레코드를 S3 Standard에서 S3 One Zone-Infrequent
Access(S3 One Zone-IA)로 전환합니다. 10년 동안 거버넌스 모드에서 S3 Object Lock을
사용합니다.
Answer: C
Explanation:
To meet the requirements of immediately accessible records for 1 year and then archived for
an additional 9 years with maximum resiliency, we can use S3 Lifecycle policy to transition
records from S3 Standard to S3 Glacier Deep Archive after 1 year. And to ensure that the
records cannot be deleted by anyone, including administrative and root users, we can use S3
Object Lock in compliance mode for a period of 10 years.
Therefore, the correct answer is option C.
Reference: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html
QUESTION NO: 507
솔루션 설계자가 다가오는 음악 이벤트를 위해 웹사이트를 최적화하고 있습니다. 공연 영상은
실시간으로 스트리밍되며, 요청 시 시청 가능합니다. 이번 행사는 전 세계 온라인 관객의
관심을 끌 것으로 예상된다.
실시간 스트리밍과 주문형 스트리밍 모두의 성능을 향상시키는 서비스는 무엇입니까?
347

IT Certification Guaranteed, The Easy Way!
A. Amazon CloudFront
B. AWS 글로벌 액셀러레이터
C. 아마존 루트 53
D. Amazon S3 전송 가속화
Answer: A
Explanation:
You can use CloudFront to deliver video on demand (VOD) or live streaming video using any
HTTP origin.
One way you can set up video workflows in the cloud is by using CloudFront together with
AWS Media Services.
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/on-demand-
streaming- video.html
QUESTION NO: 508
한 회사가 애플리케이션을 Amazon EC2 Linux 인스턴스로 마이그레이션했습니다. 이러한
EC2 인스턴스 중 하나는 일정에 따라 여러 개의 1시간 작업을 실행합니다. 이러한 작업은
서로 다른 팀에서 작성되었으며 공통 프로그래밍 언어가 없습니다. 회사는 이러한 작업이
단일 인스턴스에서 실행되는 동안 성능과 확장성에 대해 우려하고 있습니다. 솔루션 설계자는
이러한 문제를 해결하기 위한 솔루션을 구현해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Batch를 사용하여 작업을 작업으로 실행합니다. Amazon EventBridge(Amazon
CloudWatch Events)를 사용하여 작업을 예약합니다.
B. EC2 인스턴스를 컨테이너로 변환합니다. AWS App Runner를 사용하여 요청 시
컨테이너를 생성하여 작업을 작업으로 실행할 수 있습니다.
C. 작업을 AWS Lambda 함수에 복사합니다. Amazon EventBridge(Amazon CloudWatch
Events)를 사용하여 Lambda 함수를 예약합니다.
D. 작업을 실행하는 EC2 인스턴스의 Amazon 머신 이미지(AMI)를 생성합니다. AMI를
사용하여 Auto Scaling 그룹을 생성하여 인스턴스의 여러 복사본을 실행합니다.
Answer: A
Explanation:
AWS Batch is a fully managed service that enables users to run batch jobs on AWS. It can
handle different types of tasks written in different languages and run them on EC2 instances.
It also integrates with Amazon EventBridge (Amazon CloudWatch Events) to schedule jobs
based on time or event triggers. This solution will meet the requirements of performance,
scalability and low operational overhead12.
B: Convert the EC2 instance to a container. Use AWS App Runner to create the container on
demand to run the tasks as jobs. This solution will not meet the requirement of low
operational overhead, as it involves converting the EC2 instance to a container and using
AWS App Runner, which is a service that automatically builds and deploys web applications
and load balances traffic2. This is not necessary for running batch jobs.
C: Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using
Amazon EventBridge (Amazon CloudWatch Events). This solution will not meet the
requirement of performance, as AWS Lambda has a limit of 15 minutes for execution time
and 10 GB for memory allocation3. These limits may not be sufficient for running 1-hour
348

IT Certification Guaranteed, The Easy Way!
tasks.
D: Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create
an Auto Scaling group with the AMI to run multiple copies of the instance. This solution will
not meet the requirement of low operational overhead, as it involves creating and maintaining
AMIs and Auto Scaling groups, which are additional resources that need to be configured
and managed2.
Reference URL: https://docs.aws.amazon.com/whitepapers/latest/aws-overview/compute-
services.html
QUESTION NO: 509
한 회사는 수백 대의 온프레미스 가상 머신(VM)을 Amazon EC2 인스턴스로
마이그레이션했습니다. 인스턴스는 여러 Linux 배포판과 함께 다양한 Windows Server 버전을
실행합니다.
회사는 운영 체제의 인벤토리 및 업데이트를 자동화하는 솔루션을 원합니다. 또한 회사는
정기적인 월별 검토를 위해 각 인스턴스의 일반적인 취약점에 대한 요약을 필요로 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 권장해야 합니까?
A. 모든 EC2 인스턴스를 관리하도록 AWS Systems Manager Patch Manager를 설정합니다.
월별 보고서를 생성하도록 AWS Security Hub를 구성합니다.
B. 모든 EC2 인스턴스를 관리하도록 AWS Systems Manager Patch Manager를 설정합니다.
Amazon Inspector를 배포하고 월별 보고서를 구성합니다.
C. AWS Shield Advanced를 설정하고 월별 보고서를 구성합니다. AWS Config를 배포하여
EC2 인스턴스에서 패치 설치를 자동화합니다.
D. 모든 EC2 인스턴스를 모니터링하도록 계정에 Amazon GuardDuty를 설정합니다. AWS
Config를 배포하여 EC2 인스턴스에서 패치 설치를 자동화합니다.
Answer: B
Explanation:
* Understanding the Requirement: The company needs to automate inventory and updates of
diverse OS versions on EC2 instances and summarize common vulnerabilities for monthly
reviews.
* Analysis of Options:
* Systems Manager Patch Manager and Security Hub: Patch Manager automates patching,
but Security Hub is more focused on compliance and security posture rather than inventory
and vulnerability management.
* Systems Manager Patch Manager and Amazon Inspector: Patch Manager automates OS
updates, and Amazon Inspector provides vulnerability assessments, making this a
comprehensive solution for the requirements.
* AWS Shield Advanced and AWS Config: Shield Advanced is for DDoS protection, not
suitable for OS patch management and vulnerability reporting.
* Amazon GuardDuty and AWS Config: GuardDuty is for threat detection and monitoring, not
specifically for patch management and vulnerability assessments.
* Best Solution:
* Systems Manager Patch Manager and Amazon Inspector: This combination automates OS
updates and provides detailed vulnerability assessments, meeting both the inventory and
security reporting needs effectively.
References:
349

IT Certification Guaranteed, The Easy Way!
* AWS Systems Manager Patch Manager
* Amazon Inspector
QUESTION NO: 510
연구 회사에서는 온프레미스 장치를 사용하여 분석용 데이터를 생성합니다. 회사는 AWS
클라우드를 사용하여 데이터를 분석하려고 합니다. 장치는 .csv 파일을 생성하고 SMB 파일
공유에 데이터 쓰기를 지원합니다. 회사 분석가는 SQL 명령을 사용하여 데이터를 쿼리할 수
있어야 합니다. 분석가는 하루 종일 주기적으로 쿼리를 실행합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 단계 조합은 무엇입니까? (3개를
선택하세요.)
A. Amazon S3 파일 게이트웨이 모드로 온프레미스에 AWS Storage Gateway를 배포합니다.
B. Amazon FSx 파일 게이트웨이 모드로 온프레미스에 AWS Storage Gateway를 배포합니다.
C. Amazon S3에 있는 데이터를 기반으로 테이블을 생성하도록 AWS Glue 크롤러를
설정합니다.
D. EMRFS(EMR Fife System)를 사용하여 Amazon EMR 클러스터를 설정하여 Amazon S3에
있는 데이터를 쿼리합니다. 분석가에 대한 액세스를 제공합니다.
E. Amazon S3에 있는 데이터를 쿼리하도록 Amazon Redshift 클러스터를 설정합니다.
분석가에 대한 액세스를 제공합니다.
F. Amazon S3에 있는 데이터를 쿼리하도록 Amazon Athena를 설정합니다. 분석가에 대한
액세스를 제공합니다.
Answer: A C F
Explanation:
To meet the requirements of the use case in a cost-effective way, the following steps are
recommended:
* Deploy an AWS Storage Gateway on premises in Amazon S3 File Gateway mode. This will
allow the company to write the .csv files generated by the devices to an SMB file share,
which will be stored as objects in Amazon S3 buckets. AWS Storage Gateway is a hybrid
cloud storage service that integrates on-premises environments with AWS storage. Amazon
S3 File Gateway mode provides a seamless way to connect to Amazon S3 and access a
virtually unlimited amount of cloud storage1.
* Set up an AWS Glue crawler to create a table based on the data that is in Amazon S3. This
will enable the company to use standard SQL to query the data stored in Amazon S3
buckets. AWS Glue is a serverless data integration service that simplifies data preparation
and analysis. AWS Glue crawlers can automatically discover and classify data from various
sources, and create metadata tables in the AWS Glue Data Catalog2. The Data Catalog is a
central repository that stores information about data sources and how to access them3.
* Set up Amazon Athena to query the data that is in Amazon S3. This will provide the
company analysts with a serverless and interactive query service that can analyze data
directly in Amazon S3 using standard SQL. Amazon Athena is integrated with the AWS Glue
Data Catalog, so users can easily point Athena at the data source tables defined by the
crawlers. Amazon Athena charges only for the queries that are run, and offers a pay-per-
query pricing model, which makes it a cost-effective option for periodic queries4.
The other options are not correct because they are either not cost-effective or not suitable for
the use case.
Deploying an AWS Storage Gateway on premises in Amazon FSx File Gateway mode is not
350

IT Certification Guaranteed, The Easy Way!
correct because this mode provides low-latency access to fully managed Windows file shares
in AWS, which is not required for the use case. Setting up an Amazon EMR cluster with EMR
File System (EMRFS) to query the data that is in Amazon S3 is not correct because this
option involves setting up and managing a cluster of EC2 instances, which adds complexity
and cost to the solution. Setting up an Amazon Redshift cluster to query the data that is in
Amazon S3 is not correct because this option also involves provisioning and managing a
cluster of nodes, which adds overhead and cost to the solution.
References:
* What is AWS Storage Gateway?
* What is AWS Glue?
* AWS Glue Data Catalog
* What is Amazon Athena?
QUESTION NO: 511
한 회사의 온프레미스 데이터 센터에 소량의 데이터를 Amazon S3에 정기적으로 백업해야
하는 NFS 서버가 있습니다. 이러한 요구 사항을 충족하고 가장 비용 효율적인 솔루션은
무엇입니까?
A. 온프레미스 서버의 데이터를 Amazon S3에 복사하도록 AWS Glue를 설정합니다.
B. 온프레미스 서버에 AWS DataSync 에이전트를 설정하고 데이터를 Amazon S3에
동기화합니다.
C. AWS Transfer for SFTP를 사용하여 SFTP 동기화를 설정하여 온프레미스에서 Amazon
S3로 데이터를 동기화합니다.
D. 온프레미스 데이터 센터와 VPC 간에 AWS Direct Connect 연결을 설정하고 데이터를
Amazon S3에 복사합니다.
Answer: B
Explanation:
AWS DataSync is a service that makes it easy to move large amounts of data online between
on-premises storage and AWS storage services. AWS DataSync can transfer data at speeds
up to 10 times faster than open- source tools by using a purpose-built network protocol and
parallelizing data transfers. AWS DataSync also handles encryption, data integrity
verification, and bandwidth optimization. To use AWS DataSync, users need to deploy a
DataSync agent on their on-premises servers, which connects to the NFS servers and syncs
the data to Amazon S3. Users can schedule periodic or one-time sync tasks and monitor the
progress and status of the transfers.
The other options are not correct because they are either not cost-effective or not suitable for
the use case.
Setting up AWS Glue to copy the data from the on-premises servers to Amazon S3 is not
cost-effective because AWS Glue is a serverless data integration service that is mainly used
for extract, transform, and load (ETL) operations, not for simple data backup. Setting up an
SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon S3 is not
cost-effective because AWS Transfer for SFTP is a fully managed service that provides
secure file transfer using the SFTP protocol, which is more suitable for exchanging data with
third parties than for backing up data. Setting up an AWS Direct Connect connection between
the on- premises data center and a VPC, and copying the data to Amazon S3 is not cost
-effective because AWS Direct Connect is a dedicated network connection between AWS
351

IT Certification Guaranteed, The Easy Way!
and the on-premises location, which has high upfront costs and requires additional
configuration.
References:
* AWS DataSync
* How AWS DataSync works
* AWS DataSync FAQs
QUESTION NO: 512
회사에는 AWS Lake Formation이 관리하는 Amazon S3 데이터 레이크가 있습니다. 회사는
데이터 레이크의 데이터를 Amazon Aurora MySQL 데이터베이스에 저장된 운영 데이터와
결합하여 Amazon QuickSight에서 시각화를 생성하려고 합니다. 회사는 이를 시행하려고
합니다. 회사의 마케팅 팀이 데이터베이스에 있는 열의 하위 집합에만 액세스할 수 있도록 열
수준 권한 부여 어떤 솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을 충족합니까?
A. Amazon EMR을 사용하여 데이터베이스에서 QuickSight SPICE 엔진으로 직접 데이터를
수집합니다. 필수 열만 포함합니다.
B. AWS Glue Studio를 사용하여 데이터베이스에서 S3 데이터 레이크로 데이터를
수집합니다. QuickSight 사용자에게 IAM 정책을 연결하여 열 수준 액세스 제어를 시행합니다.
QuickSight에서 Amazon S3를 데이터 소스로 사용
C. AWS Glue Elastic Views를 사용하여 Amazon S3에서 데이터베이스에 대한 구체화된
보기를 생성합니다. S3 버킷 정책을 생성하여 QuickSight 사용자에 대한 열 수준 액세스
제어를 시행합니다. QuickSight에서 Amazon S3를 데이터 소스로 사용합니다.
D. Lake Formation 청사진을 사용하여 데이터베이스에서 S3 데이터 레이크로 데이터 수집
Lake Formation을 사용하여 QuickSight 사용자에 대한 열 수준 액세스 제어 시행
QuickSight에서 Amazon Athena를 데이터 소스로 사용
Answer: D
Explanation:
Enforce column-level authorization with Amazon QuickSight and AWS Lake Formation
https://aws.amazon.
com/blogs/big-data/enforce-column-level-authorization-with-amazon-quicksight-and-aws-
lake-formation/
QUESTION NO: 513
회사에 온프레미스 SFTP 파일 전송 솔루션이 있습니다. 이 회사는 파일 전송 솔루션을
확장하고 Amazon S3를 사용하여 비용을 최적화하기 위해 AWS 클라우드로
마이그레이션하고 있습니다. 회사 직원은 온프레미스 Microsoft Active Directory(AD)에 대한
자격 증명을 사용하여 새 솔루션에 액세스합니다. 회사는 현재 인증 및 파일 액세스
메커니즘을 유지하려고 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. S3 파일 게이트웨이를 구성합니다. 기존 Active Directory를 사용하여 인증하는 파일
게이트웨이에 SMB 파일 공유 생성
B. SFTP 솔루션을 실행하기 위해 Amazon EC2 인스턴스로 Auto Scaling 그룹을 구성합니다.
CPU 사용률이 60%로 확장되도록 그룹을 구성합니다.
C. SFTP 엔드포인트가 있는 AWS Transfer Family 서버를 생성합니다. AWS 디렉터리 서비스
옵션을 자격 증명 공급자로 선택합니다. AD 커넥터를 사용하여 온프레미스 Active Directory에
연결합니다.
352

IT Certification Guaranteed, The Easy Way!
D. AWS Transfer Family SFTP 엔드포인트를 생성합니다. 기존 Active Directory에 연결하기
위한 자격 증명 공급자로 AWS Directory Service 옵션을 사용하도록 엔드포인트를
구성합니다.
Answer: C
Explanation:
* AWS Transfer Family: This service provides fully managed support for file transfers directly
into and out of Amazon S3 using the SFTP, FTPS, and FTP protocols.
* SFTP Endpoints:
* Set up an AWS Transfer Family server and configure SFTP endpoints to handle the file
transfers.
* This service is scalable and managed, reducing operational overhead compared to running
an SFTP solution on EC2 instances.
* Integration with Active Directory:
* Choose the AWS Directory Service option as the identity provider for the Transfer Family
server.
* Use AD Connector to link the on-premises Active Directory with AWS, allowing employees
to use their existing AD credentials to access the SFTP service.
* Operational Efficiency: This solution leverages managed services for both file transfer and
identity management, ensuring minimal changes to the current authentication mechanisms
and reducing operational overhead.
References:
* AWS Transfer Family
* AWS Directory Service and AD Connector
QUESTION NO: 514
회사는 두 개의 가용 영역에 걸쳐 VPC에서 여러 Amazon EC2 Linux 인스턴스를 실행합니다.
인스턴스는 계층적 디렉터리 구조를 사용하는 애플리케이션을 호스팅합니다. 애플리케이션은
공유 스토리지를 동시에 빠르게 읽고 써야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. Amazon S3 버킷을 생성합니다. VPC에 있는 모든 EC2 인스턴스의 액세스를 허용합니다.
B. Amazon Elastic File System(Amazon EFS) 파일 시스템을 생성합니다. 각 EC2
인스턴스에서 EFS 파일 시스템을 탑재합니다.
C. 프로비저닝된 IOPS SSD(102) Amazon Elastic Block Store(Amazon EBS) 볼륨에 파일
시스템을 생성합니다. 모든 EC2 인스턴스에 EBS 볼륨을 연결합니다.
D. 각 EC2 인스턴스에 연결된 Amazon Elastic Block Store(Amazon EBS) 볼륨에 파일
시스템을 생성합니다. 다양한 EC2 인스턴스에서 EBS 볼륨을 동기화합니다.
Answer: B
Explanation:
it allows the EC2 instances to read and write rapidly and concurrently to shared storage
across two Availability Zones. Amazon EFS provides a scalable, elastic, and highly available
file system that can be mounted from multiple EC2 instances. Amazon EFS supports high
levels of throughput and IOPS, and consistent low latencies. Amazon EFS also supports
NFSv4 lock upgrading and downgrading, which enables high levels of concurrency.
References:
* Amazon EFS Features
353

IT Certification Guaranteed, The Easy Way!
* Using Amazon EFS with Amazon EC2
QUESTION NO: 515
일기 예보 회사는 밀리초 미만의 대기 시간으로 수백 기가바이트의 데이터를 처리해야
합니다.
이 회사는 데이터 센터에 HPC(고성능 컴퓨팅) 환경을 보유하고 있으며 예측 기능을
확장하려고 합니다.
솔루션 설계자는 대량의 지속적인 처리량을 처리할 수 있는 고가용성 클라우드 스토리지
솔루션을 식별해야 합니다. 솔루션에 저장된 파일은 전체 데이터 세트에 동시에 액세스하고
처리하는 수천 개의 컴퓨팅 인스턴스에 액세스할 수 있어야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. Lustre 스크래치 파일 시스템에 Amazon FSx 사용
B. Lustre 영구 파일 시스템에 Amazon FSx를 사용합니다.
C. 버스팅 처리량 모드와 함께 Amazon Elastic File System(Amazon EFS)을 사용합니다.
D. 프로비저닝된 처리량 모드와 함께 Amazon Elastic File System(Amazon EFS)을
사용합니다.
Answer: B
Explanation:
* Amazon FSx for Lustre: Lustre is a high-performance file system designed for workloads
that require fast storage with sustained high throughput and low latency. It integrates with
Amazon S3, making it suitable for HPC environments.
* Persistent File Systems:
* Persistent Storage: Suitable for long-term storage and recurrent use, providing durability
and availability.
* High Throughput and Low Latency: Persistent Lustre file systems can handle large amounts
of data with sub-millisecond latency, meeting the needs of high-performance computing
workloads.
* Simultaneous Access: FSx for Lustre allows thousands of compute instances to access and
process large datasets concurrently, ensuring that the high volume of data is handled
efficiently.
* Highly Available: FSx for Lustre is designed to provide high availability and is managed by
AWS, reducing the operational burden.
References:
* Amazon FSx for Lustre
* High-Performance Computing on AWS
QUESTION NO: 516
회사는 대상 그룹이 있는 Auto Scaling 그룹의 Amazon EC2 인스턴스에서 웹 애플리케이션을
실행합니다. 회사는 더 나은 사용자 경험을 위해 세션 선호도(고정 세션)와 함께 작동하도록
애플리케이션을 설계했습니다.
애플리케이션은 인터넷을 통해 엔드포인트로 공개적으로 사용할 수 있어야 합니다. 추가
보안을 위해 엔드포인트에 WAF를 적용해야 합니다. 엔드포인트에서 세션 선호도(고정
세션)를 구성해야 합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개 선택)
A. 공용 Network Load Balancer를 생성합니다. 애플리케이션 대상 그룹을 지정합니다.
B. 게이트웨이 로드 밸런서 생성 애플리케이션 대상 그룹을 지정합니다.
354

IT Certification Guaranteed, The Easy Way!
C. 퍼블릭 Application Load Balancer 생성 애플리케이션 대상 그룹을 지정합니다.
D. 두 번째 대상 그룹을 만듭니다. EC2 인스턴스에 탄력적 IP 주소 추가
E. AWS WAF에서 웹 ACL 생성 웹 ACL을 엔드포인트와 연결
Answer: C E
Explanation:
C and E are the correct answers because they allow the company to create a public endpoint
for its web application that supports session affinity (sticky sessions) and has a WAF applied
for additional security. By creating a public Application Load Balancer, the company can
distribute incoming traffic across multiple EC2 instances in an Auto Scaling group and specify
the application target group. By creating a web ACL in AWS WAF and associating it with the
Application Load Balancer, the company can protect its web application from common web
exploits. By enabling session stickiness on the Application Load Balancer, the company can
ensure that subsequent requests from a user during a session are routed to the same target.
References:
* Application Load Balancers
* AWS WAF
* Target Groups for Your Application Load Balancers
* How Application Load Balancer Works with Sticky Sessions
QUESTION NO: 517
회사에 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있습니다. EC2 인스턴스는
관련 정책이 있는 1AM 역할을 사용하여 Amazon RDS 데이터베이스에 연결합니다. 회사는
AWS Systems Manager를 사용하여 실행 중인 애플리케이션을 중단하지 않고 EC2
인스턴스를 패치하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 새로운 1AM 역할을 만듭니다. AmazonSSMManagedlnstanceCore 정책을 새 1AM 역할에
연결합니다. 새로운 1AM 역할을 EC2 인스턴스와 기존 1AM 역할에 연결합니다.
B. 오전 1시 사용자를 생성합니다. AmazonSSMManagedlnstanceCore 정책을 1AM
사용자에게 연결합니다. 1AM 사용자를 사용하여 EC2 인스턴스를 관리하도록 Systems
Manager를 구성합니다.
C. Systems Manager에서 기본 호스트 구성 관리를 활성화하여 EC2 인스턴스를 관리합니다.
D. 기존 1AM 역할에서 기존 정책을 제거합니다. 기존 1AM 역할에
AmazonSSMManagedlnstanceCore 정책을 추가합니다.
Answer: C
Explanation:
The most suitable solution for the company's requirements is to enable Default Host
Configuration Management in Systems Manager to manage the EC2 instances. This solution
will allow the company to patch the EC2 instances without disrupting the running applications
and without manually creating or modifying IAM roles or users.
Default Host Configuration Management is a feature of AWS Systems Manager that enables
Systems Manager to manage EC2 instances automatically as managed instances. A
managed instance is an EC2 instance that is configured for use with Systems Manager. The
benefits of managing instances with Systems Manager include the following:
* Connect to EC2 instances securely using Session Manager.
* Perform automated patch scans using Patch Manager.
355

IT Certification Guaranteed, The Easy Way!
* View detailed information about instances using Systems Manager Inventory.
* Track and manage instances using Fleet Manager.
* Keep SSM Agent up to date automatically.
Default Host Configuration Management makes it possible to manage EC2 instances without
having to manually create an IAM instance profile. Instead, Default Host Configuration
Management creates and applies a default IAM role to ensure that Systems Manager has
permissions to manage all instances in the Region and account where it is activated. If the
permissions provided are not sufficient for the use case, the default IAM role can be modified
or replaced with a custom role1.
The other options are not correct because they either have more operational overhead or do
not meet the requirements. Creating a new IAM role, attaching the
AmazonSSMManagedInstanceCore policy to the new IAM role, and attaching the new IAM
role and the existing IAM role to the EC2 instances is not correct because this solution
requires manual creation and management of IAM roles, which adds complexity and cost to
the solution. The AmazonSSMManagedInstanceCore policy is a managed policy that grants
permissions for Systems Manager core functionality2. Creating an IAM user, attaching the
AmazonSSMManagedInstanceCore policy to the IAM user, and configuring Systems
Manager to use the IAM user to manage the EC2 instances is not correct because this
solution requires manual creation and management of IAM users, which adds complexity and
cost to the solution. An IAM user is an identity within an AWS account that has specific
permissions for a single person or application3. Removing the existing policies from the
existing IAM role and adding the AmazonSSMManagedInstanceCore policy to the existing
IAM role is not correct because this solution may disrupt the running applications that rely on
the existing policies for accessing RDS databases. An IAM role is an identity within an AWS
account that has specific permissions for a service or entity4.
References:
* AWS managed policy: AmazonSSMManagedInstanceCore
* IAM users
* IAM roles
* Default Host Management Configuration - AWS Systems Manager
QUESTION NO: 518
솔루션 설계자는 대용량 데이터의 일괄 처리를 처리하는 애플리케이션을 만들고 있습니다.
입력 데이터는 Amazon S3에 보관되고 ou 데이터는 다른 S3 버킷에 저장됩니다. 처리를 위해
애플리케이션은 여러 Amazon EC2 인스턴스 간에 네트워크를 통해 데이터를 전송합니다.
전체 데이터 전송 비용을 줄이려면 솔루션 설계자가 무엇을 해야 합니까?
A. 모든 EC2 인스턴스를 Auto Scaling 그룹에 배치합니다.
B. 모든 EC2 인스턴스를 동일한 AWS 리전에 배치합니다.
C. 모든 EC2 인스턴스를 동일한 가용 영역에 배치합니다.
D. 모든 EC2 인스턴스를 여러 가용 영역의 프라이빗 서브넷에 배치합니다.
Answer: C
Explanation:
* Requirement Analysis: The application involves batch processing of large data transfers
between EC2 instances.
* Data Transfer Costs: Data transfer within the same Availability Zone (AZ) is typically free,
356

IT Certification Guaranteed, The Easy Way!
while cross- AZ transfers incur additional costs.
* Implementation:
* Launch all EC2 instances within the same Availability Zone.
* Ensure the instances are part of the same subnet to facilitate seamless data transfer.
* Conclusion: Placing all EC2 instances in the same AZ reduces data transfer costs
significantly without affecting the application's functionality.
References
* AWS Pricing: AWS Data Transfer Pricing
QUESTION NO: 519
회사에서 새로운 서버리스 워크로드 배포를 준비하고 있습니다. 솔루션 아키텍트는 최소
권한의 원칙을 사용하여 AWS Lambda 함수를 실행하는 데 사용되는 권한을 구성해야 합니다.
Amazon EventBridge(Amazon CloudWatch Events) 규칙이 함수를 호출합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Lambda:InvokeFunction을 작업으로, *를 주체로 사용하여 함수에 실행 역할을 추가합니다.
B. 작업으로 Lambda:InvokeFunction을, 보안 주체로 Service:amazonaws.com을 사용하여
함수에 실행 역할을 추가합니다.
C. 작업으로 Lambda:'*, 주체로 Service:events.amazonaws.com을 사용하여 함수에 리소스
기반 정책을 추가합니다.
D. 작업으로 Lambda:InvokeFunction을, 주체로 Service:events.amazonaws.com을 사용하여
함수에 리소스 기반 정책을 추가합니다.
Answer: D
Explanation:
https://docs.aws.amazon.com/eventbridge/latest/userguide/resource-based-policies-
eventbridge.html#lambda- permissions
QUESTION NO: 520
한 회사는 API를 호스팅하기 위해 Amazon EC2 Auto Scaling 그룹을 사용합니다. EC2
인스턴스는 Application Load Balancer(ALB)와 연결된 대상 그룹에 있습니다. 이 회사는
Amazon Aurora PostgreSQL 데이터베이스에 데이터를 저장합니다.
API에는 주간 유지 관리 기간이 있습니다. 회사는 API가 주간 유지 관리 기간 동안 정적 유지
관리 응답을 반환하도록 해야 합니다.
어떤 솔루션이 운영 오버헤드를 최소화하면서 이 요구 사항을 충족할 수 있을까요?
A. 키와 값을 포함하는 필드가 있는 Aurora PostgreSQL에 테이블을 만듭니다. 유지 관리
플래그에 대한 키를 만듭니다. 유지 관리 기간이 시작되면 플래그를 설정합니다. 유지 관리
플래그에 대한 테이블을 쿼리하고 플래그가 설정된 경우 유지 관리 응답을 반환하도록 API를
구성합니다. 유지 관리 기간이 끝나면 플래그를 재설정합니다.
B. Amazon Simple Queue Service(Amazon SQS) 대기열을 만듭니다. EC2 인스턴스를
대기열에 구독합니다. 유지 관리 기간이 시작되면 대기열에 메시지를 게시합니다. 인스턴스가
대기열에서 유지 관리 시작 메시지를 받으면 유지 관리 메시지를 반환하도록 API를
구성합니다.
유지 관리 기간이 끝나면 정상적인 작동을 복원하기 위해 대기열에 다른 메시지를
게시합니다.
C. 요청의 경로가 와일드카드와 일치할 때 유지 관리 응답을 반환하기 위해 ALB에 리스너
규칙을 만듭니다. 규칙 우선순위를 1로 설정합니다. 유지 관리를 수행합니다. 유지 관리
357

IT Certification Guaranteed, The Easy Way!
기간이 끝나면 리스너 규칙을 삭제합니다.
D. Amazon Simple Notification Service(Amazon SNS) 토픽을 만듭니다. EC2 인스턴스를
토픽에 구독합니다. 유지 관리 기간이 시작되면 토픽에 메시지를 게시합니다. 인스턴스가
토픽에서 유지 관리 시작 메시지를 받으면 API가 유지 관리 응답을 반환하도록 구성합니다.
유지 관리 기간이 끝나면 토픽에 다른 메시지를 게시하여 정상 작동을 복원합니다.
Answer: C
Explanation:
Creating a listener rule on the Application Load Balancer (ALB) to return a maintenance
response during the maintenance window is the most straightforward solution with the least
operational overhead. The rule can be configured to match all incoming requests and return a
custom response, and it can be easily removed once maintenance is complete.
* Option A (Aurora table flag): This adds unnecessary complexity for a temporary
maintenance response.
* Option B and D (SQS or SNS): These options introduce more components than needed for
a simple maintenance message.
AWS References:
* ALB Listener Rules
QUESTION NO: 521
솔루션 아키텍트는 퍼블릭 및 프라이빗 서브넷이 있는 VPC를 설계하고 있습니다. VPC와
서브넷은 IPv4 CIDR 블록을 사용합니다. 고가용성을 위해 3개의 가용 영역(AZ) 각각에 퍼블릭
서브넷 1개와 프라이빗 서브넷 1개가 있습니다. 인터넷 게이트웨이는 퍼블릭 서브넷에 대한
인터넷 액세스를 제공하는 데 사용됩니다. Amazon EC2 인스턴스가 소프트웨어 업데이트를
다운로드하려면 프라이빗 서브넷에 인터넷 액세스가 필요합니다.
프라이빗 서브넷에 대한 인터넷 액세스를 활성화하려면 솔루션 아키텍트가 무엇을 해야
합니까?
A. 각 AZ의 퍼블릭 서브넷당 하나씩, 3개의 NAT 게이트웨이를 생성합니다. 비VPC 트래픽을
해당 AZ의 NAT 게이트웨이로 전달하는 각 AZ에 대해 프라이빗 라우팅 테이블을 생성합니다.
B. 각 AZ의 프라이빗 서브넷당 하나씩, 3개의 NAT 인스턴스를 생성합니다. 비VPC 트래픽을
해당 AZ의 NAT 인스턴스로 전달하는 각 AZ에 대해 프라이빗 라우팅 테이블을 생성합니다.
C. 프라이빗 서브넷 중 하나에 두 번째 인터넷 게이트웨이를 생성합니다. 비VPC 트래픽을
프라이빗 인터넷 게이트웨이로 전달하는 프라이빗 서브넷의 라우팅 테이블을
업데이트합니다.
D. 퍼블릭 서브넷 중 하나에 외부 전용 인터넷 게이트웨이를 생성합니다. 비VPC 트래픽을
외부 전용 인터넷 게이트웨이로 전달하는 프라이빗 서브넷의 라우팅 테이블을
업데이트합니다.
Answer: A
Explanation:
https://aws.amazon.com/about-aws/whats-new/2018/03/introducing-amazon-vpc-nat-
gateway-in-the-aws- govcloud-us
-region/#:~:text=NAT%20Gateway%20is%20a%20highly,instances%20in%20a%20private%
20subnet.
https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html
QUESTION NO: 522
358

IT Certification Guaranteed, The Easy Way!
한 회사가 AWS에서 주문 관리 애플리케이션을 실행합니다. 이 애플리케이션을 통해 고객은
주문을 하고 신용 카드로 지불할 수 있습니다. 이 회사는 Amazon CloudFront 배포를 사용하여
애플리케이션을 제공합니다.
보안팀은 모든 수신 요청에 대한 로깅을 설정했습니다. 보안팀은 사용자가 로깅 구성을
수정하면 알림을 생성하는 솔루션이 필요합니다.
이러한 요구 사항을 충족하는 솔루션 조합은 무엇입니까? (2개를 선택하세요.)
A. 사용자가 CloudFront 배포를 생성하거나 수정할 때 호출되는 Amazon EventBridge 규칙을
구성합니다. AWS Lambda 함수를 EventBridge 규칙의 대상으로 추가합니다.
B. 애플리케이션 로드 밸런서(ALB)를 만듭니다. ALB에 대한 AWS WAF 규칙을 활성화합니다.
보안 위반을 감지하도록 AWS Config 규칙을 구성합니다.
C. CloudFront 배포 로깅의 변경 사항을 감지하는 AWS Lambda 함수를 만듭니다. Lambda
함수를 구성하여 Amazon Simple Notification Service(Amazon SNS)를 사용하여 보안 팀에
알림을 보냅니다.
D. Amazon GuardDuty를 설정합니다. GuardDuty를 구성하여 CloudFront 배포에서 결과를
모니터링합니다. 결과를 처리하기 위한 AWS Lambda 함수를 만듭니다.
E. Amazon API Gateway에서 개인 API를 만듭니다. AWS WAF 규칙을 사용하여 개인 API를
일반적인 보안 문제로부터 보호합니다.
Answer: A C
Explanation:
* A. EventBridge rule: Triggers an event whenever there is a change in CloudFront
distribution, ensuring real-time monitoring.
* B. ALB with WAF: Focuses on application-level security, not CloudFront logging.
* C. Lambda + SNS: Provides notifications upon detection of changes in logging
configuration.
* D. GuardDuty: Monitors anomalies but does not specifically address CloudFront logging
changes.
* E. Private API + WAF: Irrelevant to CloudFront logging changes.
References: Amazon EventBridge, AWS Lambda
QUESTION NO: 523
한 회사가 AWS에서 애플리케이션을 호스팅합니다. 이 애플리케이션은 사용자에게 사진을
업로드하고 Amazon S3 버킷에 사진을 저장할 수 있는 기능을 제공합니다. 이 회사는 Amazon
CloudFront와 사용자 지정 도메인 이름을 사용하여 eu-west-1 지역의 S3 버킷에 사진 파일을
업로드하려고 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까? (두 개를 선택하세요.)
A. AWS Certificate Manager(ACM)를 사용하여 us-east-1 지역에서 퍼블릭 인증서를
만듭니다. CloudFront에서 인증서를 사용합니다.
B. AWS Certificate Manager(ACM)를 사용하여 eu-west-1에 공개 인증서를 만듭니다.
CloudFront에서 인증서를 사용합니다.
C. CloudFront에서 업로드를 허용하도록 Amazon S3를 구성합니다. S3 전송 가속을
구성합니다.
D. CloudFront 원본 액세스 제어(OAC)에서 업로드를 허용하도록 Amazon S3를 구성합니다.
E. CloudFront에서 업로드를 허용하도록 Amazon S3를 구성합니다. Amazon S3 웹사이트
엔드포인트를 구성합니다.
359

IT Certification Guaranteed, The Easy Way!
Answer: A C
Explanation:
To upload photos to an S3 bucket using Amazon CloudFront with a custom domain name,
the following components are required:
* ACM in us-east-1 (Option A): When using CloudFront with HTTPS, the SSL/TLS certificate
must be created in the us-east-1 Region. AWS Certificate Manager (ACM) handles the
provisioning, management, and renewal of public certificates, making this a cost-effective
and low-maintenance solution.
* S3 Transfer Acceleration (Option C): Transfer Acceleration allows faster uploads to S3 from
CloudFront by routing traffic through AWS's edge locations. This significantly speeds up the
data upload process, especially for users that are geographically distant from the S3 bucket's
region.
* Option B (ACM in eu-west-1): CloudFront only supports certificates created in us-east-1.
* Option D and E (OAC and website endpoint): These are not ideal for handling secure
uploads or efficient data transfers in this case.
AWS References:
* Using ACM with CloudFront
* Amazon S3 Transfer Acceleration
QUESTION NO: 524
개발 팀은 성능 개선 도우미가 활성화된 범용 Amazon RDS for MySQL DB 인스턴스에 대해
매월 리소스 집약적인 테스트를 실행합니다. 테스트는 한 달에 한 번 48시간 동안 진행되며
데이터베이스를 사용하는 유일한 프로세스입니다. 팀은 DB 인스턴스의 컴퓨팅 및 메모리
속성을 줄이지 않고 테스트 실행 비용을 줄이고 싶어합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 테스트가 완료되면 DB 인스턴스를 중지합니다. 필요한 경우 DB 인스턴스를 다시
시작합니다.
B. DB 인스턴스와 함께 Auto Scaling 정책을 사용하여 테스트가 완료되면 자동으로 크기를
조정합니다.
C. 테스트가 완료되면 스냅샷을 생성합니다. DB 인스턴스를 종료하고 필요한 경우 스냅샷을
복원합니다.
D. 테스트가 완료되면 DB 인스턴스를 저용량 인스턴스로 수정합니다. 필요한 경우 DB
인스턴스를 다시 수정하십시오.
Answer: A
Explanation:
To reduce the cost of running the tests without reducing the compute and memory attributes
of the Amazon RDS for MySQL DB instance, the development team can stop the instance
when tests are completed and restart it when required. Stopping the DB instance when not in
use can help save costs because customers are only charged for storage while the DB
instance is stopped. During this time, automated backups and automated DB instance
maintenance are suspended. When the instance is restarted, it retains the same
configurations, security groups, and DB parameter groups as when it was stopped.
Reference:
Amazon RDS Documentation: Stopping and Starting a DB instance
(https://docs.aws.amazon.com
360

IT Certification Guaranteed, The Easy Way!
/AmazonRDS/latest/UserGuide/USER_StopInstance.html)
QUESTION NO: 525
미디어 회사는 Amazon S3에 영화를 저장합니다. 각 영화는 크기가 1GB에서 10GB 사이인
단일 비디오 파일에 저장됩니다.
회사는 사용자가 구매한 후 5분 이내에 영화의 스트리밍 콘텐츠를 제공할 수 있어야 합니다.
20년이 넘은 영화보다 20년 미만의 영화에 대한 수요가 더 높습니다. 회사는 수요에 따라
호스팅 서비스 비용을 최소화하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 모든 미디어 콘텐츠를 Amazon S3에 저장합니다. 영화에 대한 수요가 감소할 때 S3 수명
주기 정책을 사용하여 미디어 데이터를 Infrequent Access 계층으로 이동합니다.
B. S3 Standard에 최신 영화 비디오 파일을 저장합니다. 이전 영화 비디오 파일을 S3
Standard-Infrequent Access(S3 Standard-IA)에 저장합니다. 사용자가 오래된 영화를
주문하면 표준 검색을 사용하여 비디오 파일을 검색합니다.
C. S3 Intelligent-Tiering에 최신 영화 비디오 파일을 저장합니다. S3 Glacier 유연한 검색에
오래된 영화 비디오 파일을 저장합니다. 사용자가 오래된 영화를 주문하면 빠른 검색을
사용하여 비디오 파일을 검색합니다.
D. 최신 영화 비디오 파일을 S3 Standard에 저장합니다. S3 Glacier 유연한 검색에 오래된
영화 비디오 파일을 저장합니다. 사용자가 오래된 영화를 주문하면 대량 검색을 사용하여
비디오 파일을 검색합니다.
Answer: C
Explanation:
This solution will meet the requirements of minimizing hosting service costs based on
demand and providing the streaming content of a movie within 5 minutes of a user purchase.
S3 Intelligent-Tiering is a storage class that automatically optimizes storage costs by moving
data to the most cost-effective access tier when access patterns change. It is suitable for
data with unknown, changing, or unpredictable access patterns, such as newer movies that
may have higher demand1. S3 Glacier Flexible Retrieval is a storage class that provides low-
cost storage for archive data that is retrieved asynchronously. It offers flexible data retrieval
options from minutes to hours, and free bulk retrievals in 5-12 hours. It is ideal for backup,
disaster recovery, and offsite data storage needs2. By using expedited retrieval, the user can
access the older movie video file in 1-5 minutes, which meets the requirement of 5 minutes3.
References: 1: Amazon S3 Intelligent-Tiering Storage Class | AWS4, Overview section2:
Amazon S3 Glacier Flexible Retrieval and Glacier Deep Archive Retrieval ...1, Amazon S3
Glacier Flexible Retrieval section3:
Amazon S3 Glacier Flexible Retrieval and Glacier Deep Archive Retrieval ...1, Retrieval
Rates section.
QUESTION NO: 526
물류 회사가 운송업체와 배송 상태 정보를 공유할 수 있는 데이터 교환 플랫폼을 만들고
있습니다.
물류 회사는 모든 선적 정보와 메타데이터를 볼 수 있습니다. 회사는 선적 데이터 업데이트를
선적업체에 배포합니다.
각 운송업체는 회사와 관련된 배송 업데이트만 확인해야 합니다. 운송업체는 물류 회사에서
볼 수 있는 전체 세부 정보를 확인해서는 안 됩니다. 회사는 각 운송업체가 데이터를 공유할 수
361

IT Certification Guaranteed, The Easy Way!
있도록 Amazon Simple Notification Service(Amazon SNS) 토픽을 만듭니다. 일부 운송업체는
모바일 앱을 사용하여 배송 상태 업데이트를 제출합니다.
회사는 각 운송업체가 자사와 관련된 데이터에 구체적으로 접근할 수 있는 데이터 교환
플랫폼을 구축해야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. 모바일 앱에서 배송 업데이트를 Amazon Simple Queue Service(Amazon SQS)로
수집합니다.
SNS 토픽에 업데이트를 게시합니다. 필터 정책을 적용하여 각 메시지의 본문을 다시
작성합니다.
B. 모바일 앱에서 배송 업데이트를 Amazon Simple Queue Service(Amazon SQS)로
수집합니다.
AWS Lambda 함수를 사용하여 Amazon SQS에서 업데이트를 소비하고 각 메시지의 본문을
다시 작성합니다. 업데이트를 SNS 토픽에 게시합니다.
C. 모바일 앱에서 발송 업데이트를 두 번째 SNS 주제로 수집합니다. 발송자 SNS 주제에
업데이트를 게시합니다. 필터 정책을 적용하여 각 메시지의 본문을 다시 작성합니다.
D. 모바일 앱에서 Amazon Simple Queue Service(Amazon SQS)로 배송 업데이트를
수집합니다. Amazon EventBridge Pipes에서 메시지를 필터링하고 다시 작성합니다.
업데이트를 SNS 토픽에 게시합니다.
Answer: B
Explanation:
The best solution is to use Amazon SQS to receive updates from the mobile app and process
them with an AWS Lambda function. The Lambda function can rewrite the message body as
necessary for each shipper and then publish the updates to the appropriate SNS topic for
distribution. This setup ensures that each shipper receives only the relevant data and
minimizes operational overhead by using managed services.
* Option A (SNS filter policy): SNS does not have the capability to rewrite message bodies
before forwarding.
* Option C (Second SNS topic): Using an additional SNS topic adds unnecessary complexity
without solving the message rewriting requirement.
* Option D (EventBridge Pipes): EventBridge Pipes is more complex than necessary for this
use case, and Lambda can handle the logic more efficiently.
AWS References:
* Amazon SQS
* Amazon SNS with Lambda
QUESTION NO: 527
회사는 Amazon Elastic Block Store(Amazon EBS)가 지원하는 Amazon EC2 인스턴스에서
애플리케이션을 실행합니다. EC2 인스턴스는 최신 Amazon Linux 릴리스를 실행합니다. 회사
직원이 25GB 이상의 파일을 저장하고 검색할 때 애플리케이션에 가용성 문제가 발생합니다.
회사에는 EC2 인스턴스 간에 파일을 전송할 필요가 없는 솔루션이 필요합니다. 파일은 여러
EC2 인스턴스와 여러 가용 영역에서 사용할 수 있어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 모든 파일을 Amazon S3 버킷으로 마이그레이션합니다. 직원에게 S3 버킷의 파일에
액세스하도록 지시합니다.
B. 기존 EBS 볼륨의 스냅샷을 찍습니다. EC2 인스턴스 전반에 걸쳐 스냅샷을 EBS 볼륨으로
362

IT Certification Guaranteed, The Easy Way!
탑재합니다. 직원에게 EC2 인스턴스의 파일에 액세스하도록 지시합니다.
C. 모든 EC2 인스턴스에 Amazon Elastic File System(Amazon EFS) 파일 시스템을
탑재합니다. 직원에게 EC2 인스턴스의 파일에 액세스하도록 지시합니다.
D. EC2 인스턴스에서 Amazon 머신 이미지(AMI)를 생성합니다. 인스턴스 스토어 볼륨을
사용하는 AMI에서 새 EC2 인스턴스를 구성합니다. 직원에게 EC2 인스턴스의 파일에
액세스하도록 지시합니다.
Answer: C
Explanation:
To store and access files that are 25 GB or larger across many EC2 instances and across
multiple Availability Zones, Amazon Elastic File System (Amazon EFS) is a suitable solution.
Amazon EFS provides a simple, scalable, elastic file system that can be mounted on multiple
EC2 instances concurrently. Amazon EFS supports high availability and durability by storing
data across multiple Availability Zones within a Region.
References:
* What Is Amazon Elastic File System?
* Using EFS with EC2
QUESTION NO: 528
회사의 HTTP 애플리케이션은 NLB(Network Load Balancer) 뒤에 있습니다. NLB의 대상
그룹은 웹 서비스를 실행하는 여러 EC2 인스턴스와 함께 Amazon EC2 Auto Scaling 그룹을
사용하도록 구성됩니다.
회사는 NLB가 애플리케이션에 대한 HTTP 오류를 감지하지 못한다는 것을 알게 되었습니다.
이러한 오류는 웹 서비스를 실행하는 EC2 인스턴스를 수동으로 다시 시작해야 합니다. 회사는
사용자 정의 스크립트나 코드를 작성하지 않고 애플리케이션의 가용성을 개선해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. NLB에서 HTTP 상태 확인을 활성화합니다. 회사 응용 프로그램의 URL을 제공합니다.
B. EC2 인스턴스에 cron 작업을 추가하여 1분에 한 번씩 로컬 애플리케이션의 로그를
확인합니다. HTTP 오류가 감지되면 애플리케이션이 다시 시작됩니다.
C. NLB를 Application Load Balancer로 교체합니다. 회사 애플리케이션의 URL을 제공하여
HTTP 상태 확인을 활성화합니다. 비정상 인스턴스를 교체하도록 Auto Scaling 작업을
구성합니다.
D. NLB에 대한 UnhealthyHostCount 지표를 모니터링하는 Amazon Cloud Watch 경보를
생성합니다.경보가 ALARM 상태일 때 비정상 인스턴스를 교체하도록 Auto Scaling 작업을
구성합니다
.
Answer: C
Explanation:
Application availability: NLB cannot assure the availability of the application. This is because
it bases its decisions solely on network and TCP-layer variables and has no awareness of the
application at all. Generally, NLB determines availability based on the ability of a server to
respond to ICMP ping or to correctly complete the three-way TCP handshake. ALB goes
much deeper and is capable of determining availability based on not only a successful HTTP
GET of a particular page but also the verification that the content is as was expected based
on the input parameters.
363

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 529
회사에서 Amazon ECS를 사용하여 애플리케이션을 실행합니다. 애플리케이션은 원본
이미지의 esi/ed 버전을 생성한 다음 Amazon S3 API를 호출하여 크기가 조정된 이미지를
Amazon S3에 저장합니다.
솔루션 설계자는 애플리케이션에 Amazon S3에 액세스할 수 있는 권한이 있는지 어떻게
확인할 수 있습니까?
A. Amazon ECS에서 읽기/쓰기 액세스를 허용하도록 AWS IAM에서 S3 역할을 업데이트한
다음 컨테이너를 다시 시작합니다.
B. S3 권한이 있는 IAM 역할을 생성한 다음 작업 정의에서 해당 역할을 taskRoleAm으로
지정합니다.
C. Amazon ECS에서 Amazon S3로의 액세스를 허용하는 보안 그룹을 생성하고 ECS
클러스터에서 사용하는 시작 구성을 업데이트합니다.
D. S3 권한이 있는 IAM 사용자를 생성한 다음 이 계정으로 로그인한 동안 ECS 클러스터에
대한 Amazon EC2 인스턴스를 다시 시작합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ecs-
taskdefinition.html
QUESTION NO: 530
한 회사가 최근 고객을 위해 새로운 애플리케이션을 출시했습니다. 이 애플리케이션은 두
개의 가용성 영역에 걸쳐 여러 Amazon EC2 인스턴스에서 실행됩니다. 최종 사용자는 TCP를
사용하여 애플리케이션과 통신합니다.
애플리케이션은 고가용성이어야 하며 사용자 수가 늘어나면 자동으로 확장되어야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하려면 어떤 단계 조합이 필요할까요? (두
가지를 선택하세요.)
A. EC2 인스턴스 앞에 네트워크 로드 밸런서를 추가합니다.
B. EC2 인스턴스에 대한 자동 크기 조정 그룹을 구성합니다.
C. EC2 인스턴스 앞에 애플리케이션 로드 밸런서를 추가합니다.
D. 애플리케이션에 대한 EC2 인스턴스를 수동으로 추가합니다.
E. EC2 인스턴스 앞에 게이트웨이 로드 밸런서를 추가합니다.
Answer: A B
Explanation:
For an application requiring TCP communication and high availability:
* Network Load Balancer (NLB) is the best choice for load balancing TCP traffic because it is
designed for handling high-throughput, low-latency connections.
* Auto Scaling group ensures that the application can automatically scale based on demand,
adding or removing EC2 instances as needed, which is crucial for handling user growth.
* Option C (Application Load Balancer): ALB is primarily for HTTP/HTTPS traffic, not ideal for
TCP.
* Option D (Manual scaling): Manually adding instances does not provide the automation or
scalability required.
* Option E (Gateway Load Balancer): GLB is used for third-party virtual appliances, not for
direct application load balancing.
364

IT Certification Guaranteed, The Easy Way!
AWS References:
* Network Load Balancer
* Auto Scaling Group
QUESTION NO: 531
회사는 Amazon S3에 텍스트 파일을 저장합니다. 텍스트 파일에는 고객 채팅 메시지, 날짜 및
시간 정보, 고객 개인 식별 정보(Pll)가 포함됩니다.
회사에는 품질 관리를 위해 외부 서비스 제공업체에 대화 샘플을 제공하는 솔루션이
필요합니다. 외부 서비스 제공자는 가장 최근 대화까지 샘플 대화를 무작위로 선택해야
합니다. 회사는 고객 Pll을 외부 서비스 제공자와 공유해서는 안 됩니다. 고객 대화 수가
증가하면 솔루션도 확장되어야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 객체 Lambda 액세스 포인트를 생성합니다. 함수가 파일을 읽을 때 Pll을 수정하는 AWS
Lambda 함수를 생성합니다. 외부 서비스 공급자에게 객체 Lambda 액세스 포인트에
액세스하도록 지시합니다.
B. 모든 새 파일을 정기적으로 읽고, 파일에서 Pll을 수정하고, 수정된 파일을 다른 S3 버킷에
쓰는 Amazon EC2 인스턴스에 배치 프로세스를 생성합니다. Pll이 포함되지 않은 버킷에
액세스하도록 외부 서비스 공급자에게 지시합니다.
C. 파일 목록을 표시하고, 파일에서 Pll을 수정하고, 외부 서비스 공급자가 Pll이 수정된 파일의
새 버전을 다운로드할 수 있도록 하는 웹 애플리케이션을 Amazon EC2 인스턴스에
생성합니다.
D. Amazon DynamoDB 테이블을 생성합니다. Pll이 포함되지 않은 파일의 데이터만 읽는
AWS Lambda 함수를 생성합니다. 새 파일이 Amazon S3에 기록될 때 DynamoDB 테이블에
PII가 아닌 데이터를 저장하도록 Lambda 함수를 구성합니다. 외부 서비스 공급자에게
DynamoDB 테이블에 대한 액세스 권한을 부여합니다.
Answer: A
Explanation:
The correct solution is to create an Object Lambda Access Point and an AWS Lambda
function that redacts the PII when the function reads the file. This way, the company can use
the S3 Object Lambda feature to modify the S3 object content on the fly, without creating a
copy or changing the original object. The external service provider can access the Object
Lambda Access Point and get the redacted version of the file. This solution has the least
operational overhead because it does not require any additional storage, processing, or
synchronization. The solution also scales automatically with the number of customer
conversations and the demand from the external service provider. The other options are
incorrect because:
* Option B is using a batch process on an EC2 instance to read, redact, and write the files to
a different S3 bucket. This solution has more operational overhead because it requires
managing the EC2 instance, the batch process, and the additional S3 bucket. It also
introduces latency and inconsistency between the original and the redacted files.
* Option C is using a web application on an EC2 instance to present, redact, and download
the files. This solution has more operational overhead because it requires managing the EC2
instance, the web application, and the download process. It also exposes the original files to
the web application, which increases the risk of leaking the PII.
* Option D is using a DynamoDB table and a Lambda function to store the non-PII data from
365

IT Certification Guaranteed, The Easy Way!
the files.
This solution has more operational overhead because it requires managing the DynamoDB
table, the Lambda function, and the data transformation. It also changes the format and the
structure of the original files, which may affect the quality control process.
References:
* S3 Object Lambda
* Object Lambda Access Point
* Lambda function
QUESTION NO: 532
회사는 Application Load Balancer 뒤에서 Amazon EC2 인스턴스에서 글로벌 웹
애플리케이션을 실행합니다. 애플리케이션은 Amazon Auror a에 데이터를 저장합니다.
회사는 재해 복구 솔루션을 만들어야 하며 최대 30분의 가동 중지 시간과 잠재적인 데이터
손실을 허용할 수 있습니다. 기본 인프라가 정상일 때 솔루션은 로드를 처리할 필요가
없습니다. 이러한 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 필요한 인프라 요소를 갖춘 애플리케이션 배포 Amazon Route 53을 사용하여 액티브-
패시브 장애 조치 구성 두 번째 AWS 지역에 Aurora 복제본 생성
B. 두 번째 AWS 지역에서 애플리케이션의 축소된 배포를 호스팅합니다. Amazon Route 53을
사용하여 활성-활성 장애 조치를 구성합니다. 두 번째 지역에서 Aurora 복제본을 생성합니다.
C. 두 번째 AWS 리전에 기본 인프라 복제 Amazon Route 53을 사용하여 액티브-액티브 장애
조치 구성 최신 스냅샷에서 복원되는 Aurora 데이터베이스 생성
D. AWS Backup으로 데이터 백업 백업을 사용하여 두 번째 AWS 지역에 필요한 인프라 생성
Amazon Route 53을 사용하여 액티브-패시브 장애 조치 구성 두 번째 지역에 Aurora 두 번째
기본 인스턴스 생성
Answer: A
Explanation:
https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html
QUESTION NO: 533
회사는 매월 Amazon S3 표준 스토리지에서 약 300TB를 유지 관리합니다. S3 객체의 크기는
일반적으로 각각 약 50GB이며 글로벌 애플리케이션에 의해 멀티파트 업로드로 자주
교체됩니다. S3 객체의 수와 크기는 일정하게 유지되지만 회사의 S3 스토리지는 비용은 매달
증가하고 있습니다.
이 상황에서 솔루션 설계자는 어떻게 비용을 절감해야 합니까?
A. 멀티파트 업로드에서 Amazon S3 Transfer Acceleration으로 전환합니다.
B. 불완전한 멀티파트 업로드를 삭제하는 S3 수명 주기 정책을 활성화합니다.
C. 객체가 너무 빨리 보관되지 않도록 S3 인벤토리를 구성합니다.
D. Amazon S3에 저장된 객체 수를 줄이도록 Amazon CloudFront를 구성합니다.
Answer: B
Explanation:
This option is the most cost-effective way to reduce the S3 storage costs in this situation.
Incomplete multipart uploads are parts of objects that are not completed or aborted by the
application. They consume storage space and incur charges until they are deleted. By
enabling an S3 Lifecycle policy that deletes incomplete multipart uploads, you can
automatically remove them after a specified period of time (such as one day) and free up the
366

IT Certification Guaranteed, The Easy Way!
storage space. This will reduce the S3 storage costs and also improve the performance of
the application by avoiding unnecessary retries or errors.
Option A is not correct because switching from multipart uploads to Amazon S3 Transfer
Acceleration will not reduce the S3 storage costs. Amazon S3 Transfer Acceleration is a
feature that enables faster data transfers to and from S3 by using the AWS edge network. It
is useful for improving the upload speed of large objects over long distances, but it does not
affect the storage space or charges. In fact, it may increase the costs by adding a data
transfer fee for using the feature.
Option C is not correct because configuring S3 inventory to prevent objects from being
archived too quickly will not reduce the S3 storage costs. Amazon S3 Inventory is a feature
that provides a report of the objects and their metadata in an S3 bucket. It is useful for
managing and auditing the S3 objects, but it does not affect the storage space or charges. In
fact, it may increase the costs by generating additional S3 objects for the inventory reports.
Option D is not correct because configuring Amazon CloudFront to reduce the number of
objects stored in Amazon S3 will not reduce the S3 storage costs. Amazon CloudFront is a
content delivery network (CDN) that distributes the S3 objects to edge locations for faster and
lower latency access. It is useful for improving the download speed and availability of the S3
objects, but it does not affect the storage space or charges. In fact, it may increase the costs
by adding a data transfer fee for using the service. References:
* Managing your storage lifecycle
* Using multipart upload
* Amazon S3 Transfer Acceleration
* Amazon S3 Inventory
* What Is Amazon CloudFront?
QUESTION NO: 534
회사는 Amazon S3 버킷에 파일을 업로드하는 데 사용되는 애플리케이션을 호스팅합니다.
일단 업로드되면 파일은 5초 이내에 메타데이터를 추출하기 위해 처리됩니다. 업로드의 양과
빈도는 매 시간 몇 개의 파일에서 수백 개의 동시 업로드까지 다양합니다. 회사는 솔루션
설계자에게 이러한 요구 사항을 충족할 수 있는 비용 효율적인 아키텍처를 설계하도록
요청했습니다.
솔루션 설계자는 무엇을 추천해야 합니까?
A. S3 API 호출을 시작하도록 AWS CloudTrail 추적을 구성합니다. AWS AppSync를 사용하여
파일을 처리합니다.
B. 파일을 처리하기 위해 AWS Lambda 함수를 호출하도록 S3 버킷 내에서 객체 생성 이벤트
알림을 구성합니다.
C. 데이터를 처리하고 Amazon S3로 전송하도록 Amazon Kinesis Data Streams를
구성합니다. AWS Lambda 함수를 호출하여 파일을 처리합니다.
D. Amazon S3에 업로드된 파일을 처리하도록 Amazon Simple 알림 서비스(Amazon SNS)
주제를 구성합니다. AWS Lambda 함수를 호출하여 파일을 처리합니다.
Answer: B
Explanation:
This option is the most cost-effective and scalable way to process the files uploaded to S3.
AWS CloudTrail is used to log API calls, not to trigger actions based on them. AWS AppSync
is a service for building GraphQL APIs, not for processing files. Amazon Kinesis Data
367

IT Certification Guaranteed, The Easy Way!
Streams is used to ingest and process streaming data, not to send data to S3. Amazon SNS
is a pub/sub service that can be used to notify subscribers of events, not to process files.
References:
* Using AWS Lambda with Amazon S3
* AWS CloudTrail FAQs
* What Is AWS AppSync?
* [What Is Amazon Kinesis Data Streams?]
* [What Is Amazon Simple Notification Service?]
QUESTION NO: 535
회사는 단일 Amazon EC2 온디맨드 인스턴스에서 웹 사이트 분석 애플리케이션을
호스팅합니다. 분석 소프트웨어는 PHP로 작성되었으며 MySQL 데이터베이스를 사용합니다.
분석 소프트웨어, PHP를 제공하는 웹 서버 및 데이터베이스 서버는 모두 EC2 인스턴스에서
호스팅됩니다. 바쁜 시간 동안 애플리케이션에서 성능 저하 조짐을 보이고 있으며 5xx 오류가
표시됩니다. 회사는 애플리케이션을 원활하게 확장해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 데이터베이스를 MySQL용 Amazon RDS DB 인스턴스로 마이그레이션합니다. 웹
애플리케이션의 AMI를 생성합니다. AMI를 사용하여 두 번째 EC2 온디맨드 인스턴스를
시작합니다. Application Load Balancer를 사용하여 각 EC2 인스턴스에 로드를 분산합니다.
B. 데이터베이스를 MySQL용 Amazon RDS DB 인스턴스로 마이그레이션합니다. 웹
애플리케이션의 AMI를 생성합니다. AMI를 사용하여 두 번째 EC2 온디맨드 인스턴스를
시작합니다. Amazon Route 53 가중치 기반 라우팅을 사용하여 두 EC2 인스턴스에 로드를
분산시킵니다.
C. 데이터베이스를 Amazon Aurora MySQL DB 인스턴스로 마이그레이션합니다. EC2
인스턴스를 중지하고 인스턴스 유형을 변경하는 AWS Lambda 함수를 생성합니다. CPU
사용률이 75%를 초과하면 Lambda 함수를 호출하는 Amazon CloudWatch 경보를
생성합니다.
D. 데이터베이스를 Amazon Aurora MySQL DB 인스턴스로 마이그레이션합니다. 웹
애플리케이션의 AMI를 생성합니다. 시작 템플릿에 AMI를 적용합니다. 시작 템플릿을
사용하여 Auto Scaling 그룹을 생성합니다. 스팟 집합을 사용하도록 시작 템플릿을
구성합니다. Auto Scaling 그룹에 Application Load Balancer를 연결합니다.
Answer: D
Explanation:
Migrate the database to Amazon Aurora MySQL - this will let the DB scale on it's own; it'll
scale automatically without needing adjustment. Create AMI of the web app and using a
launch template - this will make the creating of any future instances of the app seamless.
They can then be added to the auto scaling group which will save them money as it will scale
up and down based on demand. Using a spot fleet to launch instances- This solves the
"MOST cost-effective" portion of the question as spot instances come at a huge discount at
the cost of being terminated at any time Amazon deems fit. I think this is why there's a bit of
disagreement on this. While it's the most cost effective, it would be a terrible choice if amazon
were to terminate that spot instance during a busy period.
QUESTION NO: 536
회사에는 AWS 리전에 워크로드가 있습니다. 고객은 Amazon API Gateway REST API를
368

IT Certification Guaranteed, The Easy Way!
사용하여 워크로드에 연결하고 액세스합니다. 이 회사는 Amazon Route 53을 DNS 공급자로
사용합니다. 회사는 모든 고객에게 개별적이고 안전한 URL을 제공하려고 합니다.
가장 효율적인 운영 효율성으로 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까?
(3개를 선택하세요.)
A. 등록기관에 필요한 도메인을 등록하세요. Route 53 호스팅 영역에 와일드카드 사용자 지정
도메인 이름을 생성하고 API Gateway 엔드포인트를 가리키는 영역에 기록합니다.
B. 다른 리전에 있는 AWS Certificate Manager(ACM)의 도메인과 일치하는 와일드카드
인증서를 요청합니다.
C. Route 53에서 필요에 따라 각 고객에 대한 호스팅 영역을 생성합니다. API Gateway
엔드포인트를 가리키는 영역 레코드를 생성합니다.
D. 동일한 리전에 있는 AWS Certificate Manager(ACM)의 사용자 지정 도메인 이름과
일치하는 와일드카드 인증서를 요청합니다.
E. API Gateway에서 각 고객에 대해 여러 API 엔드포인트를 생성합니다.
F. API Gateway에서 REST API용 사용자 지정 도메인 이름을 생성합니다. AWS Certificate
Manager(ACM)에서 인증서를 가져옵니다.
Answer: A D F
Explanation:
To provide individual and secure URLs for all customers using an API Gateway REST API,
you need to do the following steps:
* A. Register the required domain in a registrar. Create a wildcard custom domain name in a
Route 53 hosted zone and record in the zone that points to the API Gateway endpoint. This
step will allow you to use a custom domain name for your API instead of the default one
generated by API Gateway. A wildcard custom domain name means that you can use any
subdomain under your domain name (such as customer1.example.com or
customer2.example.com) to access your API. You need to register your domain name with a
registrar (such as Route 53 or a third-party registrar) and create a hosted zone in Route 53
for your domain name. You also need to create a record in the hosted zone that points to the
API Gateway endpoint using an alias record.
* D. Request a wildcard certificate that matches the custom domain name in AWS Certificate
Manager (ACM) in the same Region. This step will allow you to secure your API with HTTPS
using a certificate issued by ACM. A wildcard certificate means that it can match any
subdomain under your domain name (such as *.example.com). You need to request or
import a certificate in ACM that matches your custom domain name and verify that you own
the domain name. You also need to request the certificate in the same Region as your API.
* F. Create a custom domain name in API Gateway for the REST API. Import the certificate
from AWS Certificate Manager (ACM). This step will allow you to associate your custom
domain name with your API and use the certificate from ACM to enable HTTPS. You need to
create a custom domain name in API Gateway for the REST API and specify the certificate
ARN from ACM. You also need to create a base path mapping that maps a path from your
custom domain name to your API stage.
References: https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-
domains.html
https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-request.html
QUESTION NO: 537
369

IT Certification Guaranteed, The Easy Way!
한 회사는 최근 AWS로 마이그레이션했으며 프로덕션 VPC에 들어오고 나가는 트래픽을
보호하는 솔루션을 구현하려고 합니다. 이 회사는 온프레미스 데이터 센터에 검사 서버를
갖고 있었습니다. 점검서버는 트래픽 흐름 점검, 트래픽 필터링 등의 특정 작업을 수행했다.
회사는 AWS 클라우드에서도 동일한 기능을 갖기를 원합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 프로덕션 VPC에서 트래픽 검사 및 트래픽 필터링을 위해 Amazon GuardDuty를
사용합니다.
B. 트래픽 미러링을 사용하여 트래픽 검사 및 필터링을 위해 프로덕션 VPC의 트래픽을
미러링합니다.
C. AWS 네트워크 방화벽을 사용하여 프로덕션 VPC에 대한 트래픽 검사 및 트래픽 필터링에
필요한 규칙을 생성합니다.
D. AWS Firewall Manager를 사용하여 프로덕션 VPC에 대한 트래픽 검사 및 트래픽 필터링에
필요한 규칙을 생성합니다.
Answer: C
Explanation:
AWS Network Firewall supports both inspection and filtering as required
QUESTION NO: 538
한 회사는 로드 밸런싱된 프런트 엔드, 컨테이너 기반 애플리케이션 및 관계형
데이터베이스로 구성되는 전자 상거래 애플리케이션을 개발하고 있습니다. 솔루션 설계자는
수동 개입을 최대한 최소화하면서 작동하는 가용성이 뛰어난 솔루션을 만들어야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까? (2개를 선택하세요.)
A. 다중 AZ 모드에서 Amazon RDS DB 인스턴스를 생성합니다.
B. 다른 가용 영역에 Amazon RDS DB 인스턴스와 하나 이상의 복제본을 생성합니다.
C. 동적 애플리케이션 로드를 처리하기 위해 자세 기반 Docker 클러스터에 Amazon EC2를
생성합니다.
D. 동적 애플리케이션 로드를 처리하기 위해 Fargate 시작 유형을 사용하여 Amazon Elastic
Container Service(Amazon ECS) 클러스터를 생성합니다.
E. 동적 애플리케이션 로드를 처리하기 위해 Amazon EC2 시작 유형을 사용하여 Amazon
Elastic Container Service(Amazon ECS) 클러스터를 생성합니다.
Answer: A D
Explanation:
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html
1. Relational database: RDS
2. Container-based applications: ECS
"Amazon ECS enables you to launch and stop your container-based applications by using
simple API calls.
You can also retrieve the state of your cluster from a centralized service and have access to
many familiar Amazon EC2 features."
3. Little manual intervention: Fargate
You can run your tasks and services on a serverless infrastructure that is managed by AWS
Fargate.
Alternatively, for more control over your infrastructure, you can run your tasks and services
on a cluster of Amazon EC2 instances that you manage.
370

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 539
회사는 애플리케이션에 대한 실시간 데이터 수집 아키텍처를 구성해야 합니다. 회사에는
데이터가 스트리밍될 때 데이터를 변환하는 프로세스인 API와 데이터를 위한 스토리지
솔루션이 필요합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon EC2 인스턴스를 배포하여 Amazon Kinesis 데이터 스트림으로 데이터를 전송하는
API를 호스팅합니다.
Kinesis 데이터 스트림을 데이터 원본으로 사용하는 Amazon Kinesis Data Firehose 전송
스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. Kinesis Data
Firehose 전송 스트림을 사용하여 데이터를 Amazon S3로 보냅니다.
B. Amazon EC2 인스턴스를 배포하여 AWS Glue에 데이터를 전송하는 API를 호스팅합니다.
EC2 인스턴스에서 소스/대상 확인을 중지합니다. AWS Glue를 사용하여 데이터를 변환하고
데이터를 Amazon S3로 보냅니다.
C. Amazon Kinesis 데이터 스트림으로 데이터를 전송하도록 Amazon API Gateway API를
구성합니다. Kinesis 데이터 스트림을 데이터 원본으로 사용하는 Amazon Kinesis Data
Firehose 전송 스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다.
Kinesis Data Firehose 전송 스트림을 사용하여 데이터를 Amazon S3로 보냅니다.
D. 데이터를 AWS Glue로 보내도록 Amazon API Gateway API를 구성합니다. AWS Lambda
함수를 사용하여 데이터를 변환합니다. AWS Glue를 사용하여 데이터를 Amazon S3로
보냅니다.
Answer: C
Explanation:
It uses Amazon Kinesis Data Firehose which is a fully managed service for delivering real-
time streaming data to destinations such as Amazon S3. This service requires less
operational overhead as compared to option A, B, and D. Additionally, it also uses Amazon
API Gateway which is a fully managed service for creating, deploying, and managing APIs.
These services help in reducing the operational overhead and automating the data ingestion
process.
QUESTION NO: 540
회사는 여러 공급업체를 통해 Amazon S3 버킷에 저장된 디지털 자산을 배포합니다. 회사는
공급업체 AWS 계정에 이러한 S3 버킷의 객체를 다운로드하는 데 필요한 최소한의 액세스
권한이 있는지 확인하려고 합니다. 어떤 솔루션이 최소한의 운영 비용으로 이러한 요구
사항을 충족합니까? 간접비?
A. 익명 읽기 권한과 모든 버킷 나열 권한이 있는 버킷 정책을 설계합니다.
B. 사용자에게 읽기 전용 액세스 권한을 부여하는 버킷 정책을 설계합니다. 1AM 엔터티를
주체로 지정
C. 1AM 역할에 대해 지정된 읽기 전용 액세스 정책이 있는 교차 계정 1AM 역할을 생성합니다.
D. 공급업체 사용자에게 읽기 전용 액세스 권한을 부여하는 사용자 정책 및 공급업체 사용자
그룹을 생성합니다.
Answer: C
Explanation:
A cross-account IAM role is a way to grant users from one AWS account access to resources
in another AWS account. The cross-account IAM role can have a read-only access policy
371

IT Certification Guaranteed, The Easy Way!
attached to it, which allows the users to download objects from the S3 buckets without
modifying or deleting them. The cross-account IAM role also reduces the operational
overhead of managing multiple IAM users and policies in each account. The cross-account
IAM role meets all the requirements of the question, while the other options do not.
References:
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-
managing-access- example2.html
* https://aws.amazon.com/blogs/storage/setting-up-cross-account-amazon-s3-access-with-
s3-access- points/
* https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-
user_externalid.html
QUESTION NO: 541
회사에는 VPC의 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있습니다.
애플리케이션 중 하나는 Amazon S3 API를 호출하여 객체를 저장하고 읽어야 합니다. 회사의
보안 규정에 따라 애플리케이션의 트래픽은 인터넷을 통해 이동할 수 없습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. S3 게이트웨이 엔드포인트를 구성합니다.
B. 프라이빗 서브넷에 S3 버킷을 생성합니다.
C. EC2 인스턴스와 동일한 AWS 리전에 S3 버킷을 생성합니다.
D. EC2 인스턴스와 동일한 서브넷에 NAT 게이트웨이를 구성합니다.
Answer: A
Explanation:
* VPC Endpoint for S3: A gateway endpoint for Amazon S3 enables you to privately connect
your VPC to S3 without requiring an internet gateway, NAT device, VPN connection, or AWS
Direct Connect connection.
* Configuration Steps:
* In the VPC console, navigate to "Endpoints" and create a new endpoint.
* Select the service name for S3 (com.amazonaws.region.s3).
* Choose the VPC and the subnets where your EC2 instances are running.
* Update the route tables for the selected subnets to include a route pointing to the endpoint.
* Security Compliance: By configuring an S3 gateway endpoint, all traffic between the VPC
and S3 stays within the AWS network, complying with the company's security regulations to
avoid internet traversal.
References:
* VPC Endpoints for Amazon S3
QUESTION NO: 542
회사는 VPC의 프라이빗 서브넷에서 AWS Lambda 함수를 실행합니다. 서브넷에는 Amazon
EC2 NAT 인스턴스를 통해 인터넷으로 연결되는 기본 경로가 있습니다. Lambda 함수는 입력
데이터를 처리하고 해당 출력을 Amazon S3에 객체로 저장합니다.
NAT 인스턴스 네트워크의 트래픽 포화로 인해 객체를 업로드하려고 시도하는 동안
간헐적으로 Lambda 함수 시간이 초과됩니다. 회사는 인터넷을 통과하지 않고 Amazon S3에
액세스하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
372

IT Certification Guaranteed, The Easy Way!
A. EC2 NAT 인스턴스를 AWS 관리형 NAT 게이트웨이로 교체합니다.
B. VPC의 EC2 NAT 인스턴스 크기를 네트워크 최적화 인스턴스 유형으로 늘립니다.
C. VPC에서 Amazon S3에 대한 게이트웨이 엔드포인트를 프로비저닝합니다. 그에 따라
서브넷의 라우팅 테이블을 업데이트합니다.
D. 전송 게이트웨이를 프로비저닝합니다. Lambda 함수가 실행 중인 프라이빗 서브넷에 전송
게이트웨이 연결을 배치합니다.
Answer: C
Explanation:
* Gateway Endpoint for Amazon S3: A VPC endpoint for Amazon S3 allows you to privately
connect your VPC to Amazon S3 without requiring an internet gateway, NAT device, VPN
connection, or AWS Direct Connect connection.
* Provisioning the Endpoint:
* Navigate to the VPC Dashboard.
* Select "Endpoints" and create a new endpoint.
* Choose the service name for S3 (com.amazonaws.region.s3).
* Select the appropriate VPC and subnets.
* Adjust the route tables of the subnets to include the new endpoint.
* Update Route Tables: Modify the route tables of the subnets to direct traffic destined for S3
to the newly created endpoint. This ensures that traffic to S3 does not go through the NAT
instance, avoiding the saturated network and eliminating timeouts.
* Operational Efficiency: This solution minimizes operational overhead by removing
dependency on the NAT instance and avoiding internet traffic, leading to more stable and
secure S3 interactions.
References:
* VPC Endpoints for Amazon S3
* Creating a Gateway Endpoint
QUESTION NO: 543
글로벌 이벤트의 주최자는 일일 보고서를 정적 HTML 페이지로 온라인에 게시하려고 합니다.
이 페이지는 전 세계 사용자로부터 수백만 건의 조회수를 생성할 것으로 예상됩니다. 파일은
Amazon S3 버킷에 저장됩니다. 솔루션 설계자는 효율적이고 효과적인 솔루션을 설계해
달라는 요청을 받았습니다.
이를 달성하기 위해 솔루션 아키텍트는 어떤 조치를 취해야 합니까?
A. 파일에 대해 미리 서명된 URL을 생성합니다.
B. 모든 지역에 교차 지역 복제를 사용합니다.
C. Amazon Route 53의 지리 근접 기능을 사용합니다.
D. S3 버킷을 원본으로 하는 Amazon CloudFront를 사용합니다.
Answer: D
Explanation:
Amazon CloudFront is a content delivery network (CDN) that speeds up the delivery of static
and dynamic web content, such as HTML pages, images, and videos. By using CloudFront,
the HTML pages will be served to users from the edge location that is closest to them,
resulting in faster delivery and a better user experience.
CloudFront can also handle the high traffic and large number of requests expected for the
373

IT Certification Guaranteed, The Easy Way!
global event, ensuring that the HTML pages are available and accessible to users around the
world.
QUESTION NO: 544
회사는 Amazon Route 53에 도메인 이름을 등록했습니다. 이 회사는 ca-central-1 리전의
Amazon API Gateway를 백엔드 마이크로서비스 API의 공용 인터페이스로 사용합니다. 타사
서비스는 API를 안전하게 사용합니다. 회사는 타사 서비스에서 HTTPS를 사용할 수 있도록
회사의 도메인 이름 및 해당 인증서로 API 게이트웨이 URL을 설계하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. API Gateway에서 Name="Endpoint-URL" 및 Value="Company Domain Name"으로 단계
변수를 생성하여 기본 URL을 덮어씁니다. 회사의 도메인 이름과 연결된 공인 인증서를 AWS
Certificate Manager(ACM)로 가져옵니다.
B. 회사의 도메인 이름으로 Route 53 DNS 레코드를 생성합니다. 별칭 레코드가 리전 API
게이트웨이 단계 엔드포인트를 가리키도록 합니다. 회사의 도메인 이름과 연결된 공인
인증서를 us-east-1 리전의 AWS Certificate Manager(ACM)로 가져옵니다.
C. 리전 API 게이트웨이 엔드포인트를 생성합니다. API Gateway 엔드포인트를 회사의 도메인
이름과 연결합니다. 회사의 도메인 이름과 연결된 공인 인증서를 동일한 리전의 AWS
Certificate Manager(ACM)로 가져옵니다. API Gateway 엔드포인트에 인증서를 연결합니다.
API Gateway 엔드포인트로 트래픽을 라우팅하도록 Route 53을 구성합니다.
D. 리전 API 게이트웨이 엔드포인트를 생성합니다. API Gateway 엔드포인트를 회사의 도메인
이름과 연결합니다. 회사의 도메인 이름과 연결된 공인 인증서를 us-east-1 리전의 AWS
Certificate Manager(ACM)로 가져옵니다. API Gateway API에 인증서를 연결합니다.
회사의 도메인 이름으로 Route 53 DNS 레코드를 생성합니다. A 레코드가 회사의 도메인
이름을 가리키도록 합니다.
Answer: C
Explanation:
To design the API Gateway URL with the company's domain name and corresponding
certificate, the company needs to do the following: 1. Create a Regional API Gateway
endpoint: This will allow the company to create an endpoint that is specific to a region. 2.
Associate the API Gateway endpoint with the company's domain name: This will allow the
company to use its own domain name for the API Gateway URL. 3. Import the public
certificate associated with the company's domain name into AWS Certificate Manager (ACM)
in the same Region: This will allow the company to use HTTPS for secure communication
with its APIs. 4.
Attach the certificate to the API Gateway endpoint: This will allow the company to use the
certificate for securing the API Gateway URL. 5. Configure Route 53 to route traffic to the API
Gateway endpoint: This will allow the company to use Route 53 to route traffic to the API
Gateway URL using the company's domain name.
QUESTION NO: 545
회사에서는 엔지니어 팀을 위해 개별 AWS 계정을 실험하려고 합니다. 회사는 특정 달의
Amazon EC2 인스턴스 사용량이 각 계정의 특정 임계값을 초과하는 즉시 알림을 받기를
원합니다.
이 요구 사항을 가장 비용 효율적으로 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. Cost Explorer를 사용하여 서비스별 비용에 대한 일일 보고서를 생성합니다. EC2
374

IT Certification Guaranteed, The Easy Way!
인스턴스별로 보고서를 필터링합니다. 임계값이 초과되면 Amazon Simple Email
Service(Amazon SES) 알림을 보내도록 Cost Explorer를 구성합니다.
B. 비용 탐색기를 사용하여 서비스별 비용에 대한 월별 보고서를 생성합니다. EC2
인스턴스별로 보고서를 필터링합니다. 임계값이 초과되면 Amazon Simple Email
Service(Amazon SES) 알림을 보내도록 Cost Explorer를 구성합니다.
C. AWS 예산을 사용하여 각 계정에 대한 비용 예산을 생성합니다. 기간을 월 단위로
설정하세요. 범위를 EC2 인스턴스로 설정합니다. 예산에 대한 알림 임계값을 설정합니다.
임계값이 초과될 때 알림을 받도록 Amazon Simple 알림 서비스(Amazon SNS) 주제를
구성합니다.
D. AWS 비용 및 사용 보고서를 사용하여 시간별 세부 보고서를 생성합니다. 보고서 데이터를
Amazon Athena와 통합합니다. Amazon EventBridge를 사용하여 Athena 쿼리를 예약합니다.
임계값이 초과될 때 알림을 받도록 Amazon Simple 알림 서비스(Amazon SNS) 주제를
구성합니다.
Answer: C
Explanation:
AWS Budgets allows you to create budgets for your AWS accounts and set alerts when
usage exceeds a certain threshold. By creating a budget for each account, specifying the
period as monthly and the scope as EC2 instances, you can effectively track the EC2 usage
for each account and be notified when a threshold is exceeded. This solution is the most
cost-effective option as it does not require additional resources such as Amazon Athena or
Amazon EventBridge.
QUESTION NO: 546
빠르게 성장하는 전자 상거래 회사는 단일 AWS 리전에서 워크로드를 실행하고 있습니다.
솔루션 아키텍트는 다른 AWS 지역을 포함하는 재해 복구(DR) 전략을 수립해야 합니다.
회사는 지연 시간을 최소화하면서 DR 지역에서 데이터베이스를 최신 상태로 유지하기를
원합니다. DR 지역의 나머지 인프라는 감소된 용량으로 실행해야 합니다. 필요에 따라 확장할
수 있어야 합니다. 가장 낮은 RTO(복구 시간 목표)로 이러한 요구 사항을 충족하는 솔루션은
무엇입니까?
A. 파일럿 라이트 배포와 함께 Amazon Aurora 글로벌 데이터베이스 사용
B. 웜 대기 배포와 함께 Amazon Aurora 글로벌 데이터베이스 사용
C. 파일럿 라이트 배포와 함께 Amazon RDS 다중 AZ DB 인스턴스 사용
D. 웜 대기 배포와 함께 Amazon RDS 다중 AZ DB 인스턴스 사용
Answer: B
Explanation:
https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-
aws/disaster-recovery- options-in-the-cloud.html
QUESTION NO: 547
전자상거래 회사가 온프레미스 Microsoft SQL Server 데이터베이스를 AWS 클라우드로
마이그레이션할 계획입니다. 이 회사는 데이터베이스를 SQL Server Always On 가용성
그룹으로 마이그레이션해야 합니다. 클라우드 기반 솔루션은 고가용성이어야 합니다.
옵션:
A. SQL Server가 포함된 Amazon EC2 인스턴스 3개를 3개의 가용성 영역에 배포합니다. EC2
인스턴스에 Amazon Elastic Block Store(Amazon EBS) 볼륨 하나를 연결합니다.
375

IT Certification Guaranteed, The Easy Way!
B. 데이터베이스를 SQL Server용 Amazon RDS로 마이그레이션합니다. 다중 AZ 배포를
구성하고 복제본을 읽습니다.
C. SQL Server가 있는 3개의 Amazon EC2 인스턴스를 3개의 가용성 영역에 배포합니다.
Amazon FSx for Windows File Server를 스토리지 계층으로 사용합니다.
D. 3개의 가용성 영역에 SQL Server가 있는 3개의 Amazon EC2 인스턴스를 배포합니다.
Amazon S3를 스토리지 계층으로 사용합니다.
Answer: C
Explanation:
* A. EC2 with EBS: Does not support SQL Server Always On availability groups effectively.
* B. RDS Multi-AZ: Provides high availability but does not support SQL Server Always On
availability groups.
* C. EC2 with FSx for Windows: Best solution for SQL Server Always On as FSx provides
shared storage compatible with SQL Server clustering.
* D. EC2 with S3: S3 is not suitable for SQL Server storage.
References: Amazon FSx for Windows
QUESTION NO: 548
회사는 확장성 및 가용성에 대한 요구 사항을 충족하기 위해 중요한 애플리케이션을
컨테이너에서 실행하기를 원합니다. 회사는 중요한 애플리케이션의 유지 관리에 집중하는
것을 선호합니다. 회사는 컨테이너화된 워크로드를 실행하는 기본 인프라의 프로비저닝 및
관리에 대한 책임을 원하지 않습니다. 솔루션 아키텍트는 이러한 요구 사항을 충족하기 위해
무엇을 합니까?
A. Amazon EC2 인스턴스를 사용하고 인스턴스에 Docker 설치
B. Amazon EC2 작업자 노드에서 Amazon Elastic Container Service(Amazon ECS) 사용
C. AWS Fargate에서 Amazon Elastic Container Service(Amazon ECS) 사용
D. Amazon Elastic Container Service(Amazon ECS)-op6mized Amazon 머신 이미지(AMI)의
Amazon EC2 인스턴스를 사용합니다.
Answer: C
Explanation:
using AWS ECS on AWS Fargate since they requirements are for scalability and availability
without having to provision and manage the underlying infrastructure to run the containerized
workload. https://docs.aws.
amazon.com/AmazonECS/latest/userguide/what-is-fargate.html
QUESTION NO: 549
한 회사가 멀티테넌트 Amazon S3 버킷에 고객 데이터를 저장합니다. 각 고객의 데이터는
고객 고유의 접두사에 저장됩니다. 이 회사는 특정 고객의 데이터를 새 고객으로
마이그레이션해야 합니다.
소스 버킷과 동일한 AWS 지역에 있는 전용 S3 버킷. 회사는 생성 날짜 및 버전 ID와 같은 객체
메타데이터를 보존해야 합니다.
마이그레이션이 완료되면 회사는 원래 멀티테넌트 S3 버킷에서 마이그레이션된 고객의 소스
데이터를 삭제해야 합니다.
이러한 요구 사항을 가장 적은 비용으로 충족할 수 있는 솔루션 조합은 무엇입니까? (3개를
선택하세요.)
A. 대상 버킷으로 새 S3 버킷을 만듭니다. 새 버킷에서 버전 관리를 활성화합니다.
376

IT Certification Guaranteed, The Easy Way!
B. S3 일괄 처리 작업을 사용하여 지정된 접두사에서 대상 버킷으로 객체를 복사합니다.
C. S3 CopyObject API를 사용하여 대상 S3 버킷에 데이터를 복사하는 스크립트를 만듭니다.
D. 소스 버킷의 지정된 접두사에서 대상 버킷으로 기존 데이터를 복제하도록 S3 동일 지역
복제(SRR)를 구성합니다.
E. 소스 버킷의 지정된 접두사에서 대상 버킷으로 데이터를 마이그레이션하도록 AWS
DataSync를 구성합니다.
F. 데이터가 대상 버킷으로 마이그레이션된 후 S3 수명 주기 정책을 사용하여 소스 버킷에서
객체를 삭제합니다.
Answer: A B F
Explanation:
The combination of these solutions provides an efficient and automated way to migrate data
while preserving metadata and ensuring cleanup:
* Create a new S3 bucket with versioning enabled (Option A) to preserve object metadata
like version IDs during migration.
* Use S3 batch operations (Option B) to efficiently copy data from specific prefixes in the
source bucket to the destination bucket, ensuring minimal overhead.
* Use an S3 Lifecycle policy (Option F) to automatically delete the data from the source
bucket after it has been migrated, reducing manual intervention.
* Option C (CopyObject API): This approach would require more manual scripting and effort.
* Option D (Same-Region Replication): SRR is designed for ongoing replication, not for one-
time migrations.
* Option E (DataSync): DataSync adds more complexity than necessary for this task.
AWS References:
* S3 Batch Operations
* S3 Lifecycle Policies
QUESTION NO: 550
회사는 Amazon EC2 인스턴스에서 레거시 시스템을 실행하고 있습니다. 애플리케이션
코드는 수정할 수 없으며 시스템은 둘 이상의 인스턴스에서 실행될 수 없습니다. 솔루션
설계자는 시스템 복구 시간을 향상시킬 수 있는 탄력적인 솔루션을 설계해야 합니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까?
A. EC2 인스턴스에 대한 종료 방지 기능을 활성화합니다.
B. 다중 AZ 배포를 위해 EC2 인스턴스를 구성합니다.
C. 오류 발생 시 EC2 인스턴스를 복구하기 위해 Amazon CloudWatch 경보를 생성합니다.
D. 스토리지 중복성을 위해 RAID 구성을 사용하는 두 개의 Amazon Elastic Block
Store(Amazon EBS) 볼륨이 있는 EC2 인스턴스를 시작합니다.
Answer: C
Explanation:
To design a resilient solution that can improve the recovery time for the system, a solutions
architect should recommend creating an Amazon CloudWatch alarm to recover the EC2
instance in case of failure. This solution has the following benefits:
* It allows the EC2 instance to be automatically recovered when a system status check failure
occurs, such as loss of network connectivity, loss of system power, software issues on the
physical host, or hardware issues on the physical host that impact network reachability1.
377

IT Certification Guaranteed, The Easy Way!
* It preserves the instance ID, private IP addresses, Elastic IP addresses, and all instance
metadata of the original instance. A recovered instance is identical to the original instance,
except for any data that is in- memory, which is lost during the recovery process1.
* It does not require any modification of the application code or the EC2 instance
configuration. The solutions architect can create a CloudWatch alarm using the AWS
Management Console, the AWS CLI, or the CloudWatch API2.
References:
* 1: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html
* 2: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-
recover.html#ec2-instance- recover-create-alarm
QUESTION NO: 551
회사는 프라이빗 서브넷의 Amazon EC2 인스턴스에서 애플리케이션을 실행합니다.
애플리케이션은 Amazon S3 버킷에 데이터를 저장하고 검색해야 합니다. 규제 요구 사항에
따라 데이터는 공용 인터넷을 통해 이동해서는 안 됩니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. S3 버킷에 액세스하려면 NAT 게이트웨이를 배포합니다.
B. S3 버킷에 액세스하려면 AWS Storage Gateway를 배포합니다.
C. S3 버킷에 액세스하려면 S3 인터페이스 엔드포인트를 배포합니다.
D. S3 버킷에 액세스하려면 S3 게이트웨이 엔드포인트를 배포합니다.
Answer: D
Explanation:
* Understanding the Requirement: The application running in a private subnet needs to store
and retrieve data from S3 without data traveling over the public internet.
* Analysis of Options:
* NAT Gateway: Allows private subnets to access the internet but incurs additional costs and
still routes traffic through the public internet.
* AWS Storage Gateway: Provides hybrid cloud storage solutions but is not the most cost-
effective for direct S3 access from within the VPC.
* S3 Interface Endpoint: Provides private access to S3 but is generally used for specific use
cases where more granular control is required, which might be overkill and more expensive.
* S3 Gateway Endpoint: Provides private, cost-effective access to S3 from within the VPC
without routing traffic through the public internet.
* Best Solution:
* S3 Gateway Endpoint: This option meets the requirements for secure, private access to S3
from a private subnet most cost-effectively.
References:
* Amazon VPC Endpoints
* Gateway Endpoints
QUESTION NO: 552
자전거 공유 회사는 피크 운영 시간 동안 자전거 위치를 추적하기 위해 다중 계층 아키텍처를
개발하고 있습니다. 회사는 기존 분석 플랫폼에서 이러한 데이터 포인트를 사용하려고
합니다. 솔루션 설계자는 이를 지원하기 위해 가장 실행 가능한 다중 계층 옵션을 결정해야
합니다. 아키텍처 REST API에서 데이터 포인트에 액세스할 수 있어야 합니다.
378

IT Certification Guaranteed, The Easy Way!
위치 데이터를 저장하고 검색하기 위한 이러한 요구 사항을 충족하는 작업은 무엇입니까?
A. Amazon S3와 함께 Amazon Athena 사용
B. AWS Lambda와 함께 Amazon API Gateway 사용
C. Amazon Redshift와 함께 Amazon QuickSight를 사용합니다.
D. Amazon Kinesis Data Analytics와 함께 Amazon API Gateway 사용
Answer: D
Explanation:
https://aws.amazon.com/solutions/implementations/aws-streaming-data-solution-for-amazon-
kinesis/
QUESTION NO: 553
한 회사가 대규모 Amazon EC2 인스턴스에서 애플리케이션을 실행하고 있습니다.
애플리케이션은 Amazon DynamoDB 테이블의 항목을 읽고 씁니다. DynamoDB 테이블의
크기는 지속적으로 증가하지만 애플리케이션에는 지난 30일 동안의 데이터만 필요합니다.
회사에는 비용과 개발 노력을 최소화하는 솔루션이 필요합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS CloudFormation 템플릿을 사용하여 전체 솔루션을 배포합니다. 30일마다
CloudFormation 스택을 재배포하고 원본 스택을 삭제합니다.
B. AWS Marketplace에서 모니터링 애플리케이션을 실행하는 EC2 인스턴스를 사용합니다.
테이블에 새 항목이 생성될 때 Amazon DynamoDB 스트림을 사용하여 타임스탬프를
저장하도록 모니터링 애플리케이션을 구성합니다. EC2 인스턴스에서 실행되는 스크립트를
사용하여 30일보다 오래된 타임스탬프가 있는 항목을 삭제합니다.
C. 테이블에 새 항목이 생성될 때 AWS Lambda 함수를 호출하도록 Amazon DynamoDB
스트림을 구성합니다. 테이블에서 30일이 지난 항목을 삭제하도록 Lambda 함수를
구성합니다.
D. 테이블에 생성된 각 새 항목에 현재 타임스탬프에 30일을 더한 값이 있는 속성을
추가하도록 애플리케이션을 확장합니다. 해당 속성을 TTL 속성으로 사용하도록
DynamoDB를 구성합니다.
Answer: D
Explanation:
Amazon DynamoDB Time to Live (TTL) allows you to define a per-item timestamp to
determine when an item is no longer needed. Shortly after the date and time of the specified
timestamp, DynamoDB deletes the item from your table without consuming any write
throughput. TTL is provided at no extra cost as a means to reduce stored data volumes by
retaining only the items that remain current for your workload's needs.
TTL is useful if you store items that lose relevance after a specific time. The following are
example TTL use cases:
Remove user or sensor data after one year of inactivity in an application.
Archive expired items to an Amazon S3 data lake via Amazon DynamoDB Streams and AWS
Lambda.
Retain sensitive data for a certain amount of time according to contractual or regulatory
obligations.
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html
QUESTION NO: 554
379

IT Certification Guaranteed, The Easy Way!
회사에서 웹 애플리케이션으로 새로운 비디오 게임을 개발했습니다. 애플리케이션은
데이터베이스 계층에 MySQL용 Amazon RDS가 있는 VPC의 3계층 아키텍처에 있습니다.
여러 플레이어가 온라인에서 동시에 경쟁합니다. 게임 개발자는 거의 실시간으로 상위 10개
점수판을 표시하고 현재 점수를 유지하면서 게임을 중지하고 복원할 수 있는 기능을
제공하기를 원합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 웹 애플리케이션이 표시할 점수를 캐시하도록 Memcached 클러스터용 Amazon
ElastiCache를 설정합니다.
B. Redis 클러스터용 Amazon ElastiCache를 설정하여 웹 애플리케이션이 표시할 점수를
계산하고 캐시합니다.
C. 웹 애플리케이션 앞에 Amazon CloudFront 배포를 배치하여 애플리케이션 섹션에서
점수판을 캐시합니다.
D. MySQL용 Amazon RDS에서 읽기 전용 복제본을 생성하여 스코어보드를 계산하고 웹
애플리케이션에 읽기 트래픽을 제공하는 쿼리를 실행합니다.
Answer: B
Explanation:
This answer is correct because it meets the requirements of displaying a top-10 scoreboard
in near-real time and offering the ability to stop and restore the game while preserving the
current scores. Amazon ElastiCache for Redis is a blazing fast in-memory data store that
provides sub-millisecond latency to power internet-scale real-time applications. You can use
Amazon ElastiCache for Redis to set up an ElastiCache for Redis cluster to compute and
cache the scores for the web application to display. You can use Redis data structures such
as sorted sets and hashes to store and rank the scores of the players, and use Redis
commands such as ZRANGE and ZADD to retrieve and update the scores efficiently. You
can also use Redis persistence features such as snapshots and append-only files (AOF) to
enable point-in-time recovery of your data, which can help you stop and restore the game
while preserving the current scores.
References:
* https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.html
* https://redis.io/topics/data-types
* https://redis.io/topics/persistence
QUESTION NO: 555
회사는 Application Load Balancer 뒤에 있는 Amazon EC2 인스턴스에서 중요한 비즈니스
애플리케이션을 실행하고 있습니다. EC2 인스턴스는 Auto Scaling 그룹에서 실행되고
Amazon RDS DB 인스턴스에 액세스합니다. EC2 인스턴스와 DB 인스턴스가 동일하기
때문에 설계가 운영 검토를 통과하지 못했습니다. 모두 단일 가용 영역에 위치 솔루션
설계자는 두 번째 가용 영역을 사용하도록 설계를 업데이트해야 합니다. 애플리케이션의
가용성을 높이는 솔루션은 무엇입니까?
A. 각 가용 영역에 서브넷을 프로비저닝합니다. 두 가용 영역에 걸쳐 EC2 인스턴스를
배포하도록 Auto Scaling 그룹을 구성합니다. 각 네트워크에 대한 연결을 사용하여 DB
인스턴스를 구성합니다.
B. 두 가용 영역에 걸쳐 확장되는 두 개의 서브넷 프로비저닝 두 가용 영역에 EC2 인스턴스를
배포하도록 Auto Scaling 그룹 구성 각 네트워크에 대한 연결을 사용하여 DB 인스턴스 구성
C. 각 가용 영역에 서브넷 프로비저닝 두 가용 영역에 EC2 인스턴스를 배포하도록 Auto
380

IT Certification Guaranteed, The Easy Way!
Scaling 그룹 구성 다중 AZ 배포를 위한 DB 인스턴스 구성
D. 두 가용 영역에 걸쳐 확장되는 서브넷 프로비저닝 두 가용 영역에 걸쳐 EC2 인스턴스를
배포하도록 Auto Scaling 그룹 구성 다중 AZ 배포를 위한 DB 인스턴스 구성
Answer: C
Explanation:
https://aws.amazon.com/vpc/faqs/#:~:text=Can%20a%20subnet%20span%20Availability,wit
hin%20a%
20single%20Availability%20Zone.
QUESTION NO: 556
회사에 매장에 마케팅 서비스를 제공하는 애플리케이션이 있습니다. 서비스는 매장 고객의
이전 구매를 기반으로 합니다. 상점은 SFTP를 통해 거래 데이터를 회사에 업로드하고
데이터를 처리 및 분석하여 새로운 마케팅 제안을 생성합니다. 일부 파일의 크기는 200GB를
초과할 수 있습니다.
최근에 회사는 일부 상점에서 포함되어서는 안 되는 개인 식별 정보(PII)가 포함된 파일을
업로드했음을 발견했습니다. 회사는 PII가 다시 공유될 경우 관리자에게 경고를 주기를
원합니다. 회사는 또한 문제 해결을 자동화하기를 원합니다.
최소한의 개발 노력으로 이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 해야
합니까?
A. Amazon S3 버킷을 보안 전송 지점으로 사용합니다. Amazon Inspector를 사용하여 버킷의
객체를 스캔합니다. 개체에 Pll이 포함된 경우. S3 수명 주기 정책을 트리거하여 Pll이 포함된
객체를 제거합니다.
B. Amazon S3 버킷을 보안 전송 지점으로 사용합니다. Amazon Macie를 사용하여 버킷의
객체를 스캔합니다. 개체에 Pll이 포함된 경우. Amazon Simple Notification Service(Amazon
SNS)를 사용하여 관리자에게 Pll이 포함된 객체 매트를 제거하라는 알림을 트리거합니다.
C. AWS Lambda 함수에서 사용자 지정 스캔 알고리즘을 구현합니다. 객체가 버킷에 로드될
때 함수를 트리거합니다. 객체는 Rll을 포함합니다. Amazon Simple Notification
Service(Amazon SNS)를 사용하여 관리자에게 Pll이 포함된 객체를 제거하라는 알림을
트리거합니다.
D. AWS Lambda 함수에서 사용자 지정 스캔 알고리즘을 구현합니다. 객체가 버킷에 로드될
때 함수를 트리거합니다. 개체에 Pll이 포함된 경우. Amazon Simple Email Service(Amazon
STS)를 사용하여 관리자에게 알림을 트리거하고 S3 수명 주기 정책을 트리거하여 PII가
포함된 객체를 제거합니다.
Answer: B
Explanation:
To meet the requirements of detecting and alerting the administrators when PII is shared and
automating remediation with the least development effort, the best approach would be to use
Amazon S3 bucket as a secure transfer point and scan the objects in the bucket with
Amazon Macie. Amazon Macie is a fully managed data security and data privacy service that
uses machine learning and pattern matching to discover and protect sensitive data stored in
Amazon S3. It can be used to classify sensitive data, monitor access to sensitive data, and
automate remediation actions.
In this scenario, after uploading the files to the Amazon S3 bucket, the objects can be
scanned for PII by Amazon Macie, and if it detects any PII, it can trigger an Amazon Simple
Notification Service (SNS) notification to alert the administrators to remove the objects
381

IT Certification Guaranteed, The Easy Way!
containing PII. This approach requires the least development effort, as Amazon Macie
already has pre-built data classification rules that can detect PII in various formats.
Hence, option B is the correct answer.
References:
* Amazon Macie User Guide: https://docs.aws.amazon.com/macie/latest/userguide/what-is-
macie.html
* AWS Well-Architected Framework - Security Pillar:
https://docs.aws.amazon.com/wellarchitected/latest
/security-pillar/welcome.html
QUESTION NO: 557
솔루션 아키텍트는 여러 가용 영역에 배포된 웹 애플리케이션을 위한 공유 스토리지 솔루션을
설계하고 있습니다. 웹 애플리케이션은 Auto Scaling 그룹에 속한 Amazon EC2 인스턴스에서
실행됩니다. 회사는 콘텐츠를 자주 변경할 계획입니다. 솔루션은 강력한 성능을 가져야
합니다. 변경사항이 발생하는 즉시 새 콘텐츠를 일관성 있게 반환합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까? (2개 선택)
A. 개별 EC2 인스턴스에 탑재된 AWS Storage Gateway Volume Gateway iSCSI(Internet
Small Computer Systems Interface) 블록 스토리지를 사용합니다.
B. Amazon Elastic File System(Amazon EFS) 파일 시스템 생성 개별 EC2 인스턴스에 EFS
파일 시스템 탑재
C. 공유 Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성합니다. 개별 EC2 인스턴스에
EBS 볼륨을 탑재합니다.
D. AWS DataSync를 사용하여 Auto Scaling 그룹의 EC2 호스트 간에 지속적인 데이터
동기화를 수행합니다.
E. 웹 콘텐츠를 저장할 Amazon S3 버킷을 생성합니다. Cache-Control 헤더의 메타데이터를
no-cache로 설정합니다. Amazon CloudFront를 사용하여 콘텐츠를 전달합니다.
Answer: B E
Explanation:
These options are the most suitable ways to design a shared storage solution for a web
application that is deployed across multiple Availability Zones and requires strong
consistency. Option B uses Amazon Elastic File System (Amazon EFS) as a shared file
system that can be mounted on multiple EC2 instances in different Availability Zones.
Amazon EFS provides high availability, durability, scalability, and performance for file- based
workloads. It also supports strong consistency, which means that any changes made to the
file system are immediately visible to all clients. Option E uses Amazon S3 as a shared
object store that can store the web content and serve it through Amazon CloudFront, a
content delivery network (CDN). Amazon S3 provides high availability, durability, scalability,
and performance for object-based workloads. It also supports strong consistency for read-
after-write and list operations, which means that any changes made to the objects are
immediately visible to all clients. By setting the metadata for the Cache-Control header to no-
cache, the web content can be prevented from being cached by the browsers or the CDN
edge locations, ensuring that the latest content is always delivered to the users.
Option A is not suitable because using AWS Storage Gateway Volume Gateway as a shared
storage solution for a web application is not efficient or scalable. AWS Storage Gateway
Volume Gateway is a hybrid cloud storage service that provides block storage volumes that
382

IT Certification Guaranteed, The Easy Way!
can be mounted on-premises or on EC2 instances as iSCSI devices. It is useful for migrating
or backing up data to AWS, but it is not designed for serving web content or providing strong
consistency. Moreover, using Volume Gateway would incur additional costs and complexity,
and it would not leverage the native AWS storage services.
Option C is not suitable because creating a shared Amazon EBS volume and mounting it on
multiple EC2 instances is not possible or reliable. Amazon EBS is a block storage service
that provides persistent and high- performance volumes for EC2 instances. However, EBS
volumes can only be attached to one EC2 instance at a time, and they are constrained to a
single Availability Zone. Therefore, creating a shared EBS volume for a web application that
is deployed across multiple Availability Zones is not feasible. Moreover, EBS volumes do not
support strong consistency, which means that any changes made to the volume may not be
immediately visible to other clients.
Option D is not suitable because using AWS DataSync to perform continuous
synchronization of data between EC2 hosts in the Auto Scaling group is not efficient or
scalable. AWS DataSync is a data transfer service that helps you move large amounts of
data to and from AWS storage services. It is useful for migrating or archiving data, but it is
not designed for serving web content or providing strong consistency.
Moreover, using DataSync would incur additional costs and complexity, and it would not
leverage the native AWS storage services. References:
* What Is Amazon Elastic File System?
* What Is Amazon Simple Storage Service?
* What Is Amazon CloudFront?
* What Is AWS Storage Gateway?
* What Is Amazon Elastic Block Store?
* What Is AWS DataSync?
QUESTION NO: 558
회사에는 다양한 AWS 리전의 다양한 AWS 계정에 분산된 프로덕션 워크로드가 있습니다.
회사는 AWS Cost Explorer를 사용하여 비용과 사용량을 지속적으로 모니터링합니다. 회사는
워크로드의 비용 및 사용량 지출이 비정상적인 경우 알림을 받기를 원합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.)
A. 프로덕션 워크로드가 실행 중인 AWS 계정에서 AWS Cost Management 콘솔의 Cost
Explorer를 사용하여 연결 계정 예산을 생성합니다.
B. 프로덕션 워크로드가 실행 중인 ys AWS 계정에서 AWS Cost Management 콘솔의 AWS
비용 이상 탐지를 사용하여 연결된 계정 모니터를 생성합니다.
C. 프로덕션 워크로드가 실행 중인 AWS 계정에서 AWS Cost Management 콘솔의 비용 이상
탐지를 사용하여 비용 및 사용 보고서를 생성합니다.
D. 보고서를 작성하고 이메일 메시지를 보내 매주 회사에 알립니다.
E. 필수 임계값으로 구독을 생성하고 주간 요약을 사용하여 회사에 알립니다.
Answer: B E
Explanation:
AWS Cost Anomaly Detection allows you to create monitors that track the cost and usage of
your AWS resources and alert you when there is an unusual spending pattern. You can
create monitors based on different dimensions, such as AWS services, accounts, tags, or
cost categories. You can also create alert subscriptions that notify you by email or Amazon
383

IT Certification Guaranteed, The Easy Way!
SNS when an anomaly is detected. You can specify the threshold and frequency of the
alerts, and choose to receive weekly summaries of your anomalies.
Reference URLs:
1 https://aws.amazon.com/aws-cost-management/aws-cost-anomaly-detection/
2 https://docs.aws.amazon.com/cost-management/latest/userguide/getting-started-ad.html
3 https://docs.aws.amazon.com/cost-management/latest/userguide/manage-ad.html
QUESTION NO: 559
한 제조 회사가 AWS에서 보고서 생성 애플리케이션을 실행하고 있습니다. 애플리케이션은
약 20분 안에 각 보고서를 생성합니다. 애플리케이션은 단일 Amazon EC2 인스턴스에서
실행되는 모놀리스로 구축되었습니다. 애플리케이션에는 긴밀하게 결합된 모듈을 자주
업데이트해야 합니다. 회사에서 새로운 기능을 추가하면 애플리케이션을 유지 관리하기가
복잡해집니다.
회사에서 소프트웨어 모듈을 패치할 때마다 애플리케이션에 가동 중지 시간이 발생합니다.
보고서 생성은 중단된 후에 처음부터 다시 시작되어야 합니다. 회사는 애플리케이션이
유연하고 확장 가능하며 점진적으로 개선될 수 있도록 애플리케이션을 재설계하려고 합니다.
회사는 애플리케이션 가동 중지 시간을 최소화하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 최대 프로비저닝 동시성을 갖춘 단일 함수로 AWS Lambda에서 애플리케이션을
실행합니다.
B. 스팟 집합 기본 할당 전략을 사용하여 Amazon EC2 스팟 인스턴스에서 애플리케이션을
마이크로서비스로 실행합니다.
C. 서비스 자동 조정을 통해 Amazon Elastic Container Service(Amazon ECS)에서
애플리케이션을 마이크로서비스로 실행합니다.
D. 일괄 배포 전략을 사용하여 AWS Elastic Beanstalk에서 애플리케이션을 단일
애플리케이션 환경으로 실행합니다.
Answer: C
Explanation:
The solution that will meet the requirements is to run the application on Amazon Elastic
Container Service (Amazon ECS) as microservices with service auto scaling. This solution
will allow the application to be flexible, scalable, and gradually improved, as well as minimize
application downtime. By breaking down the monolithic application into microservices, the
company can decouple the modules and update them independently, without affecting the
whole application. By running the microservices on Amazon ECS, the company can leverage
the benefits of containerization, such as portability, efficiency, and isolation. By enabling
service auto scaling, the company can adjust the number of containers running for each
microservice based on demand, ensuring optimal performance and cost. Amazon ECS also
supports various deployment strategies, such as rolling update or blue/green deployment,
that can reduce or eliminate downtime during updates.
The other solutions are not as effective as the first one because they either do not meet the
requirements or introduce new challenges. Running the application on AWS Lambda as a
single function with maximum provisioned concurrency will not meet the requirements, as it
will not break down the monolith into microservices, nor will it reduce the complexity of
maintenance. Lambda functions are also limited by execution time (15 minutes), memory size
(10 GB), and concurrency quotas, which may not be sufficient for the report generation
384

IT Certification Guaranteed, The Easy Way!
application. Running the application on Amazon EC2 Spot Instances as microservices with a
Spot Fleet default allocation strategy will not meet the requirements, as it will introduce the
risk of interruptions due to spot price fluctuations. Spot Instances are not guaranteed to be
available or stable, and may be reclaimed by AWS at any time with a two-minute warning.
This may cause report generation to fail or restart from scratch. Running the application on
AWS Elastic Beanstalk as a single application environment with an all-at-once deployment
strategy will not meet the requirements, as it will not break down the monolith into
microservices, nor will it minimize application downtime. The all-at-once deployment strategy
will deploy updates to all instances simultaneously, causing a brief outage for the application.
References:
* Amazon Elastic Container Service
* Microservices on AWS
* Service Auto Scaling - Amazon Elastic Container Service
* AWS Lambda
* Amazon EC2 Spot Instances
* [AWS Elastic Beanstalk]
QUESTION NO: 560
소셜 미디어 회사는 사용자 프로필, 관계 및 상호 작용에 대한 데이터베이스를 AWS
클라우드에 저장하려고 합니다. 회사에는 데이터베이스의 변경 사항을 모니터링하는
애플리케이션이 필요합니다. 애플리케이션은 데이터 엔터티 간의 관계를 분석하고
사용자에게 권장 사항을 제공해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon Neptune을 사용하여 정보를 저장합니다. Amazon Kinesis Data Streams를
사용하여 데이터베이스의 변경 사항을 처리합니다.
B. Amazon Neptune을 사용하여 정보를 저장합니다. Neptune Streams를 사용하여
데이터베이스의 변경 사항을 처리합니다.
C. Amazon Quantum Ledger Database(Amazon QLDB)를 사용하여 정보를 저장합니다.
Amazon Kinesis Data Streams를 사용하여 데이터베이스의 변경 사항을 처리합니다.
D. Amazon Quantum Ledger Database(Amazon QLDB)를 사용하여 정보를 저장합니다.
Neptune Streams를 사용하여 데이터베이스의 변경 사항을 처리합니다.
Answer: B
Explanation:
* Amazon Neptune: Neptune is a fully managed graph database service that is optimized for
storing and querying highly connected data. It supports both property graph and RDF graph
models, making it suitable for applications that need to analyze relationships between data
entities.
* Neptune Streams: Neptune Streams captures changes to the graph and streams these
changes to other AWS services. This is useful for applications that need to monitor and
respond to changes in real-time, such as providing recommendations based on user
interactions and relationships.
* Least Operational Overhead: Using Neptune Streams directly with Amazon Neptune
ensures that the solution is tightly integrated, reducing the need for additional components
and minimizing operational overhead. This integration simplifies the architecture by
eliminating the need for a separate service like Kinesis for change processing.
385

IT Certification Guaranteed, The Easy Way!
References:
* Amazon Neptune Documentation
* Neptune Streams Documentation
QUESTION NO: 561
한 회사에서 최근 마케팅 캠페인의 효과를 측정하려고 합니다. 회사는 판매 데이터의 csv
파일에 대해 일괄 처리를 수행하고 그 결과를 1시간에 한 번씩 Amazon S3 버킷에
저장합니다. S3 2페타바이트 규모의 객체. 회사는 Amazon Athena에서 일회성 쿼리를
실행하여 특정 지역의 특정 날짜에 어떤 제품이 가장 인기가 있는지 확인합니다. 쿼리가
실패하거나 완료하는 데 예상보다 오래 걸리는 경우가 있습니다.
쿼리 성능과 안정성을 향상시키기 위해 솔루션 설계자는 어떤 조치를 취해야 합니까? (2개를
선택하세요.)
A. S3 객체 크기를 126MB 미만으로 줄입니다.
B. Amazon S3에서 날짜 및 지역별로 데이터를 분할합니다.
C. 파일을 Amazon S3에 대규모 단일 객체로 저장합니다.
D. Amazon Kinosis Data Analytics를 사용하여 일괄 처리 작업의 팬으로 쿼리를 실행합니다.
E. AWS 듀오 ETL(추출, 변환 및 로드) 프로세스를 사용하여 csv 파일을 Apache Parquet
형식으로 변환합니다.
Answer: B E
Explanation:
https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/
This solution meets the requirements of measuring the effectiveness of marketing campaigns
by performing batch processing on csv files of sales data and storing the results in an
Amazon S3 bucket once every hour.
An AWS duo ETL process can use services such as AWS Glue or AWS Data Pipeline to
extract data from S3, transform it into a more efficient format such as Apache Parquet, and
load it back into S3. Apache Parquet is a columnar storage format that can improve the query
performance and reliability of Athena by reducing the amount of data scanned, improving
compression ratio, and enabling predicate pushdown.
QUESTION NO: 562
한 회사가 각 사업부를 위한 여러 마이크로서비스로 구성된 비생산 애플리케이션을
개발했습니다. 단일 개발팀이 모든 마이크로서비스를 유지 관리합니다.
현재 아키텍처는 정적 웹 프런트엔드와 애플리케이션 로직을 포함하는 Java 기반 백엔드를
사용합니다. 이 아키텍처는 또한 회사가 Amazon EC2 인스턴스에서 호스팅하는 MySQL
데이터베이스를 사용합니다.
회사에서는 애플리케이션이 안전하고 전 세계적으로 사용 가능한지 확인해야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. Amazon CloudFront와 AWS Amplify를 사용하여 정적 웹 프런트엔드를 호스팅합니다.
마이크로서비스를 리팩토링하여 Amazon API Gateway를 사용하여 마이크로서비스가
액세스하는 AWS Lambda 함수를 사용합니다. MySQL 데이터베이스를 Amazon EC2 예약
인스턴스로 마이그레이션합니다.
B. Amazon CloudFront와 Amazon S3를 사용하여 정적 웹 프런트엔드를 호스팅합니다.
마이크로서비스가 Amazon API Gateway를 사용하여 액세스하는 AWS Lambda 함수를
사용하도록 마이크로서비스를 리팩토링합니다. MySQL 데이터베이스를 Amazon RDS for
386

IT Certification Guaranteed, The Easy Way!
MySQL로 마이그레이션합니다.
C. Amazon CloudFront와 Amazon S3를 사용하여 정적 웹 프런트엔드를 호스팅합니다.
Network Load Balancer 뒤의 대상 그룹에 있는 AWS Lambda 함수를 사용하도록
마이크로서비스를 리팩토링합니다. MySQL 데이터베이스를 Amazon RDS for MySQL로
마이그레이션합니다.
D. Amazon S3를 사용하여 정적 웹 프런트엔드를 호스팅합니다. Application Load Balancer
뒤의 대상 그룹에 있는 AWS Lambda 함수를 사용하도록 마이크로서비스를 리팩토링합니다.
MySQL 데이터베이스를 Amazon EC2 예약 인스턴스로 마이그레이션합니다.
Answer: B
Explanation:
This solution offers the least operational overhead while meeting the security and global
availability requirements:
* Amazon CloudFront and S3: Hosting the static frontend on S3 and serving it via CloudFront
provides low-latency global distribution, high availability, and security. S3 is a cost-effective
and serverless option for hosting static assets, and CloudFront ensures that the application is
cached closer to the users, reducing latency globally.
* AWS Lambda and API Gateway: Refactoring the microservices to use Lambda functions
with API Gateway allows for a fully serverless, scalable, and highly available backend. This
reduces the need for managing EC2 instances, as Lambda automatically scales to meet
demand and only charges for the actual usage.
* RDS for MySQL: Migrating the MySQL database from an EC2 instance to Amazon RDS
significantly reduces operational overhead. RDS manages backups, patching, and scaling,
and it offers high availability options (e.g., Multi-AZ).
* Option A and D involve using EC2 Reserved Instances for the database, which requires
more operational maintenance than using RDS.
* Option C suggests using a Network Load Balancer with Lambda, which adds unnecessary
complexity for this use case.
AWS References:
* Amazon S3 and CloudFront Integration
* AWS Lambda with API Gateway
* Amazon RDS for MySQL
QUESTION NO: 563
한 회사가 AWS 클라우드에서 다중 계층 추천 웹 애플리케이션을 실행하고 있습니다.
애플리케이션은 MySQL 다중 AZ OB 인스턴스용 Amazon RDS가 있는 Amazon EC2
인스턴스에서 실행됩니다. Amazon ROS는 범용 SSD(gp3) Amazon Elastic Block
Store(Amazon EBSl 볼륨)에서 2.000GB의 스토리지를 갖춘 최신 세대 DB 인스턴스로
구성됩니다. 데이터베이스 성능은 수요가 많은 기간 동안 애플리케이션에 영향을 미칩니다.
데이터베이스 관리자는 Amazon CloudWatch Logs의 로그를 분석하고 읽기 및 쓰기 IOPS
수가 20,000보다 높으면 애플리케이션 성능이 항상 저하된다는 사실을 발견했습니다.
애플리케이션 성능을 향상시키기 위해 솔루션 아키텍트는 무엇을 해야 합니까?
A. 볼륨을 마그네틱 볼륨으로 교체합니다.
B. gp3 볼륨의 IOPS 수를 늘립니다.
C. 볼륨을 프로비저닝된 IOPS SSD(Io2) 볼륨으로 교체합니다.
D. 2.000GB gp3 볼륨을 2개의 1.000GB gp3 볼륨으로 교체
387

IT Certification Guaranteed, The Easy Way!
Answer: D
Explanation:
https://aws.amazon.com/ebs/features/ Amazon EBS provides a range of options that allow
you to optimize storage performance and cost for your workload. These options are divided
into two major categories: SSD- backed storage for transactional workloads, such as
databases and boot volumes (performance depends primarily on IOPS), and HDD-backed
storage for throughput intensive workloads, such as MapReduce and log processing
(performance depends primarily on MB/s).
QUESTION NO: 564
회사는 Amazon EC2 인스턴스에서 상태 저장 프로덕션 애플리케이션을 실행합니다.
애플리케이션이 항상 실행되려면 최소 2개의 EC2 인스턴스가 필요합니다.
솔루션 설계자는 애플리케이션에 대한 가용성이 높고 내결함성이 있는 아키텍처를 설계해야
합니다. 솔루션 아키텍트는 EC2 인스턴스의 Auto Scaling 그룹을 생성합니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자가 수행해야 하는 추가 단계는 무엇입니까?
A. Auto Scaling 그룹의 최소 용량을 2로 설정합니다. 하나의 가용 영역에 하나의 온디맨드
인스턴스를 배포하고 두 번째 가용 영역에 하나의 온디맨드 인스턴스를 배포합니다.
B. Auto Scaling 그룹의 최소 용량을 4개로 설정합니다. 하나의 가용 영역에 온디맨드
인스턴스 2개를 배포하고 두 번째 가용 영역에 온디맨드 인스턴스 2개를 배포합니다.
C. Auto Scaling 그룹의 최소 용량을 2로 설정합니다. 하나의 가용 영역에 4개의 스팟
인스턴스를 배포합니다.
D. Auto Scaling 그룹의 최소 용량을 4개로 설정합니다. 하나의 가용 영역에 온디맨드
인스턴스 2개를 배포하고 두 번째 가용 영역에 스팟 인스턴스 2개를 배포합니다.
Answer: A
Explanation:
* Understanding the Requirement: The application is stateful and requires at least two EC2
instances to be running at all times, with a highly available and fault-tolerant architecture.
* Analysis of Options:
* Minimum capacity of two with instances in separate AZs: Ensures high availability by
distributing instances across multiple AZs, fulfilling the requirement of always having two
instances running.
* Minimum capacity of four: Provides redundancy but is more than what is required and
increases cost without additional benefit.
* Spot Instances: Not suitable for a stateful application requiring guaranteed availability, as
Spot Instances can be terminated at any time.
* Combination of On-Demand and Spot Instances: Mixing instance types might provide cost
savings but does not ensure the required availability for a stateful application.
* Best Solution:
* Minimum capacity of two with instances in separate AZs: This setup ensures high
availability and meets the requirement with the least cost and complexity.
References:
* Amazon EC2 Auto Scaling
* High Availability for Amazon EC2
QUESTION NO: 565
388

IT Certification Guaranteed, The Easy Way!
회사는 단일 VPC의 Amazon EC2 인스턴스에서 고가용성 이미지 처리 애플리케이션을
실행합니다. EC2 인스턴스는 여러 가용 영역에 걸쳐 여러 서브넷 내에서 실행됩니다. EC2
인스턴스는 서로 통신하지 않습니다. 그러나 EC2 인스턴스는 단일 NAT 게이트웨이를 통해
Amazon S3에서 이미지를 다운로드하고 Amazon S3에 이미지를 업로드합니다. 회사는
데이터 전송 비용을 걱정합니다. 회사가 가장 비용 효율적인 방법은 무엇입니까? 지역 데이터
전송 요금을 피하시겠습니까?
A. 각 가용 영역에서 NAT 게이트웨이를 시작합니다.
B. NAT 게이트웨이를 NAT 인스턴스로 교체
C. Amazon S3용 게이트웨이 VPC 엔드포인트 배포
D. EC2 인스턴스를 실행하기 위해 EC2 전용 호스트를 프로비저닝합니다.
Answer: A
Explanation:
In this scenario, the company wants to avoid regional data transfer charges while
downloading and uploading images from Amazon S3. To accomplish this at the lowest cost,
the NAT gateway should be launched in each availability zone that the EC2 instances are
running in. This allows the EC2 instances to route traffic through the local NAT gateway
instead of sending traffic across an availability zone boundary and incurring regional data
transfer fees. This method will help reduce the data transfer costs since inter-Availability
Zone data transfers in a single region are free of charge.
Reference:
AWS NAT Gateway documentation: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-
nat-gateway.html
QUESTION NO: 566
솔루션 설계자는 다음 JSON 텍스트를 ID 기반 정책으로 사용하여 특정 권한을 부여하려고
합니다.
389

IT Certification Guaranteed, The Easy Way!
솔루션 설계자는 이 정책을 어떤 IAM 주체에 연결할 수 있나요? (2개를 선택하세요.)
A. 역할
B. 그룹
C. 조직
D. Amazon Elastic Container Service(Amazon ECS) 리소스
E. Amazon EC2 리소스
Answer: A B
Explanation:
This JSON text is an identity-based policy that grants specific permissions. The IAM
principals that the solutions architect can attach this policy to are Role and Group. This is
because the policy is written in JSON and is an identity-based policy, which can be attached
to IAM principals such as users, groups, and roles. Identity-based policies are permissions
policies that you attach to IAM identities (users, groups, or roles) and explicitly state what that
identity is allowed (or denied) to do1. Identity-based policies are different from resource-
based policies, which define the permissions around the specific resource1. Resource-based
policies are attached to a resource, such as an Amazon S3 bucket or an Amazon EC2
instance1. Resource- based policies can also specify a principal, which is the entity that is
allowed or denied access to the resource1. Organization is not an IAM principal, but a feature
of AWS Organizations that allows you to manage multiple AWS accounts centrally2. Amazon
ECS resource and Amazon EC2 resource are not IAM principals, but AWS resources that
can have resource-based policies attached to them34.
390

IT Certification Guaranteed, The Easy Way!
References:
* Identity-based policies and resource-based policies
* AWS Organizations
* Amazon ECS task role
* Amazon EC2 instance profile
QUESTION NO: 567
한 회사는 정기적으로 GB 크기의 파일을 Amazon S3에 업로드합니다. Ihe 회사는 파일을
업로드한 후 Amazon EC2 스팟 인스턴스 집합을 사용하여 파일 형식을 트랜스코딩합니다.
회사는 온프레미스 데이터 센터에서 Amazon S3로 데이터를 업로드할 때와 Amazon S3에서
EC2 인스턴스로 데이터를 다운로드할 때 처리량을 확장해야 합니다.
gUkicn 솔루션이 이러한 요구 사항을 충족합니까? (2개를 선택하세요.)
A. S3 버킷에 직접 액세스하는 대신 S3 버킷 액세스 포인트를 사용합니다.
B. 파일을 여러 S3 버킷에 업로드합니다.
C. S3 멀티파트 업로드를 사용합니다.
D. 객체의 여러 바이트 범위를 병렬로 가져옵니다. 페
E. 파일을 업로드할 때 각 객체에 임의의 접두사를 추가합니다.
Answer: C D
Explanation:
* Requirement Analysis: The company needs to scale throughput for uploading large files to
S3 and downloading them to EC2 instances.
* S3 Multipart Uploads: This method allows for the parallel upload of parts of a file, improving
upload efficiency and reliability.
* Parallel Fetching: Fetching multiple byte-ranges in parallel from S3 improves download
performance.
* Implementation:
* For uploads, use the S3 multipart upload API to upload files in parallel.
* For downloads, use the S3 API to request multiple byte-ranges concurrently.
* Conclusion: These solutions effectively scale throughput and improve the performance of
both uploads and downloads.
References
* S3 Multipart Upload: Amazon S3 Multipart Upload
* Parallel Fetching: S3 Byte-Range Fetches
QUESTION NO: 568
회사에서는 온프레미스 데이터 세트의 보조 복사본으로 Amazon S3를 사용하려고 합니다.
회사는 이 사본에 액세스할 필요가 거의 없습니다. 스토리지 솔루션의 비용은 최소화되어야
합니다.
이러한 요구 사항을 충족하는 스토리지 솔루션은 무엇입니까?
A. S3 표준
B. S3 지능형 계층화
C. S3 표준-Infrequent Access(S3 표준-IA)
D. S3 One Zone-Infrequent Access(S3 One Zone-IA)
Answer: D
391

IT Certification Guaranteed, The Easy Way!
Explanation:
S3 One Zone-IA is a storage class that is designed for data that is accessed less frequently,
but requires rapid access when needed. Unlike other S3 Storage Classes which store data in
a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and
costs 20% less than S3 Standard-IA. This storage class meets the requirements of the
company because it provides a low-cost solution for the secondary copy of its on-premises
dataset that would rarely need to be accessed. The other storage classes are either more
expensive or not suitable for infrequently accessed data.
https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html
QUESTION NO: 569
한 회사가 6개의 Aurora Replicas를 포함하는 Amazon Aurora MySQL DB 클러스터에서
프로덕션 워크로드를 실행합니다. 이 회사는 부서 중 하나의 실시간 보고 쿼리를 Aurora
Replicas 3개에 자동으로 분산하려고 합니다. 이 3개의 복제본은 나머지 DB 클러스터와 다른
컴퓨팅 및 메모리 사양을 가지고 있습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 워크로드에 대한 사용자 정의 엔드포인트를 생성하여 사용합니다.
B. 3개 노드 클러스터 복제본을 생성하고 리더 엔드포인트를 사용합니다.
C. 선택한 세 개의 노드에 대해 인스턴스 엔드포인트를 사용합니다.
D. 리더 엔드포인트를 사용하여 읽기 전용 워크로드를 자동으로 분산합니다.
Answer: A
Explanation:
In Amazon Aurora, a custom endpoint is a feature that allows you to create a load-balanced
endpoint that directs traffic to a specific set of instances in your Aurora DB cluster. This is
particularly useful when you want to route traffic to a subset of instances that have different
configurations or when you want to isolate specific workloads (e.g., reporting queries) to
certain instances.
* Custom Endpoint: The correct solution is to create a custom endpoint that includes the
three Aurora Replicas that the department wants to use for near-real-time reporting. This
custom endpoint will distribute the reporting queries only across the three selected replicas
with the specified compute and memory configurations, ensuring that these queries do not
affect the rest of the DB cluster.
* Other Options:
* Option B (Create a three-node cluster clone): This would create a separate cluster with its
own resources, but it is not necessary and could incur additional costs. Also, it doesn't
leverage the existing replicas.
* Option C (Use any of the instance endpoints): This would involve manually managing
connections to individual instances, which is not scalable or automatic.
* Option D (Use the reader endpoint): The reader endpoint would distribute the read queries
across all replicas in the cluster, not just the selected three. This would not meet the
requirement to limit the reporting queries to only three specific replicas.
AWS References:
* Amazon Aurora Endpoints - Provides detailed information on the different types of endpoints
available in Aurora, including custom endpoints.
* Custom Endpoints in Amazon Aurora - Specific documentation on how to create and use
392

IT Certification Guaranteed, The Easy Way!
custom endpoints to direct traffic to selected instances in an Aurora cluster.
QUESTION NO: 570
회사에서 AI(인공 지능)를 사용하여 고객 서비스 통화 품질을 확인하려고 합니다. 회사는 현재
영어를 포함하여 4개 언어로 통화를 관리합니다. 회사는 앞으로 새로운 언어를 제공할
것입니다. 회사는 기계 학습(ML) 모델을 정기적으로 유지 관리할 리소스가 없습니다.
회사는 고객 서비스 통화 녹음에서 서면 감정 분석 보고서를 작성해야 합니다. 고객 서비스
통화 녹음 텍스트는 영어로 번역되어야 합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (3개를 선택하세요.)
A. Amazon Comprehend를 사용하여 오디오 녹음을 영어로 번역하십시오.
B. Amazon Lex를 사용하여 작성된 감정 분석 보고서를 생성합니다.
C. Amazon Polly를 사용하여 오디오 녹음을 텍스트로 변환합니다.
D. Amazon Transcribe를 사용하여 모든 언어의 오디오 녹음을 텍스트로 변환합니다.
E. Amazon Translate를 사용하여 모든 언어의 텍스트를 영어로 번역합니다.
F. Amazon Comprehend를 사용하여 감정 분석 보고서를 생성합니다.
Answer: D E F
Explanation:
These answers are correct because they meet the requirements of creating written sentiment
analysis reports from the customer service call recordings in any language and translating
them into English. Amazon Transcribe is a service that uses advanced machine learning
technologies to recognize speech in audio files and transcribe them into text. You can use
Amazon Transcribe to convert the audio recordings in any language into text, and specify the
language code of the source audio. Amazon Translate is a neural machine translation service
that delivers fast, high-quality, and affordable language translation. You can use Amazon
Translate to translate text in any language to English, and specify the source and target
language codes.
Amazon Comprehend is a natural language processing (NLP) service that uses machine
learning to find insights and relationships in text. You can use Amazon Comprehend to create
the sentiment analysis reports, which determine if the text is positive, negative, neutral, or
mixed.
References:
* https://docs.aws.amazon.com/transcribe/latest/dg/what-is-transcribe.html
* https://docs.aws.amazon.com/translate/latest/dg/what-is.html
* https://docs.aws.amazon.com/comprehend/latest/dg/how-sentiment.html
QUESTION NO: 571
회사는 다양한 데이터베이스에서 가져온 배치 데이터를 생성합니다. 또한 이 회사는 네트워크
센서 및 애플리케이션 API로부터 라이브 스트림 데이터를 생성합니다. 회사는 비즈니스
분석을 위해 모든 데이터를 한 곳에 통합해야 합니다. 회사는 수신 데이터를 처리한 다음 다른
Amazon S3 버킷에 데이터를 준비해야 합니다. 팀은 나중에 일회성 쿼리를 실행하고 데이터를
비즈니스 인텔리전스 도구로 가져와 핵심 성과 지표(KPI)를 표시합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를
선택하세요.)
A. 일회성 쿼리에 Amazon Athena를 사용합니다. Amazon QuickSight를 사용하여 KPI에 대한
대시보드를 생성합니다.
393

IT Certification Guaranteed, The Easy Way!
B. 일회성 쿼리에 Amazon Kinesis Data Analytics를 사용합니다. Amazon QuickSight를
사용하여 KPI에 대한 대시보드를 생성합니다.
C. 사용자 지정 AWS Lambda 함수를 생성하여 데이터베이스의 개별 레코드를 Amazon
Redshift Duster로 이동합니다.
D. AWS Glue 추출 변환 및 Toad(ETL) 작업을 사용하여 데이터를 JSON 형식으로
변환합니다. 여러 Amazon OpenSearch Service(Amazon Elasticsearch Service) 더스터에
데이터를 로드합니다.
E. AWS Lake Formation의 블루프린트를 사용하여 데이터 레이크에 수집될 수 있는 데이터를
식별합니다. AWS Glue를 사용하여 소스를 크롤링하여 데이터를 추출하고 데이터를 Apache
Parquet 형식으로 Amazon S3에 로드합니다.
Answer: A E
Explanation:
Amazon Athena is the best choice for running one-time queries on streaming data. Although
Amazon Kinesis Data Analytics provides an easy and familiar standard SQL language to
analyze streaming data in real-time, it is designed for continuous queries rather than one-
time queries[1]. On the other hand, Amazon Athena is a serverless interactive query service
that allows querying data in Amazon S3 using SQL. It is optimized for ad- hoc querying and is
ideal for running one-time queries on streaming data[2].AWS Lake Formation uses as a
central place to have all your data for analytics purposes (E). Athena integrate perfect with
S3 and can makes queries (A).
QUESTION NO: 572
한 회사가 Amazon S3에 기밀 데이터를 저장할 준비를 하고 있습니다. 규정 준수를 위해 저장
데이터는 암호화되어야 합니다. 암호화 키 사용은 감사 목적으로 기록되어야 합니다. 열쇠는
매년 교체해야 합니다.
이러한 요구 사항을 충족하고 운영상 가장 효율적인 솔루션은 무엇입니까?
A. 고객 제공 키를 사용한 서버 측 암호화(SSE-C)
B. Amazon S3 관리형 키(SSE-S3)를 사용한 서버 측 암호화
C. 수동 교체를 통한 AWS KMS 키(SSE-KMS)를 사용한 서버 측 암호화
D. 자동 교체 기능이 있는 AWS KMS 키(SSE-KMS)를 사용한 서버 측 암호화
Answer: D
Explanation:
* SSE-KMS: Server-side encryption with AWS Key Management Service (SSE-KMS)
provides robust encryption of data at rest, integrated with AWS KMS for key management
and auditing.
* Automatic Key Rotation: By enabling automatic rotation for the KMS keys, the system
ensures that keys are rotated annually without manual intervention, meeting compliance
requirements.
* Logging and Auditing: AWS KMS automatically logs all key usage and management actions
in AWS CloudTrail, providing the necessary audit logs.
* Implementation:
* Create a KMS key with automatic rotation enabled.
* Configure the S3 bucket to use SSE-KMS with the created KMS key.
* Ensure CloudTrail is enabled for logging KMS key usage.
394

IT Certification Guaranteed, The Easy Way!
* Operational Efficiency: This solution provides encryption, automatic key management, and
auditing in a seamless, fully managed way, reducing operational overhead.
References:
* AWS KMS Automatic Key Rotation
* Amazon S3 Server-Side Encryption
QUESTION NO: 573
회사는 여러 AWS 계정에서 프로덕션 및 비프로덕션 환경 워크로드를 실행하고 있습니다.
계정은 AWS Organizations의 조직에 있습니다. 회사는 비용 사용량 태그의 수정을 방지하는
솔루션을 설계해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 승인된 보안 주체 이외의 태그 수정을 방지하기 위해 사용자 지정 AWS Config 규칙을
생성합니다.
B. 태그 수정을 방지하기 위해 AWS CloudTrail에서 사용자 지정 추적을 생성합니다.
C. 인증된 주체를 제외한 태그 수정을 방지하기 위해 서비스 제어 정책(SCP)을 만듭니다.
D. 태그 수정을 방지하기 위해 사용자 지정 Amazon CloudWatch 로그를 생성합니다.
Answer: C
Explanation:
This solution meets the requirements because it uses SCPs to restrict the actions that can be
performed on cost usage tags in the organization. SCPs are a type of organization policy that
you can use to manage permissions in your organization. SCPs specify the maximum
permissions for an organization, organizational unit (OU), or account. You can use SCPs to
enforce consistent tag policies across your organization and prevent unauthorized or
accidental changes to your tags. You can also create exceptions for authorized principals,
such as administrators or auditors, who need to modify tags for legitimate purposes.
References:
* Service control policies (SCPs) - AWS Organizations
* Tag policies - AWS Organizations
QUESTION NO: 574
회사에서 내부 감사를 실시하고 있습니다. 회사는 회사의 AWS Lake Formation 데이터
레이크와 연결된 Amazon S3 버킷의 데이터에 민감한 고객 또는 직원 데이터가 포함되지
않았는지 확인하려고 합니다. 회사는 여권 번호, 신용 카드 번호 등 개인 식별 정보(Pll)나 금융
정보를 검색하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 계정에 AWS Audit Manager를 구성합니다. 감사를 위해 PCI DSS(지불 카드 산업 데이터
보안 표준)를 선택하세요.
B. S3 버킷에 Amazon S3 인벤토리를 구성합니다. 인벤토리를 쿼리하도록 Amazon Athena를
구성합니다.
C. 필요한 데이터 유형에 대해 관리형 식별자를 사용하는 데이터 검색 작업을 실행하도록
Amazon Macie를 구성합니다.
D. Amazon S3 Select를 사용하여 S3 버킷 전체에 걸쳐 보고서를 실행합니다.
Answer: C
Explanation:
Amazon Macie is a fully managed data security and data privacy service that uses machine
395

IT Certification Guaranteed, The Easy Way!
learning and pattern matching to discover and protect your sensitive data in AWS. Macie can
run data discovery jobs that use managed identifiers for various types of PII or financial
information, such as passport numbers and credit card numbers. Macie can also generate
findings that alert you to potential issues or risks with your data.
References: https://docs.aws.amazon.com/macie/latest/userguide/macie-identifiers.html
QUESTION NO: 575
한 회사가 Amazon EC2 인스턴스에서 배치 애플리케이션을 실행하고 있습니다.
애플리케이션은 여러 Amazon RDS 데이터베이스가 포함된 백엔드로 구성됩니다.
애플리케이션으로 인해 데이터베이스에 많은 리드가 발생하고 있습니다. 솔루션 설계자는
고가용성을 보장하면서 데이터베이스 읽기 수를 줄여야 합니다.
이 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. Amazon RDS 읽기 전용 복제본 추가
B. Redis용 Amazon ElastCache 사용
C. Amazon Route 53 DNS 캐싱 사용
D. Memcached용 Amazon ElastiCache 사용
Answer: A
Explanation:
This solution meets the requirement of reducing the number of database reads while
ensuring high availability for a batch application that consists of a backend with multiple
Amazon RDS databases. Amazon RDS read replicas are copies of the primary database
instance that can serve read-only traffic. You can create one or more read replicas for a
primary database instance and connect to them using a special endpoint. Read replicas can
improve the performance and availability of your application by offloading read queries from
the primary database instance.
Option B is incorrect because using Amazon ElastiCache for Redis can provide a fast, in-
memory data store that can cache frequently accessed data, but it does not support
replication from Amazon RDS databases.
Option C is incorrect because using Amazon Route 53 DNS caching can improve the
performance and availability of DNS queries, but it does not reduce the number of database
reads. Option D is incorrect because using Amazon ElastiCache for Memcached can provide
a fast, in-memory data store that can cache frequently accessed data, but it does not support
replication from Amazon RDS databases.
References:
* https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html
QUESTION NO: 576
한 회사가 새로운 애플리케이션을 출시하고 있으며 Amazon CloudWatch 대시보드에
애플리케이션 지표를 표시합니다. 회사의 제품 관리자는 이 대시보드에 정기적으로
액세스해야 합니다. 제품 관리자에게는 AWS 계정이 없습니다. 솔루션 설계자는 최소 권한
원칙에 따라 제품 관리자에 대한 액세스를 제공해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. CloudWatch 콘솔에서 대시보드를 공유합니다. 제품 관리자의 이메일 주소를 입력하고
공유 단계를 완료하세요. 대시보드에 대한 공유 가능한 링크를 제품 관리자에게 제공합니다.
B. 제품 관리자를 위해 특별히 IAM 사용자를 생성합니다. CloudWatch 읽기 전용 액세스
396

IT Certification Guaranteed, The Easy Way!
관리형 정책을 사용자에게 연결합니다. 새 로그인 자격 증명을 제품 관리자와 공유하세요.
올바른 대시보드의 브라우저 URL을 제품 관리자와 공유하세요.
C. 회사 직원을 위한 IAM 사용자를 생성하고 보기 전용 액세스 AWS 관리형 정책을 IAM
사용자에게 연결합니다. 새 로그인 자격 증명을 제품 관리자와 공유하세요. 제품 관리자에게
CloudWatch 콘솔로 이동하여 대시보드 섹션에서 이름으로 대시보드를 찾으라고 요청하세요.
D. 퍼블릭 서브넷에 배스천 서버를 배포합니다. 제품 관리자가 대시보드에 액세스해야 하는
경우 서버를 시작하고 RDP 자격 증명을 공유하세요. 배스천 서버에서 대시보드를 볼 수 있는
적절한 권한이 있는 캐시된 AWS 자격 증명을 사용하여 대시보드 URL을 열도록 브라우저가
구성되어 있는지 확인합니다.
Answer: B
Explanation:
To provide the product manager access to the Amazon CloudWatch dashboard while
following the principle of least privilege, a solution architect should create an IAM user
specifically for the product manager and attach the CloudWatch Read Only Access managed
policy to the user. This policy allows the user to view the dashboard without being able to
make any changes to it. The solution architect should then share the new login credential
with the product manager and provide them with the browser URL of the correct dashboard.
QUESTION NO: 577
회사는 ap-southeast-3 지역의 Amazon Aurora PostgreSQL 데이터베이스에 기밀 데이터를
저장합니다. 데이터베이스는 AWS Key Management Service(AWS KMS) 고객 관리형 키로
암호화됩니다. 회사는 최근 인수되었으며 데이터베이스 백업을 안전하게 공유해야 합니다.
ap-southeast-3에 있는 인수 회사의 AWS 계정을 사용합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 데이터베이스 스냅샷 생성 스냅샷을 암호화되지 않은 새 스냅샷에 복사 새 스냅샷을 인수
회사의 AWS 계정과 공유
B. 데이터베이스 스냅샷 생성 인수 회사의 AWS 계정을 KMS 키 정책에 추가 인수 회사의
AWS 계정과 스냅샷 공유
C. 다른 AWS 관리형 KMS 키를 사용하는 데이터베이스 스냅샷을 생성합니다. 인수 회사의
AWS 계정을 KMS 키 별칭에 추가합니다. 인수 회사의 AWS 계정과 스냅샷을 공유합니다.
D. 데이터베이스 스냅샷 생성 데이터베이스 스냅샷 다운로드 Amazon S3 버킷에
데이터베이스 스냅샷 업로드 인수 회사의 AWS 계정에서 액세스를 허용하도록 S3 버킷 정책
업데이트
Answer: B
Explanation:
https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying-external-
accounts.html There's no need to create another custom AWS KMS key.
https://aws.amazon.com/premiumsupport
/knowledge-center/aurora-share-encrypted-snapshot/ Give target account access to the
custom AWS KMS key within the source account 1. Log in to the source account, and go to
the AWS KMS console in the same Region as the DB cluster snapshot. 2. Select Customer-
managed keys from the navigation pane. 3. Select your custom AWS KMS key (ALREADY
CREATED) 4. From the Other AWS accounts section, select Add another AWS account, and
then enter the AWS account number of your target account. Then: Copy and share the DB
cluster snapshot
397

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 578
솔루션 설계자는 현금 회수 서비스를 위해 Amazon API Gateway에서 REST API를 설계하고
있습니다. 애플리케이션에는 계산 리소스를 위해 1GB의 메모리와 2GB의 스토리지가
필요합니다. 애플리케이션에서는 데이터가 관계형 형식이어야 합니다.
최소한의 관리 노력으로 이러한 요구 사항을 충족하는 AWS 서비스의 추가 조합은
무엇입니까? {2개를 선택하세요.)
A. 아마존 EC2
B. AWS 람다
C. 아마존 RDS
D. Amazon DynamoDB
E. Amazon Elastic Kubernetes Services(Amazon EKS)
Answer: B C
Explanation:
AWS Lambda is a service that lets users run code without provisioning or managing servers.
It automatically scales and manages the underlying compute resources for the code. It
supports multiple languages, such as Java, Python, Node.js, and Go1. By using AWS
Lambda for the REST API, the solution can meet the requirements of 1 GB of memory and
minimal administrative effort.
Amazon RDS is a service that makes it easy to set up, operate, and scale a relational
database in the cloud. It provides cost-efficient and resizable capacity while automating time-
consuming administration tasks such as hardware provisioning, database setup, patching
and backups. It supports multiple database engines, such as MySQL, PostgreSQL, Oracle,
and SQL Server2. By using Amazon RDS for the data store, the solution can meet the
requirements of 2 GB of storage and a relational format.
A: Amazon EC2. This solution will not meet the requirement of minimal administrative effort,
as Amazon EC2 is a service that provides virtual servers in the cloud that users have to
configure and manage themselves. It requires users to choose an instance type, an operating
system, a security group, and other options3.
D: Amazon DynamoDB. This solution will not meet the requirement of a relational format, as
Amazon DynamoDB is a service that provides a key-value and document database that
delivers single-digit millisecond performance at any scale. It is a non-relational or NoSQL
database that does not support joins or transactions.
E: Amazon Elastic Kubernetes Services (Amazon EKS). This solution will not meet the
requirement of minimal administrative effort, as Amazon EKS is a service that provides a fully
managed Kubernetes service that users have to configure and manage themselves. It
requires users to create clusters, nodes groups, pods, services, and other Kubernetes
resources.
Reference URL: https://aws.amazon.com/lambda/
QUESTION NO: 579
한 회사가 AWS에서 다중 계층 재고 보고 애플리케이션을 호스팅합니다. 이 회사는 필요에
따라 재고 보고서를 생성할 수 있는 비용 효율적인 솔루션이 필요합니다. 관리자는 새
보고서를 생성할 수 있어야 합니다. 보고서를 완료하는 데 약 5~10분이 걸립니다.
애플리케이션은 각 보고서를 생성하는 관리자 사용자의 이메일 주소로 보고서를 보내야
398

IT Certification Guaranteed, The Easy Way!
합니다.
옵션:
A. Amazon Elastic Container Service(Amazon ECS)를 사용하여 보고서 생성 코드를
호스팅합니다. Amazon API Gateway HTTP API를 사용하여 코드를 호출합니다. Amazon
Simple Email Service(Amazon SES)를 사용하여 보고서를 관리자 사용자에게 보냅니다.
B. Amazon EventBridge를 사용하여 예약된 AWS Lambda 함수를 호출하여 보고서를
생성합니다. Amazon Simple Notification Service(Amazon SNS)를 사용하여 보고서를 관리자
사용자에게 보냅니다.
C. Amazon Elastic Kubernetes Service(Amazon EKS)를 사용하여 보고서 생성 코드를
호스팅합니다. Amazon API Gateway REST API를 사용하여 코드를 호출합니다. Amazon
Simple Notification Service(Amazon SNS)를 사용하여 보고서를 관리자 사용자에게 보냅니다.
D. 보고서를 생성하기 위한 AWS Lambda 함수를 만듭니다. 함수 URL을 사용하여 함수를
호출합니다. Amazon Simple Email Service(Amazon SES)를 사용하여 보고서를 관리자
사용자에게 보냅니다.
Answer: D
Explanation:
Detailed Explanation:
* A. ECS + API Gateway: Overly complex and costly for an on-demand, intermittent
workload.
* B. EventBridge + SNS: EventBridge schedules are unnecessary for on-demand generation.
* C. EKS + API Gateway: Overkill for this use case, with high operational overhead.
* D. Lambda + SES: Most cost-effective and efficient solution for generating and emailing
reports on demand.
References: AWS Lambda, Amazon SES
QUESTION NO: 580
솔루션 아키텍트는 회사가 AWS에서 애플리케이션을 실행하는 비용을 최적화하도록 도와야
합니다. 애플리케이션은 아키텍처 내 컴퓨팅을 위해 Amazon EC2 인스턴스, AWS Fargate 및
AWS Lambda를 사용합니다.
EC2 인스턴스는 애플리케이션의 데이터 수집 계층을 실행합니다. EC2 사용량은 산발적이고
예측할 수 없습니다. EC2 인스턴스에서 실행되는 워크로드는 언제든지 중단될 수 있습니다.
애플리케이션 프런트 엔드는 Fargate에서 실행되고 Lambda는 API 계층을 제공합니다.
프런트엔드 활용도와 API 레이어 활용도는 내년 동안 예측 가능합니다.
이 애플리케이션을 호스팅하는 데 가장 비용 효율적인 솔루션을 제공하는 구매 옵션 조합은
무엇입니까? (2개를 선택하세요.)
A. 데이터 수집 계층에 스팟 인스턴스를 사용합니다.
B. 데이터 수집 계층에 온디맨드 인스턴스 사용
C. 프런트엔드 및 API 계층에 대한 1년 Compute Savings Plan을 구매하세요.
D. 데이터 수집 계층을 위해 1년 전체 선결제 예약 인스턴스를 구매합니다.
E. 프런트 엔드 및 API 계층에 대한 1년 EC2 인스턴스 Savings Plan을 구매합니다.
Answer: A C
Explanation:
EC2 instance Savings Plan saves 72% while Compute Savings Plans saves 66%. But
according to link, it says
399

IT Certification Guaranteed, The Easy Way!
"Compute Savings Plans provide the most flexibility and help to reduce your costs by up to
66%. These plans automatically apply to EC2 instance usage regardless of instance family,
size, AZ, region, OS or tenancy, and also apply to Fargate and Lambda usage." EC2
instance Savings Plans are not applied to Fargate or Lambda
QUESTION NO: 581
회사에는 개발 작업을 위한 여러 AWS 계정이 있습니다. 일부 직원은 지속적으로 대형
Amazon EC2 인스턴스를 사용하므로 회사는 개발 계정의 연간 예산을 초과하게 됩니다.
회사는 이러한 계정에서 AWS 리소스 생성을 중앙에서 제한하려고 합니다. 어떤 솔루션이
최소한의 개발 노력으로 이러한 요구 사항을 충족합니까?
A. 승인된 EC2 생성 프로세스를 사용하는 AWS Systems Manager 템플릿을 개발합니다.
승인된 Systems Manager 템플릿을 사용하여 EC2 인스턴스를 프로비저닝합니다.
B. AWS Organizations를 사용하여 계정을 조직 단위(OU)로 구성합니다. 서비스 제어
정책(SCP)을 정의하고 연결하여 EC2 인스턴스 유형의 사용을 제어합니다.<C>: EC2
인스턴스가 생성될 때 AWS Lambda 함수를 호출하는
Amazon EventBrid ge 규칙을 구성합니다 . 허용되지 않는 EC2 인스턴스 유형을 중지합니다.
D. 직원이 허용되는 EC2 인스턴스 유형을 생성할 수 있도록 AWS Service Catalog 제품을
설정합니다. 직원이 Service Catalog 제품을 사용해서만 EC2 인스턴스를 배포할 수 있는지
확인하세요.
Answer: B
Explanation:
AWS Organizations is a service that helps users centrally manage and govern multiple AWS
accounts. It allows users to create organizational units (OUs) to group accounts based on
business needs or other criteria. It also allows users to define and attach service control
policies (SCPs) to OUs or accounts to restrict the actions that can be performed by the
accounts1. By using AWS Organizations, the solution can centrally restrict the creation of
AWS resources in the development accounts.
A: Develop AWS Systems Manager templates that use an approved EC2 creation process.
Use the approved Systems Manager templates to provision EC2 instances. This solution will
not meet the requirement of the least development effort, as it involves developing and
maintaining custom templates for EC2 creation, and relying on the staff to use the approved
templates instead of enforcing a restriction2.
C: Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an
EC2 instance is created. Stop disallowed EC2 instance types. This solution will not meet the
requirement of the least development effort, as it involves writing custom code for Lambda
functions, and handling events and errors for EC2 creation3.
D: Set up AWS Service Catalog products for the staff to create the allowed EC2 instance
types En-sure that staff can deploy EC2 instances only by using the Service Catalog
products. This solution will not meet the requirement of the least development effort, as it
involves setting up and managing Service Catalog products for EC2 creation, and ensuring
that staff can only use Service Catalog products instead of enforcing a restriction.
Reference URL:
https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.ht
ml
400

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 582
한 회사에서 애플리케이션 테스트 중에 Amazon RDS for MySQL DB 인스턴스를
사용했습니다. 테스트 주기가 끝날 때 DB 인스턴스를 종료하기 전에 솔루션 설계자는 두 개의
백업을 생성했습니다. 솔루션 설계자는 데이터베이스 덤프를 생성하기 위해 mysqldump
유틸리티를 사용하여 첫 번째 백업을 생성했습니다. 솔루션 설계자는 RDS 종료 시 최종 DB
스냅샷 옵션을 활성화하여 두 번째 백업을 생성했습니다.
회사는 이제 새로운 테스트 주기를 계획하고 있으며 가장 최근 백업에서 새 DB 인스턴스를
생성하려고 합니다. 이 회사는 DB 인스턴스를 호스팅하기 위해 Amazon Aurora의 MySQL
호환 에디션을 선택했습니다.
어떤 솔루션이 새 DB 인스턴스를 생성합니까? (2개를 선택하세요.)
A. RDS 스냅샷을 Aurora로 직접 가져옵니다.
B. Amazon S3에 RDS 스냅샷을 업로드합니다. 그런 다음 RDS 스냅샷을 Aurora로
가져옵니다.
C. 데이터베이스 덤프를 Amazon S3에 업로드합니다. 그런 다음 데이터베이스 덤프를
Aurora로 가져옵니다.
D. AWS Database Migration Service(AWS DMS)를 사용하여 RDS 스냅샷을 Aurora로
가져옵니다.
E. 데이터베이스 덤프를 Amazon S3에 업로드합니다. 그런 다음 AWS Database Migration
Service(AWS DMS)를 사용하여 데이터베이스 덤프를 Aurora로 가져옵니다.
Answer: A C
Explanation:
These answers are correct because they meet the requirements of creating a new DB
instance from the most recent backup and using a MySQL-compatible edition of Amazon
Aurora to host the DB instance. You can import the RDS snapshot directly into Aurora if the
MySQL DB instance and the Aurora DB cluster are running the same version of MySQL. For
example, you can restore a MySQL version 5.6 snapshot directly to Aurora MySQL version
5.6, but you can't restore a MySQL version 5.6 snapshot directly to Aurora MySQL version
5.7. This method is simple and requires the fewest number of steps. You can upload the
database dump to Amazon S3 and then import the database dump into Aurora if the MySQL
DB instance and the Aurora DB cluster are running different versions of MySQL. For
example, you can import a MySQL version
5.6 database dump into Aurora MySQL version 5.7, but you can't restore a MySQL version
5.6 snapshot directly to Aurora MySQL version 5.7. This method is more flexible and allows
you to migrate across different versions of MySQL.
References:
*
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.
RDSMySQL.Import.html
*
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Migrating.
RDSMySQL.Dump.html
QUESTION NO: 583
회사는 여러 AWS 리전 및 계정에 걸쳐 리소스를 보유하고 있습니다. 새로 고용된 솔루션
설계자는 이전 직원이 발명한 리소스에 대한 세부 정보를 제공하지 않았다는 사실을
401

IT Certification Guaranteed, The Easy Way!
발견했습니다^. 솔루션 아키텍트는 모든 계정에 걸쳐 다양한 워크로드의 관계 세부 정보를
구축하고 매핑해야 합니다.
어떤 솔루션이 운영상 가장 효율적인 방식으로 이러한 요구 사항을 충족합니까?
A. AWS Systems Manager Inventory를 사용하여 상세 보기 보고서에서 지도 보기를
생성합니다.
B. AWS Step Functions를 사용하여 워크로드 세부 정보를 수집합니다. 워크로드의 아키텍처
다이어그램을 수동으로 구축합니다.
C. AWS에서 Workload Discovery를 사용하여 워크로드의 아키텍처 다이어그램을 생성합니다
.
D. AWS X-Ray를 사용하여 워크로드 세부 정보 보기 관계가 있는 아키텍처 다이어그램 구축
Answer: C
Explanation:
Workload Discovery on AWS (formerly called AWS Perspective) is a tool that visualizes AWS
Cloud workloads. It maintains an inventory of the AWS resources across your accounts and
Regions, mapping relationships between them, and displaying them in a web UI. It also
allows you to query AWS Cost and Usage Reports, search for resources, save and export
architecture diagrams, and more1. By using Workload Discovery on AWS, the solution can
build and map the relationship details of the various workloads across all accounts with the
least operational effort.
A: Use AWS Systems Manager Inventory to generate a map view from the detailed view
report. This solution will not meet the requirement of building and mapping the relationship
details of the various workloads across all accounts, as AWS Systems Manager Inventory is
a feature that collects metadata from your managed instances and stores it in a central
Amazon S3 bucket. It does not provide a map view or architecture diagrams of the
workloads2.
B: Use AWS Step Functions to collect workload details Build architecture diagrams of the
work-loads manually. This solution will not meet the requirement of the least operational
effort, as it involves creating and managing state machines to orchestrate the workload
details collection, and building architecture diagrams manually.
D: Use AWS X-Ray to view the workload details Build architecture diagrams with
relationships. This solution will not meet the requirement of the least operational effort, as it
involves instrumenting your applications with X-Ray SDKs to collect workload details, and
building architecture diagrams manually.
Reference URL: https://aws.amazon.com/solutions/implementations/workload-discovery-on-
aws/
QUESTION NO: 584
회사는 계약 문서를 보관해야 합니다. 계약은 5년 동안 지속됩니다. 회사는 5년 동안 문서를
덮어쓰거나 삭제할 수 없도록 해야 합니다. 회사는 미사용 문서를 암호화하고 매년 암호화
키를 자동으로 교체해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하기 위해 솔루션 설계자가 수행해야 하는
단계 조합은 무엇입니까? (2개를 선택하세요.)
A. Amazon S3에 문서를 저장합니다. 거버넌스 모드에서 S3 객체 잠금을 사용합니다.
B. Amazon S3에 문서를 저장합니다. 규정 준수 모드에서 S3 객체 잠금을 사용합니다.
C. Amazon S3 관리형 암호화 키(SSE-S3)로 서버 측 암호화를 사용합니다. 키 순환을
402

IT Certification Guaranteed, The Easy Way!
구성합니다.
D. AWS Key Management Service(AWS KMS) 고객 관리형 키로 서버 측 암호화를
사용합니다.
키 순환을 구성합니다.
E. AWS Key Management Service(AWS KMS) 고객 제공(가져온) 키로 서버 측 암호화를
사용합니다. 키 순환을 구성합니다.
Answer: B D
Explanation:
Consider using the default aws/s3 KMS key if: You're uploading or accessing S3 objects
using AWS Identity and Access Management (IAM) principals that are in the same AWS
account as the AWS KMS key. You don't want to manage policies for the KMS key. Consider
using a customer managed key if: You want to create, rotate, disable, or define access
controls for the key. You want to grant cross-account access to your S3 objects. You can
configure the policy of a customer managed key to allow access from another account.
https://repost.aws/knowledge-center/s3-object-encryption-keys
QUESTION NO: 585
회사는 AWS Organizations의 조직을 사용하여 애플리케이션이 포함된 AWS 계정을
관리합니다. 회사는 조직 내에 전용 모니터링 회원 계정을 설정합니다. 회사는 Amazon
CloudWatch를 사용하여 계정 전체의 관측 가능성 데이터를 쿼리하고 시각화하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 모니터링 계정에 대해 CloudWatch 교차 계정 관찰 기능을 활성화합니다. 모니터링
계정에서 제공하는 AWS CloudFormation 템플릿을 각 AWS 계정에 배포하여 모니터링
계정과 데이터를 공유합니다.
B. 조직 루트 조직 단위(OU) 아래의 모니터링 계정에서 CloudWatch에 대한 액세스를
제공하도록 서비스 제어 정책(SCP)을 설정합니다.
C. 모니터링 계정에 새로운 1AM 사용자를 구성합니다. 각 AWS 계정에서 계정의 CloudWatch
데이터를 쿼리하고 시각화할 수 있도록 오전 1시 정책을 구성합니다. 새 I AM 사용자에게 새
오전 1시 정책을 연결합니다.
D. 모니터링 계정에 새로운 1AM 사용자를 생성합니다. 각 AWS 계정에서 교차 계정 1AM
정책을 생성합니다. 새로운 1AM 사용자에게 1AM 정책을 연결합니다.
Answer: A
Explanation:
CloudWatch cross-account observability is a feature that allows you to monitor and
troubleshoot applications that span multiple accounts within a Region. You can seamlessly
search, visualize, and analyze your metrics, logs, traces, and Application Insights
applications in any of the linked accounts without account boundaries1.
To enable CloudWatch cross-account observability, you need to set up one or more AWS
accounts as monitoring accounts and link them with multiple source accounts. A monitoring
account is a central AWS account that can view and interact with observability data shared
by other accounts. A source account is an individual AWS account that shares observability
data and resources with one or more monitoring accounts1.
To create links between monitoring accounts and source accounts, you can use the
CloudWatch console, the AWS CLI, or the AWS API. You can also use AWS Organizations
to link accounts in an organization or organizational unit to the monitoring account1.
403

IT Certification Guaranteed, The Easy Way!
CloudWatch provides a CloudFormation template that you can deploy in each source
account to share observability data with the monitoring account. The template creates a sink
resource in the monitoring account and an observability link resource in the source account.
The template also creates the necessary IAM roles and policies to allow cross-account
access to the observability data2.
Therefore, the solution that meets the requirements of the question is to enable CloudWatch
cross-account observability for the monitoring account and deploy the CloudFormation
template provided by the monitoring account in each AWS account to share the data with the
monitoring account.
The other options are not valid because:
* Service control policies (SCPs) are a type of organization policy that you can use to
manage permissions in your organization. SCPs offer central control over the maximum
available permissions for all accounts in your organization, allowing you to ensure your
accounts stay within your organization's access control guidelines3. SCPs do not provide
access to CloudWatch in the monitoring account, but rather restrict the actions that users and
roles can perform in the source accounts. SCPs are not required to enable CloudWatch
cross-account observability, as the CloudFormation template creates the necessary IAM
roles and policies for cross-account access2.
* IAM users are entities that you create in AWS to represent the people or applications that
use them to interact with AWS. IAM users can have permissions to access the resources in
your AWS account4.
Configuring a new IAM user in the monitoring account and an IAM policy in each AWS
account to have access to query and visualize the CloudWatch data in the account is not a
valid solution, as it does not enable CloudWatch cross-account observability. This solution
would require the IAM user to switch between different accounts to view the observability
data, which is not seamless and efficient. Moreover, this solution would not allow the IAM
user to search, visualize, and analyze metrics, logs, traces, and Application Insights
applications across multiple accounts in a single place1.
* Cross-account IAM policies are policies that allow you to delegate access to resources that
are in different AWS accounts that you own. You attach a cross-account policy to a user or
group in one account, and then specify which accounts the user or group can access5.
Creating a new IAM user in the monitoring account and cross-account IAM policies in each
AWS account is not a valid solution, as it does not enable CloudWatch cross-account
observability. This solution would also require the IAM user to switch between different
accounts to view the observability data, which is not seamless and efficient. Moreover, this
solution would not allow the IAM user to search, visualize, and analyze metrics, logs, traces,
and Application Insights applications across multiple accounts in a single place1.
References: CloudWatch cross-account observability, CloudFormation template for
CloudWatch cross- account observability, Service control policies, IAM users, Cross-account
IAM policies
QUESTION NO: 586
회사는 Amazon EC2 인스턴스와 Amazon Elastic Block Store(Amazon EBS) 볼륨을 사용하여
애플리케이션을 실행합니다. 회사는 규정 준수 요구 사항을 충족하기 위해 매일 각 EBS
볼륨에 대해 하나의 스냅샷을 생성합니다. 회사는 EBS 볼륨 스냅샷이 실수로 삭제되는 것을
방지하는 아키텍처를 구현하려고 합니다. 솔루션은 스토리지 관리자 사용자의 관리 권한을
404

IT Certification Guaranteed, The Easy Way!
변경해서는 안 됩니다.
최소한의 관리 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 스냅샷 삭제 권한이 있는 오전 1시 역할을 생성합니다. 새 EC2 인스턴스에 역할을
연결합니다. 스냅샷을 삭제하려면 새 EC2 인스턴스에서 AWS CLI를 사용하세요.
B. 스냅샷 삭제를 거부하는 오전 1시 정책을 만듭니다. 스토리지 관리자 사용자에게 정책을
연결합니다.
C. 스냅샷에 태그를 추가합니다. 태그가 있는 EBS 스냅샷에 대해 휴지통에 보관 규칙을
만듭니다.
D. 삭제를 방지하기 위해 EBS 스냅샷을 잠급니다.
Answer: D
Explanation:
EBS snapshots are point-in-time backups of EBS volumes that can be used to restore data or
create new volumes. EBS snapshots can be locked to prevent accidental deletion using a
feature called EBS Snapshot Lock. When a snapshot is locked, it cannot be deleted by any
user, including the root user, until it is unlocked.
The lock policy can also specify a retention period, after which the snapshot can be deleted.
This solution will meet the requirements with the least administrative effort, as it does not
require any code development or policy changes.
References:
* 1 explains how to lock and unlock EBS snapshots using EBS Snapshot Lock.
* 2 describes the concept and benefits of EBS snapshots.
QUESTION NO: 587
한 게임 회사가 AWS에서 브라우저 기반 애플리케이션을 호스팅하고 있습니다. 애플리케이션
사용자는 Amazon S3에 저장된 수많은 비디오와 이미지를 소비합니다. 본 내용은 모든
이용자에게 동일합니다.
응용 프로그램의 인기가 높아졌으며 전 세계적으로 수백만 명의 사용자가 이러한 미디어
파일에 액세스하고 있습니다. 회사는 원본에 대한 부하를 줄이면서 사용자에게 파일을
제공하려고 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 웹 서버 앞에 AWS Global Accelerator 액셀러레이터를 배포합니다.
B. S3 버킷 앞에 Amazon CloudFront 웹 배포를 배포합니다.
C. 웹 서버 앞에 Redis용 Amazon ElastiCache 인스턴스를 배포합니다.
D. 웹 서버 앞에 Memcached 인스턴스용 Amazon ElastiCache를 배포합니다.
Answer: B
Explanation:
ElastiCache, enhances the performance of web applications by quickly retrieving information
from fully- managed in-memory data stores. It utilizes Memcached and Redis, and manages
to considerably reduce the time your applications would, otherwise, take to read data from
disk-based databases. Amazon CloudFront supports dynamic content from HTTP and
WebSocket protocols, which are based on the Transmission Control Protocol (TCP) protocol.
Common use cases include dynamic API calls, web pages and web applications, as well as
an application's static files such as audio and images. It also supports on-demand media
streaming over HTTP. AWS Global Accelerator supports both User Datagram Protocol (UDP)
405

IT Certification Guaranteed, The Easy Way!
and TCP-based protocols. It is commonly used for non-HTTP use cases, such as gaming,
IoT and voice over IP. It is also good for HTTP use cases that need static IP addresses or
fast regional failover
QUESTION NO: 588
회사는 Oracle Database Enterprise Edition에서 애플리케이션을 실행합니다. 회사는
애플리케이션과 데이터베이스를 AWS로 마이그레이션해야 합니다. 회사는 AWS로
마이그레이션하는 동안 BYOL(Bring Your Own License) 모델을 사용할 수 있습니다.
애플리케이션은 권한 있는 액세스가 필요한 타사 데이터베이스 기능을 사용합니다.
솔루션 설계자는 데이터베이스 마이그레이션을 위한 솔루션을 설계해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 기본 도구를 사용하여 데이터베이스를 Oracle용 Amazon RDS로 마이그레이션합니다.
타사 기능을 AWS Lambda로 대체합니다.
B. 기본 도구를 사용하여 Oracle용 Amazon RDS Custom으로 데이터베이스를
마이그레이션합니다. 타사 기능을 지원하도록 새 데이터베이스 설정을 사용자 지정합니다.
C. AWS Database Migration Service {AWS DMS)를 사용하여 데이터베이스를 Amazon
DynamoDB로 마이그레이션합니다.
타사 기능을 지원하도록 새 데이터베이스 설정을 사용자 정의합니다.
D. AWS Database Migration Service(AWS DMS)를 사용하여 PostgreSQL용 Amazon RDS로
데이터베이스를 마이그레이션합니다. 타사 기능에 대한 종속성을 제거하려면 애플리케이션
코드를 다시 작성하세요.
Answer: B
Explanation:
* Amazon RDS Custom for Oracle: This service allows you to bring your own Oracle
Database licenses and provides the flexibility to customize the database settings, making it
suitable for applications that require privileged access and third-party database features.
* BYOL (Bring Your Own License):
* RDS Custom supports the BYOL model, allowing you to use your existing Oracle licenses
and comply with licensing requirements.
* This helps in leveraging existing investments and reducing migration costs.
* Customization and Third-Party Features:
* RDS Custom allows for deeper customization of the database environment compared to
standard RDS instances.
* This makes it possible to support the third-party features that your application relies on
without significant changes.
* Migration Process:
* Use native Oracle tools like Data Pump or RMAN to migrate the database to RDS Custom.
* Customize the database settings post-migration to ensure compatibility with third-party
features.
References:
* Amazon RDS Custom for Oracle
* Migrating to Amazon RDS Custom
QUESTION NO: 589
한 회사는 Amazon Route 53 지연 시간 기반 라우팅을 사용하여 전 세계 사용자를 위해 UDP
406

IT Certification Guaranteed, The Easy Way!
기반 애플리케이션으로 요청을 라우팅하고 있습니다. 애플리케이션은 미국에 있는 회사의
온프레미스 데이터 센터에 있는 중복 서버에서 호스팅됩니다. 아시아, 유럽. 회사의 규정 준수
요구 사항에는 애플리케이션이 온프레미스에서 호스팅되어야 한다고 명시되어 있습니다.
회사는 애플리케이션의 성능과 가용성을 개선하기를 원합니다. 솔루션 설계자는 이러한 요구
사항을 충족하기 위해 무엇을 해야 합니까?
A. A 온프레미스 엔드포인트를 처리하기 위해 3개의 AWS 리전에서 3개의 NLB(Network Load
Balancer)를 구성합니다. AWS Global Accelerator를 사용하여 액셀러레이터를 생성하고
NLB를 엔드포인트로 등록합니다. 가속기 DNS를 가리키는 CNAME을 사용하여
애플리케이션에 대한 액세스 제공
B. 온프레미스 엔드포인트를 처리하기 위해 3개의 AWS 리전에 3개의 Application Load
Balancer(ALB)를 구성합니다. AWS Global Accelerator를 사용하여 액셀러레이터를 생성하고
ALB를 엔드포인트로 등록 액셀러레이터 DNS를 가리키는 CNAME을 사용하여
애플리케이션에 대한 액세스 제공
C. 온프레미스 엔드포인트를 처리하기 위해 3개의 AWS 리전에 3개의 NLB(Network Load
Balancer)를 구성합니다. Route 53에서 3개의 NLB를 가리키는 지연 시간 기반 레코드를
생성합니다. Amazon CloudFront 배포의 오리진으로 사용 CloudFront DNS를 가리키는
CNAME을 사용하여 애플리케이션에 대한 액세스 제공
D. 온프레미스 엔드포인트를 처리하기 위해 3개의 AWS 리전에 3개의 Application Load
Balancer(ALB)를 구성합니다. Route 53에서 3개의 ALB를 가리키는 지연 시간 기반 레코드를
생성하고 이를 Amazon CloudFront 배포의 오리진으로 사용합니다. - CloudFront DNS를
가리키는 CNAME을 사용하여 애플리케이션에 대한 액세스를 제공합니다.
Answer: A
Explanation:
https://aws.amazon.com/step-
functions/#:~:text=AWS%20Step%20Functions%20is%20a,machine%
20learning%20(ML)%20pipelines.
"A common use case for AWS Step Functions is a task that requires human intervention (for
example, an approval process). Step Functions makes it easy to coordinate the components
of distributed applications as a series of steps in a visual workflow called a state machine.
You can quickly build and run state machines to execute the steps of your application in a
reliable and scalable fashion. (https://aws.amazon.com/pt/blogs
/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-
amazon-api-gateway/)"
QUESTION NO: 590
회사는 재생성할 수 없는 많은 파일을 생성하는 애플리케이션에 대해 Amazon S3 스토리지
비용을 최적화해야 합니다. 각 파일은 약 5MB이며 Amazon S3 표준 스토리지에 저장됩니다.
회사는 파일을 삭제하기 전에 4년 동안 파일을 저장해야 합니다. 파일에 즉시 액세스할 수
있어야 합니다. 파일은 객체 생성 후 처음 30일 동안 자주 액세스되지만 처음 30일 이후에는
거의 액세스되지 않습니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 객체 생성 후 30일 후에 파일을 S3 Glacier Instant Retrieval로 이동하는 S3 수명 주기
정책을 생성합니다. 객체 생성 후 4년이 지나면 파일을 삭제합니다.
B. 객체 생성 후 30일 후에 파일을 S3 One Zone-Infrequent Access(S3 One Zone-IA)로
이동하는 S3 수명 주기 정책을 생성합니다. 객체 생성 후 4년이 지나면 파일을 삭제합니다.
407

IT Certification Guaranteed, The Easy Way!
C. 객체 생성 후 30일 후에 파일을 S3 Standard-Infrequent Access(S3 Standard-IA)로
이동하는 S3 수명 주기 정책을 생성합니다. 객체 생성 후 4년이 지나면 파일을 삭제합니다.
D. 객체 생성 후 30일 후에 파일을 S3 Standard-Infrequent Access(S3 Standard-IA)로
이동하는 S3 수명 주기 정책을 생성합니다. 객체 생성 후 4년이 지나면 파일을 S3 Glacier
유연한 검색으로 이동합니다.
Answer: C
Explanation:
* Amazon S3 Standard-IA: This storage class is designed for data that is accessed less
frequently but requires rapid access when needed. It offers lower storage costs compared to
S3 Standard while still providing high availability and durability.
* Access Patterns: Since the files are frequently accessed in the first 30 days and rarely
accessed afterward, transitioning them to S3 Standard-IA after 30 days aligns with their
access patterns and reduces storage costs significantly.
* Lifecycle Policy: Implementing a lifecycle policy to transition the files to S3 Standard-IA
ensures automatic management of the data lifecycle, moving files to a lower-cost storage
class without manual intervention. Deleting the files after 4 years further optimizes costs by
removing data that is no longer needed.
References:
* Amazon S3 Storage Classes
* S3 Lifecycle Configuration
QUESTION NO: 591
한 회사에서 여러 AWS 계정에 대한 로깅 솔루션을 구축하려고 합니다. 회사는 현재 모든
계정의 로그를 중앙 집중식 계정에 저장합니다. 회사는 VPC 흐름 로그와 AWS CloudTrail
로그를 저장하기 위해 중앙 집중식 계정에 Amazon S3 버킷을 생성했습니다. 모든 로그는
빈번한 분석을 위해 30일 동안 가용성이 높아야 하고, 백업 목적으로 추가로 60일 동안
보관되어야 하며, 생성 후 90일 후에 삭제되어야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 생성 후 30일이 지나면 객체를 S3 Standard 스토리지 클래스로 전환합니다. 90일 후에
객체를 삭제하도록 Amazon S3에 지시하는 만료 작업을 작성합니다.
B. 생성 후 30일이 지나면 S3 Standard-Infrequent Access(S3 Standard-IA) 스토리지
클래스로 객체를 전환합니다. 90일 후에 모든 객체를 S3 Glacier 유연한 검색 스토리지
클래스로 이동합니다. 90일 후에 객체를 삭제하도록 Amazon S3에 지시하는 만료 작업을
작성합니다.
C. 생성 후 30일이 지나면 객체를 S3 Glacier 유연한 검색 스토리지 클래스로 전환합니다.
Amazon S3에 90일 후에 객체를 삭제하도록 지시하는 만료 작업을 작성합니다.
D. 생성 후 30일이 지나면 객체를 S3 One Zone-Infrequent Access(S3 One Zone-IA) 스토리지
클래스로 전환합니다. 90일 후에 모든 객체를 S3 Glacier 유연한 검색 스토리지 클래스로
이동합니다. 90일 후에 객체를 삭제하도록 Amazon S3에 지시하는 만료 작업을 작성합니다.
Answer: D
Explanation:
* Understanding the Requirement: The company needs logs to be highly available for 30
days for frequent analysis, retained for an additional 60 days for backup, and deleted after 90
days.
* Analysis of Options:
408

IT Certification Guaranteed, The Easy Way!
* Transition to S3 Standard after 30 days: Keeps logs in the same high-availability storage,
not cost- effective.
* Transition to S3 Standard-IA, then Glacier Flexible Retrieval after 90 days: Adds
unnecessary cost and complexity since objects need to be accessible for only 30 days and
then retained for 60 days.
* Transition to Glacier Flexible Retrieval after 30 days: Not suitable for frequent access
required in the first 30 days.
* Transition to S3 One Zone-IA after 30 days, then Glacier Flexible Retrieval: Provides cost-
effective storage for infrequently accessed logs after the initial 30-day period, then moves to
the cheapest long-term storage before deletion.
* Best Solution:
* Transition to S3 One Zone-IA after 30 days, then Glacier Flexible Retrieval: This solution
meets the requirements for high availability, cost-effective storage for backup, and scheduled
deletion with the least cost.
References:
* Amazon S3 Storage Classes
* Managing your storage lifecycle
QUESTION NO: 592
한 회사는 최근 메시지 처리 시스템을 AWS로 마이그레이션했습니다. 시스템은 Amazon EC2
인스턴스에서 실행되는 ActiveMQ 대기열로 메시지를 수신합니다. 메시지는 Amazon
EC2에서 실행되는 소비자 애플리케이션에 의해 처리됩니다. 소비자 애플리케이션은
메시지를 처리하고 결과를 Amazon EC2의 MySQL 데이터베이스에 기록합니다. 회사는 이
애플리케이션이 운영상의 복잡성과 함께 가용성이 높기를 원합니다. 가장 높은 가용성을
제공하는 아키텍처는 무엇입니까?
A. 다른 가용 영역에 두 번째 ActiveMQ 서버 추가 다른 가용 영역에 추가 소비자 EC2
인스턴스를 추가합니다. MySQL 데이터베이스를 다른 가용 영역에 복제합니다.
B. 두 개의 가용 영역에 걸쳐 구성된 활성/대기 브로커와 함께 Amazon MO를 사용합니다.
다른 가용 영역에 추가 소비자 EC2 인스턴스를 추가합니다. MySQL 데이터베이스를 다른
가용 영역에 복제합니다.
C. 두 개의 가용 영역에 걸쳐 구성된 활성/대기 블로터와 함께 Amazon MO를 사용합니다.
다른 가용 영역에 추가 소비자 EC2 인스턴스를 추가합니다. 다중 AZ가 활성화된 Amazon
ROS 또는 MySQL을 사용합니다.
D. 두 개의 가용 영역에 걸쳐 구성된 활성/대기 브로커와 함께 Amazon MQ를 사용합니다. 두
개의 가용 영역에 걸쳐 소비자 EC2 인스턴스에 대한 Auto Scaling 그룹을 추가합니다. 다중
AZ가 활성화된 MySQL용 Amazon RDS를 사용합니다.
Answer: D
Explanation:
Amazon S3 is a highly scalable and durable object storage service that can store and retrieve
any amount of data from anywhere on the web1. Users can configure the application to
upload images directly from each user's browser to Amazon S3 through the use of a
presigned URL. A presigned URL is a URL that gives access to an object in an S3 bucket for
a limited time and with a specific action, such as uploading an object2.
Users can generate a presigned URL programmatically using the AWS SDKs or AWS CLI.
By using a presigned URL, users can reduce coupling within the application and improve
409

IT Certification Guaranteed, The Easy Way!
website performance, as they do not need to send the images to the web server first.
AWS Lambda is a serverless compute service that runs code in response to events and
automatically manages the underlying compute resources3. Users can configure S3 Event
Notifications to invoke an AWS Lambda function when an image is uploaded. S3 Event
Notifications is a feature that allows users to receive notifications when certain events
happen in an S3 bucket, such as object creation or deletion. Users can configure S3 Event
Notifications to invoke a Lambda function that resizes the image and stores it back in the
same or a different S3 bucket. This way, users can offload the image resizing task from the
web server to Lambda.
QUESTION NO: 593
회사는 하이브리드 네트워크 아키텍처를 설계해야 합니다. 회사의 워크로드는 현재 AWS
클라우드와 온프레미스 데이터 센터에 저장되어 있습니다. 워크로드의 통신을 위해 한 자릿수
지연 시간이 필요합니다. 회사는 AWS Transit Gateway 전송 게이트웨이를 사용하여 여러
VPC를 연결합니다. 어떤 조합을 사용합니까? 단계 중 가장 비용 효율적으로 이러한 요구
사항을 충족할 수 있는 단계는 무엇입니까? (2개를 선택하세요.)
A. 각 VPC에 대한 AWS Site-to-Site VPN 연결을 설정합니다.
B. AWS Direct Connect 게이트웨이를 VPC에 연결된 전송 게이트웨이와 연결합니다.
C. AWS Direct Connect 게이트웨이에 대한 AWS Site-to-Site VPN 연결을 설정합니다.
D. AWS Direct Connect 연결을 설정합니다. Direct Connect 게이트웨이에 대한 VIF(전송 가상
인터페이스)를 생성합니다.
E. AWS Site-to-Site VPN 연결을 VPC에 연결된 전송 게이트웨이와 연결합니다.
Answer: B D
Explanation:
* AWS Direct Connect: Provides a dedicated network connection from your on-premises data
center to AWS, ensuring low latency and consistent network performance.
* Direct Connect Gateway Association:
* Direct Connect Gateway: Acts as a global network transit hub to connect VPCs across
different AWS regions.
* Association with Transit Gateway: Enables communication between on-premises data
centers and multiple VPCs connected to the transit gateway.
* Transit Virtual Interface (VIF):
* Create Transit VIF: To connect Direct Connect with a transit gateway.
* Setup Steps:
* Establish a Direct Connect connection.
* Create a transit VIF to the Direct Connect gateway.
* Associate the Direct Connect gateway with the transit gateway attached to the VPCs.
* Cost Efficiency: This combination avoids the recurring costs and potential performance
variability of VPN connections, providing a robust, low-latency hybrid network solution.
References:
* AWS Direct Connect
* Transit Gateway and Direct Connect Gateway
QUESTION NO: 594
회사는 Amazon EC2 인스턴스에서 고성능 컴퓨팅(HPC) 워크로드를 실행할 계획입니다.
410

IT Certification Guaranteed, The Easy Way!
워크로드에는 긴밀하게 결합된 노드 간 통신을 통해 지연 시간이 짧은 네트워크 성능과 높은
네트워크 처리량이 필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 클러스터 배치 그룹의 일부가 되도록 EC2 인스턴스를 구성합니다.
B. 전용 인스턴스 테넌시를 사용하여 EC2 인스턴스를 시작합니다.
C. EC2 인스턴스를 스팟 인스턴스로 시작합니다.
D. EC2 인스턴스가 시작될 때 온디맨드 용량 예약을 구성합니다.
Answer: A
Explanation:
* Cluster Placement Group: This type of placement group is designed to provide low-latency
network performance and high throughput by grouping instances within a single Availability
Zone. It is ideal for applications that require tightly coupled node-to-node communication.
* Configuration:
* When launching EC2 instances, specify the option to launch them in a cluster placement
group.
* This ensures that the instances are physically located close to each other, reducing latency
and increasing network throughput.
* Benefits:
* Low-Latency Communication: Instances in a cluster placement group benefit from
enhanced networking capabilities, enabling low-latency communication.
* High Network Throughput: The network performance within a cluster placement group is
optimized for high throughput, which is essential for HPC workloads.
References:
* Placement Groups
* High Performance Computing on AWS
QUESTION NO: 595
회사에서 내부 브라우저 기반 애플리케이션을 실행합니다. 애플리케이션은 Application Load
Balancer 뒤의 Amazon EC2 인스턴스에서 실행됩니다. 인스턴스는 여러 가용 영역에 걸쳐
Amazon EC2 Auto Scaling 그룹에서 실행됩니다. Auto Scaling 그룹은 업무 시간 동안 최대
20개의 인스턴스를 확장하고 밤새 2개의 인스턴스로 축소합니다. 직원들은 애플리케이션이
오전 중반까지는 잘 실행되지만 하루가 시작될 때 애플리케이션이 매우 느리다고 불평합니다.
직원 불만을 해결하고 비용을 최소로 유지하려면 스케일링을 어떻게 변경해야합니까?
A. 사무실 개장 직전에 원하는 용량을 20으로 설정하는 예약된 작업을 구현합니다.
B. 더 낮은 CPU 임계값에서 트리거되는 단계 조정 작업을 구현하고 휴지 기간을 줄입니다.
C. 낮은 CPU 임계값에서 트리거되는 대상 추적 작업을 구현하고 휴지 기간을 줄입니다.
D. 사무실 개장 직전에 최소 및 최대 수용 인원을 20명으로 설정하는 예약된 작업을
구현합니다.
Answer: C
Explanation:
This option will scale up capacity faster in the morning to improve performance, but will still
allow capacity to scale down during off hours. It achieves this as follows: * A target tracking
action scales based on a CPU utilization target. By triggering at a lower CPU threshold in the
morning, the Auto Scaling group will start scaling up sooner as traffic ramps up, launching
411

IT Certification Guaranteed, The Easy Way!
instances before utilization gets too high and impacts performance. * Decreasing the
cooldown period allows Auto Scaling to scale more aggressively, launching more instances
faster until the target is reached. This speeds up the ramp-up of capacity. * However, unlike a
scheduled action to set a fixed minimum/maximum capacity, with target tracking the group
can still scale down during off hours based on demand. This helps minimize costs.
QUESTION NO: 596
한 회사에는 고객의 주문을 처리하는 3계층 웹 애플리케이션이 있습니다. 웹 계층은
Application Load Balancer 뒤에 있는 Amazon EC2 인스턴스로 구성됩니다. 처리 계층은 EC2
인스턴스로 구성됩니다.
이 회사는 Amazon Simple Queue Service(Amazon SQS)를 사용하여 웹 계층과 처리 계층을
분리했습니다. 스토리지 계층은 Amazon DynamoDB를 사용합니다.
피크 타임에 일부 사용자는 주문 처리 지연 및 중단을 보고합니다. 이 회사는 이러한 지연 중에
EC2 인스턴스가 100% CPU 사용률로 실행되고 SQS 대기열이 가득 찬다는 것을
알아챘습니다. 피크 타임은 가변적이고 예측할 수 없습니다.
회사는 애플리케이션의 성능을 개선해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon EC2 자동 확장에 예약된 확장을 사용하여 피크 사용 시간 동안 처리 계층
인스턴스를 확장합니다. CPU 사용률 메트릭을 사용하여 확장 시기를 결정합니다.
B. DynamoDB 백엔드 티어 앞에 Amazon ElastiCache for Redis를 사용합니다. 타겟 활용도를
메트릭으로 사용하여 확장 시기를 결정합니다.
C. 웹 계층에 대한 응답을 캐시하기 위해 Amazon CloudFront 배포를 추가합니다. HTTP 대기
시간을 척도로 사용하여 확장 시기를 결정합니다.
D. Amazon EC2 자동 확장 대상 추적 정책을 사용하여 처리 계층 인스턴스를 확장합니다.
ApproximateNumberOfMessages 속성을 사용하여 확장 시기를 결정합니다.
Answer: D
Explanation:
The issue in this case is related to the processing tier, where EC2 instances are
overwhelmed at peak times, causing delays. Option D, using an Amazon EC2 Auto Scaling
target tracking policy based on the ApproximateNumberOfMessages in the SQS queue, is
the best solution.
* Auto Scaling with Target Tracking:
* Target tracking policies dynamically scale out or in based on a specific metric. For this use
case, you can monitor the ApproximateNumberOfMessages in the SQS queue. When the
number of messages (orders) in the queue increases, the Auto Scaling group will scale out
more EC2 instances to handle the additional load, ensuring that the queue doesn't build up
and cause delays.
* This solution is ideal for handling variable and unpredictable peak times, as Auto Scaling
can automatically adjust based on real-time load rather than scheduled times.
* Why Not the Other Options?:
* Option A (Scheduled Scaling): Scheduled scaling works well for predictable peak times, but
this company experiences unpredictable peak usage, making scheduled scaling less
effective.
* Option B (ElastiCache for Redis): Adding a caching layer would help if DynamoDB were the
bottleneck, but in this case, the issue is CPU overload on EC2 instances in the processing
412

IT Certification Guaranteed, The Easy Way!
tier.
* Option C (CloudFront): CloudFront would help cache static content from the web tier, but it
wouldn't resolve the issue of the processing tier's overloaded EC2 instances.
AWS References:
* Amazon EC2 Auto Scaling Target Tracking
* Amazon SQS ApproximateNumberOfMessages
QUESTION NO: 597
회사는 AWS Organizations를 사용하여 다양한 부서의 여러 AWS 계정을 관리합니다. 마스터
계정에는 프로젝트 보고서가 포함된 Amazon S3 버킷이 있습니다. 회사는 이 S3 버킷에 대한
액세스를 AWS Organizations의 조직 내 계정 사용자로만 제한하려고 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 조직 ID에 대한 참조와 함께 aws:PrincipalOrgID 전역 조건 키를 S3 버킷 정책에
추가합니다.
B. 각 부서에 대한 조직 단위(OU)를 만듭니다. S3 버킷 정책에 aws:PrincipalOrgPaths 전역
조건 키를 추가합니다.
C. AWS CloudTrail을 사용하여 CreateAccount, InviteAccountToOrganization,
LeaveOrganization 및 RemoveAccountFromOrganization 이벤트를 모니터링합니다. 이에
따라 S3 버킷 정책을 업데이트하십시오.
D. S3 버킷에 액세스해야 하는 각 사용자에게 태그를 지정합니다. S3 버킷 정책에
aws:PrincipalTag 전역 조건 키를 추가합니다.
Answer: A
Explanation:
https://aws.amazon.com/blogs/security/control-access-to-aws-resources-by-using-the-aws-
organization-of- iam-principals/ The aws:PrincipalOrgID global key provides an alternative to
listing all the account IDs for all AWS accounts in an organization. For example, the following
Amazon S3 bucket policy allows members of any account in the XXX organization to add an
object into the examtopics bucket.
{"Version": "2020-09-10",
"Statement": {
"Sid": "AllowPutObject",
"Effect": "Allow",
"Principal": "*",
"Action": "s3:PutObject",
"Resource": "arn:aws:s3:::examtopics/*",
"Condition": {"StringEquals":
{"aws:PrincipalOrgID":["XXX"]}}}}
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html
QUESTION NO: 598
한 회사가 VPC 내부의 여러 Amazon EC2 인스턴스에 애플리케이션을 호스팅합니다. 이
회사는 각 고객이 Amazon S3에 관련 정보를 저장하도록 전용 Amazon S3 버킷을 만듭니다.
회사에서는 EC2 인스턴스에서 실행되는 애플리케이션이 회사의 AWS 계정에 속한 S3
버킷에만 안전하게 액세스할 수 있도록 하려고 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
413

IT Certification Guaranteed, The Easy Way!
A. VPC에 연결된 Amazon S3에 대한 게이트웨이 엔드포인트를 생성합니다. 애플리케이션에
필요한 특정 버킷에만 액세스를 제공하도록 오전 1시 인스턴스 프로필 정책을
업데이트합니다.
B. Amazon S3에만 액세스할 수 있는 보안 그룹이 있는 공용 서브넷에 NAT 게이트웨이를
만듭니다. NAT 게이트웨이를 사용하도록 경로 테이블을 업데이트합니다.
C. VPC에 연결된 Amazon S3에 대한 게이트웨이 엔드포인트를 생성합니다. Deny 작업과
다음 조건 키로 1AM 인스턴스 프로필 정책을 업데이트합니다.
D. 공용 서브넷에 NAT 게이트웨이를 만듭니다. NAT 게이트웨이를 사용하도록 경로 테이블을
업데이트합니다. 거부 작업과 다음 조건 키를 사용하여 모든 버킷에 대한 버킷 정책을
지정합니다.
Answer: A
QUESTION NO: 599
회사는 여러 가용 영역에 배포된 Amazon RDS 인스턴스에서 실행되는 데이터베이스를
호스팅합니다. 회사는 정기적으로 데이터베이스에 대해 스크립트를 실행하여 데이터베이스에
추가된 새 항목을 보고합니다. 데이터베이스에 대해 실행되는 스크립트는 중요한
애플리케이션의 성능에 부정적인 영향을 미칩니다. 회사는 최소한의 비용으로 애플리케이션
성능을 향상시켜야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 활성 연결이 가장 적은 인스턴스를 식별하는 기능을 스크립트에 추가합니다. 해당
인스턴스에서 읽어 전체 새 항목을 보고하도록 스크립트를 구성합니다.
B. 데이터베이스의 읽기 전용 복제본을 생성합니다. 총 새 항목을 보고하기 위해 읽기 전용
복제본만 쿼리하도록 스크립트를 구성합니다.
C. 하루가 끝날 때마다 데이터베이스의 해당 날짜에 대한 새 항목을 수동으로 내보내도록 개발
팀에 지시합니다.
D. Amazon ElastiCache를 사용하여 스크립트가 데이터베이스에 대해 실행하는 일반 쿼리를
캐시합니다.
Answer: B
Explanation:
A read replica is a copy of the primary database that supports read-only queries. By creating
a read replica, you can offload the read workload from the primary database and improve its
performance. The script can query the read replica without affecting the critical application
that uses the primary database. This solution also has the least operational overhead, as you
do not need to modify the script, export the data manually, or manage a cache cluster.
References:
* Working with PostgreSQL, MySQL, and MariaDB Read Replicas
* Amazon RDS Performance Insights
QUESTION NO: 600
회사에는 매일 수백 개의 파일을 생성하는 온프레미스 비즈니스 애플리케이션이 있습니다.
이러한 파일은 SMB 파일 공유에 저장되며 애플리케이션 서버에 대한 지연 시간이 짧은
연결이 필요합니다. 새로운 회사 정책에는 애플리케이션에서 생성된 모든 파일을 AWS에
복사해야 한다고 명시되어 있습니다. 이미 AWS에 대한 VPN 연결이 있습니다.
애플리케이션 개발 팀은 애플리케이션을 AWS로 이동하는 데 필요한 코드를 수정할 시간이
없습니다. 애플리케이션이 AWS로 파일을 복사할 수 있도록 솔루션 아키텍트는 어떤
414

IT Certification Guaranteed, The Easy Way!
서비스를 권장해야 합니까?
A. Amazon Elastic File System(Amazon EFS)
B. Windows 파일 서버용 Amazon FSx
C. AWS Snowball
D. AWS 스토리지 게이트웨이
Answer: D
Explanation:
* Understanding the Requirement: The company needs to copy files generated by an on-
premises application to AWS without modifying the application code. The files are stored on
an SMB file share and require a low-latency connection to the application servers.
* Analysis of Options:
* Amazon Elastic File System (EFS): EFS is designed for Linux-based workloads and does
not natively support SMB file shares.
* Amazon FSx for Windows File Server: FSx supports SMB file shares but would require
changes to the application or additional infrastructure to connect on-premises systems.
* AWS Snowball: Suitable for large data transfers but not for continuous, low-latency file
copying.
* AWS Storage Gateway: Provides a hybrid cloud storage solution, supporting SMB file
shares and enabling seamless copying of files to AWS without requiring changes to the
application.
* Best Solution:
* AWS Storage Gateway: This service meets the requirement for a low-latency, seamless file
transfer solution from on-premises to AWS without modifying the application code.
References:
* AWS Storage Gateway
* Amazon FSx for Windows File Server
QUESTION NO: 601
회사는 의료 애플리케이션의 데이터를 저장해야 합니다. 애플리케이션의 데이터는 자주
변경됩니다. 새로운 규정은 저장된 데이터의 모든 수준에서 감사 z 액세스를 요구합니다.
회사는 스토리지 용량이 부족한 온프레미스 인프라에서 애플리케이션을 호스팅합니다.
솔루션 설계자는 새로운 규정을 만족하면서 기존 데이터를 AWS로 안전하게
마이그레이션해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS DataSync를 사용하여 기존 데이터를 Amazon S3로 이동합니다. AWS CloudTrail을
사용하여 데이터 이벤트를 기록합니다.
B. AWS Snowcone을 사용하여 기존 데이터를 Amazon $3로 이동합니다. AWS CloudTrail을
사용하여 관리 이벤트를 기록합니다.
C. Amazon S3 Transfer Acceleration을 사용하여 기존 데이터를 Amazon S3로 이동합니다.
AWS CloudTrail을 사용하여 데이터 이벤트를 기록합니다.
D. AWS Storage Gateway를 사용하여 기존 데이터를 Amazon S3로 이동합니다. AWS
CloudTrail을 사용하여 관리 이벤트를 기록합니다.
Answer: A
Explanation:
415

IT Certification Guaranteed, The Easy Way!
This answer is correct because it meets the requirements of securely migrating the existing
data to AWS and satisfying the new regulation. AWS DataSync is a service that makes it
easy to move large amounts of data online between on-premises storage and Amazon S3.
DataSync automatically encrypts data in transit and verifies data integrity during transfer.
AWS CloudTrail is a service that records AWS API calls for your account and delivers log
files to Amazon S3. CloudTrail can log data events, which show the resource operations
performed on or within a resource in your AWS account, such as S3 object-level API activity.
By using CloudTrail to log data events, you can audit access at all levels of the stored data.
References:
* https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html
* https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-
cloudtrail.html
QUESTION NO: 602
회사에는 Amazon API Gateway에서 호출하는 AWS Lambda 함수에서 실행되는 상태 비저장
웹 애플리케이션이 있습니다. 회사 v는 지역 장애 조치 기능을 제공하기 위해 여러 AWS
지역에 애플리케이션을 배포하려고 합니다.
트래픽을 여러 지역으로 라우팅하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 각 지역에 대한 Amazon Route 53 상태 확인을 생성합니다. 활성-활성 장애 조치 구성을
사용합니다.
B. 각 리전에 대한 오리진이 있는 Amazon CloudFront 배포를 생성합니다. CloudFront 상태
확인을 사용하여 트래픽을 라우팅합니다.
C. 전송 게이트웨이를 생성합니다. Transit Gateway를 각 리전의 API Gateway 엔드포인트에
연결합니다. 요청을 라우팅하도록 전송 게이트웨이를 구성합니다.
D. 기본 리전에서 Application Load Balancer를 생성합니다. 각 리전의 API 게이트웨이
엔드포인트 호스트 이름을 가리키도록 대상 그룹을 설정합니다.
Answer: C
Explanation:
This answer is correct because it provides Regional failover capabilities for the online gaming
application by using AWS Global Accelerator. AWS Global Accelerator is a networking
service that helps you improve the availability, performance, and security of your public
applications. Global Accelerator provides two global static public IPs that act as a fixed entry
point to your application endpoints, such as NLBs, in different AWS Regions. Global
Accelerator uses the AWS global network to route traffic to the optimal regional endpoint
based on health, client location, and policies that you configure. Global Accelerator also
terminates TCP and UDP traffic at the edge locations, which reduces the number of hops and
improves the network performance.
By adding AWS Global Accelerator in front of the NLBs, you can achieve Regional failover for
your online gaming application.
References:
* https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html
* https://aws.amazon.com/global-accelerator/
QUESTION NO: 603
회사는 AWS CloudTrail 로그를 3년 동안 보관해야 합니다. 회사는 상위 계정의 AWS
416

IT Certification Guaranteed, The Easy Way!
Organizations를 사용하여 일련의 AWS 계정에 걸쳐 CloudTrail을 시행하고 있습니다.
CloudTrail 대상 S3 버킷은 S3 버전 관리가 활성화된 상태로 구성됩니다. 3년 후에 현재
객체를 삭제하는 S3 수명 주기 정책이 마련되어 있습니다.
S3 버킷을 사용한 지 4년이 지난 후 S3 버킷 지표를 보면 객체 수가 계속해서 증가한 것으로
나타났습니다. 그러나 S3 버킷으로 전달되는 새로운 CloudTrail 로그 수는 일관되게
유지되었습니다.
가장 비용 효과적인 방식으로 3년이 넘은 객체를 삭제하는 솔루션은 무엇입니까?
A. 3년 후에 객체가 만료되도록 조직의 중앙 집중식 CloudTrail 추적을 구성합니다.
B. 현재 버전은 물론 이전 버전도 삭제하도록 S3 수명 주기 정책을 구성합니다.
C. Amazon S3에서 3년이 넘은 객체를 열거하고 삭제하는 AWS Lambda 함수를 생성합니다.
D. 상위 계정을 S3 버킷에 전달되는 모든 객체의 소유자로 구성합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/awscloudtrail/latest/userguide/best-practices-
security.html#:~:text=The%
20CloudTrail%20trail,time%20has%20passed.
QUESTION NO: 604
한 회사가 VPC에서 비디오 스트리밍 웹 애플리케이션을 호스팅합니다. 이 회사는 네트워크
로드 밸런서(NLB)를 사용하여 실시간 데이터 처리를 위한 TCP 트래픽을 처리합니다.
애플리케이션에 대한 무단 액세스 시도가 있었습니다.
회사에서는 최소한의 아키텍처 변경으로 애플리케이션 보안을 개선해 승인되지 않은
애플리케이션 액세스 시도를 방지하고자 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 승인되지 않은 트래픽을 필터링하기 위해 NLB에 직접 일련의 AWS WAF 규칙을
구현합니다.
B. 신뢰할 수 있는 IP 주소만 허용하는 보안 그룹으로 NLB를 다시 만듭니다.
C. 엄격한 IP 주소 허용 목록으로 구성된 기존 NLB와 병렬로 두 번째 NLB를 배포합니다.
D. AWS Shield Advanced를 사용하면 강화된 DDoS 보호 기능을 제공하고 무단 액세스
시도를 방지할 수 있습니다.
Answer: D
QUESTION NO: 605
한 회사가 AWS에서 모바일 앱을 구축하고 있습니다. 회사는 수백만 명의 사용자로 범위를
확장하려고 합니다. 회사는 승인된 사용자가 모바일 장치에서 회사의 콘텐츠를 볼 수 있도록
플랫폼을 구축해야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을
권장해야 합니까?
A. 퍼블릭 Amazon S3 버킷에 콘텐츠를 게시합니다. AWS Key Management Service(AWS
KMS) 키를 사용하여 콘텐츠를 스트리밍합니다.
B. 콘텐츠 스트리밍을 위해 모바일 앱과 AWS 환경 간에 IPsec VPN을 설정합니다.
C. Amazon CloudFront를 사용하여 서명된 URL을 제공하여 콘텐츠를 스트리밍합니다.
D. 모바일 앱과 AWS 환경 간에 AWS Client VPN을 설정하여 콘텐츠를 스트리밍합니다.
Answer: C
Explanation:
417

IT Certification Guaranteed, The Easy Way!
Amazon CloudFront is a content delivery network (CDN) that securely delivers data, videos,
applications, and APIs to customers globally with low latency and high transfer speeds.
CloudFront supports signed URLs that provide authorized access to your content. This
feature allows the company to control who can access their content and for how long,
providing a secure and scalable solution for millions of users.
QUESTION NO: 606
솔루션 설계자는 모든 신규 사용자가 특정 복잡성 요구 사항과 IAM 사용자 암호에 대한 필수
교체 기간을 갖기를 원합니다. 이를 달성하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 전체 AWS 계정에 대한 전반적인 비밀번호 정책을 설정합니다.
B. AWS 계정의 각 IAM 사용자에 대한 비밀번호 정책을 설정합니다.
C. 타사 공급업체 소프트웨어를 사용하여 비밀번호 요구 사항 설정
D. Create_newuser 이벤트에 Amazon CloudWatch 규칙을 연결하여 적절한 요구 사항에 따라
암호를 설정합니다.
Answer: A
Explanation:
This option is the most efficient because it sets an overall password policy for the entire AWS
account, which is a way to specify complexity requirements and mandatory rotation periods
for IAM user passwords1. It also meets the requirement of setting a password policy for all
new users, as the password policy applies to all IAM users in the account. This solution
meets the requirement of setting specific complexity requirements and mandatory rotation
periods for IAM user passwords. Option B is less efficient because it sets a password policy
for each IAM user in the AWS account, which is not possible as password policies can only
be set at the account level. Option C is less efficient because it uses third-party vendor
software to set password requirements, which is not necessary as IAM provides a built-in
way to set password policies. Option D is less efficient because it attaches an Amazon
CloudWatch rule to the Create_newuser event to set the password with the appropriate
requirements, which is not possible as CloudWatch rules cannot modify IAM user passwords.
QUESTION NO: 607
IoT 회사는 사용자의 수면에 대한 데이터를 수집하는 센서가 있는 매트리스를 출시하고
있습니다. 센서는 데이터를 Amazon S3 버킷으로 보냅니다. 센서는 각 매트리스에 대해 매일
밤 약 2MB의 데이터를 수집합니다. 회사는 각 매트리스에 대한 데이터를 처리하고 요약해야
합니다. 결과는 가능한 한 빨리 사용할 수 있어야 합니다. 데이터 처리에는 1GB의 메모리가
필요하며 30초 이내에 완료됩니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. Scalajob과 함께 AWS Glue를 사용합니다.
B. Apache Spark 스크립트와 함께 Amazon EMR을 사용합니다.
C. Python 스크립트와 함께 AWS Lambda를 사용합니다.
D. PySpark 작업과 함께 AWS Glue를 사용합니다.
Answer: C
Explanation:
AWS Lambda charges you based on the number of invocations and the execution time of
your function. Since the data processing job is relatively small (2 MB of data), Lambda is a
cost-effective choice. You only pay for the actual usage without the need to provision and
418

IT Certification Guaranteed, The Easy Way!
maintain infrastructure.
QUESTION NO: 608
회사에는 Oracle 데이터베이스를 사용하여 고객 정보를 처리하고 저장하는 온프레미스
서버가 있습니다. 회사는 AWS 데이터베이스 서비스를 사용하여 더 높은 가용성을 달성하고
애플리케이션 성능을 향상하려고 합니다. 또한 회사는 기본 데이터베이스 시스템에서 보고
작업을 오프로드하려고 합니다.
어떤 솔루션이 운영상 가장 효율적인 방식으로 이러한 요구 사항을 충족합니까?
A. AWS Database Migration Service(AWS DMS)를 사용하여 여러 AWS 리전에서 Amazon
RDS DB 인스턴스를 생성합니다. 보고 기능이 기본 DB 인스턴스와 별도의 DB 인스턴스를
가리키도록 합니다.
B. 단일 AZ 배포에서 Amazon RDS를 사용하여 Oracle 데이터베이스를 생성합니다. 기본 DB
인스턴스와 동일한 영역에 읽기 전용 복제본을 생성합니다. 보고 기능을 읽기 전용
복제본으로 보냅니다.
C. 다중 AZ 클러스터 배포에 배포된 Amazon RDS를 사용하여 Oracle 데이터베이스 생성
클러스터 배포에서 리더 인스턴스를 사용하도록 보고 기능에 지시
D. 다중 AZ 인스턴스 배포에 배포된 Amazon RDS를 사용하여 Amazon Aurora
데이터베이스를 생성합니다. 보고 기능을 리더 인스턴스에 지시합니다.
Answer: D
Explanation:
Amazon Aurora is a fully managed relational database that is compatible with MySQL and
PostgreSQL. It provides up to five times better performance than MySQL and up to three
times better performance than PostgreSQL. It also provides high availability and durability by
replicating data across multiple Availability Zones and continuously backing up data to
Amazon S31. By using Amazon RDS deployed in a Multi-AZ instance deployment to create
an Amazon Aurora database, the solution can achieve higher availability and improve
application performance.
Amazon Aurora supports read replicas, which are separate instances that share the same
underlying storage as the primary instance. Read replicas can be used to offload read-only
queries from the primary instance and improve performance. Read replicas can also be used
for reporting functions2. By directing the reporting functions to the reader instances, the
solution can offload reporting from its primary database system.
A: Use AWS Database Migration Service (AWS DMS) to create an Amazon RDS DB
instance in multiple AWS Regions Point the reporting functions toward a separate DB
instance from the pri-mary DB instance.
This solution will not meet the requirement of using an AWS database service, as AWS DMS
is a service that helps users migrate databases to AWS, not a database service itself. It also
involves creating multiple DB instances in different Regions, which may increase complexity
and cost.
B: Use Amazon RDS in a Single-AZ deployment to create an Oracle database Create a read
replica in the same zone as the primary DB instance. Direct the reporting functions to the
read replica. This solution will not meet the requirement of achieving higher availability, as a
Single-AZ deployment does not provide failover protection in case of an Availability Zone
outage. It also involves using Oracle as the database engine, which may not provide better
performance than Aurora.
419

IT Certification Guaranteed, The Easy Way!
C: Use Amazon RDS deployed in a Multi-AZ cluster deployment to create an Oracle
database Di-rect the reporting functions to use the reader instance in the cluster deployment.
This solution will not meet the requirement of improving application performance, as Oracle
may not provide better performance than Aurora. It also involves using a cluster deployment,
which is only supported for Aurora, not for Oracle.
Reference URL: https://aws.amazon.com/rds/aurora/
QUESTION NO: 609
한 회사가 이 웹 사이트에서 Amazon CloudFront를 사용하고 있습니다. 회사는 CloudFront
배포에서 로깅을 활성화했으며 로그는 회사의 Amazon S3 버킷 중 하나에 저장됩니다. 회사는
로그에 대한 고급 분석을 수행하고 시각화를 구축해야 합니다. 이러한 요구 사항을
충족하려면 솔루션 아키텍트가 무엇을 해야 합니까?
A. Amazon Athena에서 표준 SQL 쿼리를 사용하여 S3 버킷의 CloudFront 토그를
분석합니다. AWS Glue로 결과 시각화
B. Amazon Athena에서 표준 SQL 쿼리를 사용하여 S3 버킷의 CloudFront 토그를 분석합니다.
Amazon QuickSight로 결과 시각화
C. Amazon DynamoDB에서 표준 SQL 쿼리를 사용하여 S3 버킷의 CloudFront 로그를
분석합니다. AWS Glue로 결과 시각화
D. Amazon DynamoDB에서 표준 SQL 쿼리를 사용하여 S3 버킷의 CtoudFront 로그를
분석합니다. Amazon QuickSight로 결과 시각화
Answer: B
Explanation:
https://docs.aws.amazon.com/quicksight/latest/user/welcome.html
Using Athena to query the CloudFront logs in the S3 bucket and QuickSight to visualize the
results is the best solution because it is cost-effective, scalable, and requires no
infrastructure setup. It also provides a robust solution that enables the company to perform
advanced analysis and build interactive visualizations without the need for a dedicated team
of developers.
QUESTION NO: 610
미디어 회사가 Amazon EC2 인스턴스에서 실행되는 비디오 변환 도구를 사용하고 있습니다.
비디오 변환 도구는 Windows EC2 인스턴스와 Linux EC2 인스턴스의 조합에서 실행됩니다.
각 비디오 파일의 크기는 수십 기가바이트입니다. 비디오 변환 도구는 가능한 한 짧은 시간
내에 비디오 파일을 처리해야 합니다. 이 회사에는 비디오 변환 도구를 호스팅하는 모든 EC2
인스턴스에 마운트할 수 있는 단일 중앙 집중식 파일 스토리지 솔루션이 필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 하드 디스크 드라이브(HDD) 스토리지를 사용하여 Amazon FSx for Windows File Server를
배포합니다.
B. SSD(Solid State Drive) 스토리지를 사용하여 Windows 파일 서버용 Amazon FSx를
배포합니다.
C. 최대 I/O 성능 모드로 Amazon Elastic File System(Amazon EFS)을 배포합니다.
D. 일반 용도 성능 모드로 Amazon Elastic File System(Amazon EFS)을 배포합니다.
Answer: C
Explanation:
Amazon EFS with Max I/O performance mode is designed for workloads that require high
420

IT Certification Guaranteed, The Easy Way!
levels of parallelism, such as video processing across multiple EC2 instances. EFS provides
shared file storage that can be mounted on both Windows and Linux EC2 instances, and the
Max I/O mode ensures the best performance for handling large files and concurrent access
across multiple instances.
* Option A and B (FSx for Windows File Server): FSx for Windows File Server is optimized for
Windows workloads and would not be ideal for Linux instances or high-throughput, parallel
workloads.
* Option D (EFS General Purpose mode): General Purpose mode offers lower latency but
doesn't support the high throughput needed for large, concurrent workloads.
AWS References:
* Amazon EFS Performance Modes
QUESTION NO: 611
회사의 SAP 애플리케이션에는 온-프레미스 환경에 백엔드 SQL Server 데이터베이스가
있습니다. 회사는 온프레미스 애플리케이션과 데이터베이스 서버를 AWS로
마이그레이션하려고 합니다. 회사에는 SAP 데이터베이스의 높은 요구 사항을 충족하는
인스턴스 유형이 필요합니다. 온프레미스 성능 데이터에 따르면 SAP 애플리케이션과
데이터베이스 모두 메모리 활용도가 높은 것으로 나타났습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 애플리케이션에 컴퓨팅 최적화 인스턴스 제품군을 사용합니다. 데이터베이스에 메모리
최적화 인스턴스 제품군을 사용합니다.
B. 애플리케이션과 데이터베이스 모두에 스토리지 최적화 인스턴스 제품군을 사용합니다.
C. 애플리케이션과 데이터베이스 모두에 메모리 최적화 인스턴스 제품군을 사용합니다.
D. 애플리케이션에 고성능 컴퓨팅(HPC) 최적화 인스턴스 제품군을 사용합니다.
데이터베이스에 메모리 최적화 인스턴스 제품군을 사용합니다.
Answer: C
Explanation:
* Memory Optimized Instances: These instances are designed to deliver fast performance for
workloads that process large data sets in memory. They are ideal for high-performance
databases like SAP and applications with high memory utilization.
* High Memory Utilization: Both the SAP application and the SQL Server database have high
memory demands as per the on-premises performance data. Memory optimized instances
provide the necessary memory capacity and performance.
* Instance Types:
* For the SAP application, using a memory optimized instance ensures the application has
sufficient memory to handle the high workload efficiently.
* For the SQL Server database, memory optimized instances ensure optimal database
performance with high memory throughput.
* Operational Efficiency: Using the same instance family for both the application and the
database simplifies management and ensures both components meet performance
requirements.
References:
* Amazon EC2 Instance Types
* SAP on AWS
421

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 612
한 회사가 최근 한 AWS 지역에서 고가용성을 갖춘 새로운 제품을 출시했습니다. 이 제품은
Amazon Elastic Container Service(Amazon ECS)에서 실행되는 애플리케이션, 퍼블릭
Application Load Balancer(ALB) 및 Amazon DynamoDB 테이블로 구성되어 있습니다. 이
회사는 여러 지역에서 애플리케이션을 고가용성으로 만들어 줄 솔루션을 원합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (세 가지를 선택하십시오.)
A. 다른 지역에서 새 ALB를 통해 액세스할 수 있는 새 ECS 클러스터에 애플리케이션을
배포합니다.
B. Amazon Route 53 장애 조치 레코드를 생성합니다.
C. DynamoDB 테이블을 수정하여 DynamoDB 전역 테이블을 생성합니다.
D. 동일한 지역에서 새 ALB를 통해 액세스할 수 있는 Amazon Elastic Kubernetes
Service(Amazon EKS) 클러스터에 애플리케이션을 배포합니다.
E. DynamoDB 테이블을 수정하여 글로벌 보조 인덱스(GSI)를 생성합니다.
F. 애플리케이션에 대한 AWS PrivateLink 엔드포인트를 생성합니다.
Answer: A B C
Explanation:
To make the application highly available across regions:
* Deploy the application in a different region using a new ECS cluster and ALB to ensure
regional redundancy.
* Use Route 53 failover routing to automatically direct traffic to the healthy region in case of
failure.
* Use DynamoDB Global Tables to ensure the database is replicated and available across
multiple regions, supporting read and write operations in each region.
* Option D (EKS cluster in the same region): This does not provide regional redundancy.
* Option E (Global Secondary Indexes): GSIs improve query performance but do not provide
multi- region availability.
* Option F (PrivateLink): PrivateLink is for secure communication, not for cross-region high
availability.
AWS References:
* DynamoDB Global Tables
* Amazon ECS with ALB
QUESTION NO: 613
한 회사가 AWS에서 애플리케이션을 시작하고 있습니다. 애플리케이션은 애플리케이션
로드(ALB)를 사용하여 단일 대상 그룹에 있는 최소 2개의 Amazon EC2 인스턴스로 트래픽을
전달합니다.
인스턴스는 각 환경의 Auto Scaling 그룹에 있습니다. 회사에는 개발 및 생산 환경이
필요합니다. 프로덕션 환경에는 트래픽이 많은 기간이 있습니다.
개발 환경을 가장 비용 효율적으로 구성하는 솔루션은 무엇입니까?
A. 하나의 EC2 인스턴스를 대상으로 하도록 개발 환경에서 대상 그룹을 재구성합니다.
B. ALB 밸런싱 알고리즘을 최소 미해결 요청으로 변경합니다.
C. 두 환경 모두에서 EC2 인스턴스의 크기를 줄입니다.
D. 개발 환경의 Auto Scaling 그룹에서 최대 EC2 인스턴스 수를 줄입니다.
Answer: D
422

IT Certification Guaranteed, The Easy Way!
Explanation:
This option will configure the development environment in the most cost-effective way as it
reduces the number of instances running in the development environment and therefore
reduces the cost of running the application. The development environment typically requires
less resources than the production environment, and it is unlikely that the development
environment will have periods of high traffic that would require a large number of instances.
By reducing the maximum number of instances in the development environment's Auto
Scaling group, the company can save on costs while still maintaining a functional
development environment.
QUESTION NO: 614
회사에는 재무, 데이터 분석 및 개발 부서에 대한 별도의 AWS 계정이 있습니다. 비용과 보안
문제로 인해 회사는 각 AWS 계정이 사용할 수 있는 서비스를 제어하려고 합니다. 어떤
솔루션이 최소한의 운영 오버헤드로 이러한 요구 사항을 충족합니까?
A. AWS Systems Manager 템플릿을 사용하여 각 부서에서 사용할 수 있는 AWS 서비스를
제어합니다.
B. AWS Organizations의 각 부서에 대한 조직 단위(OU)를 생성합니다. SCP(서비스 제어
정책)를 OU에 연결합니다.
C. AWS CloudFormation을 사용하여 각 부서에서 사용할 수 있는 AWS 서비스만 자동으로
프로비저닝합니다.
D. 특정 AWS 서비스의 사용을 관리하고 제어하기 위해 AWS 계정의 AWS Service Catalog에
제품 목록을 설정합니다.
Answer: B
Explanation:
* AWS Organizations: AWS Organizations allows you to create multiple AWS accounts and
manage them centrally. You can organize accounts into organizational units (OUs) and apply
policies to these units.
* Organizational Units (OUs):
* Create separate OUs for each department: finance, data analytics, and development.
* Place the respective AWS accounts for each department into their corresponding OUs.
* Service Control Policies (SCPs):
* SCPs are policies that can restrict which AWS services and actions are available to
accounts in an OU.
* Create SCPs to define which services each department can use and attach these policies
to the appropriate OUs.
* SCPs apply to all IAM users, groups, and roles within the accounts in the OU, providing
centralized control over service usage.
* Operational Efficiency: Using AWS Organizations and SCPs provides a scalable and
centralized way to manage permissions across multiple accounts with minimal operational
overhead.
References:
* AWS Organizations
* Service Control Policies
QUESTION NO: 615
423

IT Certification Guaranteed, The Easy Way!
한 회사가 사내 직원을 위한 개인 웹사이트를 게시하려고 합니다. 웹사이트는 여러 HTML
페이지와 이미지 파일로 구성되어 있습니다. 웹사이트는 HTTPS를 통해서만 사용할 수
있어야 하며 사내 직원만 사용할 수 있어야 합니다. 솔루션 아키텍트는 웹사이트 파일을
Amazon S3 버킷에 저장할 계획입니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 소스 IP 주소가 온프레미스 환경의 퍼블릭 IP 주소가 아닌 경우 액세스를 거부하는 S3 버킷
정책을 만듭니다. S3 버킷을 가리키도록 Amazon Route 53 별칭 레코드를 설정합니다.
온프레미스 직원에게 별칭 레코드를 제공하여 직원에게 웹사이트에 대한 액세스 권한을
부여합니다.
B. 웹사이트 액세스를 제공하기 위해 S3 액세스 포인트를 만듭니다. 소스 IP 주소가
온프레미스 환경의 공용 IP 주소가 아닌 경우 액세스를 거부하는 액세스 포인트 정책을
첨부합니다. 직원들에게 웹사이트에 대한 액세스 권한을 부여하기 위해 온프레미스 직원에게
S3 액세스 포인트 별칭을 제공합니다.
C. S3 버킷에 대해 구성된 OAC(Origin Access Control)를 포함하는 Amazon CloudFront
배포를 만듭니다. SSL을 위해 AWS Certificate Manager를 사용합니다. 온프레미스 IP 주소에
대한 액세스를 허용하는 IP 설정 규칙이 있는 AWS WAF를 사용합니다. CloudFront 배포를
가리키도록 Amazon Route 53 별칭 레코드를 설정합니다.
D. S3 버킷에 대해 구성된 OAC(Origin Access Control)를 포함하는 Amazon CloudFront
배포를 만듭니다. 버킷의 객체에 대한 CloudFront 서명 URL을 만듭니다. CloudFront 배포를
가리키도록 Amazon Route 53 별칭 레코드를 설정합니다. 서명된 URL을 온프레미스
직원에게 제공하여 직원에게 웹사이트에 대한 액세스 권한을 부여합니다.
Answer: C
Explanation:
This solution uses CloudFront to serve the website securely over HTTPS using AWS
Certificate Manager (ACM) for SSL certificates. Origin Access Control (OAC) ensures that
only CloudFront can access the S3 bucket directly. AWS WAF with an IP set rule restricts
access to the website, allowing only the on-premises IP address. Route 53 is used to create
an alias record pointing to the CloudFront distribution. This setup ensures secure, private
access to the website with low administrative overhead.
* Option A and B: S3 bucket policies and access points do not provide HTTPS support, nor
do they offer the same level of security as CloudFront with WAF.
* Option D: Signed URLs are more suitable for temporary, expiring access rather than a
permanent solution for on-premises employees.
AWS References:
* Amazon CloudFront with Origin Access Control
QUESTION NO: 616
한 회사는 Amazon RDS(또는 PostgreSQL)를 사용하여 us-east-1 지역에서 애플리케이션을
실행합니다. 이 회사는 또한 기계 학습(ML) 모델을 사용하여 깔끔한 실시간 보고서를
기반으로 연간 수익을 예측합니다. 이 보고서는 PostgreSQL 데이터베이스용 동일한 RDS를
사용하여 생성됩니다. 데이터베이스 성능은 업무 시간 동안 느려집니다. 이 회사는
데이터베이스 성능을 개선해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. 지역 간 읽기 복제본을 만듭니다. 읽기 복제본에서 생성될 보고서를 구성합니다.
B. RDS for PostgreSQL에 대한 Multi-AZ DB 인스턴스 배포를 활성화합니다. 스탠바이
424

IT Certification Guaranteed, The Easy Way!
데이터베이스에서 생성될 보고서를 구성합니다.
C. AWS Data Migration Service(AWS DMS)를 사용하여 새 데이터베이스로 데이터를
논리적으로 복제합니다. 새 데이터베이스에서 생성될 보고서를 구성합니다.
D. us-east-1에 읽기 복제본을 만듭니다. 읽기 복제본에서 생성될 보고서를 구성합니다.
Answer: D
Explanation:
To improve the performance of the primary RDS PostgreSQL database during business
hours and reduce the load, the best solution is to create a read replica in the same region
(us-east-1). This will offload the read- heavy operations (like generating reports) to the replica,
reducing the burden on the primary instance, which improves overall performance.
Additionally, read replicas provide near real-time replication, making them ideal for real-time
reporting use cases.
* Option A (cross-Region read replica): This adds unnecessary latency for real-time reporting
and increased costs due to cross-region data transfer.
* Option B (Multi-AZ): Multi-AZ deployments are for high availability and disaster recovery but
won't offload the read traffic, as the standby database cannot serve read requests.
* Option C (AWS DMS replication): This adds complexity and is not as cost-effective as using
an RDS read replica for the same region.
AWS References:
* Amazon RDS Read Replicas
* Amazon RDS Performance Best Practices
QUESTION NO: 617
한 회사는 Application Load Balancer 뒤에 있는 Amazon EC2 온디맨드 인스턴스 그룹에서
생산 중인 상태 비저장 웹 애플리케이션을 실행합니다. 애플리케이션은 영업일 기준으로 매일
8시간 동안 사용량이 많습니다. 애플리케이션 사용량은 보통이고 밤새 꾸준합니다. 주말에는
애플리케이션 사용량이 낮습니다.
회사는 애플리케이션의 가용성에 영향을 주지 않고 EC2 비용을 최소화하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 전체 워크로드에 스팟 인스턴스를 사용합니다.
B. 기준 사용량 수준을 위해 예약 인스턴스를 사용합니다. 애플리케이션에 필요한 추가 용량을
위해 스팟 인스턴스를 사용합니다.
C. 기본 사용량 수준으로 온디맨드 인스턴스를 사용합니다. 애플리케이션에 필요한 추가
용량을 위해 스팟 인스턴스를 사용합니다.
D. 기준 사용량 수준에는 전용 인스턴스를 사용합니다. 애플리케이션에 필요한 추가 용량을
위해 온디맨드 인스턴스를 사용합니다.
Answer: B
Explanation:
Reserved is cheaper than on demand the company has. And it's meet the availabilty (HA)
requirement as to spot instance that can be disrupted at any time. PRICING BELOW. On-
Demand: 0% There's no commitment from you. You pay the most with this option. Reserved :
40%-60%1-year or 3-year commitment from you.
You save money from that commitment. Spot 50%-90% Ridiculously inexpensive because
there's no commitment from the AWS side.
425

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 618
온라인 비디오 게임 회사는 게임 서버에 대해 매우 낮은 대기 시간을 유지해야 합니다. 게임
서버는 Amazon EC2 인스턴스에서 실행됩니다. 회사에는 초당 수백만 건의 UDP 인터넷
트래픽 요청을 처리할 수 있는 솔루션이 필요합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 인터넷 트래픽에 필요한 프로토콜과 포트로 Application Load Balancer를 구성합니다. EC2
인스턴스를 대상으로 지정합니다.
B. 인터넷 트래픽에 대한 게이트웨이 로드 밸런서를 구성합니다. EC2 인스턴스를 대상으로
지정합니다.
C. 인터넷 트래픽에 필요한 프로토콜과 포트로 Network Load Balancer를 구성합니다. EC2
인스턴스를 대상으로 지정합니다.
D. 별도의 AWS 지역에 있는 EC2 인스턴스에서 동일한 게임 서버 세트를 시작합니다. 인터넷
트래픽을 두 EC2 인스턴스 세트로 라우팅합니다.
Answer: C
Explanation:
The most cost-effective solution for the online video game company is to configure a Network
Load Balancer with the required protocol and ports for the internet traffic and specify the EC2
instances as the targets. This solution will enable the company to handle millions of UDP
requests per second with ultra-low latency and high performance.
A Network Load Balancer is a type of Elastic Load Balancing that operates at the connection
level (Layer 4) and routes traffic to targets (EC2 instances, microservices, or containers)
within Amazon VPC based on IP protocol data. A Network Load Balancer is ideal for load
balancing of both TCP and UDP traffic, as it is capable of handling millions of requests per
second while maintaining high throughput at ultra-low latency. A Network Load Balancer also
preserves the source IP address of the clients to the back-end applications, which can be
useful for logging or security purposes1.
QUESTION NO: 619
한 회사가 애플리케이션을 AWS로 마이그레이션하고 있습니다. 애플리케이션은 서로 다른
계정에 배포됩니다. 회사는 AWS Organizations를 사용하여 중앙에서 계정을 관리합니다.
회사의 보안 팀에는 회사의 모든 계정에 대한 SSO(Single Sign-On) 솔루션이 필요합니다.
회사는 온프레미스 자체 관리형 Microsoft Active Directory에서 사용자 및 그룹을 계속
관리해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS SSO 콘솔에서 AWS Single Sign-On(AWS SSO)을 활성화합니다. Microsoft Active
Directory용 AWS Directory Service를 사용하여 단방향 포리스트 트러스트 또는 단방향
도메인 트러스트를 생성하여 회사의 자체 관리형 Microsoft Active Directory를 AWS SSO와
연결합니다.
B. AWS SSO 콘솔에서 AWS Single Sign-On(AWS SSO)을 활성화합니다. Microsoft Active
Directory용 AWS Directory Service를 사용하여 회사의 자체 관리형 Microsoft Active
Directory를 AWS SSO와 연결하기 위한 양방향 포리스트 신뢰를 생성합니다.
C. AWS 디렉터리 서비스를 사용합니다. 회사의 자체 관리형 Microsoft Active Directory와
양방향 신뢰 관계를 구축합니다.
D. 온프레미스에 ID 공급자(IdP)를 배포합니다. AWS SSO 콘솔에서 AWS Single Sign-
426

IT Certification Guaranteed, The Easy Way!
On(AWS SSO)을 활성화합니다.
Answer: A
Explanation:
To provide single sign-on (SSO) across all the company's accounts while continuing to
manage users and groups in its on-premises self-managed Microsoft Active Directory, the
solution is to enable AWS Single Sign-On (SSO) from the AWS SSO console and create a
one-way forest trust or a one-way domain trust to connect the company's self-managed
Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft
Active Directory. This solution is described in the AWS documentation
QUESTION NO: 620
글로벌 마케팅 회사에는 ap-southeast-2 지역 및 eu-west-1 지역에서 실행되는
애플리케이션이 있습니다.
eu-west-1의 VPC에서 실행되는 애플리케이션은 ap-southeast-2의 VPC에서 실행되는
데이터베이스와 안전하게 통신해야 합니다.
이러한 요구 사항을 충족하는 네트워크 설계는 무엇입니까?
A. eu-west-1 VPC와 ap-southeast-2 VPC 간에 VPC 피어링 연결을 생성합니다. ap-
southeast-2 보안 그룹의 데이터베이스 서버 IP 주소에서 오는 트래픽을 허용하는 인바운드
규칙을 eu-west-1 애플리케이션 보안 그룹에 생성합니다.
B. ap-southeast-2 VPC와 eu-west-1 VPC 간에 VPC 피어링 연결을 구성합니다. 서브넷 경로
테이블을 업데이트합니다. eu-west-1에 있는 애플리케이션 서버의 보안 그룹 ID를 참조하는
ap-southeast-2 데이터베이스 보안 그룹에서 인바운드 규칙을 생성합니다.
C. ap-southeast-2 VPC와 eu-west-1 VPC 간에 VPC 피어링 연결을 구성합니다. 서브넷
라우팅 테이블 업데이트 ap-southeast-2 데이터베이스 보안 그룹에서 eu-west-1 애플리케이션
서버 IP 주소의 트래픽을 허용하는 인바운드 규칙을 생성합니다.
D. eu-west-1 VPC와 ap-southeast-2 VPC 간에 피어링 연결이 있는 전송 게이트웨이를
생성합니다. 전송 게이트웨이가 올바르게 피어링되고 라우팅이 구성되면 eu-west-1에 있는
애플리케이션 서버의 보안 그룹 ID를 참조하는 데이터베이스 보안 그룹에 인바운드 규칙을
생성합니다.
Answer: C
Explanation:
"You cannot reference the security group of a peer VPC that's in a different Region. Instead,
use the CIDR block of the peer VPC." https://docs.aws.amazon.com/vpc/latest/peering/vpc-
peering-security-groups.html
QUESTION NO: 621
회사의 애플리케이션은 Amazon EC2 인스턴스에 배포되고 이벤트 기반 아키텍처에 AWS
Lambda 함수를 사용합니다. 회사는 다른 AWS 계정에서 비생산 개발 환경을 사용하여 회사가
기능을 프로덕션에 배포하기 전에 새로운 기능을 테스트합니다.
프로덕션 인스턴스는 다른 시간대에 있는 고객 때문에 지속적으로 사용됩니다. 이 회사는
주중 업무 시간에만 비프로덕션 인스턴스를 사용합니다. 이 회사는 주말에는 비프로덕션
인스턴스를 사용하지 않습니다. 이 회사는 AWS에서 애플리케이션을 실행하는 데 드는
비용을 최적화하려고 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. 온디맨드 인스턴스(또는 프로덕션 인스턴스)를 사용하세요. 주말에만 프로덕션이 아닌
427

IT Certification Guaranteed, The Easy Way!
인스턴스에는 전용 호스트를 사용하세요.
B. 운영 인스턴스와 비운영 인스턴스에 예약 인스턴스를 사용합니다. 사용하지 않을 때는
비운영 인스턴스를 종료합니다.
C. 프로덕션 인스턴스에 대해 Compute Savings Plans를 사용합니다. 비프로덕션 인스턴스에
대해 On-Demand Instances를 사용합니다. 사용하지 않을 때 비프로덕션 인스턴스를
종료합니다.
D. 프로덕션 인스턴스에는 전용 호스트를 사용합니다. 비프로덕션 인스턴스에는 EC2
인스턴스 절약 플랜을 사용합니다.
Answer: C
Explanation:
Compute Savings Plans offer the most flexible and cost-effective solution for the production
instances, as they provide significant savings (up to 66%) for both EC2 and AWS Lambda
usage, while allowing flexibility in the type of instance family, size, and even region. For
nonproduction instances, using On-Demand Instances ensures you only pay for the
instances when they are running, and shutting them down during off- hours further optimizes
cost.
Key AWS features:
* Compute Savings Plans: Provide savings based on consistent usage, making it ideal for
production environments with steady load.
* On-Demand Instances: Suitable for nonproduction environments that are used
intermittently. Shutting them down when not in use avoids unnecessary costs.
* AWS Documentation: According to AWS's cost optimization best practices, using a
combination of Savings Plans for production and On-Demand Instances for nonproduction
environments that are used sparingly results in optimal cost savings.
QUESTION NO: 622
솔루션 설계자는 Amazon S3를 사용하여 새로운 디지털 미디어 애플리케이션의 스토리지
아키텍처를 설계하고 있습니다. 미디어 파일은 가용 영역 손실에 대해 복원력이 있어야
합니다. 일부 파일은 자주 액세스되는 반면 다른 파일은 예측할 수 없는 패턴으로 거의
액세스되지 않습니다. 솔루션 설계자는 미디어 파일 저장 및 검색 비용을 최소화해야 합니다.
이러한 요구 사항을 충족하는 스토리지 옵션은 무엇입니까?
A. S3 표준
B. S3 지능형 계층화
C. S3 표준-Infrequent Access(S3 표준-IA)
D. S3 One Zone-Infrequent Access(S3 One Zone-IA)
Answer: B
Explanation:
S3 Intelligent-Tiering - Perfect use case when you don't know the frequency of access or
irregular patterns of usage.
Amazon S3 offers a range of storage classes designed for different use cases. These include
S3 Standard for general-purpose storage of frequently accessed data; S3 Intelligent-Tiering
for data with unknown or changing access patterns; S3 Standard-Infrequent Access (S3
Standard-IA) and S3 One Zone-Infrequent Access (S3 One Zone-IA) for long-lived, but less
frequently accessed data; and Amazon S3 Glacier (S3 Glacier) and Amazon S3 Glacier
Deep Archive (S3 Glacier Deep Archive) for long-term archive and digital preservation. If you
428

IT Certification Guaranteed, The Easy Way!
have data residency requirements that can't be met by an existing AWS Region, you can use
the S3 Outposts storage class to store your S3 data on-premises. Amazon S3 also offers
capabilities to manage your data throughout its lifecycle. Once an S3 Lifecycle policy is set,
your data will automatically transfer to a different storage class without any changes to your
application.
https://aws.amazon.com/getting-started/hands-on/getting-started-using-amazon-s3-
intelligent-tiering/?
nc1=h_ls
QUESTION NO: 623
한 회사가 Amazon EC2 인스턴스에 새 애플리케이션을 배포하고 있습니다. 애플리케이션은
Amazon Elastic Block Store(Amazon EBS) 볼륨에 데이터를 씁니다. 회사는 EBS 볼륨에
기록된 모든 데이터가 저장 시 암호화되었는지 확인해야 합니다.
이 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EBS 암호화를 지정하는 IAM 역할을 생성합니다. EC2 인스턴스에 역할을 연결합니다.
B. EBS 볼륨을 암호화된 볼륨으로 생성합니다. EBS 볼륨을 EC2 인스턴스에 연결합니다.
C. 키가 Encrypt이고 값이 True인 EC2 인스턴스 태그를 생성합니다. ESS 수준에서 암호화가
필요한 모든 인스턴스에 태그를 지정합니다.
D. 계정에서 EBS 암호화를 시행하는 AWS Key Management Service(AWS KMS) 키 정책을
생성합니다. 키 정책이 활성 상태인지 확인합니다.
Answer: B
Explanation:
The solution that will meet the requirement of ensuring that all data that is written to the EBS
volumes is encrypted at rest is B. Create the EBS volumes as encrypted volumes and attach
the encrypted EBS volumes to the EC2 instances. When you create an EBS volume, you can
specify whether to encrypt the volume. If you choose to encrypt the volume, all data written to
the volume is automatically encrypted at rest using AWS- managed keys. You can also use
customer-managed keys (CMKs) stored in AWS KMS to encrypt and protect your EBS
volumes. You can create encrypted EBS volumes and attach them to EC2 instances to
ensure that all data written to the volumes is encrypted at rest.
QUESTION NO: 624
한 회사에는 Amazon S3 앞의 Amazon CloudFront에 호스팅된 정적 웹사이트가 있습니다.
정적 웹사이트는 데이터베이스 백엔드를 사용합니다. 이 회사는 웹사이트가 웹사이트의 Git
저장소에서 이루어진 업데이트를 반영하지 않는다는 것을 알아차립니다. 이 회사는 Git
저장소와 Amazon S3 간의 CI/CD(Continuous Integration and Continuous Delivery)
파이프라인을 확인합니다. 이 회사는 웹훅이 제대로 구성되었는지, CI/CD 파이프라인이
성공적인 배포를 나타내는 메시지를 보내고 있는지 확인합니다.
솔루션 아키텍트는 웹사이트에 업데이트를 표시하는 솔루션을 구현해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 애플리케이션 로드 밸런서를 추가합니다.
B. 웹 애플리케이션의 데이터베이스 계층에 Redis 또는 Memcached용 Amazon
ElastiCache를 추가합니다.
C. CloudFront 캐시를 무효화합니다.
D. AWS Certificate Manager(ACM)를 사용하여 웹사이트의 SSL 인증서를 검증합니다.
429

IT Certification Guaranteed, The Easy Way!
Answer: C
Explanation:
Amazon CloudFront is a content delivery network (CDN) service that caches copies of your
content at edge locations around the world. This helps improve performance by serving
content from the edge nearest to the user. However, when the content in Amazon S3 (your
origin) is updated, those updates may not immediately reflect on the website if they are
cached at the CloudFront edge locations.
The issue described in the question suggests that the CI/CD pipeline is functioning correctly,
and updates are being deployed to S3. However, since CloudFront caches this content, the
edge locations may still be serving outdated content, causing the updates to not be reflected
on the website.
To resolve this issue, you need to invalidate the CloudFront cache. By invalidating the cache,
CloudFront will remove the outdated content and retrieve the latest version from the S3
origin.
AWS documentation on this process:
* CloudFront cache invalidation allows you to clear items from the cache so that CloudFront
retrieves the latest version from the origin. You can create invalidation requests via the AWS
Management Console, AWS CLI, or SDKs.
* AWS CloudFront Documentation
Why the other options are incorrect:
* A. Add an Application Load Balancer: ALBs are used to distribute incoming application
traffic and are not relevant to caching or serving content from CloudFront.
* B. Add Amazon ElastiCache for Redis or Memcached: This would help in caching database
queries but has no relation to static website content hosted on CloudFront and S3.
* D. Use AWS Certificate Manager (ACM): ACM is used for managing SSL/TLS certificates
and is unrelated to the issue of content not being updated on CloudFront.
QUESTION NO: 625
회사는 Amazon API Gateway를 사용하여 타사 서비스 공급자가 액세스하는 REST API를
관리합니다. 회사는 SQL 주입 및 크로스 사이트 스크립팅 공격으로부터 REST API를
보호해야 합니다.
이러한 요구 사항을 충족하는 가장 운영 효율적인 솔루션은 무엇입니까?
A. AWS Shield를 구성합니다.
B. AWS WAR 구성
C. Amazon CloudFront 배포를 사용하여 API 게이트웨이 설정 CloudFront에서 AWS Shield를
구성합니다.
D. Amazon CloudFront 배포로 API Gateway를 설정합니다. CloudFront에서 AWS WAF 구성
Answer: D
Explanation:
* Amazon API Gateway with CloudFront: API Gateway allows you to create, deploy, and
manage APIs, while CloudFront provides a CDN to deliver content with low latency and high
transfer speeds.
* AWS WAF (Web Application Firewall):
* AWS WAF can be configured in CloudFront to protect against common web exploits,
including SQL injection and cross-site scripting (XSS).
430

IT Certification Guaranteed, The Easy Way!
* WAF allows you to create custom rules to block specific attack patterns and can be
managed centrally.
* Configuration:
* Deploy your APIs using Amazon API Gateway.
* Set up an Amazon CloudFront distribution in front of the API Gateway.
* Configure AWS WAF on the CloudFront distribution to apply security rules.
* Operational Efficiency: This solution provides robust protection with minimal operational
overhead by leveraging managed AWS services, ensuring that your APIs are secure without
extensive custom implementation.
References:
* Using AWS WAF to Protect Your APIs
* How CloudFront Works with AWS WAF
QUESTION NO: 626
글로벌 전자 상거래 회사는 AWS에서 중요한 워크로드를 실행합니다. 워크로드는 다중 AZ
배포용으로 구성된 PostgreSQL DB 인스턴스용 Amazon RDS를 사용합니다.
회사에서 데이터베이스 장애 조치가 발생할 때 고객이 애플리케이션 시간 초과를
보고했습니다. 회사에는 장애 조치 시간을 줄이기 위해 탄력적인 솔루션이 필요합니다. 어떤
솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon RDS 프록시를 생성합니다. DB 인스턴스에 프록시를 할당합니다.
B. DB 인스턴스에 대한 읽기 전용 복제본을 생성합니다. 읽기 트래픽을 읽기 전용 복제본으로
이동합니다.
C. 성능 개선 도우미를 활성화합니다. CPU 로드를 모니터링하여 시간 초과를 식별합니다.
D. 정기적인 자동 스냅샷 생성 자동 스냅샷을 여러 AWS 리전에 복사
Answer: A
Explanation:
* Amazon RDS Proxy: RDS Proxy is a fully managed, highly available database proxy that
makes applications more resilient to database failures by pooling and sharing connections,
and it can automatically handle database failovers.
* Reduced Failover Time: By using RDS Proxy, the connection management between the
application and the database is improved, reducing failover times significantly. RDS Proxy
maintains connections in a connection pool and reduces the time required to re-establish
connections during a failover.
* Configuration:
* Create an RDS Proxy instance.
* Configure the proxy to connect to the RDS for PostgreSQL DB instance.
* Modify the application configuration to use the RDS Proxy endpoint instead of the direct
database endpoint.
* Operational Benefits: This solution provides high availability and reduces application
timeouts during failovers with minimal changes to the application code.
References:
* Amazon RDS Proxy
* Setting Up RDS Proxy
QUESTION NO: 627
431

IT Certification Guaranteed, The Easy Way!
솔루션 설계자는 원하는 Amazon EC2 용량에 도달하기 전에 야간 일괄 처리 작업이 1시간
동안 자동으로 확장되는 것을 관찰합니다. 최대 용량은 매일 밤 동일하며 배치 작업은 항상
오전 1시에 시작됩니다. 솔루션 아키텍트는 원하는 EC2 용량에 빠르게 도달하고 배치 작업이
완료된 후 Auto Scaling 그룹을 축소할 수 있는 비용 효율적인 솔루션을 찾아야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. Auto Scaling 그룹의 최소 용량을 늘립니다.
B. Auto Scaling 그룹의 최대 용량을 늘립니다.
C. 원하는 컴퓨팅 수준까지 확장하도록 예약된 확장을 구성합니다.
D. 각 조정 작업 중에 더 많은 EC2 인스턴스를 추가하도록 조정 정책을 변경합니다.
Answer: C
Explanation:
By configuring scheduled scaling, the solutions architect can set the Auto Scaling group to
automatically scale up to the desired compute level at a specific time (IAM) when the batch
job starts and then automatically scale down after the job is complete. This will allow the
desired EC2 capacity to be reached quickly and also help in reducing the cost.
QUESTION NO: 628
회사는 디지털 미디어 스트리밍 애플리케이션을 호스팅하기 위해 Amazon Elastic Kubernetes
Service(Amazon EKS) 클러스터를 생성해야 합니다. EKS 클러스터는 저장을 위해 Amazon
Elastic Block Store(Amazon EBS) 볼륨이 지원하는 관리형 노드 그룹을 사용합니다. 회사는
AWS Key Management Service(AWS KMS)에 저장된 고객 관리형 키를 사용하여 모든 저장
데이터를 암호화해야 합니다. 최소한의 운영 오버헤드로 이 요구 사항을 충족할 수 있는 작업
조합은 무엇입니까? (2개를 선택하세요.)
A. 고객 관리 키를 사용하여 데이터 암호화를 수행하는 Kubernetes 플러그인을 사용합니다.
B. EKS 클러스터를 생성한 후 EBS 볼륨을 찾습니다. 고객 관리형 키를 사용하여 암호화를
활성화합니다.
C. EKS 클러스터가 생성될 AWS 리전에서 기본적으로 EBS 암호화를 활성화합니다. 고객
관리형 키를 기본 키로 선택합니다.
D. EKS 클러스터 생성 고객 관리형 키에 권한을 부여하는 권한이 있는 IAM 역할을
생성합니다. 역할을 EKS 클러스터와 연결합니다.
E. 고객 관리 키를 EKS 클러스터에 Kubernetes 비밀로 저장합니다. 고객 관리형 키를
사용하여 EBS 볼륨을 암호화합니다.
Answer: C D
Explanation:
EBS encryption by default is a feature that enables encryption for all new EBS volumes and
snapshots created in a Region1. EBS encryption by default uses a service managed key or a
customer managed key that is stored in AWS KMS1. EBS encryption by default is suitable for
scenarios where data at rest must be encrypted by using a customer managed key, such as
the digital media streaming application in the scenario1.
To meet the requirements of the scenario, the solutions architect should enable EBS
encryption by default in the AWS Region where the EKS cluster will be created. The solutions
architect should select the customer managed key as the default key for encryption1. This
way, all new EBS volumes and snapshots created in that Region will be encrypted by using
the customer managed key.
432

IT Certification Guaranteed, The Easy Way!
EKS encryption provider support is a feature that enables envelope encryption of Kubernetes
secrets in EKS with a customer managed key that is stored in AWS KMS2. Envelope
encryption means that data is encrypted by data encryption keys (DEKs) using AES-GCM;
DEKs are encrypted by key encryption keys (KEKs) according to configuration in AWS
KMS3. EKS encryption provider support is suitable for scenarios where secrets must be
encrypted by using a customer managed key, such as the digital media streaming application
in the scenario2.
To meet the requirements of the scenario, the solutions architect should create the EKS
cluster and create an IAM role that has a policy that grants permission to the customer
managed key. The solutions architect should associate the role with the EKS cluster2. This
way, the EKS cluster can use envelope encryption of Kubernetes secrets with the customer
managed key.
QUESTION NO: 629
회사는 들어오는 요청을 처리하는 온프레미스 서버 집합에서 컨테이너화된 웹
애플리케이션을 호스팅합니다. 요청 건수가 빠르게 증가하고 있습니다. 온프레미스 서버는
증가된 요청 수를 처리할 수 없습니다. 회사는 최소한의 코드 변경과 최소한의 개발 노력으로
애플리케이션을 AWS로 이전하기를 원합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon Elastic Container Service(Amazon ECS)에서 AWS Fargate를 사용하여 Service
Auto Scaling으로 컨테이너화된 웹 애플리케이션을 실행합니다. Application Load Balancer를
사용하여 수신 요청을 분산합니다.
B. 두 개의 Amazon EC2 인스턴스를 사용하여 컨테이너화된 웹 애플리케이션을
호스팅합니다. Application Load Balancer를 사용하여 수신 요청 분산
C. 지원되는 언어 중 하나를 사용하는 새 코드와 함께 AWS Lambda를 사용합니다. 로드를
지원하는 여러 Lambda 함수를 생성합니다. Amazon API Gateway를 Lambda 함수의
진입점으로 사용합니다.
D. AWS ParallelCluster와 같은 고성능 컴퓨팅(HPC) 솔루션을 사용하여 적절한 규모로 수신
요청을 처리할 수 있는 HPC 클러스터를 구축합니다.
Answer: A
Explanation:
AWS Fargate is a serverless compute engine that lets users run containers without having to
manage servers or clusters of Amazon EC2 instances1. Users can use AWS Fargate on
Amazon Elastic Container Service (Amazon ECS) to run the containerized web application
with Service Auto Scaling. Amazon ECS is a fully managed container orchestration service
that supports both Docker and Kubernetes2. Service Auto Scaling is a feature that allows
users to adjust the desired number of tasks in an ECS service based on CloudWatch metrics,
such as CPU utilization or request count3. Users can use AWS Fargate on Amazon ECS to
migrate the application to AWS with minimum code changes and minimum development
effort, as they only need to package their application in containers and specify the CPU and
memory requirements.
Users can also use an Application Load Balancer to distribute the incoming requests. An
Application Load Balancer is a load balancer that operates at the application layer and routes
traffic to targets based on the content of the request. Users can register their ECS tasks as
targets for an Application Load Balancer and configure listener rules to route requests to
433

IT Certification Guaranteed, The Easy Way!
different target groups based on path or host headers. Users can use an Application Load
Balancer to improve the availability and performance of their web application.
QUESTION NO: 630
회사는 온프레미스 NAS(Network Attached Storage) 시스템을 사용하여 HPC(고성능 컴퓨팅)
워크로드에 파일 공유를 제공합니다. 회사는 지연 시간에 민감한 HPC 워크로드와 스토리지를
AWS 클라우드로 마이그레이션하려고 합니다. 회사는 파일 시스템에서 NFS 및 SMB 다중
프로토콜 액세스를 제공할 수 있어야 합니다.
가장 짧은 대기 시간으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? (2개를
선택하세요.)
A. 컴퓨팅 최적화 EC2 인스턴스를 클러스터 배치 그룹에 배포합니다.
B. 컴퓨팅 최적화 EC2 인스턴스를 파티션 배치 그룹에 배포합니다.
C. EC2 인스턴스를 Amazon FSx for Lustre 파일 시스템에 연결합니다.
D. EC2 인스턴스를 Amazon FSx for OpenZFS 파일 시스템에 연결합니다.
E. EC2 인스턴스를 Amazon FSx for NetApp ONTAP 파일 시스템에 연결합니다.
Answer: A E
Explanation:
A cluster placement group is a logical grouping of EC2 instances within a single Availability
Zone that are placed close together to minimize network latency. This is suitable for latency-
sensitive HPC workloads that require high network performance. A compute optimized EC2
instance is an instance type that has a high ratio of vCPUs to memory, which is ideal for
compute-intensive applications. Amazon FSx for NetApp ONTAP is a fully managed service
that provides NFS and SMB multi-protocol access from the file system, as well as features
such as data deduplication, compression, thin provisioning, and snapshots. This solution will
meet the requirements with the least latency, as it leverages the low-latency network and
storage performance of AWS.
References:
* 1 explains how cluster placement groups work and their benefits.
* 2 describes the characteristics and use cases of compute optimized EC2 instances.
* 3 provides an overview of Amazon FSx for NetApp ONTAP and its features.
QUESTION NO: 631
한 회사가 프로덕션 Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터에서 실행될
애플리케이션을 개발 중입니다. EKS 클러스터에는 온디맨드 인스턴스로 프로비저닝되는
관리형 노드 그룹이 있습니다.
회사에는 개발 작업을 위한 전용 EKS 클러스터가 필요합니다. 회사는 애플리케이션의
복원력을 테스트하기 위해 개발 클러스터를 자주 사용하지 않습니다. EKS 클러스터는 모든
노드를 관리해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 스팟 인스턴스만 포함하는 관리형 노드 그룹을 생성합니다.
B. 두 개의 관리형 노드 그룹을 생성합니다. 온디맨드 인스턴스로 하나의 노드 그룹을
프로비저닝합니다. 스팟 인스턴스로 두 번째 노드 그룹을 프로비저닝합니다.
C. 스팟 인스턴스를 사용하는 시작 구성이 있는 Auto Scaling 그룹을 생성합니다. EKS
클러스터에 노드를 추가하도록 사용자 데이터를 구성합니다.
D. 온디맨드 인스턴스만 포함하는 관리형 노드 그룹을 생성합니다.
434

IT Certification Guaranteed, The Easy Way!
Answer: A
Explanation:
Spot Instances are EC2 instances that are available at up to a 90% discount compared to
On-Demand prices.
Spot Instances are suitable for stateless, fault-tolerant, and flexible workloads that can
tolerate interruptions.
Spot Instances can be reclaimed by EC2 when the demand for On-Demand capacity
increases, but they provide a two-minute warning before termination. EKS managed node
groups automate the provisioning and lifecycle management of nodes for EKS clusters.
Managed node groups can use Spot Instances to reduce costs and scale the cluster based
on demand. Managed node groups also support features such as Capacity Rebalancing and
Capacity Optimized allocation strategy to improve the availability and resilience of Spot
Instances. This solution will meet the requirements most cost-effectively, as it leverages the
lowest-priced EC2 capacity and does not require any manual intervention.
References:
* 1 explains how to create and use managed node groups with EKS.
* 2 describes how to use Spot Instances with managed node groups.
* 3 provides an overview of Spot Instances and their benefits.
QUESTION NO: 632
회사는 Amazon API Gateway를 사용하여 동일한 VPC에서 두 개의 REST API가 포함된
프라이빗 게이트웨이를 실행합니다. BuyStock RESTful 웹 서비스는 CheckFunds RESTful 웹
서비스를 호출하여 주식을 구매하기 전에 충분한 자금을 사용할 수 있는지 확인합니다.
회사는 BuyStock RESTful 웹 서비스가 VPC 대신 인터넷을 통해 CheckFunds RESTful 웹
서비스를 호출한다는 사실을 VPC 흐름 로그에서 확인했습니다. 솔루션 아키텍트는 API가
VPC를 통해 통신하도록 솔루션을 구현해야 합니다.
코드를 가장 적게 변경하여 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
(올바른 옵션을 선택하고 AWS Certified Solutions Architect – Associate(SAA-C03) 학습
매뉴얼 또는 문서에서 자세한 설명을 제공하십시오.)
A. 인증을 위해 HTTP 헤더에 X-APl-Key 헤더를 추가합니다.
B. 인터페이스 엔드포인트를 사용합니다.
C. 게이트웨이 엔드포인트를 사용합니다.
D. 두 REST API 사이에 Amazon Simple Queue Service(Amazon SQS) 대기열을 추가합니다.
Answer: B
Explanation:
Using an interface endpoint will allow the BuyStock RESTful web service and the
CheckFunds RESTful web service to communicate through the VPC without any changes to
the code. An interface endpoint creates an elastic network interface (ENI) in the customer's
VPC, and then configures the route tables to route traffic from the APIs to the ENI. This will
ensure that the two APIs will communicate through the VPC without any changes to the
code.
QUESTION NO: 633
회사는 다른 팀이 액세스할 수 있도록 하루에 한 번씩 데이터베이스를 Amazon S3로 내보내야
합니다. 내보낸 개체 크기는 2GB에서 5GB 사이입니다. 데이터에 대한 S3 액세스 패턴은
435

IT Certification Guaranteed, The Easy Way!
가변적이며 빠르게 변화합니다. 데이터는 즉시 사용할 수 있어야 하며 최대 3개월 동안
액세스할 수 있어야 합니다. 회사에는 검색 시간을 늘리지 않는 가장 비용 효율적인 솔루션이
필요합니다. 이러한 요구 사항을 충족하려면 회사에서 어떤 S3 스토리지 클래스를 사용해야
합니까?
A. S3 지능형 계층화
B. S3 Glacier 즉시 검색
C. S3 표준
D. S3 표준-Infrequent Access(S3 표준-IA)
Answer: D
Explanation:
S3 Intelligent-Tiering is a cost-optimized storage class that automatically moves data to the
most cost- effective access tier based on changing access patterns. Although it offers cost
savings, it also introduces additional latency and retrieval time into the data retrieval process,
which may not meet the requirement of
"immediately available" data. On the other hand, S3 Standard-Infrequent Access (S3
Standard-IA) provides low cost storage with low latency and high throughput performance. It
is designed for infrequently accessed data that can be recreated if lost, and can be retrieved
in a timely manner if required. It is a cost-effective solution that meets the requirement of
immediately available data and remains accessible for up to 3 months.
QUESTION NO: 634
한 회사가 MySQL을 사용하는 온프레미스 온라인 트랜잭션 처리(OLTP) 데이터베이스를
AWS 관리형 데이터베이스 관리 시스템으로 마이그레이션할 계획입니다. 여러 보고 및 분석
애플리케이션이 주말과 매월 말에 온프레미스 데이터베이스를 많이 사용합니다. 클라우드
기반 솔루션은 주말과 매월 말에 읽기가 많은 급증을 처리할 수 있어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 데이터베이스를 Amazon Aurora MySQL 클러스터로 마이그레이션합니다. Aurora Auto
Scaling을 구성하여 복제본을 사용하여 급증을 처리합니다.
B. MySQL을 실행하는 Amazon EC2 인스턴스로 데이터베이스를 마이그레이션합니다. 임시
스토리지가 있는 EC2 인스턴스 유형을 사용합니다. 인스턴스에 Amazon EBS Provisioned
IOPS SSD(io2) 볼륨을 연결합니다.
C. 데이터베이스를 Amazon RDS for MySQL 데이터베이스로 마이그레이션합니다. RDS for
MySQL 데이터베이스를 Multi-AZ 배포에 맞게 구성하고 자동 스케일링을 설정합니다.
D. 데이터베이스에서 Amazon Redshift로 마이그레이션합니다. OLTP 및 분석 애플리케이션
모두에 대한 데이터베이스로 Amazon Redshift를 사용합니다.
Answer: A
Explanation:
* A. Aurora MySQL: Handles OLTP workloads efficiently with built-in replication and auto-
scaling capabilities.
* B. EC2 with MySQL: Requires heavy manual maintenance and does not scale seamlessly.
* C. RDS for MySQL: Limited in auto-scaling compared to Aurora.
* D. Redshift: Primarily for OLAP, not suitable for OLTP workloads.
References: Amazon Aurora
QUESTION NO: 635
436

IT Certification Guaranteed, The Easy Way!
회사는 Amazon S3에 데이터를 저장해야 하며 데이터가 변경되는 것을 방지해야 합니다.
회사는 Amazon S3에 업로드된 새 객체가 회사가 객체를 수정하기로 결정할 때까지 불특정
시간 동안 변경 불가능한 상태로 유지되기를 원합니다. 회사 AWS 계정의 특정 사용자만
객체를 삭제할 수 있습니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야
합니까?
A. S3 Glacier 볼트 생성 WORM(Write-Once, Read-Many) 볼트 잠금 정책을 객체에
적용합니다.
B. S3 객체 잠금이 활성화된 S3 버킷 생성 버전 관리 활성화 보존 기간을 100년으로 설정
거버넌스 모드를 새 객체에 대한 S3 버킷의 기본 보존 모드로 사용
C. S3 버킷 생성 AWS CloudTrail을 사용하여 객체를 수정하는 모든 S3 API 이벤트를 랙에
알림 시 회사가 보유하고 있는 백업 버전에서 수정된 객체를 복원합니다.
D. S3 객체 잠금이 활성화된 S3 버킷 생성 버전 관리 활성화 객체에 법적 보존 추가 객체를
삭제해야 하는 사용자의 IAM 정책에 s3 PutObjectLegalHold 권한 추가
Answer: D
Explanation:
"The Object Lock legal hold operation enables you to place a legal hold on an object version.
Like setting a retention period, a legal hold prevents an object version from being overwritten
or deleted. However, a legal hold doesn't have an associated retention period and remains in
effect until removed." https://docs.aws.
amazon.com/AmazonS3/latest/userguide/batch-ops-legal-hold.html
QUESTION NO: 636
한 회사가 AWS 클라우드에서 새로운 내부 웹 애플리케이션을 설계하고 있습니다. 새로운
애플리케이션은 AWS 관리 서비스에서 여러 직원의 사용자 이름과 비밀번호를 안전하게
검색하고 저장해야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. 직원 자격 증명을 AWS Systems Manager Parameter Store에 저장합니다. AWS Cloud
Formation과 BatchGetSecretValue API를 사용하여 Parameter Store에서 사용자 이름과
비밀번호를 검색합니다.
B. AWS Secrets Manager에 직원 자격 증명을 저장합니다. AWS Cloud Formation과 AWS
Batch를 BatchGetSecretValue API와 함께 사용하여 Secrets Manager에서 사용자 이름과
비밀번호를 검색합니다.
C. 직원 자격 증명을 AWS Systems Manager Parameter Store에 저장합니다. AWS Cloud
Formation과 AWS Batch를 BatchGetSecretValue API와 함께 사용하여 Parameter Store에서
사용자 이름과 비밀번호를 검색합니다.
D. AWS Secrets Manager에 직원 자격 증명을 저장합니다. AWS Cloud Formation과
BatchGetSecretValue API를 사용하여 Secrets Manager에서 사용자 이름과 비밀번호를
검색합니다.
Answer: D
Explanation:
AWS Secrets Manager is the best solution for securely storing and managing sensitive
information, such as usernames and passwords. Secrets Manager provides automatic
rotation, fine-grained access control, and encryption of credentials. It is designed to integrate
easily with other AWS services, such as CloudFormation, to automate the retrieval of secrets
437

IT Certification Guaranteed, The Easy Way!
via the BatchGetSecretValue API.
Secrets Manager has a lower operational overhead than manually managing credentials, and
it offers features like automatic secret rotation that reduce the need for human intervention.
* Option A and C (Parameter Store): While Systems Manager Parameter Store can store
secrets, Secrets Manager provides more specialized capabilities for securely managing and
rotating credentials with less operational overhead.
* Option B and C (AWS Batch): Introducing AWS Batch unnecessarily complicates the
solution.
Secrets Manager already provides simple API calls for retrieving secrets without needing an
additional service.
AWS References:
* AWS Secrets Manager
* Secrets Manager with CloudFormation
QUESTION NO: 637
한 회사는 최근 해양 조사에서 얻은 200TB의 데이터를 AWS Snowball Edge Storage
Optimized 디바이스에 복사합니다. 이 회사는 석유 및 가스 매장지를 찾기 위해 AWS에
호스팅되는 고성능 컴퓨팅(HPC) 클러스터를 보유하고 있습니다. 솔루션 아키텍트는
Snowball Edge Storage Optimized 디바이스의 데이터에 대한 일관된 밀리초 미만의 지연
시간과 높은 처리량 액세스를 클러스터에 제공해야 합니다. 회사는 디바이스를 AWS로 다시
보내고 있습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon S3 버킷을 생성합니다. 데이터를 S3 버킷으로 가져옵니다. S3 버킷을 사용하도록
AWS Storage Gateway 파일 게이트웨이를 구성합니다. HPC 클러스터 인스턴스에서 파일
게이트웨이에 액세스합니다.
B. Amazon S3 버킷을 생성합니다. 데이터를 S3 버킷으로 가져옵니다. Lustre 파일 시스템용
Amazon FSx를 구성하고 이를 S3 버킷과 통합합니다. HPC 클러스터 인스턴스에서 FSx for
Lustre 파일 시스템에 액세스합니다.
C. Amazon S3 버킷과 Amazon Elastic File System(Amazon EFS) 파일 시스템을 생성합니다.
데이터를 S3 버킷으로 가져옵니다. S3 버킷의 데이터를 EFS 파일 시스템에 복사합니다. HPC
클러스터 인스턴스에서 EFS 파일 시스템에 액세스합니다.
D. Lustre 파일 시스템용 Amazon FSx를 생성합니다. 데이터를 FSx for Lustre 파일
시스템으로 직접 가져옵니다. HPC 클러스터 인스턴스에서 FSx for Lustre 파일 시스템에
액세스합니다.
Answer: B
Explanation:
To provide the HPC cluster with consistent sub-millisecond latency and high-throughput
access to the data on the Snowball Edge Storage Optimized devices, a solutions architect
should configure an Amazon FSx for Lustre file system, and integrate it with an Amazon S3
bucket. This solution has the following benefits:
* It allows the HPC cluster to access the data on the Snowball Edge devices using a POSIX-
compliant file system that is optimized for fast processing of large datasets1.
* It enables the data to be imported from the Snowball Edge devices into the S3 bucket using
the AWS Snow Family Console or the AWS CLI2. The data can then be accessed from the
FSx for Lustre file system using the S3 integration feature3.
438

IT Certification Guaranteed, The Easy Way!
* It supports high availability and durability of the data, as the FSx for Lustre file system can
automatically copy the data to and from the S3 bucket3. The data can also be accessed from
other AWS services or applications using the S3 API4.
References:
* 1: https://aws.amazon.com/fsx/lustre/
* 2: https://docs.aws.amazon.com/snowball/latest/developer-guide/using-adapter.html
* 3: https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-fs-linked-data-repo.html
* 4: https://docs.aws.amazon.com/fsx/latest/LustreGuide/export-data-repo.html
QUESTION NO: 638
회사는 Amazon EC2에서 콘텐츠 관리 시스템(CMS)을 사용하는 웹 사이트를 운영합니다.
CMS는 단일 EC2 인스턴스에서 실행되며 데이터 계층에 Amazon Aurora MySQL 다중 AZ DB
인스턴스를 사용합니다. 웹 사이트 이미지는 EC2 인스턴스 내부에 탑재된 Amazon Elastic
Block Store(Amazon EBS) 볼륨에 저장됩니다.
웹 사이트의 성능과 복원력을 개선하기 위해 솔루션 설계자가 취해야 하는 작업 조합은
무엇입니까? (2개를 선택하세요.)
A. 웹사이트 이미지를 모든 EC2 인스턴스에 탑재된 Amazon S3 버킷으로 이동합니다.
B. 기본 EC2 인스턴스의 NFS 공유를 사용하여 웹사이트 이미지를 공유합니다. 이 공유를
다른 EC2 인스턴스에 마운트합니다.
C. 웹 사이트 이미지를 모든 EC2 인스턴스에 탑재된 Amazon Elastic File System(Amazon
EFS) 파일 시스템으로 이동합니다.
D. 기존 EC2 인스턴스에서 Amazon 머신 이미지(AMI) 생성 AMI를 사용하여 Auto Scaling
그룹의 일부로 Application Load Balancer 뒤에 새 인스턴스를 프로비저닝합니다. 최소 2개의
인스턴스를 유지하도록 Auto Scaling 그룹을 구성합니다. 웹 사이트에 대한 AWS Global
Accelerator에서 액셀러레이터를 구성합니다.
E. 기존 EC2 인스턴스에서 Amazon 머신 이미지(AMI)를 생성합니다. AMI를 사용하여 Auto
Scaling 그룹의 일부로 Application Load Balancer 뒤에 새 인스턴스를 프로비저닝합니다.
최소 2개의 인스턴스를 유지하도록 Auto Scaling 그룹을 구성합니다. 웹 사이트에 대한
Amazon CloudFront 배포를 구성합니다.
Answer: C E
Explanation:
Option C provides moving the website images onto an Amazon EFS file system that is
mounted on every EC2 instance. Amazon EFS provides a scalable and fully managed file
storage solution that can be accessed concurrently from multiple EC2 instances. This
ensures that the website images can be accessed efficiently and consistently by all
instances, improving performance In Option E The Auto Scaling group maintains a minimum
of two instances, ensuring resilience by automatically replacing any unhealthy instances.
Additionally, configuring an Amazon CloudFront distribution for the website further improves
performance by caching content at edge locations closer to the end-users, reducing latency
and improving content delivery.
Hence combining these actions, the website's performance is improved through efficient
image storage and content delivery
QUESTION NO: 639
한 회사의 AWS 클라우드에 수백 개의 Amazon EC2 Linux 기반 인스턴스가 있습니다. 시스템
439

IT Certification Guaranteed, The Easy Way!
관리자는 공유 SSH 키를 사용하여 인스턴스를 관리했습니다. 최근 감사 후 회사 보안 팀은
모든 공유 키를 제거하도록 명령했습니다. 솔루션 아키텍트는 EC2 인스턴스에 대한 보안
액세스를 제공하는 솔루션을 설계해야 합니다.
최소한의 관리 오버헤드로 이 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Systems Manager Session Manager를 사용하여 EC2 인스턴스에 연결합니다.
B. AWS Security Token Service(AWS STS)를 사용하여 요청 시 일회용 SSH 키를
생성합니다.
C. 배스천 인스턴스 집합에 대한 공유 SSH 액세스를 허용합니다. 배스천 인스턴스에서 SSH
액세스만 허용하도록 다른 모든 인스턴스를 구성합니다.
D. Amazon Cognito 사용자 지정 권한 부여자를 사용하여 사용자를 인증합니다. AWS
Lambda 함수를 호출하여 임시 SSH 키를 생성합니다.
Answer: A
Explanation:
Session Manager is a fully managed AWS Systems Manager capability. With Session
Manager, you can manage your Amazon Elastic Compute Cloud (Amazon EC2) instances,
edge devices, on-premises servers, and virtual machines (VMs). You can use either an
interactive one-click browser-based shell or the AWS Command Line Interface (AWS CLI).
Session Manager provides secure and auditable node management without the need to open
inbound ports, maintain bastion hosts, or manage SSH keys. Session Manager also allows
you to comply with corporate policies that require controlled access to managed nodes, strict
security practices, and fully auditable logs with node access details, while providing end
users with simple one-click cross-platform access to your managed nodes.
https://docs.aws.amazon.com/systems-manager/latest/userguide
/session-manager.html
QUESTION NO: 640
회사는 매월 통화 기록 파일을 저장합니다. 사용자는 통화 후 1년 이내에 파일에 무작위로
액세스하지만 1년 이후에는 파일에 자주 액세스하지 않습니다. 이 회사는 사용자에게 1년
미만의 파일을 가능한 한 빨리 쿼리하고 검색할 수 있는 기능을 제공하여 솔루션을
최적화하려고 합니다. 오래된 파일을 검색하는 데 있어 지연은 허용됩니다.
어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?
A. Amazon S3 Glacier Instant Retrieval에 태그가 있는 개별 파일을 저장합니다. 태그를
쿼리하여 S3 Glacier Instant Retrieval에서 파일을 검색합니다.
B. Amazon S3 Intelligent-Tiering에 개별 파일을 저장합니다. S3 수명 주기 정책을 사용하여
1년 후 파일을 S3 Glacier Flexible Retrieval로 이동합니다. Amazon Athena를 사용하여
Amazon S3에 있는 파일을 쿼리하고 검색합니다. S3 Glacier Select를 사용하여 S3 Glacier에
있는 파일을 쿼리하고 검색합니다.
C. Amazon S3 Standard 스토리지에 태그가 있는 개별 파일을 저장합니다. Amazon S3
Standard 스토리지의 각 아카이브에 대한 검색 메타데이터를 저장합니다. S3 수명 주기
정책을 사용하여 1년 후에 파일을 S3 Glacier Instant Retrieval로 이동합니다. Amazon S3에서
메타데이터를 검색하여 파일을 쿼리하고 검색합니다.
D. Amazon S3 Standard 스토리지에 개별 파일을 저장합니다. S3 수명 주기 정책을 사용하여
1년 후에 파일을 S3 Glacier Deep Archive로 이동합니다. Amazon RDS에 검색 메타데이터를
저장합니다. Amazon RDS에서 파일을 쿼리합니다. S3 Glacier Deep Archive에서 파일을
440

IT Certification Guaranteed, The Easy Way!
검색합니다.
Answer: B
Explanation:
"For archive data that needs immediate access, such as medical images, news media
assets, or genomics data, choose the S3 Glacier Instant Retrieval storage class, an archive
storage class that delivers the lowest cost storage with milliseconds retrieval. For archive
data that does not require immediate access but needs the flexibility to retrieve large sets of
data at no cost, such as backup or disaster recovery use cases, choose S3 Glacier Flexible
Retrieval (formerly S3 Glacier), with retrieval in minutes or free bulk retrievals in 5-12 hours."
https://aws.amazon.com/about-aws/whats-new/2021/11/amazon-s3-glacier-instant-retrieval-
storage- class/
QUESTION NO: 641
회사에서는 S3 버킷의 PII를 어떻게 감지하고 보안 팀에 알릴 수 있나요?
A. Amazon Macie를 사용합니다. SensitiveData 결과에 대한 EventBridge 규칙을 만들고 SNS
알림을 보냅니다.
B. Amazon GuardDuty를 사용합니다. CRITICAL 결과에 대한 EventBridge 규칙을 만들고
SNS 알림을 보냅니다.
C. Amazon Macie를 사용합니다. SensitiveData:S3Object/Personal 검색 결과에 대한
EventBridge 규칙을 만들고 SQS 알림을 보냅니다.
D. Amazon GuardDuty를 사용합니다. CRITICAL 결과에 대한 EventBridge 규칙을 만들고
SQS 알림을 보냅니다.
Answer: A
* Amazon Macie is purpose-built for detecting PII in S3.
* Option A uses EventBridge to filter SensitiveData findings and notify via SNS, meeting the
requirements.
* Options B and D involve GuardDuty, which is not designed for PII detection.
* Option C uses SQS, which is less suitable for immediate notifications.
QUESTION NO: 642
트랜잭션 처리 회사에는 Amazon EC2 인스턴스에서 실행되는 주간 스크립트 배치 작업이
있습니다. EC2 인스턴스는 Auto Scaling 그룹에 있습니다. 트랜잭션 수는 다양할 수 있지만 각
실행 시 기록되는 기본 CPU 사용률은 60% 이상입니다. 회사는 작업이 실행되기 30분 전에
용량을 프로비저닝해야 합니다.
현재 엔지니어링에서는 Auto Scaling 그룹 매개변수를 수동으로 수정하여 이 작업을
완료합니다. 회사에는 Auto Scaling 그룹 수에 필요한 용량 추세를 분석할 리소스가 없습니다.
회사에는 Auto Scaling 그룹의 용량을 수정하는 자동화된 방법이 필요합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Auto Scaling 그룹에 대한 동적 조정 정책을 생성합니다. CPU 사용률 지표를 기준으로
60%로 확장하도록 정책을 구성합니다.
B. Auto Scaling 그룹에 대한 예약된 조정 정책을 생성합니다. 원하는 용량, 최소 용량, 최대
용량을 적절하게 설정하세요. 반복을 매주로 설정합니다. 시작 시간을 30분으로 설정합니다.
일괄 작업이 실행되기 전입니다.
C. Auto Scaling 그룹에 대한 예측 조정 정책을 생성합니다. 예측에 따라 확장되도록 정책을
구성합니다. 확장 지표를 CPU 사용률로 설정합니다. 측정항목의 목표 값을 60%로
441

IT Certification Guaranteed, The Easy Way!
설정합니다. 정책에서 작업이 실행되기 30분 전에 인스턴스가 사전 실행되도록 설정합니다.
D. Auto Scaling 그룹의 CPU 사용률 지표 값이 60%에 도달하면 AWS Lamda 함수를
호출하는 Amazon EventBridge 이벤트를 생성합니다. Auto Scaling 그룹의 원하는 용량과
최대 용량을 20% 늘리도록 Lambda 함수를 구성합니다.
Answer: C
Explanation:
This option is the most efficient because it uses a predictive scaling policy for the Auto
Scaling group, which is a type of scaling policy that uses machine learning to predict capacity
requirements based on historical data from CloudWatch1. It also configures the policy to
scale based on forecast, which enables the Auto Scaling group to adjust its capacity in
advance of traffic changes. It also sets the scaling metric to CPU utilization and the target
value for the metric to 60%, which aligns with the baseline CPU utilization that is noted on
each run. It also sets the instances to pre-launch 30 minutes before the jobs run, which
ensures that enough capacity is provisioned before the weekly scripted batch jobs start. This
solution meets the requirement of provisioning the capacity 30 minutes before the jobs run
with the least operational overhead. Option A is less efficient because it uses a dynamic
scaling policy for the Auto Scaling group, which is a type of scaling policy that adjusts your
Auto Scaling group's capacity in response to changing demand2. However, this does not
provide a way to provision the capacity 30 minutes before the jobs run, as it only reacts to
changing traffic. Option B is less efficient because it uses a scheduled scaling policy for the
Auto Scaling group, which is a type of scaling policy that lets you scale your Auto Scaling
group based on a schedule that you create3. However, this does not provide a way to scale
based on forecast or CPU utilization, as it only scales based on predefined metrics and
policies. Option D is less efficient because it uses an Amazon EventBridge event to invoke an
AWS Lambda function when the CPU utilization metric value for the Auto Scaling group
reaches 60%, which is a way to trigger serverless functions based on events. However, this
does not provide a way to provision the capacity 30 minutes before the jobs run, as it only
reacts to changing traffic.
QUESTION NO: 643
한 회사가 AWS에서 3티어 애플리케이션을 구축하고 있습니다. 프레젠테이션 계층은 정적 웹
사이트를 제공합니다. 논리 계층은 컨테이너화된 애플리케이션입니다. 이 응용 프로그램은
관계형 데이터베이스에 데이터를 저장합니다. 회사는 배포를 단순화하고 운영 비용을
절감하기를 원합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon S3를 사용하여 정적 콘텐츠를 호스팅합니다. 컴퓨팅 성능을 위해 AWS Fargate와
함께 Amazon Elastic Container Service(Amazon ECS)를 사용하십시오. 데이터베이스에
관리형 Amazon RDS 클러스터를 사용합니다.
B. Amazon CloudFront를 사용하여 정적 콘텐츠를 호스팅합니다. 컴퓨팅 성능을 위해 Amazon
EC2와 함께 Amazon Elastic Container Service(Amazon ECS)를 사용하세요. 데이터베이스에
관리형 Amazon RDS 클러스터를 사용합니다.
C. Amazon S3를 사용하여 정적 콘텐츠를 호스팅합니다. 컴퓨팅 성능을 위해 AWS Fargate와
함께 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용하십시오. 데이터베이스에
관리형 Amazon RDS 클러스터를 사용합니다.
D. Amazon EC2 예약 인스턴스를 사용하여 정적 콘텐츠를 호스팅합니다. 컴퓨팅 성능을 위해
442

IT Certification Guaranteed, The Easy Way!
Amazon EC2와 함께 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용하십시오.
데이터베이스에 관리형 Amazon RDS 클러스터를 사용합니다.
Answer: A
Explanation:
Amazon S3 is an object storage service that offers industry-leading scalability, data
availability, security, and performance. You can use Amazon S3 to host static content for
your website, such as HTML files, images, videos, etc. Amazon Elastic Container Service
(Amazon ECS) is a fully managed container orchestration service that allows you to run and
scale containerized applications on AWS. AWS Fargate is a serverless compute engine for
containers that works with both Amazon ECS and Amazon EKS. Fargate makes it easy for
you to focus on building your applications by removing the need to provision and manage
servers. You can use Amazon ECS with AWS Fargate for compute power for your
containerized application logic tier.
Amazon RDS is a managed relational database service that makes it easy to set up, operate,
and scale a relational database in the cloud. You can use a managed Amazon RDS cluster
for the database tier of your application. This solution will simplify deployment and reduce
operational costs for your three-tier application. References:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html
https://docs.aws.amazon.com
/AmazonRDS/latest/UserGuide/Welcome.html
QUESTION NO: 644
회사는 AWS에서 여러 Windows 워크로드를 실행합니다. 회사 직원은 두 개의 Amazon EC2
인스턴스에서 호스팅되는 Windows 파일 공유를 사용합니다. 파일 공유는 서로 간에 데이터를
동기화하고 중복 복사본을 유지합니다. 이 회사는 사용자가 현재 파일에 액세스하는 방식을
보존하는 고가용성 및 내구성 스토리지 솔루션을 원합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 모든 데이터를 Amazon S3로 마이그레이션 사용자가 파일에 액세스할 수 있도록 IAM 인증
설정
B. Amazon S3 파일 게이트웨이를 설정합니다. 기존 EC2 인스턴스에 S3 파일 게이트웨이를
탑재합니다.
C. 파일 공유 환경을 다중 AZ 구성을 사용하여 Windows 파일 서버용 Amazon FSx로
확장합니다. 모든 데이터를 Windows 파일 서버용 FSx로 마이그레이션합니다.
D. 다중 AZ 구성을 사용하여 파일 공유 환경을 Amazon Elastic File System(Amazon
EFS)으로 확장합니다. 모든 데이터를 Amazon EFS로 마이그레이션합니다.
Answer: C
Explanation:
https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEFS.html Amazon
FSx for Windows File Server provides fully managed Microsoft Windows file servers, backed
by a fully native Windows file system.
https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html
QUESTION NO: 645
솔루션 설계자는 건물 내 비즈니스 임차인의 시간당 에너지 소비량을 저장하는 워크로드를
443

IT Certification Guaranteed, The Easy Way!
설계하고 있습니다. 센서는 각 테넌트의 사용량을 합산하는 HTTP 요청을 통해
데이터베이스를 제공합니다. 솔루션 설계자는 가능하면 관리형 서비스를 사용해야 합니다.
솔루션 아키텍트가 독립 구성 요소를 추가하면 워크로드에 앞으로 더 많은 기능이 제공될
것입니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Lambda 함수와 함께 Amazon API Gateway를 사용하여 센서로부터 데이터를
수신하고, 데이터를 처리하고, Amazon DynamoDB 테이블에 데이터를 저장합니다.
B. Amazon EC2 인스턴스의 Auto Scaling 그룹에서 지원하는 Elastic Load Balancer를
사용하여 센서에서 데이터를 수신하고 처리합니다. Amazon S3 버킷을 사용하여 처리된
데이터를 저장합니다.
C. AWS Lambda 함수와 함께 Amazon API Gateway를 사용하여 센서로부터 데이터를
수신하고, 데이터를 처리하고, Amazon EC2 인스턴스의 Microsoft SQL Server Express
데이터베이스에 데이터를 저장합니다.
D. Amazon EC2 인스턴스의 Auto Scaling 그룹에서 지원하는 Elastic Load Balancer를
사용하여 센서에서 데이터를 수신하고 처리합니다. Amazon Elastic File System(Amazon
EFS) 공유 파일 시스템을 사용하여 처리된 데이터를 저장합니다.
Answer: A
Explanation:
To use an event-driven programming model with AWS Lambda and reduce operational
overhead, Amazon API Gateway and Amazon DynamoDB are suitable solutions. Amazon
API Gateway can receive the data from the sensors and invoke AWS Lambda functions to
process the data. AWS Lambda can run code without provisioning or managing servers, and
scale automatically with the incoming requests. Amazon DynamoDB can store the data in a
fast and flexible NoSQL database that can handle any amount of data with consistent
performance.
References:
* What Is Amazon API Gateway?
* What Is AWS Lambda?
* What Is Amazon DynamoDB?
QUESTION NO: 646
회사에서 이미지 처리를 위한 2계층 애플리케이션을 운영하고 있습니다. 애플리케이션은
각각 1개의 퍼블릭 서브넷과 1개의 프라이빗 서브넷이 있는 2개의 가용 영역을 사용합니다. 웹
계층용 ALB(Application Load Balancer)는 퍼블릭 서브넷을 사용합니다. 애플리케이션 계층의
Amazon EC2 인스턴스는 프라이빗 서브넷을 사용합니다.
사용자는 응용 프로그램이 예상보다 느리게 실행되고 있다고 보고합니다. 웹 서버 로그
파일의 보안 감사 결과 애플리케이션이 소수의 IP 주소로부터 수백만 건의 불법 요청을 받고
있는 것으로 나타났습니다. 솔루션 설계자는 회사가 보다 영구적인 솔루션을 조사하는 동안
즉각적인 성능 문제를 해결해야 합니다.
이 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까?
A. 웹 계층에 대한 인바운드 보안 그룹을 수정합니다. 리소스를 소비하는 IP 주소에 대한 거부
규칙을 추가합니다.
B. 웹 계층 서브넷에 대한 네트워크 ACL을 수정합니다. 리소스를 소비하는 IP 주소에 대한
인바운드 거부 규칙 추가
C. 애플리케이션 계층에 대한 인바운드 보안 그룹을 수정합니다. 리소스를 소비하는 IP 주소에
444

IT Certification Guaranteed, The Easy Way!
대한 거부 규칙을 추가합니다.
D. 애플리케이션 계층 서브넷에 대한 네트워크 ACL을 수정합니다. 리소스를 소비하는 IP
주소에 대한 인바운드 거부 규칙 추가
Answer: B
Explanation:
Deny the request from the first entry at the public subnet, dont allow it to cross and get to the
private subnet.
In this scenario, the security audit reveals that the application is receiving millions of
illegitimate requests from a small number of IP addresses. To address this issue, it is
recommended to modify the network ACL (Access Control List) for the web tier subnets. By
adding an inbound deny rule specifically targeting the IP addresses that are consuming
resources, the network ACL can block the illegitimate traffic at the subnet level before it
reaches the web servers. This will help alleviate the excessive load on the web tier and
improve the application's performance.
QUESTION NO: 647
회사의 애플리케이션은 여러 가용 영역에 있는 Amazon EC2 인스턴스에서 실행됩니다.
애플리케이션은 타사 애플리케이션에서 실시간 데이터를 수집해야 합니다.
회사에는 수집된 원시 데이터를 Amazon S3 버킷에 배치하는 데이터 수집 솔루션이
필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 데이터 수집을 위한 Amazon Kinesis 데이터 스트림을 생성합니다. Kinesis 데이터
스트림을 사용하기 위해 Amazon Kinesis Data Firehose 전송 스트림을 생성합니다. S3
버킷을 전송 스트림의 대상으로 지정합니다.
B. AWS Database Migration Service(AWS DMS)에서 데이터베이스 마이그레이션 작업을
생성합니다. EC2 인스턴스의 복제 인스턴스를 소스 엔드포인트로 지정합니다. S3 버킷을
대상 엔드포인트로 지정합니다. 기존 데이터를 마이그레이션하고 지속적인 변경 사항을
복제하도록 마이그레이션 유형을 설정합니다.
C. EC2 인스턴스에서 AWS DataSync 에이전트를 생성하고 구성합니다. EC2 인스턴스에서
S3 버킷으로 데이터를 전송하도록 DataSync 작업을 구성합니다.
D. 데이터 수집을 위해 애플리케이션에 대한 AWS Direct Connect 연결을 생성합니다.
Amazon Kinesis Data Firehose 전송 스트림을 생성하여 애플리케이션에서 직접 PUT 작업을
사용합니다. S3 버킷을 전송 스트림의 대상으로 지정합니다.
Answer: A
Explanation:
The solution that will meet the requirements is to create Amazon Kinesis data streams for
data ingestion, create Amazon Kinesis Data Firehose delivery streams to consume the
Kinesis data streams, and specify the S3 bucket as the destination of the delivery streams.
This solution will allow the company's application to ingest real-time data from third-party
applications and place the ingested raw data in an S3 bucket. Amazon Kinesis data streams
are scalable and durable streams that can capture and store data from hundreds of
thousands of sources. Amazon Kinesis Data Firehose is a fully managed service that can
deliver streaming data to destinations such as S3, Amazon Redshift, Amazon OpenSearch
Service, and Splunk. Amazon Kinesis Data Firehose can also transform and compress the
data before delivering it to S3.
445

IT Certification Guaranteed, The Easy Way!
The other solutions are not as effective as the first one because they either do not support
real-time data ingestion, do not work with third-party applications, or do not use S3 as the
destination. Creating database migration tasks in AWS Database Migration Service (AWS
DMS) will not support real-time data ingestion, as AWS DMS is mainly designed for migrating
relational databases, not streaming data. AWS DMS also requires replication instances,
source endpoints, and target endpoints to be compatible with specific database engines and
versions. Creating and configuring AWS DataSync agents on the EC2 instances will not work
with third-party applications, as AWS DataSync is a service that transfers data between on-
premises storage systems and AWS storage services, not between applications. AWS
DataSync also requires installing agents on the source or destination servers. Creating an
AWS Direct Connect connection to the application for data ingestion will not use S3 as the
destination, as AWS Direct Connect is a service that establishes a dedicated network
connection between on-premises and AWS, not between applications and storage services.
AWS Direct Connect also requires a physical connection to an AWS Direct Connect location.
References:
* Amazon Kinesis
* Amazon Kinesis Data Firehose
* AWS Database Migration Service
* AWS DataSync
* AWS Direct Connect
QUESTION NO: 648
마케팅 팀은 다가올 다종목 행사를 위한 캠페인을 구축하고자 합니다. 이 팀은 지난 5년간의
뉴스 기사를 PDF 형식으로 보유하고 있습니다. 이 팀은 뉴스 기사의 내용과 감정에 대한
통찰력을 추출하는 솔루션이 필요합니다. 이 솔루션은 Amazon Textract를 사용하여 뉴스
기사를 처리해야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. 추출된 통찰력을 분석을 위해 Amazon Athena에 제공합니다. 추출된 통찰력과 분석 내용을
Amazon S3 버킷에 저장합니다.
B. 추출된 통찰력을 Amazon DynamoDB 테이블에 저장합니다. Amazon SageMaker를
사용하여 감정 모델을 구축합니다.
C. 추출된 통찰력을 분석을 위해 Amazon Comprehend에 제공합니다. 분석을 Amazon S3
버킷에 저장합니다.
D. 추출된 인사이트를 Amazon S3 버킷에 저장합니다. Amazon QuickSight를 사용하여
데이터를 시각화하고 분석합니다.
Answer: C
Explanation:
Amazon Textract can extract text from the PDFs, and Amazon Comprehend is the most
suitable service to analyze the extracted text for sentiment and insights. Comprehend offers a
fully managed, low-operational overhead solution for analyzing text data. The results can
then be stored in an Amazon S3 bucket, ensuring scalability and easy access.
* Option A: Athena is for querying structured data and is not suitable for sentiment analysis.
* Option B: SageMaker adds complexity and is not necessary when Comprehend can handle
sentiment analysis natively.
* Option D: QuickSight is used for visualization and analytics, but it does not provide
446

IT Certification Guaranteed, The Easy Way!
sentiment analysis.
AWS References:
* Amazon Comprehend
* Amazon Textract
QUESTION NO: 649
회사는 온프레미스 데이터 센터에서 여러 워크로드를 실행합니다. 회사의 데이터 센터는
회사의 확장되는 비즈니스 요구 사항을 충족할 만큼 빠르게 확장할 수 없습니다. 회사는
AWS로의 마이그레이션을 계획하기 위해 온프레미스 서버 및 워크로드에 대한 사용량 및
구성 데이터를 수집하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Migration Hub에서 홈 AWS 리전을 설정합니다. AWS Systems Manager를 사용하여
온프레미스 서버에 대한 데이터를 수집합니다.
B. AWS Migration Hub에서 홈 AWS 리전을 설정합니다. AWS Application Discovery
Service를 사용하여 온프레미스 서버에 대한 데이터를 수집합니다.
C. AWS Schema Conversion Tool(AWS SCT)을 사용하여 관련 템플릿을 생성합니다. AWS
Trusted Advisor를 사용하여 온프레미스 서버에 대한 데이터를 수집합니다.
D. AWS SCT(AWS Schema Conversion Tool)를 사용하여 관련 템플릿을 생성합니다. AWS
Database Migration Service(AWS DMS)를 사용하여 온프레미스 서버에 대한 데이터를
수집합니다.
Answer: B
Explanation:
The most suitable solution for the company's requirements is to set the home AWS Region in
AWS Migration Hub and use AWS Application Discovery Service to collect data about the on-
premises servers. This solution will enable the company to gather usage and configuration
data of its on-premises servers and workloads, and plan a migration to AWS.
AWS Migration Hub is a service that simplifies and accelerates migration tracking by
aggregating migration status information into a single console. Users can view the discovered
servers, group them into applications, and track the migration status of each application from
the Migration Hub console in their home Region. The home Region is the AWS Region where
users store their migration data, regardless of which Regions they migrate into1.
AWS Application Discovery Service is a service that helps users plan their migration to AWS
by collecting usage and configuration data about their on-premises servers and databases.
Application Discovery Service is integrated with AWS Migration Hub and supports two
methods of performing discovery: agentless discovery and agent-based discovery. Agentless
discovery can be performed by deploying the Application Discovery Service Agentless
Collector through VMware vCenter, which collects static configuration data and utilization
data for virtual machines (VMs) and databases. Agent-based discovery can be performed by
deploying the AWS Application Discovery Agent on each of the VMs and physical servers,
which collects static configuration data, detailed time-series system-performance information,
inbound and outbound network connections, and processes that are running2.
The other options are not correct because they do not meet the requirements or are not
relevant for the use case. Using the AWS Schema Conversion Tool (AWS SCT) to create the
relevant templates and using AWS Trusted Advisor to collect data about the on-premises
servers is not correct because this solution is not suitable for collecting usage and
447

IT Certification Guaranteed, The Easy Way!
configuration data of on-premises servers and workloads. AWS SCT is a tool that helps users
convert database schemas and code objects from one database engine to another, such as
from Oracle to PostgreSQL3. AWS Trusted Advisor is a service that provides best practice
recommendations for cost optimization, performance, security, fault tolerance, and service
limits4. Using the AWS Schema Conversion Tool (AWS SCT) to create the relevant
templates and using AWS Database Migration Service (AWS DMS) to collect data about the
on-premises servers is not correct because this solution is not suitable for collecting usage
and configuration data of on-premises servers and workloads. As mentioned above, AWS
SCT is a tool that helps users convert database schemas and code objects from one
database engine to another.
AWS DMS is a service that helps users migrate relational databases, non-relational
databases, and other types of data stores to AWS with minimal downtime5.
References:
* Home Region - AWS Migration Hub
* What is AWS Application Discovery Service? - AWS Application Discovery Service
* AWS Schema Conversion Tool - Amazon Web Services
* What Is Trusted Advisor? - Trusted Advisor
* What Is AWS Database Migration Service? - AWS Database Migration Service
QUESTION NO: 650
한 회사에서 인기 있는 소셜 미디어 웹사이트를 운영하고 있습니다. 웹사이트는 사용자에게
이미지를 업로드하여 다른 사용자와 공유할 수 있는 기능을 제공합니다. 회사는 이미지에
부적절한 콘텐츠가 포함되지 않았는지 확인하고 싶습니다.
회사는 개발 노력을 최소화하는 솔루션이 필요합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. Amazon Comprehend를 사용하여 부적절한 콘텐츠를 감지합니다. 신뢰도가 낮은 예측에는
인적 검토를 사용합니다.
B. Amazon Rekognition을 사용하여 부적절한 콘텐츠를 감지합니다. 신뢰도가 낮은 예측에는
인적 검토를 사용합니다.
C. Amazon SageMaker를 사용하여 부적절한 콘텐츠를 감지합니다. 신뢰도가 낮은 예측에
레이블을 지정하려면 정답을 사용합니다.
D. AWS Fargate를 사용하여 사용자 지정 기계 학습 모델을 배포하여 부적절한 콘텐츠를
감지합니다. 신뢰도가 낮은 예측에 레이블을 지정하려면 정답을 사용합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html?pg=ln&sec=ft
https://docs.aws.amazon.com/rekognition/latest/dg/a2i-rekognition.html
QUESTION NO: 651
의료 기록 회사가 Amazon EC2 인스턴스에서 애플리케이션을 호스팅하고 있습니다.
애플리케이션은 Amazon S3에 저장된 고객 데이터 파일을 처리합니다. EC2 인스턴스는
퍼블릭 서브넷에서 호스팅됩니다. EC2 인스턴스는 인터넷을 통해 Amazon S3에
액세스하지만 다른 네트워크 액세스는 필요하지 않습니다.
새로운 요구 사항에서는 파일 전송을 위한 네트워크 트래픽이 개인 경로를 사용하고 인터넷을
통해 전송되지 않도록 규정하고 있습니다.
448

IT Certification Guaranteed, The Easy Way!
솔루션 설계자는 이 요구 사항을 충족하기 위해 네트워크 아키텍처에 대해 어떤 변경을
권장해야 합니까?
A. NAT 게이트웨이를 생성합니다. NAT 게이트웨이를 통해 Amazon S3로 트래픽을 보내도록
퍼블릭 서브넷의 라우팅 테이블을 구성합니다.
B. S3 접두사 목록에 대한 트래픽만 허용되도록 아웃바운드 트래픽을 제한하도록 EC2
인스턴스에 대한 보안 그룹을 구성합니다.
C. EC2 인스턴스를 프라이빗 서브넷으로 이동합니다. Amazon S3용 VPC 엔드포인트를
생성하고 엔드포인트를 프라이빗 서브넷의 라우팅 테이블에 연결합니다.
D. VPC에서 인터넷 게이트웨이를 제거합니다. AWS Direct Connect 연결을 설정하고 Direct
Connect 연결을 통해 Amazon S3로 트래픽을 라우팅합니다.
Answer: C
Explanation:
To meet the new requirement of transferring files over a private route, the EC2 instances
should be moved to private subnets, which do not have direct access to the internet. This
ensures that the traffic for file transfers does not go over the internet. To enable the EC2
instances to access Amazon S3, a VPC endpoint for Amazon S3 can be created. VPC
endpoints allow resources within a VPC to communicate with resources in other services
without the traffic being sent over the internet. By linking the VPC endpoint to the route table
for the private subnets, the EC2 instances can access Amazon S3 over a private connection
within the VPC.
QUESTION NO: 652
회사는 3계층 애플리케이션을 온프레미스에서 AWS로 마이그레이션하려고 합니다. 웹
계층과 애플리케이션 계층은 타사 VM(가상 머신)에서 실행됩니다. 데이터베이스 계층은
MySQL에서 실행됩니다.
회사는 아키텍처를 최소한으로 변경하여 애플리케이션을 마이그레이션해야 합니다. 또한
회사에는 데이터를 특정 시점으로 복원할 수 있는 데이터베이스 솔루션이 필요합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 웹 계층과 애플리케이션 계층을 프라이빗 서브넷의 Amazon EC2 인스턴스로
마이그레이션합니다. 데이터베이스 계층을 프라이빗 서브넷의 MySQL용 Amazon RDS로
마이그레이션합니다.
B. 웹 계층을 퍼블릭 서브넷의 Amazon EC2 인스턴스로 마이그레이션합니다. 애플리케이션
계층을 프라이빗 서브넷의 EC2 인스턴스로 마이그레이션합니다. 데이터베이스 계층을
프라이빗 서브넷의 Amazon Aurora MySQL로 마이그레이션합니다.
C. 웹 계층을 퍼블릭 서브넷의 Amazon EC2 인스턴스로 마이그레이션합니다. 애플리케이션
계층을 프라이빗 서브넷의 EC2 인스턴스로 마이그레이션합니다. 데이터베이스 계층을
프라이빗 서브넷의 MySQL용 Amazon RDS로 마이그레이션합니다.
D. 웹 계층과 애플리케이션 계층을 퍼블릭 서브넷의 Amazon EC2 인스턴스로
마이그레이션합니다. 데이터베이스 계층을 퍼블릭 서브넷의 Amazon Aurora MySQL로
마이그레이션합니다.
Answer: C
Explanation:
The solution that meets the requirements with the least operational overhead is to migrate
the web tier to Amazon EC2 instances in public subnets, migrate the application tier to EC2
449

IT Certification Guaranteed, The Easy Way!
instances in private subnets, and migrate the database tier to Amazon RDS for MySQL in
private subnets. This solution allows the company to migrate its three-tier application to AWS
by making minimal changes to the architecture, as it preserves the same web, application,
and database tiers and uses the same MySQL database engine. The solution also provides a
database solution that can restore data to a specific point in time, as Amazon RDS for
MySQL supports automated backups and point-in-time recovery. This solution also reduces
the operational overhead by using managed services such as Amazon EC2 and Amazon
RDS, which handle tasks such as provisioning, patching, scaling, and monitoring.
The other solutions do not meet the requirements as well as the first one because they either
involve more changes to the architecture, do not provide point-in-time recovery, or do not
follow best practices for security and availability. Migrating the database tier to Amazon
Aurora MySQL would require changing the database engine and potentially modifying the
application code to ensure compatibility. Migrating the web tier and the application tier to
public subnets would expose them to more security risks and reduce their availability in case
of a subnet failure. Migrating the database tier to public subnets would also compromise its
security and performance. References:
* Migrate Your Application Database to Amazon RDS
* Amazon RDS for MySQL
* Amazon Aurora MySQL
* Amazon VPC
QUESTION NO: 653
회사에 고객 주문을 처리하는 애플리케이션이 있습니다. 이 회사는 주문을 Amazon Aurora
데이터베이스에 저장하는 Amazon EC2 인스턴스에서 애플리케이션을 호스팅합니다. 가끔
트래픽이 높을 때: 워크로드가 주문을 충분히 빠르게 처리하지 못하는 경우가 있습니다.
가능한 한 빨리 데이터베이스에 주문을 안정적으로 기록하려면 솔루션 설계자가 무엇을 해야
합니까?
A. 트래픽이 많을 때 EC2 인스턴스의 인스턴스 크기를 늘립니다. Amazon Simple 알림
서비스(Amazon SNS)에 주문을 작성합니다. 데이터베이스 엔드포인트에서 SNS 주제를
구독하세요.
B. Amazon Simple Queue Service(Amazon SQS) 대기열에 주문을 작성합니다. Application
Load Balancer 뒤에 있는 Auto Scaling 그룹의 EC2 인스턴스를 사용하여 SQS 대기열에서
읽고 주문을 데이터베이스로 처리합니다.
C. Amazon Simple Notification Service(Amazon SNS)에 주문 쓰기 데이터베이스
엔드포인트에서 SNS 주제를 구독합니다. Application Load Balancer 뒤에 있는 Auto Scaling
그룹의 EC2 인스턴스를 사용하여 SNS 주제에서 읽습니다.
D. EC2 인스턴스가 CPU 임계값 한도에 도달하면 Amazon Simple Queue Service(Amazon
SQS) 대기열에 쓰기 명령을 보냅니다. Application Load Balancer 뒤의 Auto Scaling 그룹에서
EC2 인스턴스의 예약된 조정을 사용하여 SQS 대기열에서 읽고 주문을 데이터베이스로
처리합니다.
Answer: B
Explanation:
Amazon SQS is a fully managed message queuing service that can decouple and scale
microservices, distributed systems, and serverless applications. By writing orders to an SQS
queue, the application can handle spikes in traffic without losing any orders. The EC2
450

IT Certification Guaranteed, The Easy Way!
instances in an Auto Scaling group can read from the SQS queue and process orders into
the database at a steady pace. The Application Load Balancer can distribute the load across
the EC2 instances and provide health checks. This solution meets all the requirements of the
question, while the other options do not. References:
* https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/welcome.html
* https://aws.amazon.com/architecture/serverless/
* https://aws.amazon.com/sqs/
QUESTION NO: 654
한 회사는 최근 전 세계 고객을 대상으로 소매 웹사이트를 구축한다고 발표했습니다. 웹
사이트는 Elastic Load Balancer 뒤의 여러 Amazon EC2 인스턴스에서 실행됩니다.
인스턴스는 여러 가용 영역에 걸쳐 Auto Scaling 그룹에서 실행됩니다.
회사는 고객이 웹 사이트에 액세스하는 데 사용하는 장치에 따라 다양한 버전의 콘텐츠를
고객에게 제공하려고 합니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 조치 조합을 취해야 합니까? (2개를
선택하세요.)
A. 여러 버전의 콘텐츠를 캐시하도록 Amazon CloudFront를 구성합니다.
B. 트래픽을 다른 인스턴스로 전달하도록 Network Load Balancer의 호스트 헤더를
구성합니다.
C. User-Agent 헤더를 기반으로 사용자에게 특정 객체를 보내도록 Lambda@Edge 함수를
구성합니다.
D. AWS Global Accelerator를 구성합니다. NLB(Network Load Balancer)로 요청을
전달합니다. 다른 EC2 인스턴스에 대한 호스트 기반 라우팅을 설정하도록 NLB를 구성합니다.
E. AWS Global Accelerator를 구성합니다. NLB(Network Load Balancer)로 요청을
전달합니다. 다른 EC2 인스턴스에 대한 경로 기반 라우팅을 설정하도록 NLB를 구성합니다.
Answer: A C
Explanation:
For C: IMPROVED USER EXPERIENCE Lambda@Edge can help improve your users'
experience with your websites and web applications across the world, by letting you
personalize content for them without sacrificing performance. Real-time Image
Transformation You can customize your users' experience by transforming images on the fly
based on the user characteristics. For example, you can resize images based on the viewer's
device type-mobile, desktop, or tablet. You can also cache the transformed images at
CloudFront Edge locations to further improve performance when delivering images.
https://aws.amazon.com
/lambda/edge/
QUESTION NO: 655
한 회사에서 AWS Lambd와 함께 이벤트 기반 프로그래밍 모델을 사용하려고 합니다. 회사는
Java 11에서 실행되는 Lambda 함수의 시작 지연 시간을 줄이고 싶어합니다. 회사는
애플리케이션에 대한 엄격한 지연 시간 요구 사항을 갖고 있지 않습니다. 회사는 기능이
확장될 때 콜드 스타트와 이상치 지연 시간을 줄이고 싶어합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. Lambda 프로비저닝 동시성을 구성합니다.
B. Lambda 함수의 제한 시간을 늘립니다.
451

IT Certification Guaranteed, The Easy Way!
C. Lambda 함수의 메모리를 늘립니다.
D. Lambda SnapStart를 구성합니다.
Answer: D
Explanation:
To reduce startup latency for Lambda functions that run on Java 11, Lambda SnapStart is a
suitable solution.
Lambda SnapStart is a feature that enables faster cold starts and lower outlier latencies for
Java 11 functions.
Lambda SnapStart uses a pre-initialized Java Virtual Machine (JVM) to run the functions,
which reduces the initialization time and memory footprint. Lambda SnapStart does not incur
any additional charges.
References:
* Lambda SnapStart for Java 11 Functions
* Lambda SnapStart FAQs
QUESTION NO: 656
한 회사가 AWS에서 자체 관리형 DNS 서비스를 구현했습니다. 솔루션은 다음으로
구성됩니다.
* 다양한 AWS 지역의 Amazon EC2 인스턴스
* 표준 액셀러레이터의 끝점 AWS Global Accelerator
회사는 DDoS 공격으로부터 솔루션을 보호하려고 합니다. 이 요구 사항을 충족하려면 솔루션
설계자가 무엇을 해야 합니까?
A. AWS Shield Advanced 구독 보호할 리소스로 액셀러레이터 추가
B. AWS Shield Advanced 구독 EC2 인스턴스를 보호할 리소스로 추가
C. 속도 기반 규칙을 포함하는 AWS WAF 웹 ACL을 생성합니다. 웹 ACL을 액셀러레이터와
연결합니다.
D. 속도 기반 규칙을 포함하는 AWS WAF 웹 ACL을 생성합니다. 웹 ACL을 EC2 인스턴스와
연결합니다.
Answer: A
Explanation:
AWS Shield is a managed service that provides protection against Distributed Denial of
Service (DDoS) attacks for applications running on AWS. AWS Shield Standard is
automatically enabled to all AWS customers at no additional cost. AWS Shield Advanced is
an optional paid service. AWS Shield Advanced provides additional protections against more
sophisticated and larger attacks for your applications running on Amazon Elastic Compute
Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator,
and Route 53.
https://docs.aws.amazon.com/waf/latest/developerguide/ddos-event-mitigation-logic-gax.html
QUESTION NO: 657
한 회사는 전 세계에 고객을 두고 있습니다. 이 회사는 자동화를 사용하여 시스템과 네트워크
인프라를 보호하고자 합니다. 이 회사의 보안 팀은 인프라에 대한 모든 증분적 변경 사항을
추적하고 감사할 수 있어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. AWS Organizations를 사용하여 인프라를 설정합니다. AWS Config를 사용하여 변경
452

IT Certification Guaranteed, The Easy Way!
사항을 추적합니다.
B. AWS Cloud Formation을 사용하여 인프라를 설정합니다. AWS Config를 사용하여 변경
사항을 추적합니다.
C. AWS Organizations를 사용하여 인프라를 설정합니다. AWS Service Catalog를 사용하여
변경 사항을 추적합니다.
D. AWS Cloud Formation을 사용하여 인프라를 설정합니다. AWS Service Catalog를
사용하여 변경 사항을 추적합니다.
Answer: B
Explanation:
AWS CloudFormation allows for the automated, repeatable setup of infrastructure, reducing
human error and ensuring consistency. AWS Config provides the ability to track changes in
the infrastructure, ensuring that all changes are logged and auditable, which satisfies the
requirement for tracking incremental changes.
* Option A and C (AWS Organizations): AWS Organizations manage multiple accounts, but
they are not designed for infrastructure setup or change tracking.
* Option D (Service Catalog): Service Catalog is used for deploying products, not for setting
up infrastructure or tracking changes.
AWS References:
* AWS Config
* AWS CloudFormation
QUESTION NO: 658
한 회사는 다음 구성 요소로 구성된 새로운 다중 계층 웹 애플리케이션을 설계하고 있습니다.
* Auto Scaling 그룹의 일부로 Amazon EC2 인스턴스에서 실행되는 웹 및 애플리케이션 서버
* 데이터 저장을 위한 Amazon RDS DB 인스턴스
솔루션 설계자는 웹 서버만 액세스할 수 있도록 애플리케이션 서버에 대한 액세스를 제한해야
합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 애플리케이션 서버 앞에 AWS PrivateLink를 배포합니다. 웹 서버만 애플리케이션 서버에
액세스할 수 있도록 네트워크 ACL을 구성합니다.
B. 애플리케이션 서버 앞에 VPC 엔드포인트를 배포합니다. 웹 서버만 애플리케이션 서버에
액세스할 수 있도록 보안 그룹을 구성합니다.
C. 애플리케이션 서버의 Auto Scaling 그룹이 포함된 대상 그룹으로 Network Load Balancer를
배포합니다. 웹 서버만 애플리케이션 서버에 액세스할 수 있도록 네트워크 ACL을 구성합니다
.
D. 애플리케이션 서버의 Auto Scaling 그룹이 포함된 대상 그룹을 사용하여 Application Load
Balancer를 배포합니다. 웹 서버만 애플리케이션 서버에 액세스할 수 있도록 보안 그룹을
구성합니다.
Answer: D
Explanation:
* Application Load Balancer (ALB): ALB is suitable for routing HTTP/HTTPS traffic to the
application servers. It provides advanced routing features and integrates well with Auto
Scaling groups.
* Target Group Configuration:
* Create a target group for the application servers and register the Auto Scaling group with
this target group.
453

IT Certification Guaranteed, The Easy Way!
* Configure the ALB to forward requests from the web servers to the application servers.
* Security Group Setup:
* Configure the security group of the application servers to only allow traffic from the web
servers' security group.
* This ensures that only the web servers can access the application servers, meeting the
requirement to limit access.
* Benefits:
* Security: Using security groups to restrict access ensures a secure environment where only
intended traffic is allowed.
* Scalability: ALB works seamlessly with Auto Scaling groups, ensuring the application can
handle varying loads efficiently.
References:
* Application Load Balancer
* Security Groups for Your VPC
QUESTION NO: 659
회사는 온프레미스에서 Oracle 데이터베이스를 실행합니다. 회사는 AWS로
마이그레이션하는 과정의 일환으로 데이터베이스를 사용 가능한 최신 버전으로
업그레이드하려고 합니다. 또한 회사는 데이터베이스에 대한 재해 복구(DR)를 설정하려고
합니다. 회사는 일반 운영 및 DR 설정을 위한 운영 오버헤드를 최소화해야 합니다. 또한
회사는 데이터베이스의 기본 운영 체제에 대한 액세스를 유지해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Oracle 데이터베이스를 Amazon EC2 인스턴스로 마이그레이션합니다. 다른 AWS
리전으로 데이터베이스 복제를 설정합니다.
B. Oracle 데이터베이스를 Oracle용 Amazon RDS로 마이그레이션합니다. 스냅샷을 다른
AWS 리전에 복제하려면 교차 리전 자동 백업을 활성화하십시오.
C. Oracle 데이터베이스를 Oracle용 Amazon RDS Custom으로 마이그레이션합니다. 다른
AWS 리전에 있는 데이터베이스에 대한 읽기 전용 복제본을 생성합니다.
D. Oracle 데이터베이스를 Oracle용 Amazon RDS로 마이그레이션합니다. 다른 가용 영역에
대기 데이터베이스를 생성합니다.
Answer: C
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-custom.html and
https://docs.aws.amazon.
com/AmazonRDS/latest/UserGuide/working-with-custom-oracle.html
QUESTION NO: 660
회사에 VPC의 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있습니다. 애플리케이션
중 하나는 Amazon S3 API를 호출하여 객체를 저장하고 읽어야 합니다. 회사의 보안 규정에
따라 응용 프로그램의 트래픽은 인터넷을 통해 이동할 수 없습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. S3 인터페이스 끝점을 구성합니다.
B. S3 게이트웨이 엔드포인트를 구성합니다.
C. 프라이빗 서브넷에 S3 버킷을 생성합니다.
D. EC2 인스턴스와 동일한 리전에 S3 버킷을 생성합니다.
454

IT Certification Guaranteed, The Easy Way!
Answer: B
Explanation:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-
endpoints.html#types-of-vpc- endpoints-for-s3
https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html
QUESTION NO: 661
회사에 1,000개의 Amazon EC2 Linux 인스턴스에서 실행되는 프로덕션 워크로드가 있습니다.
워크로드는 타사 소프트웨어에 의해 구동됩니다. 회사는 중요한 보안 취약성을 수정하기 위해
가능한 한 빨리 모든 EC2 인스턴스에서 타사 소프트웨어를 패치해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. AWS Lambda 함수를 생성하여 모든 EC2 인스턴스에 패치를 적용합니다.
B. 모든 EC2 인스턴스에 패치를 적용하도록 AWS Systems Manager Patch Manager를
구성합니다.
C. AWS Systems Manager 유지 관리 기간을 예약하여 모든 EC2 인스턴스에 패치를
적용합니다.
D. AWS Systems Manager Run Command를 사용하여 모든 EC2 인스턴스에 패치를
적용하는 사용자 지정 명령을 실행합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/systems-manager/latest/userguide/about-windows-app-
patching.html
QUESTION NO: 662
한 회사가 Amazon S3에서 데이터 레이크를 호스팅하고 있습니다. 데이터 레이크는 다양한
데이터 소스에서 Apache Parquet 형식으로 데이터를 수집합니다. 회사는 수집된 데이터를
준비하기 위해 여러 변환 단계를 사용합니다. 이 단계에는 이상 항목 필터링, 데이터를 표준
날짜 및 시간 값으로 정규화, 분석을 위한 집계 생성이 포함됩니다.
회사는 변환된 데이터를 데이터 분석가가 액세스하는 S3 버킷에 저장해야 합니다. 회사에는
코드가 필요하지 않은 데이터 변환을 위해 사전 구축된 솔루션이 필요합니다. 솔루션은
데이터 계보 및 데이터 프로파일링을 제공해야 합니다. 회사는 회사 전체의 직원과 데이터
변환 단계를 공유해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 데이터를 변환하도록 AWS Glue Studio 시각적 캔버스를 구성합니다. AWS Glue 작업을
사용하여 변화 단계를 직원과 공유하세요.
B. 데이터를 변환하도록 Amazon EMR Serverless를 구성합니다. EMR Serveriess 작업을
사용하여 직원과 변환 단계를 공유하십시오.
C. 데이터를 변환하도록 AWS Glue DataBrew를 구성합니다. DataBrew 레시피를 사용하여
변환 단계를 직원과 공유하세요.
D. 데이터용 Amazon Athena 테이블을 생성합니다. Athena SQL 쿼리를 작성하여 데이터를
변환합니다. Athena SQL 쿼리를 직원과 공유하세요.
Answer: C
Explanation:
The most suitable solution for the company's requirements is to configure AWS Glue
455

IT Certification Guaranteed, The Easy Way!
DataBrew to transform the data and share the transformation steps with employees by using
DataBrew recipes. This solution will provide a prebuilt solution for data transformation that
does not require code, and will also provide data lineage and data profiling. The company
can easily share the data transformation steps with employees throughout the company by
using DataBrew recipes.
AWS Glue DataBrew is a visual data preparation tool that makes it easy for data analysts
and data scientists to clean and normalize data for analytics or machine learning by up to
80% faster. Users can upload their data from various sources, such as Amazon S3, Amazon
RDS, Amazon Redshift, Amazon Aurora, or Glue Data Catalog, and use a point-and-click
interface to apply over 250 built-in transformations. Users can also preview the results of
each transformation step and see how it affects the quality and distribution of the data1.
A DataBrew recipe is a reusable set of transformation steps that can be applied to one or
more datasets. Users can create recipes from scratch or use existing ones from the
DataBrew recipe library. Users can also export, import, or share recipes with other users or
groups within their AWS account or organization2.
DataBrew also provides data lineage and data profiling features that help users understand
and improve their data quality. Data lineage shows the source and destination of each
dataset and how it is transformed by each recipe step. Data profiling shows various statistics
and metrics about each dataset, such as column
QUESTION NO: 663
회사는 AWS Organizations에서 AWS 계정을 관리합니다. 계정에 대해 AWS 1AM Identity
Center(AWS Single Sign-On) 및 AWS Control Tower가 구성되어 있습니다. 회사는 모든
계정에 걸쳐 여러 사용자 권한을 관리하려고 합니다.
권한은 여러 1AM 사용자가 사용하며 개발자 팀과 관리자 팀 간에 분할되어야 합니다. 각
팀에는 서로 다른 권한이 필요합니다. 회사는 두 팀 모두에 고용된 신규 사용자를 포함하는
솔루션을 원합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 1AM ID 센터(또는 각 계정)에서 개별 사용자를 생성합니다. 1AM ID 센터에서 별도의
개발자 및 관리자 그룹을 생성합니다. 사용자를 적절한 그룹에 할당합니다. 각 그룹에 대한
사용자 지정 1AM 정책을 생성하여 세분화된 권한을 설정합니다.
B. 각 계정에 대해 1AM ID 센터에서 개별 사용자를 만듭니다. 1AM Identity Center에서 별도의
개발자 및 관리자 그룹을 만듭니다. 사용자를 적절한 그룹에 할당합니다. 세분화된 권한을
위해 필요에 따라 AWS 관리형 1AM 정책을 각 사용자에게 연결합니다.
C. 1AM Identity Center에서 개별 사용자를 생성합니다. 1AM Identity Center에서 새로운
개발자 및 관리자 그룹을 생성합니다. 각 그룹에 적합한 오전 1시 정책을 포함하는 새 권한
집합을 만듭니다. 적절한 계정에 새 그룹을 할당합니다. 새 그룹에 새 권한 집합을 할당합니다.
새 사용자가 고용되면 해당 그룹에 추가합니다.
D. 1AM Identity Center에서 개별 사용자를 생성합니다. 각 사용자에 대한 적절한 오전 1시
정책을 포함하는 새 권한 집합을 만듭니다. 사용자를 적절한 계정에 할당합니다. 특정 계정
내에서 사용자에게 추가 오전 1시 권한을 부여합니다. 새로운 사용자가 고용되면 이들을 1AM
ID 센터에 추가하고 계정에 할당합니다.
Answer: C
Explanation:
This solution meets the requirements with the least operational overhead because it
456

IT Certification Guaranteed, The Easy Way!
leverages the features of IAM Identity Center and AWS Control Tower to centrally manage
multiple user permissions across all the accounts. By creating new groups and permission
sets, the company can assign fine-grained permissions to the developer and administrator
teams based on their roles and responsibilities. The permission sets are applied to the
groups at the organization level, so they are automatically inherited by all the accounts in the
organization. When new users are hired, the company only needs to add them to the
appropriate group in IAM Identity Center, and they will automatically get the permissions
assigned to that group. This simplifies the user management and reduces the manual effort
of assigning permissions to each user individually.
References:
* Managing access to AWS accounts and applications
* Managing permissions sets
* Managing groups
QUESTION NO: 664
회사에서 외부 감사인과 회계 데이터를 공유하려고 합니다. 데이터는 프라이빗 서브넷에
상주하는 Amazon RDS DB 인스턴스에 저장됩니다. 감사자는 자체 AWS 계정이 있으며 자체
데이터베이스 사본이 필요합니다.
회사가 감사자와 데이터베이스를 공유하는 가장 안전한 방법은 무엇입니까?
A. 데이터베이스의 읽기 전용 복제본을 생성합니다. 감사자 액세스 권한을 부여하도록 IAM
표준 데이터베이스 인증을 구성합니다.
B. 데이터베이스 내용을 텍스트 파일로 내보냅니다. 파일을 Amazon S3 버킷에 저장합니다.
감사자를 위한 새 IAM 사용자를 생성합니다. 사용자에게 S3 버킷에 대한 액세스 권한을
부여합니다.
C. 데이터베이스의 스냅샷을 Amazon S3 버킷에 복사합니다. IAM 사용자를 생성합니다.
사용자의 키를 감사자와 공유하여 $3 버킷의 개체에 대한 액세스 권한을 부여합니다.
D. 데이터베이스의 암호화된 스냅샷을 생성합니다. 감사자와 스냅샷을 공유합니다. AWS Key
Management Service(AWS KMS) 암호화 키에 대한 액세스를 허용합니다.
Answer: D
Explanation:
This answer is correct because it meets the requirements of sharing the database with the
auditor in a secure way. You can create an encrypted snapshot of the database by using
AWS Key Management Service (AWS KMS) to encrypt the snapshot with a customer
managed key. You can share the snapshot with the auditor by modifying the permissions of
the snapshot and specifying the AWS account ID of the auditor. You can also allow access to
the AWS KMS encryption key by adding a key policy statement that grants permissions to the
auditor's account. This way, you can ensure that only the auditor can access and restore the
snapshot in their own AWS account.
References:
* https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ShareSnapshot.html
* https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html#key-policy-
default-allow- root-enable-iam
QUESTION NO: 665
한 회사가 AWS 클라우드에서 호스팅되는 게임 애플리케이션을 위한 공유 스토리지 솔루션을
457

IT Certification Guaranteed, The Easy Way!
설계하고 있습니다. 회사는 SMB 클라이언트를 사용하여 데이터 솔루션에 액세스할 수 있는
기능이 필요하며 완전히 관리되어야 합니다.
이러한 요구 사항을 충족하는 AWS 솔루션은 무엇입니까?
A. 데이터를 탑재 가능한 파일 시스템으로 공유하는 AWS DataSync 작업을 생성합니다. 파일
시스템을 애플리케이션 서버에 탑재합니다.
B. Amazon EC2 Windows 인스턴스 생성 인스턴스에 Windows 파일 공유 역할 설치 및 구성
애플리케이션 서버를 파일 공유에 연결
C. Windows 파일 서버용 Amazon FSx 파일 시스템 생성 원본 서버에 파일 시스템 연결
애플리케이션 서버를 파일 시스템에 연결
D. Amazon S3 버킷 생성 애플리케이션에 IAM 역할을 할당하여 S3 버킷에 대한 액세스 권한
부여 S3 버킷을 애플리케이션 서버에 마운트
Answer: C
Explanation:
Amazon FSx for Windows File Server (Amazon FSx) is a fully managed, highly available, and
scalable file storage solution built on Windows Server that uses the Server Message Block
(SMB) protocol. It allows for Microsoft Active Directory integration, data deduplication, and
fully managed backups, among other critical enterprise features.
https://aws.amazon.com/blogs/storage/accessing-smb-file-shares-remotely-with-amazon- fsx
-for-windows-file-server/
QUESTION NO: 666
회사는 온프레미스 서버를 사용하여 애플리케이션을 호스팅합니다. 회사의 스토리지 용량이
부족합니다.
애플리케이션은 블록 스토리지와 NFS 스토리지를 모두 사용합니다. 회사는 기존
애플리케이션을 재설계하지 않고 로컬 캐싱을 지원하는 고성능 솔루션이 필요합니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 작업 조합을 수행해야 합니까?
(2개를 선택하세요.)
A. Amazon S3를 온프레미스 서버에 파일 시스템으로 탑재합니다.
B. NFS 스토리지를 대체할 AWS Storage Gateway 파일 게이트웨이를 배포합니다.
C. AWS Snowball Edge를 배포하여 온프레미스 서버에 NFS 마운트를 프로비저닝합니다.
D. 블록 스토리지를 대체할 AWS Storage Gateway 볼륨 게이트웨이 배포
E. Amazon Elastic File System(Amazon EFS) 볼륨을 배포하고 온프레미스 서버에
탑재합니다.
Answer: B D
Explanation:
https://aws.amazon.com/storagegateway/file/
File Gateway provides a seamless way to connect to the cloud in order to store application
data files and backup images as durable objects in Amazon S3 cloud storage. File Gateway
offers SMB or NFS-based access to data in Amazon S3 with local caching. It can be used for
on-premises applications, and for Amazon EC2-based applications that need file protocol
access to S3 object storage.
https://aws.amazon.com/storagegateway/volume/
Volume Gateway presents cloud-backed iSCSI block storage volumes to your on-premises
applications.
458

IT Certification Guaranteed, The Easy Way!
Volume Gateway stores and manages on-premises data in Amazon S3 on your behalf and
operates in either cache mode or stored mode. In the cached Volume Gateway mode, your
primary data is stored in Amazon S3, while retaining your frequently accessed data locally in
the cache for low latency access.
QUESTION NO: 667
회사는 AWS 클라우드에서 호스팅되는 미디어 애플리케이션을 위한 공유 스토리지 솔루션을
구현하고 있습니다. 회사는 SMB 클라이언트를 사용하여 데이터에 액세스할 수 있는 기능이
필요합니다. 솔루션은 완전히 관리되어야 합니다.
어떤 AWS 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Storage Gateway 볼륨 게이트웨이를 생성합니다. 필요한 클라이언트 프로토콜을
사용하는 파일 공유를 생성합니다. 응용 프로그램 서버를 파일 공유에 연결합니다.
B. AWS Storage Gateway 테이프 게이트웨이 구성 생성(Amazon S3를 사용하여 테이프
게이트웨이에 애플리케이션 서버 연결
C. Amazon EC2 Windows 인스턴스 생성 인스턴스에 Windows 파일 공유 역할을 설치 및
구성합니다. 응용 프로그램 서버를 파일 공유에 연결합니다.
D. Amazon FSx for Windows 파일 서버 타일 시스템을 생성합니다. fie 시스템을 원본 서버에
연결합니다. 애플리케이션 서버를 파일 시스템에 연결
Answer: D
Explanation:
https://aws.amazon.com/fsx/lustre/
Amazon FSx has native support for Windows file system features and for the industry-
standard Server Message Block (SMB) protocol to access file storage over a network.
https://docs.aws.amazon.com/fsx/latest
/WindowsGuide/what-is.html
QUESTION NO: 668
한 회사가 AWS에서 데이터 레이크를 호스팅하고 있습니다. 데이터 레이크는 Amazon S3 및
PostgreSQL용 Amazon RDS의 데이터로 구성됩니다. 회사에는 데이터 시각화를 제공하고
데이터 레이크 내의 모든 데이터 소스를 포함하는 보고 솔루션이 필요합니다. 회사의
관리팀만이 모든 시각화에 대한 전체 액세스 권한을 가져야 합니다. 회사의 나머지 부분은
제한된 액세스 권한만 가져야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon QuickSight에서 분석을 생성합니다. 모든 데이터 소스를 연결하고 새
데이터세트를 생성합니다. 데이터를 시각화하기 위해 대시보드를 게시합니다. 적절한 IAM
역할과 대시보드를 공유합니다.
B. Amazon OuickSighl에서 분석을 생성합니다. 모든 데이터 소스를 연결하고 새
데이터세트를 생성합니다. 데이터를 시각화하기 위해 대시보드를 게시합니다. 적절한 사용자
및 그룹과 대시보드를 공유합니다.
C. Amazon S3의 데이터에 대한 AWS Glue 테이블과 크롤러를 생성합니다. AWS Glue 추출,
변환 및 로드(ETL) 작업을 생성하여 보고서를 생성합니다. 보고서를 Amazon S3에
게시합니다. S3 버킷 정책을 사용하여 보고서에 대한 액세스를 제한합니다.
D. Amazon S3의 데이터에 대한 AWS Glue 테이블과 크롤러를 생성합니다. Amazon Athena
Federated Query를 사용하여 PoslgreSQL용 Amazon RDS 내의 데이터에 액세스합니다.
Amazon Athena를 사용하여 보고서를 생성합니다. 보고서를 Amazon S3에 게시합니다. S3
459

IT Certification Guaranteed, The Easy Way!
버킷 정책을 사용하여 보고서에 대한 액세스를 제한합니다.
Answer: B
Explanation:
Amazon QuickSight is a data visualization service that allows you to create interactive
dashboards and reports from various data sources, including Amazon S3 and Amazon RDS
for PostgreSQL. You can connect all the data sources and create new datasets in
QuickSight, and then publish dashboards to visualize the data. You can also share the
dashboards with the appropriate users and groups, and control their access levels using IAM
roles and permissions.
Reference: https://docs.aws.amazon.com/quicksight/latest/user/working-with-data-
sources.html
QUESTION NO: 669
소셜 미디어 회사는 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스에서
애플리케이션을 실행합니다. ALB는 Amazon CloudFront 배포의 오리진입니다. 이
애플리케이션은 Amazon S3 버킷에 10억 개 이상의 이미지가 저장되어 있으며 초당 수천
개의 이미지를 처리합니다. 회사는 이미지 크기를 동적으로 조정하고 고객에게 적절한 형식을
제공하기를 원합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EC2 인스턴스에 외부 이미지 관리 라이브러리를 설치합니다. 이미지 관리 라이브러리를
사용하여 이미지를 처리합니다.
B. CloudFront 오리진 요청 정책을 생성합니다. 정책을 사용하여 자동으로 이미지 크기를
조정하고 요청의 User-Agent HTTP 헤더를 기반으로 적절한 형식을 제공합니다.
C. 외부 이미지 관리 라이브러리와 함께 Lambda@Edge 함수를 사용합니다. Lambda@Edge
함수를 이미지를 제공하는 CloudFront 동작과 연결합니다.
D. CloudFront 응답 헤더 정책을 생성합니다. 정책을 사용하여 자동으로 이미지 크기를
조정하고 요청의 User-Agent HTTP 헤더를 기반으로 적절한 형식을 제공합니다.
Answer: C
Explanation:
Lambda@Edge is a service that allows you to run Lambda functions at CloudFront edge
locations. It can be used to modify requests and responses that flow through CloudFront.
CloudFront origin request policy is a policy that controls the values (URL query strings, HTTP
headers, and cookies) that are included in requests that CloudFront sends to the origin. It can
be used to collect additional information at the origin or to customize the origin response.
CloudFront response headers policy is a policy that specifies the HTTP headers that
CloudFront removes or adds in responses that it sends to viewers. It can be used to add
security or custom headers to responses.
Based on these definitions, the solution that will meet the requirements with the least
operational overhead is:
C: Use a Lambda@Edge function with an external image management library. Associate the
Lambda@Edge function with the CloudFront behaviors that serve the images.
This solution would allow the application to use a Lambda@Edge function to resize the
images dynamically and serve appropriate formats to clients based on the User-Agent HTTP
header in the request. The Lambda@Edge function would run at the edge locations, reducing
latency and load on the origin. The application code would only need to include an external
460

IT Certification Guaranteed, The Easy Way!
image management library that can perform image manipulation tasks1.
QUESTION NO: 670
솔루션 아키텍트는 AWS 클라우드에서 고성능 컴퓨팅(HPC) 워크로드를 호스팅해야 합니다.
워크로드는 수백 개의 Amazon EC2 인스턴스에서 실행되며, 대규모 데이터 세트의 분산
처리를 가능하게 하기 위해 공유 파일 시스템에 대한 병렬 액세스가 필요합니다. 데이터
세트는 여러 인스턴스에서 동시에 액세스됩니다. 워크로드는 1ms 이내의 액세스 지연 시간이
필요합니다. 처리가 완료된 후 엔지니어는 수동 후처리를 위해 데이터 세트에 액세스해야
합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon Elastic File System(Amazon EFS)을 공유 파일 시스템으로 사용합니다. Amazon
EFS에서 데이터 세트에 액세스합니다.
B. 공유 파일 시스템으로 사용할 Amazon S3 버킷을 마운트합니다. S3 버킷에서 직접
후처리를 수행합니다.
C. Amazon FSx for Lustre를 공유 파일 시스템으로 사용합니다. 후처리를 위해 파일 시스템을
Amazon S3 버킷에 연결합니다.
D. AWS Resource Access Manager를 구성하여 Amazon S3 버킷을 공유하면 모든
인스턴스에 마운트하여 처리 및 사후 처리를 수행할 수 있습니다.
Answer: C
Explanation:
Amazon FSx for Lustre is the ideal solution for high-performance computing (HPC) workloads
that require parallel access to a shared file system with low latency. FSx for Lustre is
designed specifically to meet the needs of such workloads, offering sub-millisecond latencies,
which makes it well-suited for the 1 ms latency requirement mentioned in the question.
Here is why FSx for Lustre is the best fit:
* Parallel File System: FSx for Lustre is a parallel file system that can scale across hundreds
of Amazon EC2 instances, providing high throughput and low-latency access to data. It is
optimized for processing large datasets in parallel, which is essential for HPC workloads.
* Low Latency: FSx for Lustre is capable of providing access latencies well within 1 ms,
making it ideal for performance-sensitive workloads like HPC.
* Seamless Integration with Amazon S3: FSx for Lustre can be linked to an Amazon S3
bucket. This integration allows data to be imported from S3 into FSx for Lustre before the
workload begins and exported back to S3 after processing. This feature is crucial for manual
postprocessing because it enables engineers to access the dataset in S3 after processing.
* Performance: FSx for Lustre is built for workloads that require high performance, such as
machine learning, analytics, media processing, and financial simulations, which are typical for
HPC environments.
In contrast:
* Amazon EFS (Option A): While EFS provides shared file storage and scales across multiple
EC2 instances, it does not offer the same level of performance or sub-millisecond latencies
as FSx for Lustre. EFS is more suited for general-purpose workloads, not high-performance
computing.
* Mounting S3 as a file system (Option B and D): S3 is object storage, not a file system
designed for low-latency access and parallel processing. Mounting S3 buckets directly or
using AWS Resource Access Manager to share the bucket would not meet the low-latency (1
461

IT Certification Guaranteed, The Easy Way!
ms) or performance requirements needed for HPC workloads.
Therefore, Amazon FSx for Lustre (Option C) is the most appropriate and verified solution for
this scenario.
AWS References:
* Amazon FSx for Lustre
* Best Practices for High Performance Computing (HPC)
* Amazon FSx and Amazon S3 Integration
QUESTION NO: 671
회사는 중요한 애플리케이션에 대한 애플리케이션 로그 파일을 10년 동안 보관해야 합니다.
애플리케이션 팀은 문제 해결을 위해 지난 달의 로그에 정기적으로 액세스하지만, 1개월이
지난 로그에는 거의 액세스하지 않습니다. 애플리케이션은 매월 10TB가 넘는 로그를
생성합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 스토리지 옵션은 무엇입니까?
A. Amazon S3에 로그 저장 AWS Backup을 사용하여 1개월 이상 된 로그를 S3 Glacier Deep
Archive로 이동합니다.
B. Amazon S3에 로그를 저장합니다. S3 수명 주기 정책을 사용하여 1개월 이상 된 로그를 S3
Glacier Deep Archive로 이동합니다.
C. Amazon CloudWatch Logs에 로그 저장 AWS Backup을 사용하여 1개월 이상 된 로그를 S3
Glacier Deep Archive로 이동
D. Amazon CloudWatch Logs에 로그를 저장합니다. Amazon S3 수명 주기 정책을 사용하여
1개월 이상 된 로그를 S3 Glacier Deep Archive로 이동합니다.
Answer: B
Explanation:
You need S3 to be able to archive the logs after one month. Cannot do that with CloudWatch
Logs.
QUESTION NO: 672
회사에는 생산 및 개발이라는 두 개의 AWS 계정이 있습니다. 회사는 Development 계정의
코드 변경 사항을 Production 계정으로 푸시해야 합니다. 알파 단계에서는 개발팀의 수석
개발자 2명만 프로덕션 계정에 액세스하면 됩니다. 베타 단계에서는 테스트를 수행하기 위해
더 많은 개발자가 액세스해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 각 계정에서 AWS Management Console을 사용하여 두 개의 정책 문서를 생성합니다.
액세스가 필요한 개발자에게 정책을 할당합니다.
B. 개발 계정에서 오전 1시 역할을 생성합니다. 프로덕션 계정에 오전 1시 역할 액세스 권한을
부여합니다.
개발자가 역할을 맡도록 허용
C. 프로덕션 계정에서 IAM 역할을 생성합니다. 개발 계정을 지정하는 신뢰 정책 정의 개발자가
역할을 맡을 수 있도록 허용
D. 프로덕션 계정에 IAM 그룹을 생성합니다. 프로덕션 계정을 지정하는 신뢰 정책에 그룹을
보안 주체로 추가합니다. 그룹에 개발자를 추가합니다.
Answer: C
Explanation:
* Understanding the Requirement: Developers in the Development account need to push
462

IT Certification Guaranteed, The Easy Way!
code changes to the Production account, with phased access control for different stages of
the project.
* Analysis of Options:
* Policy Documents in Each Account: This approach increases complexity and is harder to
manage compared to role-based access.
* IAM Role in Development Account: Roles in the Development account cannot directly
control access to resources in the Production account.
* IAM Role in Production Account: Creating a role in the Production account with a trust
policy that allows the Development account to assume it provides controlled, secure access.
* IAM Group in Production Account: This approach does not provide the required cross-
account access control.
* Best Solution:
* IAM Role in the Production Account: This method allows precise control over who can
access the Production account from the Development account, with the ability to manage
permissions and access levels effectively.
References:
* IAM Roles with Cross-Account Access
* Creating a Role for Cross-Account Access
QUESTION NO: 673
지난 주 프로덕션 배포 중에 IAM 사용자가 회사 계정의 AWS 리소스에 대한 여러 구성을
변경했습니다. 솔루션 설계자는 몇 가지 보안 그룹 규칙이 원하는 대로 구성되지 않았다는
사실을 알게 되었습니다. 솔루션 설계자는 어떤 IAM 사용자가 변경 작업을 담당했는지
확인하려고 합니다.
솔루션 아키텍트는 원하는 정보를 찾기 위해 어떤 서비스를 사용해야 합니까?
A. Amazon GuardDuty
B. 아마존 인스펙터
C. AWS CloudTrail
D. AWS 구성
Answer: C
Explanation:
The best option is to use AWS CloudTrail to find the desired information. AWS CloudTrail is a
service that enables governance, compliance, operational auditing, and risk auditing of AWS
account activities.
CloudTrail can be used to log all changes made to resources in an AWS account, including
changes made by IAM users, EC2 instances, AWS management console, and other AWS
services. By using CloudTrail, the solutions architect can identify the IAM user who made the
configuration changes to the security group rules.
QUESTION NO: 674
한 회사는 최근 애플리케이션 마이그레이션 이니셔티브에 대한 지원을 위해 AWS 관리형
서비스 공급자(MSP) 파트너와 계약을 체결했습니다. 솔루션 설계자는 기존 AWS 계정의
Amazon 머신 이미지(AMI)를 MSP 파트너의 AWS 계정과 공유해야 합니다. AMI는 Amazon
Elastic Block Store(Amazon EBS)의 지원을 받으며 고객 관리형 고객 마스터 키(CMK)를
사용하여 EBS 볼륨 스냅샷을 암호화합니다.
463

IT Certification Guaranteed, The Easy Way!
솔루션 설계자가 MSP 파트너의 AWS 계정과 AMI를 공유하는 가장 안전한 방법은
무엇입니까?
A. 암호화된 AMI 및 스냅샷을 공개적으로 사용할 수 있도록 합니다. MSP 파트너의 AWS
계정이 키를 사용할 수 있도록 CMK의 키 정책을 수정합니다.
B. AMI의 launchPermission 속성을 수정합니다. MSP 파트너의 AWS 계정과만 AMI를
공유하십시오. MSP 파트너의 AWS 계정이 키를 사용할 수 있도록 CMK의 키 정책을
수정합니다.
C. AMI의 launchPermission 속성을 수정합니다. AMI를 MSP 파트너의 AWS 계정과만
공유합니다. 암호화를 위해 MSP 파트너가 소유한 새 CMK를 신뢰하도록 CMK의 키 정책을
수정합니다.
D. 소스 계정에서 MSP 파트너의 AWS 계정에 있는 Amazon S3 버킷으로 AMI를
내보냅니다.MSP 파트너가 소유한 CMK로 S3 버킷을 암호화하고 MSP 파트너의 AWS
계정에서 AMI를 복사하고 시작합니다.
Answer: B
Explanation:
Share the existing KMS key with the MSP external account because it has already been used
to encrypt the AMI snapshot. https://docs.aws.amazon.com/kms/latest/developerguide/key-
policy-modifying-external- accounts.html
QUESTION NO: 675
결제 처리 회사는 고객과의 모든 음성 통신을 녹음하고 오디오 파일을 Amazon S3 버킷에
저장합니다. 회사는 오디오 파일에서 텍스트를 캡처해야 합니다. 회사는 고객에게 속한
개인식별정보(Pll)를 텍스트에서 삭제해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. Amazon Kinesis Video Streams를 사용하여 오디오 파일을 처리합니다. AWS Lambda
함수를 사용하여 알려진 Pll 패턴을 스캔합니다.
B. 오디오 파일이 S3 버킷에 업로드되면 AWS Lambda 함수를 호출하여 Amazon Textract
작업을 시작하여 통화 녹음을 분석합니다.
C. Pll 수정이 활성화된 Amazon Transcribe 전사 작업을 구성합니다. 오디오 파일이 S3 버킷에
업로드되면 AWS Lambda 함수를 호출하여 녹음 작업을 시작합니다. 출력을 별도의 S3
버킷에 저장합니다.
D. 전사가 활성화된 오디오 파일을 수집하는 Amazon Connect 고객 응대 흐름을 생성합니다.
알려진 Pll 패턴을 검색하는 AWS Lambda 함수를 포함합니다. 오디오 파일이 S3 버킷에
업로드될 때 Amazon EventBridge(Amazon CloudWatch Events)를 사용하여 고객 응대
흐름을 시작합니다.
Answer: C
Explanation:
"Sensitive data redaction replaces personally identifiable information (PII) in the text
transcript and the audio file. A redacted transcript replaces the original text with [PII]; a
redacted audio file replaces spoken personal information with silence. This parameter is
useful for protecting customer information." https://docs.aws.
amazon.com/transcribe/latest/dg/call-analytics-insights.html#call-analytics-insights-redaction
QUESTION NO: 676
464

IT Certification Guaranteed, The Easy Way!
회사는 MySQL 데이터베이스를 실행하는 자체 Amazon EC2 인스턴스를 관리합니다. 회사는
수요가 증가하거나 감소함에 따라 복제 및 확장을 수동으로 관리하고 있습니다. 회사에는
필요에 따라 데이터베이스 계층에 컴퓨팅 용량을 추가하거나 제거하는 프로세스를
단순화하는 새로운 솔루션이 필요합니다. 또한 솔루션은 최소한의 운영 노력으로 향상된
성능, 확장성 및 내구성을 제공해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Aurora MySQL용 Amazon Aurora Serverless로 데이터베이스를 마이그레이션합니다.
B. Aurora PostgreSQL용 Amazon Aurora Serverless로 데이터베이스를 마이그레이션합니다.
C. 데이터베이스를 하나의 더 큰 MySQL 데이터베이스로 결합합니다. 더 큰 EC2
인스턴스에서 더 큰 데이터베이스를 실행하십시오.
D. 데이터베이스 계층에 대한 EC2 Auto Scaling 그룹을 생성합니다. 기존 데이터베이스를 새
환경으로 마이그레이션합니다.
Answer: A
Explanation:
https://aws.amazon.com/rds/aurora/serverless/
QUESTION NO: 677
한 회사에 Amazon S3에 백업되는 시간에 민감한 대량의 데이터를 생성하는 온프레미스
애플리케이션이 있습니다. 애플리케이션이 성장했으며 인터넷 대역폭 제한에 대한 사용자
불만이 있습니다. 솔루션 아키텍트는 Amazon S3에 시기적절하게 백업하고 내부 사용자의
인터넷 연결에 미치는 영향을 최소화할 수 있는 장기 솔루션을 설계해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS VPN 연결을 설정하고 VPC 게이트웨이 엔드포인트를 통해 모든 트래픽을 프록시
처리합니다.
B. 새로운 AWS Direct Connect 연결을 설정하고 이 새로운 연결을 통해 백업 트래픽을
전달합니다.
C. 매일 AWS Snowball 디바이스 주문 Snowball 디바이스에 데이터를 로드하고 매일
디바이스를 AWS에 반환합니다.
D. AWS Management Console을 통해 지원 티켓을 제출하고 계정에서 S3 서비스 제한
제거를 요청합니다.
Answer: B
Explanation:
To address the issue of bandwidth limitations on the company's on-premises application, and
to minimize the impact on internal user connectivity, a new AWS Direct Connect connection
should be established to direct backup traffic through this new connection. This solution will
offer a secure, high-speed connection between the company's data center and AWS, which
will allow the company to transfer data quickly without consuming internet bandwidth.
Reference:
AWS Direct Connect documentation: https://aws.amazon.com/directconnect/
QUESTION NO: 678
회사는 애플리케이션에 대한 실시간 데이터 수집 아키텍처를 구성해야 합니다. 회사에는
데이터가 스트리밍될 때 데이터를 변환하는 프로세스인 API와 데이터를 위한 스토리지
솔루션이 필요합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
465

IT Certification Guaranteed, The Easy Way!
A. Amazon EC2 인스턴스를 배포하여 Amazon Kinesis 데이터 스트림으로 데이터를 전송하는
API를 호스팅합니다.
Kinesis 데이터 스트림을 데이터 원본으로 사용하는 Amazon Kinesis Data Firehose 전송
스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다. Kinesis Data
Firehose 전송 스트림을 사용하여 데이터를 Amazon S3로 보냅니다.
B. Amazon EC2 인스턴스를 배포하여 AWS Glue에 데이터를 전송하는 API를 호스팅합니다.
EC2 인스턴스에서 소스/대상 확인을 중지합니다. AWS Glue를 사용하여 데이터를 변환하고
데이터를 Amazon S3로 보냅니다.
C. Amazon Kinesis 데이터 스트림으로 데이터를 전송하도록 Amazon API Gateway API를
구성합니다. Kinesis 데이터 스트림을 데이터 원본으로 사용하는 Amazon Kinesis Data
Firehose 전송 스트림을 생성합니다. AWS Lambda 함수를 사용하여 데이터를 변환합니다.
Kinesis Data Firehose 전송 스트림을 사용하여 데이터를 Amazon S3로 보냅니다.
D. 데이터를 AWS Glue로 보내도록 Amazon API Gateway API를 구성합니다. AWS Lambda
함수를 사용하여 데이터를 변환합니다. AWS Glue를 사용하여 데이터를 Amazon S3로
보냅니다.
Answer: C
QUESTION NO: 679
회사는 Windows 기반 애플리케이션을 온프레미스에서 AWS 클라우드로 마이그레이션하려고
합니다. 애플리케이션에는 비즈니스 계층과 Microsoft SQL Server가 포함된 데이터베이스
계층이라는 세 가지 계층이 있습니다. 회사는 기본 백업 및 데이터 품질 서비스와 같은 SQL
Server의 특정 기능을 사용하려고 합니다. 또한 회사는 계층 간 처리를 위해 파일을 공유해야
합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 아키텍처를 어떻게 설계해야 합니까?
A. Amazon 인스턴스에서 세 가지를 모두 호스팅합니다. 계층 간 파일 공유를 위해 Mmazon
FSx 파일 게이트웨이를 사용합니다.
B. Amazon EC2 인스턴스에서 세 가지를 모두 호스팅합니다. 계층 간 Windows 파일 공유를
위해 Amazon FSx를 사용합니다.
C. Amazon EC2 인스턴스에서 애플리케이션 계층과 비즈니스 계층을 호스팅합니다. Amazon
RDS에서 데이터베이스 계층을 호스팅합니다. 계층 간 파일 공유를 위해 Amazon Elastic File
system(Amazon EFS)을 사용합니다.
D. Amazon EC2 인스턴스에서 애플리케이션 계층과 비즈니스 계층을 호스팅합니다. Amazon
RDS에서 데이터베이스 계층을 호스팅합니다. 계층 간 파일 공유를 위해 프로비저닝된 IOPS
SSD(io2) Amazon Elastic Block Store(Amazon EBS) 볼륨을 사용합니다.
Answer: B
Explanation:
This solution will allow the company to host all three tiers on Amazon EC2 instances while
using Amazon FSx for Windows File Server to provide Windows-based file sharing between
the tiers. This will allow the company to use specific features of SQL Server, such as native
backups and Data Quality Services, while sharing files for processing between the tiers.
QUESTION NO: 680
한 회사는 최근 웹 애플리케이션을 AWS 클라우드로 마이그레이션했습니다. 이 회사는
Amazon EC2 인스턴스를 사용하여 여러 프로세스를 실행하여 애플리케이션을 호스팅합니다.
466

IT Certification Guaranteed, The Easy Way!
프로세스에는 정적 콘텐츠를 제공하는 Apache 웹 서버가 포함됩니다. Apache 웹 서버는
사용자 세션을 위해 로컬 Redis 서버를 사용하는 PHP 애플리케이션에 요청합니다.
회사는 가용성이 높고 AWS 관리형 솔루션을 사용하도록 아키텍처를 재설계하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Elastic Beanstalk를 사용하여 정적 콘텐츠와 PHP 애플리케이션을 호스팅합니다.
EC2 인스턴스를 퍼블릭 서브넷에 배포하도록 Elastic Beanstalk를 구성합니다. 퍼블릭 IP
주소를 할당합니다.
B. AWS Lambda를 사용하여 정적 콘텐츠와 PHP 애플리케이션을 호스팅합니다. Amazon API
Gateway REST API를 사용하여 Lambda 함수에 대한 요청을 프록시합니다. 도메인 이름에
응답하도록 API 게이트웨이 CORS 구성을 설정합니다. 세션 정보를 처리하도록 Redis용
Amazon ElastiCache 구성
C. EC2 인스턴스에 백엔드 코드를 유지합니다. 다중 AZ가 활성화된 Redis용 Amazon
ElastiCache 클러스터 생성 클러스터 모드에서 Redis용 ElastiCache 클러스터 구성
프런트엔드 리소스를 Amazon S3에 복사 EC2 인스턴스를 참조하도록 백엔드 코드 구성
D. 정적 콘텐츠를 호스팅하도록 구성된 S3 버킷에 대한 Amazon S3 엔드포인트를 사용하여
Amazon CloudFront 배포를 구성합니다. PHP 애플리케이션에 대해 AWS Fargate 작업을
실행하는 Amazon Elastic Container Service(Amazon ECS) 서비스를 대상으로 하는
Application Load Balancer를 구성합니다. 여러 가용 영역에서 실행되는 Redis용 Amazon
ElastiCache 클러스터를 사용하도록 PHP 애플리케이션을 구성합니다.
Answer: D
Explanation:
* Understanding the Requirement: The company needs to redesign the architecture to be
highly available and use AWS managed solutions for hosting a web application with static
content, PHP application, and Redis for user sessions.
* Analysis of Options:
* AWS Elastic Beanstalk: Suitable for simplifying deployment but may not provide the desired
flexibility and control for complex architectures.
* AWS Lambda and API Gateway: Not ideal for hosting a stateful PHP application and
handling static content. Adding complexity without significant benefit.
* EC2 instance with ElastiCache and S3: Provides some high availability but involves
managing EC2 instances, which increases operational overhead.
* CloudFront with S3, ALB, ECS with Fargate, and ElastiCache: This solution leverages fully
managed AWS services for each component, ensuring high availability and scalability.
* Best Solution:
* CloudFront with S3, ALB, ECS with Fargate, and ElastiCache: This combination of services
meets the requirements for a highly available and managed solution, ensuring optimal
performance and minimal operational overhead.
References:
* Amazon CloudFront
* Amazon S3
* Amazon ECS with Fargate
* Amazon ElastiCache for Redis
QUESTION NO: 681
회사는 데이터베이스 계층에 대한 기본 백업 설정으로 Amazon RDS를 사용합니다. 회사는
467

IT Certification Guaranteed, The Easy Way!
규제 요구 사항을 충족하기 위해 데이터베이스를 정기적으로 백업해야 합니다. 회사는
백업본을 30일 동안 보관해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 매일 RDS 스냅샷을 생성하는 AWS Lambda 함수를 작성합니다.
B. RDS 데이터베이스를 수정하여 자동 백업의 보존 기간을 30일로 설정하세요.
C. AWS Systems Manager 유지 관리 기간을 사용하여 RDS 백업 보존 기간을 수정합니다.
D. AWS CLI를 사용하여 매일 수동 스냅샷을 생성합니다. RDS 백업 보존 기간을 수정합니다.
Answer: B
Explanation:
* Current Backup Settings: By default, Amazon RDS creates automated backups with a
retention period of 7 days.
* Regulatory Requirements: The requirement is to retain daily backups for 30 days.
* Adjusting Retention Period: You can modify the RDS instance settings to increase the
automated backup retention period to 30 days.
* Operational Overhead: This solution is the simplest as it leverages existing automated
backups and requires minimal intervention.
* Implementation: The change can be made via the AWS Management Console, AWS CLI,
or AWS SDKs.
References
* Amazon RDS Backups: Amazon RDS Documentation
QUESTION NO: 682
한 회사가 AWS Fargate 클러스터를 사용하여 Amazon Elastic Kubernetes Service(Amazon
EKS)에 새 애플리케이션을 배포하고 있습니다. 애플리케이션에는 데이터 지속성을 위한
스토리지 솔루션이 필요합니다. 솔루션은 가용성이 높고 내결함성이 있어야 합니다. 또한
솔루션은 여러 애플리케이션 컨테이너 간에 공유되어야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EKS 작업자 노드가 배치된 동일한 가용 영역에 Amazon Elastic Block Store(Amazon EBS)
볼륨을 생성합니다. EKS 클러스터의 StorageClass 객체에 볼륨을 등록합니다. EBS 다중
연결을 사용하여 컨테이너 간에 데이터를 공유합니다.
B. Amazon Elastic File System(Amazon EFS) 파일 시스템을 생성합니다. EKS 클러스터의
StorageClass 객체에 파일 시스템을 등록합니다. 모든 컨테이너에 동일한 파일 시스템을
사용합니다.
C. Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성합니다. EKS 클러스터의
StorageClass 객체에 볼륨을 등록합니다. 모든 용기에 동일한 용량을 사용하십시오.
D. EKS 작업자 노드가 배치된 동일한 가용 영역에 Amazon Elastic File System(Amazon EFS)
파일 시스템을 생성합니다. EKS 클러스터의 StorageClass 객체에 파일 시스템을 등록합니다.
파일 시스템 간에 데이터를 동기화하는 AWS Lambda 함수를 생성합니다.
Answer: B
Explanation:
Amazon EFS is a fully managed, elastic, and scalable file system that can be shared
between multiple containers. It provides high availability and fault tolerance by replicating
data across multiple Availability Zones. Amazon EFS is compatible with Amazon EKS and
AWS Fargate, and can be registered in a StorageClass object on an EKS cluster. Amazon
468

IT Certification Guaranteed, The Easy Way!
EBS volumes are not supported by AWS Fargate, and cannot be shared between multiple
containers without using EBS Multi-Attach, which has limitations and performance
implications. EBS Multi-Attach also requires the volumes to be in the same Availability Zone
as the worker nodes, which reduces availability and fault tolerance. Synchronizing data
between multiple EFS file systems using AWS Lambda is unnecessary, complex, and prone
to errors. References:
* Amazon EFS Storage Classes
* Amazon EKS Storage Classes
* Amazon EBS Multi-Attach
QUESTION NO: 683
회사는 Amazon S3를 데이터 레이크로 사용합니다. 회사에는 데이터 파일을 업로드하기 위해
SFTP를 사용해야 하는 새로운 파트너가 있습니다. 솔루션 설계자는 운영 오버헤드를
최소화하는 가용성이 높은 SFTP 솔루션을 구현해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Transfer Family를 사용하여 공개적으로 액세스 가능한 엔드포인트로 SFTP 지원
서버를 구성합니다. S3 데이터 레이크를 대상으로 선택합니다.
B. Amazon S3 파일 게이트웨이를 SFTP 서버로 사용 S3 파일 게이트웨이 엔드포인트 URL을
새 파트너에게 노출 S3 파일 게이트웨이 엔드포인트를 새 파트너와 공유
C. VPC의 프라이빗 서브넷에서 Amazon EC2 인스턴스를 시작합니다. VPN을 사용하여 EC2
인스턴스에 파일을 업로드하도록 새 파트너에게 지시합니다. EC2 인스턴스에서 cron 작업
스크립트를 실행하여 S3 데이터 레이크에 파일 업로드
D. VPC의 프라이빗 서브넷에서 Amazon EC2 인스턴스를 시작합니다. EC2 인스턴스 앞에
Network Load Balancer(NLB)를 배치합니다. NLB용 SFTP 리스너 포트를 생성합니다. 새
파트너와 NLB 호스트 이름을 공유합니다. EC2 인스턴스에서 cron 작업 스크립트를 실행하여
S3 데이터 레이크에 파일을 업로드합니다.
Answer: A
Explanation:
This option is the most cost-effective and simple way to enable SFTP access to the S3 data
lake. AWS Transfer Family is a fully managed service that supports secure file transfers over
SFTP, FTPS, and FTP protocols. You can create an SFTP-enabled server with a public
endpoint and associate it with your S3 bucket. You can also use AWS Identity and Access
Management (IAM) roles and policies to control access to your S3 data lake. The service
scales automatically to handle any volume of file transfers and provides high availability and
durability. You do not need to provision, manage, or patch any servers or load balancers.
Option B is not correct because Amazon S3 File Gateway is not an SFTP server. It is a
hybrid cloud storage service that provides a local file system interface to S3. You can use it
to store and retrieve files as objects in S3 using standard file protocols such as NFS and
SMB. However, it does not support SFTP protocol, and it requires deploying a file gateway
appliance on-premises or on EC2.
Option C is not cost-effective or scalable because it requires launching and managing an
EC2 instance in a private subnet and setting up a VPN connection for the new partner. This
would incur additional costs for the EC2 instance, the VPN connection, and the data transfer.
It would also introduce complexity and security risks to the solution. Moreover, it would
require running a cron job script on the EC2 instance to upload files to the S3 data lake,
469

IT Certification Guaranteed, The Easy Way!
which is not efficient or reliable.
Option D is not cost-effective or scalable because it requires launching and managing
multiple EC2 instances in a private subnet and placing a NLB in front of them. This would
incur additional costs for the EC2 instances, the NLB, and the data transfer. It would also
introduce complexity and security risks to the solution. Moreover, it would require running a
cron job script on the EC2 instances to upload files to the S3 data lake, which is not efficient
or reliable. References:
* What Is AWS Transfer Family?
* What Is Amazon S3 File Gateway?
* What Is Amazon EC2?
* [What Is Amazon Virtual Private Cloud?]
* [What Is a Network Load Balancer?]
QUESTION NO: 684
회사에는 Amazon EC2 인스턴스에서 실행되는 레거시 데이터 처리 애플리케이션이
있습니다. 데이터는 순차적으로 처리되지만 결과의 순서는 중요하지 않습니다.
애플리케이션은 모놀리식 아키텍처를 사용합니다. 회사가 증가하는 수요를 충족하기 위해
애플리케이션을 확장할 수 있는 유일한 방법은 인스턴스 크기를 늘리는 것입니다.
이 회사의 개발자는 Amazon Elastic Container Service(Amazon ECS)에서 마이크로서비스
아키텍처를 사용하도록 애플리케이션을 다시 작성하기로 결정했습니다.
솔루션 설계자는 마이크로서비스 간 통신을 위해 무엇을 권장해야 합니까?
A. Amazon Simple Queue Service(Amazon SQS) 대기열을 생성합니다. 데이터 생산자에
코드를 추가하고 데이터를 큐로 보냅니다. 큐의 데이터를 처리하기 위해 데이터 소비자에
코드를 추가합니다.
B. Amazon Simple 알림 서비스(Amazon SNS) 주제를 생성합니다. 데이터 생산자에 코드를
추가하고 주제에 알림을 게시합니다. 주제를 구독하려면 데이터 소비자에 코드를 추가하세요.
C. 메시지를 전달하는 AWS Lambda 함수를 생성합니다. 데이터 객체로 Lambda 함수를
호출하려면 데이터 생산자에 코드를 추가하세요. Lambda 함수에서 전달된 데이터 객체를
수신하려면 데이터 소비자에 코드를 추가하세요.
D. Amazon DynamoDB 테이블을 생성합니다. DynamoDB 스트림을 활성화합니다. 데이터
생산자에 코드를 추가하여 테이블에 데이터를 삽입합니다. DynamoDB Streams API를
사용하여 새 테이블 항목을 감지하고 데이터를 검색하도록 데이터 소비자에 코드를
추가합니다.
Answer: A
Explanation:
Queue has Limited throughput (300 msg/s without batching, 3000 msg/s with batching
whereby up-to 10 msg per batch operation; Msg duplicates not allowed in the queue (exactly-
once delivery); Msg order is preserved (FIFO); Queue name must end with .fifo
QUESTION NO: 685
한 회사는 Amazon CloudFront 배포를 사용하여 웹 사이트의 콘텐츠 페이지를 제공합니다.
회사는 고객이 회사 웹사이트에 접속할 때 TLS 인증서를 사용하는지 확인해야 합니다. 회사는
Tl S 인증서의 생성 및 갱신을 자동화하려고 합니다.
가장 효율적인 운영 효율성으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. CloudFront 보안 정책을 사용하여 인증서를 생성합니다.
470

IT Certification Guaranteed, The Easy Way!
B. CloudFront OAC(오리진 액세스 제어)를 사용하여 인증서를 생성합니다.
C. AWS Certificate Manager(ACM)를 사용하여 인증서를 생성합니다. 도메인에 대해 DNS
검증을 사용하십시오.
D. AWS Certificate Manager(ACM)를 사용하여 인증서를 생성합니다. 도메인에 대한 이메일
검증을 사용하십시오.
Answer: C
Explanation:
* Understanding the Requirement: The company needs to ensure clients use a TLS
certificate when accessing the website and automate the creation and renewal of TLS
certificates.
* Analysis of Options:
* CloudFront Security Policy: Not applicable for creating certificates.
* CloudFront Origin Access Control (OAC): Controls access to origins, not relevant for TLS
certificate creation.
* AWS Certificate Manager (ACM) with DNS Validation: Provides automated certificate
management, including creation and renewal, with minimal manual intervention. DNS
validation is automated and does not require manual intervention as email validation does.
* AWS Certificate Manager (ACM) with Email Validation: Requires manual intervention to
approve validation emails, which increases operational effort.
* Best Solution:
* AWS Certificate Manager (ACM) with DNS Validation: Ensures automated and efficient
certificate management with the least operational effort.
References:
* AWS Certificate Manager (ACM)
* DNS Validation in ACM
QUESTION NO: 686
한 회사에서 Amazon Aurora MySQL DB 클러스터를 저장용으로 사용하는 다중 계층 웹
애플리케이션을 호스팅하고 있습니다. 애플리케이션 계층은 Amazon EC2 인스턴스에서
호스팅됩니다. 회사의 IT 보안 지침에 따르면 데이터베이스 자격 증명은 14일마다 암호화되고
교체되어야 합니다. 최소한의 운영 노력으로 이 요구 사항을 충족하려면 솔루션 설계자가
무엇을 해야 합니까?
A. 새 AWS Key Management Service(AWS KMS) 암호화 키 생성 AWS Secrets Manager를
사용하여 적절한 자격 증명과 함께 KMS 키를 사용하는 새 비밀을 생성합니다. 비밀을 Aurora
DB 클러스터와 연결합니다. 사용자 지정 교체 기간을 14로 구성합니다. 날
B. AWS Systems Manager Parameter Store에서 두 개의 매개변수를 생성합니다. 하나는
문자열 매개변수인 사용자 이름용이고 다른 하나는 비밀번호용으로 SecureStnng 유형을
사용합니다. 비밀번호 매개변수로 AWS Key Management Service(AWS KMS) 암호화를
선택하고 이를 로드합니다. 애플리케이션 계층의 매개변수 14일마다 암호를 교체하는 AWS
Lambda 함수를 구현합니다.
C. AWS KMS(AWS Key Management Service) 암호화 Amazon Elastic File System(Amazon
EFS) 파일 시스템에 자격 증명이 포함된 파일을 저장합니다. 애플리케이션 계층의 모든 EC2
인스턴스에 EFS 파일 시스템을 탑재합니다. 애플리케이션이 파일을 읽을 수 있고 슈퍼
사용자만 파일을 수정할 수 있도록 파일 시스템의 파일에 대한 액세스를 제한합니다.
14일마다 Aurora에서 키를 교체하고 파일에 새 자격 증명을 쓰는 AWS Lambda 함수를
471

IT Certification Guaranteed, The Easy Way!
구현합니다.
D. 애플리케이션이 자격 증명을 로드하는 데 사용하는 AWS Key Management Service(AWS
KMS) 암호화 Amazon S3 버킷에 자격 증명이 포함된 파일을 저장합니다. 올바른 자격 증명이
사용되도록 정기적으로 애플리케이션에 파일을 다운로드합니다. 14일마다 Aurora 자격
증명을 교체하고 이러한 자격 증명을 S3 버킷의 파일에 업로드하는 AWS Lambda 함수
Answer: A
Explanation:
https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-manager-rotate-credentials-
amazon-rds- database-types-oracle/
QUESTION NO: 687
솔루션 설계자는 Amazon EC2 인스턴스를 호스팅하는 VPC 네트워크를 보호해야 합니다.
EC2 인스턴스에는 매우 민감한 데이터가 포함되어 있고 프라이빗 서브넷이 포함되어
있습니다. 회사 정책에 따라 VPC에서 실행되는 EC2 인스턴스는 인터넷에서 승인된 타사
소프트웨어 저장소에만 액세스할 수 있습니다. 타사의 URL을 사용하는 소프트웨어 제품
업데이트의 경우 기타 인터넷 트래픽을 차단해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 아웃바운드 트래픽을 AWS 네트워크 방화벽으로 라우팅하도록 프라이빗 서브넷의 라우팅
테이블을 업데이트합니다. 도메인 목록 규칙 그룹 구성
B. AWS WAF 웹 ACL을 설정합니다. 소스 및 대상 IP 주소 범위 집합을 기반으로 트래픽
요청을 필터링하는 사용자 지정 규칙 집합을 만듭니다.
C. 엄격한 인바운드 보안 그룹 역할 구현 URL을 지정하여 인터넷의 승인된 소프트웨어
저장소로만 트래픽을 허용하는 아웃바운드 규칙을 구성합니다.
D. EC2 인스턴스 앞에 Application Load Balancer(ALB)를 구성합니다. 아웃바운드 트래픽을
ALB로 전달합니다. 인터넷에 대한 아웃바운드 액세스를 위해 ALB 대상 그룹의 URL 기반
규칙 리스너를 사용합니다.
Answer: A
Explanation:
Send the outbound connection from EC2 to Network Firewall. In Network Firewall, create
stateful outbound rules to allow certain domains for software patch download and deny all
other domains. https://docs.aws.
amazon.com/network-firewall/latest/developerguide/suricata-examples.html#suricata-
example-domain- filtering
QUESTION NO: 688
한 회사에 Amazon uynamoUb에 기반을 둔 데이터 저장소가 있는 모바일 채팅
애플리케이션이 있습니다. 사용자는 가능한 짧은 대기 시간으로 새 메시지를 읽기를
원합니다. 솔루션 설계자는 애플리케이션 변경을 최소화하는 최적의 솔루션을 설계해야
합니다.
솔루션 설계자는 어떤 방법을 선택해야 합니까?
A. 새 메시지 테이블에 대해 Amazon DynamoDB Accelerator(DAX)를 구성합니다.
DAXendpoint를 사용하도록 코드를 업데이트합니다.
B. 증가된 읽기 로드를 처리하기 위해 DynamoDB 읽기 복제본을 추가합니다. 읽기 전용
복제본의 읽기 엔드포인트를 가리키도록 애플리케이션을 업데이트합니다.
C. DynamoDB의 새 메시지 테이블에 대한 읽기 용량 단위 수를 두 배로 늘립니다. 기존
472

IT Certification Guaranteed, The Easy Way!
DynamoDB 엔드포인트를 계속 사용합니다.
D. Redis용 Amazon ElastiCache 캐시를 애플리케이션 스택에 추가합니다. DynamoDB 대신
Redis 캐시 엔드포인트를 가리키도록 애플리케이션을 업데이트합니다.
Answer: A
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-high-latency/
Amazon DynamoDB Accelerator (DAX) is a fully managed in-memory cache for DynamoDB
that improves the performance of DynamoDB tables by up to 10 times and provides
microsecond level of response time at any scale. It is compatible with DynamoDB API
operations and requires minimal code changes to use1. By configuring DAX for the new
messages table, the solution can reduce the latency for reading new messages with minimal
application changes.
B: Add DynamoDB read repticas to handle the increased read load. Update the application to
point to the read endpoint for the read replicas. This solution will not work, as DynamoDB
does not support read replicas as a feature. Read replicas are available for Amazon RDS,
not for DynamoDB2.
C: Double the number of read capacity units for the new messages table in DynamoDB.
Continue to use the existing DynamoDB endpoint. This solution will not meet the requirement
of reading new messages with as little latency as possible, as increasing the read capacity
units will only increase the throughput of DynamoDB, not the performance or latency3.
D: Add an Amazon ElastiCache for Redis cache to the application stack. Update the
application to point to the Redis cache endpoint instead of DynamoDB. This solution will not
meet the requirement of minimal application changes, as adding ElastiCache for Redis will
require significant code changes to implement caching logic, such as querying cache first,
updating cache after writing to DynamoDB, and invalidating cache when needed.
Reference URL: https://aws.amazon.com/dynamodb/dax/
QUESTION NO: 689
개발 팀은 개발 VPC 내부의 Amazon EC2 인스턴스에서 호스팅되는 새로운 애플리케이션을
출시했습니다. 솔루션 아키텍트는 동일한 계정에 새 VPC를 생성해야 합니다. 새 VPC는 ​​개발
VPC와 피어링됩니다. 개발 VPC의 VPC CIDR 블록은 192.168.00/24입니다. 솔루션
아키텍트는 새 VPC에 대한 CIDR 블록을 생성해야 합니다. CIDR 블록은 개발 VPC에 대한
VPC 피어링 연결에 유효해야 합니다.
이러한 요구 사항을 충족하는 가장 작은 CIOR 블록은 무엇입니까?
A. 10.0.1.0/32
B. 192.168.0.0/24
C. 192.168.1.0/32
D. 10.0.1.0/24
Answer: D
Explanation:
The allowed block size is between a /28 netmask and /16 netmask. The CIDR block must not
overlap with any existing CIDR block that's associated with the VPC.
https://docs.aws.amazon.com/vpc/latest/userguide
/configure-your-vpc.html
473

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 690
회사의 로컬 데이터 센터에 Docker 컨테이너를 사용하는 애플리케이션이 있습니다. 이
애플리케이션은 호스트의 볼륨에 영구 데이터를 저장하는 컨테이너 호스트에서 실행됩니다.
컨테이너 인스턴스는 저장된 영구 데이터를 사용합니다.
회사는 서버나 스토리지 인프라를 관리하고 싶지 않기 때문에 애플리케이션을 완전 관리형
서비스로 이동하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 자체 관리형 노드와 함께 Amazon Elastic Kubernetes Service(Amazon EKS)를
사용합니다. Amazon EC2 인스턴스에 연결된 Amazon Elastic Block Store(Amazon EBS)
볼륨을 생성합니다. EBS 볼륨을 컨테이너에 탑재된 영구 볼륨으로 사용합니다.
B. AWS Fargate 시작 유형과 함께 Amazon Elastic Container Service(Amazon ECS)를
사용합니다. Amazon Elastic File System(Amazon EFS) 볼륨을 생성합니다. EFS 볼륨을
컨테이너에 탑재된 영구 스토리지 볼륨으로 추가합니다.
C. AWS Fargate 시작 유형과 함께 Amazon Elastic Container Service(Amazon ECS)를
사용합니다. Amazon S3 버킷을 생성합니다. S3 버킷을 컨테이너에 탑재된 영구 스토리지
볼륨으로 매핑합니다.
D. Amazon EC2 시작 유형과 함께 Amazon Elastic Container Service(Amazon ECS)를
사용합니다. Amazon Elastic File System(Amazon EFS) 볼륨을 생성합니다. EFS 볼륨을
컨테이너에 탑재된 영구 스토리지 볼륨으로 추가합니다.
Answer: B
Explanation:
This solution meets the requirements because it allows the company to move the application
to a fully managed service without managing any servers or storage infrastructure. AWS
Fargate is a serverless compute engine for containers that runs the Amazon ECS tasks. With
Fargate, the company does not need to provision, configure, or scale clusters of virtual
machines to run containers. Amazon EFS is a fully managed file system that can be
accessed by multiple containers concurrently. With EFS, the company does not need to
provision and manage storage capacity. EFS provides a simple interface to create and
configure file systems quickly and easily. The company can use the EFS volume as a
persistent storage volume mounted in the containers to store the persistent data. The
company can also use the EFS mount helper to simplify the mounting process. References:
Amazon ECS on AWS Fargate, Using Amazon EFS file systems with Amazon ECS, Amazon
EFS mount helper.
QUESTION NO: 691
한 회사가 Amazon EC2 인스턴스 집합에서 프로덕션 애플리케이션을 실행합니다.
애플리케이션은 Amazon SQS 대기열에서 데이터를 읽고 메시지를 병렬로 처리합니다.
메시지 볼륨은 예측할 수 없으며 트래픽이 간헐적으로 발생하는 경우가 많습니다. 이
애플리케이션은 가동 중지 시간 없이 지속적으로 메시지를 처리해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 필요한 최대 용량을 처리하려면 스팟 인스턴스만 사용하십시오.
B. 예약 인스턴스를 독점적으로 사용하여 필요한 최대 용량을 처리합니다.
C. 기본 용량으로 예약 인스턴스를 사용하고 추가 용량을 처리하려면 스팟 인스턴스를
사용합니다.
474

IT Certification Guaranteed, The Easy Way!
D. 기본 용량으로 예약 인스턴스를 사용하고 추가 용량을 처리하려면 온디맨드 인스턴스를
사용합니다.
Answer: D
Explanation:
We recommend that you use On-Demand Instances for applications with short-term, irregular
workloads that cannot be interrupted.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.
html
QUESTION NO: 692
한 회사가 AWS 클라우드에서 애플리케이션을 구축하고 있습니다. 이 애플리케이션은
Application Load Balancer(ALB) 뒤의 Amazon EC2 인스턴스에서 호스팅됩니다. 이 회사는
DNS에 Amazon Route 53을 사용합니다.
해당 회사에는 DDoS 공격을 감지하기 위한 사전 대응이 가능한 관리형 솔루션이 필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. AWS Config를 활성화합니다. DDoS 공격을 감지하는 AWS Config 관리 규칙을
구성합니다.
B. ALB에서 AWS WAF 활성화 DDoS 공격을 탐지하고 방지하기 위한 규칙이 있는 AWS WAF
웹 ACL을 만듭니다. 웹 ACL을 ALB와 연결합니다.
C. ALB 액세스 로그를 Amazon S3 버킷에 저장합니다. Amazon GuardDuty를 구성하여 DDoS
공격을 감지하고 자동화된 예방 조치를 취합니다.
D. AWS Shield Advanced를 구독합니다. Route 53에서 호스팅된 영역을 구성합니다. ALB
리소스를 보호된 리소스로 추가합니다.
Answer: D
Explanation:
AWS Shield Advanced is designed to provide enhanced protection against DDoS attacks
with proactive engagement and response capabilities, making it the best solution for this
scenario.
* AWS Shield Advanced: This service provides advanced protection against DDoS attacks. It
includes detailed attack diagnostics, 24/7 access to the AWS DDoS Response Team (DRT),
and financial protection against DDoS-related scaling charges. Shield Advanced also
integrates with Route 53 and the Application Load Balancer (ALB) to ensure comprehensive
protection for your web applications.
* Route 53 and ALB Protection: By adding your Route 53 hosted zones and ALB resources to
AWS Shield Advanced, you ensure that these components are covered under the enhanced
protection plan.
Shield Advanced actively monitors traffic and provides real-time attack mitigation, minimizing
the impact of DDoS attacks on your application.
* Why Not Other Options?:
* Option A (AWS Config): AWS Config is a configuration management service and does not
provide DDoS protection or detection capabilities.
* Option B (AWS WAF): While AWS WAF can help mitigate some types of attacks, it does
not provide the comprehensive DDoS protection and proactive engagement offered by Shield
Advanced.
* Option C (GuardDuty): GuardDuty is a threat detection service that identifies potentially
475

IT Certification Guaranteed, The Easy Way!
malicious activity within your AWS environment, but it is not specifically designed to provide
DDoS protection.
AWS References:
* AWS Shield Advanced - Overview of AWS Shield Advanced and its DDoS protection
capabilities.
* Integrating AWS Shield Advanced with Route 53 and ALB - Detailed guidance on how to
protect Route 53 and ALB with AWS Shield Advanced.
QUESTION NO: 693
한 회사는 최근 Amazon EC2 인스턴스용 운영 체제 버전 패치 및 설치된 소프트웨어에 대한
정보를 중앙 집중화하기 위해 새로운 감사 시스템을 배포했습니다. 솔루션 아키텍트는 EC2
Auto Scaling 그룹을 통해 프로비저닝된 모든 인스턴스가 시작 및 종료되는 즉시 감사
시스템에 보고서를 성공적으로 보내도록 해야 합니다. 어떤 솔루션이 이러한 목표를 가장
효율적으로 달성합니까?
A. 예약된 AWS Lambda 함수를 사용하고 모든 EC2 인스턴스에서 원격으로 스크립트를
실행하여 데이터를 감사 시스템으로 보냅니다.
B. EC2 Auto Scaling 수명 주기 후크를 사용하여 인스턴스가 시작 및 종료될 때 감사 시스템에
데이터를 보내는 사용자 지정 스크립트를 실행합니다.
C. EC2 Auto Scaling 시작 구성을 사용하여 사용자 데이터를 통해 사용자 지정 스크립트를
실행하여 인스턴스가 시작 및 종료될 때 감사 시스템에 데이터를 보냅니다.
D. 인스턴스 운영 체제에서 사용자 지정 스크립트를 실행하여 감사 시스템에 데이터를
보냅니다. 인스턴스가 시작되고 종료될 때 EC2 Auto Scaling 그룹이 호출할 스크립트를
구성합니다.
Answer: B
Explanation:
Amazon EC2 Auto Scaling offers the ability to add lifecycle hooks to your Auto Scaling
groups. These hooks let you create solutions that are aware of events in the Auto Scaling
instance lifecycle, and then perform a custom action on instances when the corresponding
lifecycle event occurs. (https://docs.aws.amazon.com
/autoscaling/ec2/userguide/lifecycle-hooks.html)
QUESTION NO: 694
한 회사는 Amazon S3 버킷을 데이터 레이크 스토리지 플랫폼으로 사용합니다. S3 버킷에는
여러 팀과 수백 개의 애플리케이션에서 무작위로 액세스하는 엄청난 양의 데이터가 포함되어
있습니다. 회사는 S3 스토리지 비용을 절감하고 자주 액세스하는 객체에 대한 즉각적인
가용성을 제공하고자 합니다. 이러한 요구 사항을 충족하는 가장 운영 효율적인 솔루션은
무엇입니까?
A. 객체를 S3 Intelligent-Tiering 스토리지 클래스로 전환하는 S3 수명 주기 규칙을
생성합니다.
B. Amazon S3 Glacier에 객체를 저장합니다. S3 Select를 사용하여 애플리케이션에 데이터에
대한 액세스 권한을 제공합니다.
C. S3 스토리지 클래스 분석의 데이터를 사용하여 객체를 S3 Standard-Infrequent Access(S3
Standard-IA) 스토리지 클래스로 자동 전환하는 S3 수명 주기 규칙을 생성합니다.
D. S3 Standard-Infrequent Access(S3 Standard-IA) 스토리지 클래스로 객체 전환
애플리케이션에서 객체에 액세스할 때 객체를 S3 Standard 스토리지 클래스로 전환하는
476

IT Certification Guaranteed, The Easy Way!
AWS Lambda 함수를 생성합니다.
Answer: A
Explanation:
* Amazon S3 Intelligent-Tiering: This storage class is designed to optimize costs by
automatically moving data between two access tiers (frequent and infrequent) when access
patterns change. It provides cost savings without performance impact or operational
overhead.
* S3 Lifecycle Rules: By creating an S3 Lifecycle rule, the company can automatically
transition objects to the Intelligent-Tiering storage class. This eliminates the need for manual
intervention and ensures that objects are moved to the most cost-effective storage tier based
on their access patterns.
* Operational Efficiency: Intelligent-Tiering requires no additional management and delivers
immediate availability for frequently accessed objects. This makes it the most operationally
efficient solution for the given requirements.
References:
* Amazon S3 Intelligent-Tiering
* S3 Lifecycle Policies
QUESTION NO: 695
한 회사가 Amazon EC2 인스턴스 집합에서 3계층 전자상거래 애플리케이션을 호스팅합니다.
인스턴스는 ALB(Application Load Balancer) 뒤의 Auto Scaling 그룹에서 실행됩니다. 모든
전자상거래 데이터는 ManaDB 다중 AZ DB 인스턴스용 Amazon RDS에 저장됩니다. 회사는
트랜잭션 중 고객 세션 관리를 최적화하기를 원합니다. 애플리케이션은 세션 데이터를 내구성
있게 저장해야 합니다. 솔루션이 이러한 요구 사항을 충족할 수 있을까요? (2개 선택)
A. ALB에서 고정 세션 기능(세션 선호도)을 설정합니다.
B. Amazon DynamoDB 테이블을 사용하여 고객 세션 정보 저장
C. Amazon Cognito 사용자 풀을 배포하여 사용자 세션 정보 관리
D. Redis용 Amazon ElastiCache 클러스터를 배포하여 고객 세션 정보 저장
E. 애플리케이션에서 AWS Systems Manager Application Manager를 사용하여 사용자 세션
정보를 관리합니다.
Answer: A D
Explanation:
https://aws.amazon.com/caching/session-management/
QUESTION NO: 696
한 회사가 AWS에서 애플리케이션을 실행하고 있습니다. 애플리케이션의 사용량이 일관되지
않습니다. 애플리케이션은 AWS Direct Connect를 사용하여 온프레미스 MySQL 호환
데이터베이스에 연결합니다. 온프레미스 데이터베이스는 최소 2GiB의 메모리를 지속적으로
사용합니다.
회사는 온프레미스 데이터베이스를 관리형 AWS 서비스로 마이그레이션하려고 합니다.
회사는 자동 확장 기능을 사용하여 예상치 못한 워크로드 증가를 관리하려고 합니다.
최소한의 관리 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 기본 읽기 및 쓰기 용량 설정으로 Amazon DynamoDB 데이터베이스를 프로비저닝합니다.
B. 최소 용량이 1 Aurora 용량 단위(ACU)인 Amazon Aurora 데이터베이스를
프로비저닝합니다.
477

IT Certification Guaranteed, The Easy Way!
C. 최소 용량이 1 Aurora 용량 단위(ACU)인 Amazon Aurora Serverless v2 데이터베이스를
프로비저닝합니다.
D. 2GiB 메모리로 MySQL 데이터베이스용 Amazon RDS를 프로비저닝합니다.
Answer: C
Explanation:
it allows the company to migrate the on-premises database to a managed AWS service that
supports auto scaling capabilities and has the least administrative overhead. Amazon Aurora
Serverless v2 is a configuration of Amazon Aurora that automatically scales compute
capacity based on workload demand. It can scale from hundreds to hundreds of thousands of
transactions in a fraction of a second. Amazon Aurora Serverless v2 also supports MySQL-
compatible databases and AWS Direct Connect connectivity. References:
* Amazon Aurora Serverless v2
* Connecting to an Amazon Aurora DB Cluster
QUESTION NO: 697
한 회사가 AWS 클라우드에서 다중 계층 퍼블릭 웹 애플리케이션을 호스팅합니다. 웹
애플리케이션은 Amazon EC2 인스턴스에서 실행되고 데이터베이스는 Amazon RDS에서
실행됩니다. 이 회사는 다가올 휴일 주말에 매출이 크게 증가할 것으로 예상합니다. 솔루션
아키텍트는 2분을 넘지 않는 세부성으로 웹 애플리케이션의 성능을 분석하는 솔루션을
구축해야 합니다.
이 요구 사항을 충족하기 위해 솔루션 아키텍트는 무엇을 해야 할까요?
A. Amazon CloudWatch 로그를 Amazon Redshift로 보냅니다. Amazon QuickSight를
사용하여 추가 분석을 수행합니다.
B. 모든 EC2 인스턴스에서 자세한 모니터링을 활성화합니다. Amazon CloudWatch 메트릭을
사용하여 추가 분석을 수행합니다.
C. Amazon CloudWatch Logs에서 EC2 로그를 가져오는 AWS Lambda 함수를 만듭니다.
Amazon CloudWatch 메트릭을 사용하여 추가 분석을 수행합니다.
D. EC2 로그를 Amazon S3로 보냅니다. Amazon Redshift를 사용하여 S3 버킷에서 tog를
가져와 Amazon QuickSight로 추가 분석을 위한 원시 데이터를 처리합니다.
Answer: B
Explanation:
To analyze the performance of the web application with granularity of no more than 2
minutes, enabling detailed monitoring on EC2 instances is the best solution. By default,
CloudWatch provides metrics at a 5- minute interval. Enabling detailed monitoring allows you
to collect metrics at 1-minute intervals, which will give you the level of granularity you need to
analyze performance during peak traffic.
Amazon CloudWatch metrics can then be used to analyze CPU utilization, memory usage,
disk I/O, and network throughput, among other performance-related metrics, at the desired
granularity.
* Option A: Sending CloudWatch logs to Redshift for analysis is unnecessary and
overcomplicated for simple performance analysis, which can be done using CloudWatch
metrics alone.
* Option C: Fetching EC2 logs via Lambda adds complexity, and CloudWatch metrics already
provide the required data for performance analysis.
* Option D: Sending logs to S3 and using Redshift for analysis is also more complex than
478

IT Certification Guaranteed, The Easy Way!
necessary for simple performance monitoring.
AWS References:
* Monitoring Amazon EC2 with CloudWatch
* Amazon CloudWatch Detailed Monitoring
QUESTION NO: 698
회사에서 온프레미스 레거시 애플리케이션을 AWS로 마이그레이션하려고 합니다.
애플리케이션은 온프레미스 ERP(전사적 자원 관리) 시스템에서 고객 주문 파일을
수집합니다. 그런 다음 애플리케이션은 파일을 SFTP 서버에 업로드합니다. 애플리케이션은
매시간 주문 파일을 확인하는 예약된 작업을 사용합니다.
회사에는 이미 온프레미스 네트워크에 연결된 AWS 계정이 있습니다. AWS의 새로운
애플리케이션은 기존 ERP 시스템과의 통합을 지원해야 합니다. 새로운 애플리케이션은
안전하고 탄력적이어야 하며 SFTP 프로토콜을 사용하여 ERP 시스템의 주문을 즉시
처리해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 두 개의 가용 영역에 AWS Transfer Family SFTP 인터넷 연결 서버를 생성합니다. Amazon
S3 스토리지를 사용하세요. 주문 파일을 처리하는 AWS Lambda 함수를 생성합니다. S3
이벤트 알림을 사용하여 s3: ObjectCreated: * 이벤트를 Lambda 함수로 보냅니다.
B. 하나의 가용 영역에 AWS Transfer Family SFTP 인터넷 연결 서버를 생성합니다. Amazon
Elastic File System(Amazon EFS) 스토리지를 사용합니다. 주문 파일을 처리하는 AWS
Lambda 함수를 생성합니다. Transfer Family 관리형 워크플로를 사용하여 Lambda 함수를
호출합니다.
C. 두 개의 가용 영역에 AWS Transfer Family SFTP 내부 서버를 생성합니다. Amazon Elastic
File System(Amazon EFS) 스토리지를 사용합니다. 주문 파일을 처리하기 위해 AWS Step
Functions 상태 머신을 생성합니다. Amazon EventBridge Scheduler를 사용하면 상태
시스템을 호출하여 Amazon EFS에서 주문 파일을 주기적으로 확인할 수 있습니다.
D. 두 개의 가용 영역에 AWS Transfer Family SFTP 내부 서버를 생성합니다. Amazon S3
스토리지를 사용하세요. 주문 파일을 처리하는 AWS Lambda 함수를 생성합니다. Transfer
Family 관리형 워크플로를 사용하여 Lambda 함수를 호출합니다.
Answer: D
Explanation:
This solution meets the requirements because it uses the following components and features
:
* AWS Transfer Family SFTP internal server: This allows the application to securely transfer
order files from the on-premises ERP system to AWS using the SFTP protocol over a private
connection. The internal server is deployed in two Availability Zones for high availability and
fault tolerance.
* Amazon S3 storage: This provides scalable, durable, and cost-effective object storage for
the order files. Amazon S3 also supports encryption at rest and in transit, as well as lifecycle
policies and versioning for data protection and compliance.
* AWS Lambda function: This enables the application to process the order files in a
serverless manner, without provisioning or managing servers. The Lambda function can
perform any custom logic or transformation on the order files, such as validating, parsing, or
enriching the data.
* Transfer Family managed workflow: This simplifies the orchestration of the file processing
479

IT Certification Guaranteed, The Easy Way!
tasks by triggering the Lambda function as soon as a file is uploaded to the SFTP server. The
managed workflow also provides error handling, retry policies, and logging capabilities.
QUESTION NO: 699
금융 회사는 매우 민감한 데이터를 처리해야 합니다. 회사는 데이터를 Amazon S3 버킷에
저장할 것입니다. 회사는 데이터가 전송 중이거나 저장 중일 때 암호화되도록 해야 합니다.
회사는 AWS 클라우드 외부에서 암호화 키를 관리해야 합니다. 어떤 솔루션이 충족해야
할까요? 이러한 요구 사항은 무엇입니까?
A. AWS Key Management Service(AWS KMS) 고객 관리형 키를 사용하는 서버 측
암호화(SSE)로 S3 버킷의 데이터를 암호화합니다.
B. AWS Key Management Service(AWS KMS) AWS 관리형 키를 사용하는 서버 측
암호화(SSE)로 S3 버킷의 데이터를 암호화합니다.
C. 기본 서버 측 암호화(SSE)를 사용하여 S3 버킷의 데이터를 암호화합니다.
D. S3 버킷에 데이터를 저장하기 전에 회사 데이터 센터의 데이터를 암호화합니다.
Answer: D
Explanation:
This option is the only solution that meets the requirements because it allows the company to
encrypt the data with its own encryption keys and tools outside the AWS Cloud. By
encrypting the data at the company's data center before storing the data in the S3 bucket, the
company can ensure that the data is encrypted in transit and at rest, and that the company
has full control over the encryption keys and processes. This option also avoids the need to
use any AWS encryption services or features, which may not be compatible with the
company's security policies or compliance standards.
A: Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS
Key Management Service (AWS KMS) customer managed key. This option does not meet
the requirements because it does not allow the company to manage the encryption keys
outside the AWS Cloud. Although the company can create and use its own customer
managed key in AWS KMS, the key is still stored and managed by AWS KMS, which is a
service within the AWS Cloud. Moreover, the company still needs to use the AWS encryption
features and APIs to encrypt and decrypt the data in the S3 bucket, which may not be
compatible with the company's security policies or compliance standards.
B: Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS
Key Management Service (AWS KMS) AWS managed key. This option does not meet the
requirements because it does not allow the company to manage the encryption keys outside
the AWS Cloud. In this option, the company uses the default AWS managed key in AWS
KMS, which is created and managed by AWS on behalf of the company. The company has
no control over the key rotation, deletion, or recovery policies. Moreover, the company still
needs to use the AWS encryption features and APIs to encrypt and decrypt the data in the
S3 bucket, which may not be compatible with the company's security policies or compliance
standards.
C: Encrypt the data in the S3 bucket with the default server-side encryption (SSE). This
option does not meet the requirements because it does not allow the company to manage the
encryption keys outside the AWS Cloud. In this option, the company uses the default server-
side encryption with Amazon S3 managed keys (SSE-S3), which is applied to every bucket in
Amazon S3. The company has no visibility or control over the encryption keys, which are
480

IT Certification Guaranteed, The Easy Way!
managed by Amazon S3. Moreover, the company still needs to use the AWS encryption
features and APIs to encrypt and decrypt the data in the S3 bucket, which may not be
compatible with the company's security policies or compliance standards.
References:
* 1 Protecting data with encryption - Amazon Simple Storage Service
* 2 Protecting data with server-side encryption - Amazon Simple Storage Service
* 3 Protecting data by using client-side encryption - Amazon Simple Storage Service
* 4 AWS Key Management Service Concepts - AWS Key Management Service
QUESTION NO: 700
회사에는 온프레미스 iSCSI(Internet Small Computer Systems Interface) 네트워크 스토리지
서버가 여러 대 있습니다. 회사는 AWS 클라우드로 이동하여 이러한 서버의 수를 줄이고
싶어합니다. 솔루션 설계자는 자주 사용되는 데이터에 대한 짧은 대기 시간 액세스를
제공하고 최소한의 인프라 변경으로 온프레미스 서버에 대한 종속성을 줄여야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon S3 파일 게이트웨이 배포
B. Amazon S3에 대한 백업과 함께 Amazon Elastic Block Store(Amazon EBS) 스토리지 배포
C. 저장된 볼륨으로 구성된 AWS Storage Gateway 볼륨 게이트웨이 배포
D. 캐시된 볼륨으로 구성된 AWS Storage Gateway 볼륨 게이트웨이를 배포합니다.
Answer: D
Explanation:
* Storage Gateway Volume Gateway (Cached Volumes): This configuration allows you to
store your primary data in Amazon S3 while retaining frequently accessed data locally in a
cache for low-latency access.
* Low-Latency Access: Frequently accessed data is cached locally on-premises, providing
low-latency access while the less frequently accessed data is stored cost-effectively in
Amazon S3.
* Implementation:
* Deploy a Storage Gateway appliance on-premises or in a virtual environment.
* Configure it as a volume gateway with cached volumes.
* Create volumes and configure your applications to use these volumes.
* Minimal Infrastructure Changes: This solution integrates seamlessly with existing on-
premises infrastructure, requiring minimal changes and reducing dependency on on-
premises storage servers.
References:
* AWS Storage Gateway Volume Gateway
* Volume Gateway Cached Volumes
QUESTION NO: 701
한 회사에서 Amazon Elastic Container Service(Amazon ECS)를 사용하여 하이브리드
환경에서 온프레미스 애플리케이션을 실행하려고 합니다. 애플리케이션은 현재 온프레미스
컨테이너에서 실행됩니다.
회사에는 온프레미스, 하이브리드 또는 클라우드 환경에서 확장할 수 있는 단일 컨테이너
솔루션이 필요합니다. 회사는 AWS 클라우드에서 새로운 애플리케이션 컨테이너를 실행해야
하며 HTTP 트래픽용 로드 밸런서를 사용해야 합니다.
481

IT Certification Guaranteed, The Easy Way!
이러한 요구 사항을 충족하는 작업 조합은 무엇입니까? (2개를 선택하세요.)
A. 클라우드 애플리케이션 컨테이너에 대해 AWS Fargate 시작 유형을 사용하는 ECS
클러스터를 설정합니다. 온프레미스 애플리케이션 컨테이너에 Amazon ECS Anywhere 외부
시작 유형을 사용합니다.
B. 클라우드 ECS 서비스용 Application Load Balancer 설정
C. 클라우드 ECS 서비스용 Network Load Balancer를 설정합니다.
D. AWS Fargate 시작 유형을 사용하는 ECS 클러스터를 설정합니다. 클라우드 애플리케이션
컨테이너 및 온프레미스 애플리케이션 컨테이너에 Fargate를 사용합니다.
E. 클라우드 애플리케이션 컨테이너에 Amazon EC2 시작 유형을 사용하는 ECS 클러스터를
설정합니다. 온프레미스 애플리케이션 컨테이너에 대해 AWS Fargate 시작 유형과 함께
Amazon ECS Anywhere를 사용하십시오.
Answer: A B
Explanation:
* Understanding the Requirement: The company needs a container solution that can scale
across on- premises, hybrid, and cloud environments, with a load balancer for HTTP traffic.
* Analysis of Options:
* Fargate Launch Type and ECS Anywhere: Using Fargate for cloud-based containers and
ECS Anywhere for on-premises containers provides a unified management experience
across environments without needing to manage infrastructure.
* Application Load Balancer: Suitable for HTTP traffic and can distribute requests to the ECS
services, ensuring scalability and performance.
* Network Load Balancer: Typically used for TCP/UDP traffic, not specifically optimized for
HTTP traffic.
* EC2 Launch Type for ECS and ECS Anywhere with Fargate: Involves managing
infrastructure for EC2 instances, increasing operational overhead.
* Best Combination of Solutions:
* ECS with Fargate Launch Type and ECS Anywhere: This provides flexibility and scalability
across hybrid environments with minimal operational overhead.
* Application Load Balancer: Optimized for HTTP traffic, ensuring efficient load distribution
and scaling for the ECS services.
References:
* Amazon ECS on AWS Fargate
* Amazon ECS Anywhere
* Application Load Balancer
QUESTION NO: 702
회사에는 ALB(Application Load Balancer) 뒤의 단일 가용 영역에 있는 Amazon EC2 Auto
Scaling 그룹의 프런트 엔드 웹 서버 6개를 실행하는 다중 계층 애플리케이션이 있습니다.
솔루션 설계자는 애플리케이션을 수정하지 않고도 가용성을 높이기 위해 인프라를 수정해야
합니다.
솔루션 설계자는 고가용성을 제공하기 위해 어떤 아키텍처를 선택해야 합니까?
A. 각 tv/o 지역에서 3개의 인스턴스를 사용하는 Auto Scaling 그룹을 생성합니다.
B. 두 개의 가용 영역 각각에서 세 개의 인스턴스를 사용하도록 Auto Scaling 그룹을
수정합니다.
C. 다른 리전에서 더 많은 인스턴스를 빠르게 생성하는 데 사용할 수 있는 Auto Scaling
482

IT Certification Guaranteed, The Easy Way!
템플릿을 생성합니다.
D. 라운드 로빈 구성에서 Amazon EC2 인스턴스 앞의 ALB를 변경하여 웹 계층에 대한 트래픽
균형을 조정합니다.
Answer: B
Explanation:
High availability can be enabled for this architecture quite simply by modifying the existing
Auto Scaling group to use multiple availability zones. The ASG will automatically balance the
load so you don't actually need to specify the instances per AZ.
QUESTION NO: 703
회사에서는 기본 온프레미스 파일 스토리지 볼륨에 대한 재해 복구 계획을 구현하려고
합니다. 파일 스토리지 볼륨은 로컬 스토리지 서버의 iSCSI(Internet Small Computer Systems
Interface) 장치에서 마운트됩니다. 파일 스토리지 볼륨에는 수백 테라바이트(TB)의 데이터가
저장됩니다.
회사는 최종 사용자가 대기 시간 없이 온프레미스 시스템의 모든 파일 형식에 즉시 액세스할
수 있기를 원합니다.
회사의 기존 인프라를 최소한으로 변경하면서 이러한 요구 사항을 충족하는 솔루션은
무엇입니까?
A. Amazon S3 파일 게이트웨이를 온프레미스에서 호스팅되는 가상 머신(VM)으로
프로비저닝합니다. 로컬 캐시를 10TB로 설정합니다. NFS 프로토콜을 통해 파일에
액세스하도록 기존 애플리케이션을 수정합니다. 재해로부터 복구하려면 Amazon EC2
인스턴스를 프로비저닝하고 파일이 포함된 S3 버킷을 탑재합니다.
B. AWS Storage Gateway 테이프 게이트웨이를 프로비저닝합니다. 데이터 백업 솔루션을
사용하여 모든 기존 데이터를 가상 테이프 라이브러리에 백업하십시오. 초기 백업이 완료된
후 야간에 실행되도록 데이터 백업 솔루션을 구성합니다. 재해로부터 복구하려면 Amazon
EC2 인스턴스를 프로비저닝하고 가상 테이프 라이브러리의 볼륨에서 Amazon Elastic Block
Store(Amazon EBS) 볼륨으로 데이터를 복원합니다.
C. AWS Storage Gateway 볼륨 게이트웨이 캐시 볼륨을 프로비저닝합니다. 로컬 캐시를
10TB로 설정합니다. iSCSI를 사용하여 볼륨 게이트웨이 캐시 볼륨을 기존 파일 서버에
탑재합니다. 모든 파일을 저장 볼륨에 복사합니다. 스토리지 볼륨의 예약된 스냅샷을
구성합니다. 재해로부터 복구하려면 스냅샷을 Amazon Elastic Block Store(Amazon EBS)
볼륨으로 복원하고 EBS 볼륨을 Amazon EC2 인스턴스에 연결합니다.
D. 기존 파일 스토리지 볼륨과 동일한 양의 디스크 공간으로 AWS Storage Gateway 볼륨
게이트웨이 저장 볼륨을 프로비저닝합니다. iSCSI를 사용하여 볼륨 게이트웨이 저장 볼륨을
기존 파일 서버에 탑재하고 모든 파일을 스토리지 볼륨에 복사합니다. 스토리지 볼륨의
예약된 스냅샷을 구성합니다. 재해로부터 복구하려면 스냅샷을 Amazon Elastic Block
Store(Amazon EBS) 볼륨으로 복원하고 EBS 볼륨을 Amazon EC2 인스턴스에 연결합니다.
Answer: D
Explanation:
"The company wants to ensure that end users retain immediate access to all file types from
the on-premises systems " - Cached volumes: low latency access to most recent data - Store
d volumes: entire dataset is on premise, scheduled backups to S3 Hence Volume Gateway
stored volume is the apt choice.
QUESTION NO: 704
483

IT Certification Guaranteed, The Easy Way!
회사에는 동일한 AWS 계정 내의 us-west-2 리전에 위치한 두 개의 VPC가 있습니다. 회사는
이러한 VPC 간의 네트워크 트래픽을 허용해야 합니다. 매달 VPC 간에 약 500GB의 데이터
전송이 발생합니다.
이러한 VPC를 연결하는 가장 비용 효율적인 솔루션은 무엇입니까?
A. AWS Transit Gateway를 구현하여 VPC를 연결합니다. VPC 간 통신을 위해 전송
게이트웨이를 사용하도록 각 VPC의 라우팅 테이블을 업데이트합니다.
B. VPC 간에 AWS Site-to-Site VPN 터널을 구현합니다. VPC 간 통신에 VPN 터널을
사용하도록 각 VPC의 라우팅 테이블을 업데이트합니다.
C. VPC 간에 VPC 피어링 연결을 설정합니다. VPC 간 통신에 VPC 피어링 연결을 사용하도록
각 VPC의 라우팅 테이블을 업데이트합니다.
D. VPC 간에 1GB AWS Direct Connect 연결을 설정합니다. VPC 간 통신에 Direct Connect
연결을 사용하도록 각 VPC의 라우팅 테이블을 업데이트합니다.
Answer: D
Explanation:
To connect two VPCs in the same Region within the same AWS account, VPC peering is the
most cost- effective solution. VPC peering allows direct network traffic between the VPCs
without requiring a gateway, VPN connection, or AWS Transit Gateway. VPC peering also
does not incur any additional charges for data transfer between the VPCs.
References:
* What Is VPC Peering?
* VPC Peering Pricing
QUESTION NO: 705
회사의 웹사이트에서는 사용자에게 다운로드 가능한 과거 실적 보고서를 제공합니다.
웹사이트에는 전 세계적으로 회사의 웹사이트 요구 사항을 충족할 수 있도록 확장할 수 있는
솔루션이 필요합니다. 솔루션은 비용 효율적이어야 하며, 인프라 리소스 프로비저닝을
제한하고, 가능한 가장 빠른 응답 시간을 제공해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 조합을 권장해야 합니까?
A. Amazon CloudFront 및 Amazon S3
B. AWS Lambda 및 Amazon DynamoDB
C. Amazon EC2 Auto Scaling을 갖춘 Application Load Balancer
D. 내부 Application Load Balancer가 있는 Amazon Route 53
Answer: A
Explanation:
Cloudfront for rapid response and s3 to minimize infrastructure.
QUESTION NO: 706
한 회사가 여러 대륙에 걸쳐 도시의 온도, 습도, 기압 데이터를 수집합니다. 사이트당 매일
수집되는 평균 데이터 양은 500GB입니다. 각 사이트에는 고속 인터넷 연결이 제공됩니다.
회사의 일기 예보 애플리케이션은 단일 지역을 기반으로 하며 매일 데이터를 분석합니다.
이러한 모든 글로벌 사이트에서 데이터를 집계하는 가장 빠른 방법은 무엇입니까?
A. 대상 버킷에서 Amazon S3 Transfer Acceleration을 활성화합니다. 멀티파트 업로드를
사용하여 사이트 데이터를 대상 버킷에 직접 업로드합니다.
B. 가장 가까운 AWS 리전에 있는 Amazon S3 버킷에 사이트 데이터를 업로드합니다. S3 교차
484

IT Certification Guaranteed, The Easy Way!
리전 복제를 사용하여 객체를 대상 버킷에 복사합니다.
C. 가장 가까운 AWS 리전으로 데이터를 전송하도록 AWS Snowball 작업을 매일 예약합니다.
S3 교차 리전 복제를 사용하여 객체를 대상 버킷에 복사합니다.
D. 가장 가까운 지역의 Amazon EC2 인스턴스에 데이터를 업로드합니다. Amazon Elastic
Block Store(Amazon EBS) 볼륨에 데이터를 저장합니다. 하루에 한 번씩 EBS 스냅샷을 찍어
중앙 리전에 복사합니다. 중앙 지역에 EBS 볼륨을 복원하고 매일 데이터 분석을 실행합니다.
Answer: A
Explanation:
You might want to use Transfer Acceleration on a bucket for various reasons, including the
following:
You have customers that upload to a centralized bucket from all over the world.
You transfer gigabytes to terabytes of data on a regular basis across continents.
You are unable to utilize all of your available bandwidth over the Internet when uploading to
Amazon S3.
https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html
https://aws.amazon.com/s3/transfer-
acceleration/#:~:text=S3%20Transfer%20Acceleration%20(S3TA)%
20reduces,to%20S3%20for%20remote%20applications:
"Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3
by as much as 50-
500% for long-distance transfer of larger objects. Customers who have either web or mobile
applications with widespread users or applications hosted far away from their S3 bucket can
experience long and variable upload and download speeds over the Internet"
https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html
"Improved throughput - You can upload parts in parallel to improve throughput."
QUESTION NO: 707
한 회사에서 Amazon EC2 데이터와 여러 Amazon S3 버킷에 대한 백업 전략을 구현하려고
합니다. 규정 요구 사항으로 인해 회사는 특정 기간 동안 백업 파일을 보관해야 합니다. 회사는
보관기간 동안 해당 파일을 변경해서는 안 됩니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Backup을 사용하여 거버넌스 모드에서 볼트 잠금이 있는 백업 볼트를 생성합니다.
필요한 백업 계획을 생성합니다.
B. Amazon Data Lifecycle Manager를 사용하여 필요한 자동 스냅샷 정책을 생성합니다.
C. Amazon S3 파일 게이트웨이를 사용하여 백업을 생성합니다. 적절한 S3 수명주기 관리를
구성합니다.
D. AWS Backup을 사용하여 규정 준수 모드에서 볼트 잠금이 있는 백업 볼트를 생성합니다.
필요한 백업 계획을 생성합니다.
Answer: D
Explanation:
AWS Backup is a fully managed service that allows you to centralize and automate data
protection of AWS services across compute, storage, and database. AWS Backup Vault Lock
is an optional feature of a backup vault that can help you enhance the security and control
over your backup vaults. When a lock is active in Compliance mode and the grace time is
over, the vault configuration cannot be altered or deleted by a customer, account/data owner,
485

IT Certification Guaranteed, The Easy Way!
or AWS. This ensures that your backups are available for you until they reach the expiration
of their retention periods and meet the regulatory requirements. References:
https://docs.aws.
amazon.com/aws-backup/latest/devguide/vault-lock.html
QUESTION NO: 708
회사에서 AWS에 새로운 공개 웹 애플리케이션을 배포하고 있습니다. 애플리케이션은
ALB(Application Load Balancer) 뒤에서 실행됩니다. 애플리케이션은 외부 CA(인증 기관)에서
발급한 SSL/TLS 인증서를 사용하여 에지에서 암호화해야 합니다. 인증서가 만료되기 전에
매년 인증서를 교체해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. AWS Certificate Manager(ACM)를 사용하여 SSL/TLS 인증서를 발급합니다. 인증서를
ALB에 적용합니다.
관리형 갱신 기능을 사용하여 인증서를 자동으로 교체합니다.
B. AWS Certificate Manager(ACM)를 사용하여 SSL/TLS 인증서를 발급합니다. 인증서에서
키 자료를 가져옵니다. 인증서를 ALB에 적용합니다. 관리형 갱신 기능을 사용하여 인증서를
자동으로 교체합니다.
C. AWS Certificate Manager(ACM) 사설 인증 기관을 사용하여 루트 CA에서 SSL/TLS
인증서를 발급합니다. 인증서를 ALB에 적용합니다. 관리형 갱신 기능을 사용하여 인증서를
자동으로 교체합니다.
D. AWS Certificate Manager(ACM)를 사용하여 SSL/TLS 인증서를 가져옵니다. 인증서를
ALB에 적용합니다. Amazon EventBridge(Amazon CloudWatch Events)를 사용하여 인증서가
만료될 때 알림을 보냅니다. 인증서를 수동으로 교체합니다.
Answer: D
Explanation:
https://www.amazonaws.cn/en/certificate-
manager/faqs/#Managed_renewal_and_deployment
QUESTION NO: 709
전자 상거래 회사는 주문 처리 작업을 완료하기 위해 여러 서버리스 기능과 AWS 서비스가
포함된 분산 애플리케이션을 구축하고 있습니다. 이러한 작업에는 워크플로의 일부로 수동
승인이 필요합니다. 솔루션 설계자는 주문 처리 애플리케이션을 위한 아키텍처를 설계해야
합니다. 솔루션은 여러 AWS Lambda 기능을 반응형 서버리스 애플리케이션에 결합할 수
있어야 합니다. 또한 솔루션은 다음에서 실행되는 데이터와 서비스를 조율해야 합니다.
Amazon EC2 인스턴스, 컨테이너 또는 온프레미스 서버 어떤 솔루션이 최소한의 운영
오버헤드로 이러한 요구 사항을 충족합니까?
A. AWS Step Functions를 사용하여 애플리케이션을 구축합니다.
B. AWS Glue 작업에 모든 애플리케이션 구성 요소를 통합합니다.
C. Amazon Simple Queue Service(Amazon SQS)를 사용하여 애플리케이션 구축
D. AWS Lambda 함수 및 Amazon EventBridge(Amazon CloudWatch Events) 이벤트를
사용하여 애플리케이션 구축
Answer: A
Explanation:
AWS Step Functions is a fully managed service that makes it easy to build applications by
coordinating the components of distributed applications and microservices using visual
486

IT Certification Guaranteed, The Easy Way!
workflows. With Step Functions, you can combine multiple AWS Lambda functions into
responsive serverless applications and orchestrate data and services that run on Amazon
EC2 instances, containers, or on-premises servers. Step Functions also allows for manual
approvals as part of the workflow. This solution meets all the requirements with the least
operational overhead.
https://aws.amazon.com/step-
functions/#:~:text=AWS%20Step%20Functions%20is%20a,machine%
20learning%20(ML)%20pipelines.
QUESTION NO: 710
한 회사에서 테스트 환경의 애플리케이션에 AWS CloudFormatlon 스택을 사용하려고 합니다.
회사는 공개 액세스를 차단하는 Amazon S3 버킷에 CloudFormation 템플릿을 저장합니다.
회사는 테스트 환경을 생성하기 위한 특정 사용자 요청을 기반으로 S3 버킷의 템플릿에
CloudFormation 액세스 권한을 부여하려고 합니다. 솔루션은 보안 모범 사례를 따라야
합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon S3용 게이트웨이 VPC 엔드포인트를 생성합니다. S3 객체 URL을 사용하도록
CloudFormation 스택 구성
B. S3 버킷을 대상으로 하는 Amazon API Gateway REST API를 생성합니다. API 게이트웨이
URL을 사용하도록 CloudFormat10n 스택 구성 _
C. 템플릿 객체에 대해 미리 서명된 URL을 생성합니다. 미리 서명된 URL을 사용하도록
CloudFormation 스택을 구성합니다.
D. S3 버킷의 템플릿 객체에 대한 공개 액세스를 허용합니다. 테스트 환경 생성 후 공개 접근
차단
Answer: C
Explanation:
it allows CloudFormation to access the template in the S3 bucket without granting public
access or creating additional resources. A presigned URL is a URL that is signed with the
access key of an IAM user or role that has permission to access the object. The presigned
URL can be used by anyone who receives it, but it expires after a specified time. By creating
a presigned URL for the template object and configuring the CloudFormation stack to use it,
the company can grant CloudFormation access to the template based on specific user
requests and follow security best practices. References:
* Using Amazon S3 Presigned URLs
* Using Amazon S3 Buckets
QUESTION NO: 711
전자 상거래 회사는 AWS 클라우드에 테라바이트 규모의 고객 데이터를 저장합니다.
데이터에는 개인 식별 정보(Pll)가 포함되어 있습니다. 회사는 세 가지 애플리케이션에서
데이터를 사용하려고 합니다. 애플리케이션 중 하나만 Pll을 처리해야 합니다. 다른 두 응용
프로그램이 데이터를 처리하기 전에 Pll을 제거해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon DynamoDB 테이블에 데이터를 저장합니다. 각 애플리케이션이 요청하는
데이터를 가로채서 처리하는 프록시 애플리케이션 계층을 만듭니다.
B. Amazon S3 버킷에 데이터를 저장합니다. 요청 애플리케이션에 데이터를 반환하기 전에
487

IT Certification Guaranteed, The Easy Way!
S3 객체 Lambda를 사용하여 데이터를 처리하고 변환합니다.
C. 데이터를 처리하고 변환된 데이터를 3개의 개별 Amazon S3 버킷에 저장하여 각
애플리케이션이 고유한 사용자 지정 데이터 세트를 갖게 합니다. 각 애플리케이션이 해당 S3
버킷을 가리키도록 합니다.
D. 데이터를 처리하고 변환된 데이터를 3개의 별도 Amazon DynamoDB 테이블에 저장하여
각 애플리케이션이 고유한 사용자 지정 데이터 세트를 갖게 합니다. 각 애플리케이션이 해당
DynamoDB 테이블을 가리키도록 합니다.
Answer: B
Explanation:
https://aws.amazon.com/blogs/aws/introducing-amazon-s3-object-lambda-use-your-code-to-
process-data-as-it- is-being-retrieved-from-s3/ S3 Object Lambda is a new feature of Amazon
S3 that enables customers to add their own code to process data retrieved from S3 before
returning it to the application. By using S3 Object Lambda, the data can be processed and
transformed in real-time, without the need to store multiple copies of the data in separate S3
buckets or DynamoDB tables.
In this case, the Pll can be removed from the data by the code added to S3 Object Lambda
before returning the data to the two applications that do not need to process Pll. The one
application that requires Pll can be pointed to the original S3 bucket where the Pll is still
stored.
Using S3 Object Lambda is the simplest and most cost-effective solution, as it eliminates the
need to maintain multiple copies of the same data in different buckets or tables, which can
result in additional storage costs and operational overhead.
QUESTION NO: 712
한 회사에 전 세계 학생들에게 주문형 교육 비디오를 제공하는 애플리케이션이 있습니다.
또한 이 애플리케이션을 사용하면 승인된 콘텐츠 개발자가 비디오를 업로드할 수 있습니다.
데이터는 us-east-2 리전의 Amazon S3 버킷에 저장됩니다.
회사는 eu-west-2 리전에 S3 버킷을, ap-southeast-1 리전에 S3 버킷을 생성했습니다. 회사는
데이터를 새로운 S3 버킷에 복제하려고 합니다. 회사는 eu-west-2 및 ap-southeast-1
근처에서 비디오를 업로드하는 개발자와 비디오를 스트리밍하는 학생의 대기 시간을
최소화해야 합니다.
애플리케이션을 가장 적게 변경하여 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까?
(2개를 선택하세요.)
A. us-east-2 S3 버킷에서 eu-west-2 S3 버킷으로의 단방향 복제를 구성합니다. us-east-2 S3
버킷에서 ap-southeast-1 S3 버킷으로의 단방향 복제를 구성합니다.
B. us-east-2 S3 버킷에서 eu-west-2 S3 버킷으로의 단방향 복제를 구성합니다. eu-west-2 S3
버킷에서 ap-southeast-1 S3 버킷으로의 단방향 복제를 구성합니다.
C. 세 지역 모두에 있는 S3 버킷 간에 양방향(양방향) 복제를 구성합니다.
D. S3 다중 지역 액세스 포인트를 생성합니다. 비디오 스트리밍을 위해 다중 지역 액세스
포인트의 Amazon 리소스 이름(ARN)을 사용하도록 애플리케이션을 수정합니다. 비디오
업로드용 애플리케이션을 수정하지 마십시오.
E. S3 다중 지역 액세스 포인트 생성 비디오 스트리밍 및 업로드에 다중 지역 액세스 포인트의
Amazon 리소스 이름(ARN)을 사용하도록 애플리케이션을 수정합니다.
Answer: A E
Explanation:
488

IT Certification Guaranteed, The Easy Way!
These two steps will meet the requirements with the fewest changes to the application
because they will enable the company to replicate the data to the new S3 buckets and
minimize latency for both video streaming and uploads. One-way replication from the us-
east-2 S3 bucket to the other two S3 buckets will ensure that the data is synchronized across
all three regions. The company can use S3 Cross-Region Replication (CRR) to automatically
copy objects across buckets in different AWS Regions. CRR can help the company achieve
lower latency and compliance requirements by keeping copies of their data in different
regions. Creating an S3 Multi-Region Access Point and modifying the application to use its
ARN will allow the company to access the data through a single global endpoint. An S3 Multi-
Region Access Point is a globally unique name that can be used to access objects stored in
S3 buckets across multiple regions. It automatically routes requests to the closest S3 bucket
with the lowest latency. By using an S3 Multi-Region Access Point, the company can simplify
the application architecture and improve the performance and reliability of the application.
References:
* Replicating objects
* Multi-Region Access Points in Amazon S3
QUESTION NO: 713
솔루션 아키텍트는 Amazon S3 버킷의 파일을 Amazon Elastic File System(Amazon EFS)
파일 시스템과 다른 S3 버킷으로 복사해야 합니다. 파일은 계속해서 복사되어야 합니다. 새
파일은 원본 S3 버킷에 지속적으로 추가됩니다. 복사된 파일은 원본 파일이 변경된 경우에만
덮어써야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 대상 S3 버킷과 EFS 파일 시스템 모두에 대한 AWS DataSync 위치를 생성합니다. 대상 S3
버킷 및 EFS 파일 시스템에 대한 작업을 생성합니다. 변경된 데이터만 전송하도록 전송
모드를 설정하세요.
B. AWS Lambda 함수를 생성합니다. 파일 시스템을 함수에 마운트합니다. Amazon S3에서
파일이 생성되고 변경될 때 함수를 호출하도록 S3 이벤트 알림을 설정합니다. 파일 시스템과
대상 S3 버킷에 파일을 복사하는 기능을 구성합니다.
C. 대상 S3 버킷과 EFS 파일 시스템 모두에 대한 AWS DataSync 위치를 생성합니다. 대상 S3
버킷 및 EFS 파일 시스템에 대한 작업을 생성합니다. 모든 데이터를 전송하려면 전송 모드를
설정하세요.
D. 파일 시스템과 동일한 VPC에서 Amazon EC2 인스턴스를 시작합니다. 파일 시스템을
마운트합니다. 원본 S3 버킷에서 변경된 모든 객체를 대상 S3 버킷 및 탑재된 파일 시스템에
정기적으로 동기화하는 스크립트를 만듭니다.
Answer: A
Explanation:
AWS DataSync is a service that makes it easy to move large amounts of data between AWS
storage services and on-premises storage systems. AWS DataSync can copy files from an
S3 bucket to an EFS file system and another S3 bucket continuously, as well as overwrite
only the files that have changed in the source. This solution will meet the requirements with
the least operational overhead, as it does not require any code development or manual
intervention.
References:
* 4 explains how to create AWS DataSync locations for different storage services.
489

IT Certification Guaranteed, The Easy Way!
* 5 describes how to create and configure AWS DataSync tasks for data transfer.
* 6 discusses the different transfer modes that AWS DataSync supports.
QUESTION NO: 714
회사는 웹사이트에서 호스팅하는 설문 조사를 사용하여 고객 만족도를 추적합니다. 설문
조사는 때때로 매시간 수천 명의 고객에게 도달합니다. 설문 조사 결과는 현재 이메일
메시지로 회사에 전송되므로 회사 직원이 수동으로 결과를 검토하고 고객 감정을 평가할 수
있습니다.
이 회사는 고객 설문 조사 프로세스를 자동화하려고 합니다. 설문 조사 결과는 지난 12개월
동안 사용할 수 있어야 합니다.
이러한 요구 사항을 가장 확장 가능한 방식으로 충족할 수 있는 솔루션은 무엇일까요?
A. 설문 조사 결과 데이터를 Amazon Simple Queue Service(Amazon SQS) 대기열에 연결된
Amazon API Gateway 엔드포인트로 보냅니다. SQS 대기열을 폴링하고, 감정 분석을 위해
Amazon Comprehend를 호출하고, 결과를 Amazon DynamoDB 테이블에 저장하는 AWS
Lambda 함수를 만듭니다.
모든 레코드에 대한 TTL을 미래 365일로 설정합니다.
B. Amazon EC2 인스턴스에서 실행되는 API로 설문 조사 결과 데이터를 보냅니다. API를
구성하여 설문 조사 결과를 Amazon DynamoDB 테이블에 새 레코드로 저장하고, 감정 분석을
위해 Amazon Comprehend를 호출하고, 두 번째 DynamoDB 테이블에 결과를 저장합니다.
모든 레코드의 TTL을 다음과 같이 설정합니다.
365일 후의 미래.
C. 설문 조사 결과 데이터를 Amazon S3 버킷에 씁니다. S3 Event Notifications를 사용하여
AWS Lambda 함수를 호출하여 데이터를 읽고 Amazon Rekognition을 호출하여 감정 분석을
수행합니다. 감정 분석 결과를 두 번째 S3 버킷에 저장합니다. 각 버킷에서 S3 Lifecycle
정책을 사용하여 365일 후에 객체를 만료시킵니다.
D. 설문 조사 결과 데이터를 Amazon Simple Queue Service(Amazon SQS) 대기열에 연결된
Amazon API Gateway 엔드포인트로 보냅니다. SQS 대기열을 구성하여 감정 분석을 위해
Amazon Lex를 호출하고 결과를 Amazon DynamoDB 테이블에 저장하는 AWS Lambda
함수를 호출합니다. 모든 레코드의 TTL을 미래 365일로 설정합니다.
Answer: A
Explanation:
This solution is the most scalable and efficient way to handle large volumes of survey data
while automating sentiment analysis:
* API Gateway and SQS: The survey results are sent to API Gateway, which forwards the
data to an SQS queue. SQS can handle large volumes of messages and ensures that
messages are not lost.
* AWS Lambda: Lambda is triggered by polling the SQS queue, where it processes the
survey data.
* Amazon Comprehend: Comprehend is used for sentiment analysis, providing insights into
customer satisfaction.
* DynamoDB with TTL: Results are stored in DynamoDB with a Time to Live (TTL) attribute
set to expire after 365 days, automatically removing old data and reducing storage costs.
* Option B (EC2 API): Running an API on EC2 requires more maintenance and scalability
management compared to API Gateway.
* Option C (S3 and Rekognition): Amazon Rekognition is for image and video analysis, not
490

IT Certification Guaranteed, The Easy Way!
sentiment analysis.
* Option D (Amazon Lex): Amazon Lex is used for building conversational interfaces, not
sentiment analysis.
AWS References:
* Amazon Comprehend for Sentiment Analysis
* Amazon SQS
* DynamoDB TTL
QUESTION NO: 715
한 회사에서 MySQL 데이터베이스를 온프레미스에서 AWS로 마이그레이션하려고 합니다. 이
회사는 최근 비즈니스에 큰 영향을 미치는 데이터베이스 중단을 경험했습니다. 이런 일이
다시 발생하지 않도록 회사는 데이터 손실을 최소화하고 모든 트랜잭션을 최소 두 개의
노드에 저장하는 안정적인 AWS 기반 데이터베이스 솔루션을 원합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 3개의 가용 영역에 있는 3개 노드에 대한 동기식 복제를 통해 Amazon RDS DB 인스턴스를
생성합니다.
B. 데이터를 동기식으로 복제할 수 있도록 다중 AZ 기능이 활성화된 Amazon RDS MySQL
DB 인스턴스를 생성합니다.
C. Amazon RDS MySQL DB 인스턴스를 생성한 다음 데이터를 동기식으로 복제하는 별도의
AWS 리전에 읽기 전용 복제본을 생성합니다.
D. AWS Lambda 함수를 트리거하여 데이터를 Amazon RDS MySQL DB 인스턴스에
동기식으로 복제하는 MySQL 엔진이 설치된 Amazon EC2 인스턴스를 생성합니다.
Answer: B
Explanation:
Q: What does Amazon RDS manage on my behalf?
Amazon RDS manages the work involved in setting up a relational database: from
provisioning the infrastructure capacity you request to installing the database software. Once
your database is up and running, Amazon RDS automates common administrative tasks
such as performing backups and patching the software that powers your database. With
optional Multi-AZ deployments, Amazon RDS also manages synchronous data replication
across Availability Zones with automatic failover.
https://aws.amazon.com/rds/faqs/
QUESTION NO: 716
회사는 Amazon S3 Standard 스토리지에 데이터 객체를 저장합니다. 한 솔루션 설계자는
30일 후에는 데이터의 75%에 거의 액세스하지 않는다는 사실을 발견했습니다. 회사는 동일한
고가용성과 복원성을 유지하면서 모든 데이터에 즉시 액세스할 수 있어야 하지만 스토리지
비용을 최소화하려고 합니다.
이러한 요구 사항을 충족하는 스토리지 솔루션은 무엇입니까?
A. 30일 후에 데이터 객체를 S3 Glacier Deep Archive로 이동합니다.
B. 30일 후에 데이터 객체를 S3 Standard-Infrequent Access(S3 Standard-IA)로 이동합니다.
C. 30일 후에 데이터 객체를 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 이동합니다.
D. 데이터 객체를 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 즉시 이동합니다.
Answer: B
Explanation:
491

IT Certification Guaranteed, The Easy Way!
Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days - will
meet the requirements of keeping the data immediately accessible with high availability and
resiliency, while minimizing storage costs. S3 Standard-IA is designed for infrequently
accessed data, and it provides a lower storage cost than S3 Standard, while still offering the
same low latency, high throughput, and high durability as S3 Standard.
QUESTION NO: 717
회사에는 여러 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있습니다. 각 EC2
인스턴스에는 여러 Amazon Elastic Block Store(Amazon EBS) 데이터 볼륨이 연결되어
있습니다. 애플리케이션의 EC2 인스턴스 구성과 데이터를 밤에 백업해야 합니다.
애플리케이션도 복구 가능해야 합니다. 다른 AWS 리전에서 어떤 솔루션이 운영상 가장
효율적인 방식으로 이러한 요구 사항을 충족합니까?
A. 애플리케이션 EBS 볼륨의 야간 스냅샷을 예약하고 해당 스냅샷을 다른 리전에 복사하는
AWS Lambda 함수를 작성합니다.
B. AWS Backup을 사용하여 야간 백업을 수행하여 백업 계획을 만듭니다. 백업을 다른 지역에
복사 애플리케이션의 EC2 인스턴스를 리소스로 추가
C. AWS Backup을 사용하여 야간 백업을 수행하여 백업 계획을 생성합니다. 백업을 다른
리전에 복사합니다. 애플리케이션의 EBS 볼륨을 리소스로 추가합니다.
D. 애플리케이션 EBS 볼륨의 야간 스냅샷을 예약하고 해당 스냅샷을 다른 가용 영역에
복사하는 AWS Lambda 함수를 작성합니다.
Answer: B
Explanation:
The most operationally efficient solution to meet these requirements would be to create a
backup plan by using AWS Backup to perform nightly backups and copying the backups to
another Region. Adding the application's EBS volumes as resources will ensure that the
application's EC2 instance configuration and data are backed up, and copying the backups to
another Region will ensure that the application is recoverable in a different AWS Region.
QUESTION NO: 718
한 회사는 AWS에서 호스팅되는 서비스 솔루션으로 고성능 컴퓨팅(HPC) 워크로드를 구축할
계획입니다. 16개의 AmazonEC2Ltnux 인스턴스 그룹에는 노드 간 통신을 위해 가능한 최저
지연 시간이 필요합니다. 또한 인스턴스에는 고성능 스토리지를 위한 공유 블록 장치 볼륨이
필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 먼지떨이 배치 그룹을 사용합니다. Amazon EBS 다중 연결을 사용하여 단일 프로비저닝된
IOPS SSD Amazon Elastic Block Store(Amazon E BS) 볼륨을 모든 인스턴스에 연결합니다.
B. 클러스터 배치 그룹을 사용합니다. Amazon Elastic File System(Amazon EFS)을 사용하여
인스턴스 전반에 걸쳐 공유 거짓말 시스템을 생성합니다.
C. 파티션 배치 그룹을 사용합니다. Amazon Elastic File System(Amazon EFS)을 사용하여
인스턴스 전체에 공유 타일 시스템을 생성합니다.
D. 스프레드 배치 그룹을 사용합니다. Amazon EBS 다중 연결을 사용하여 단일 프로비저닝된
IOPS SSD Amazon Elastic Block Store(Amazon EBS) 볼륨을 모든 인스턴스에 연결합니다.
Answer: A
Explanation:
1. lowest possible latency + node to node ==> cluster placement(must be within one AZ), so
492

IT Certification Guaranteed, The Easy Way!
C, D out
2. For EBS Multi-Attach, up to 16 instances can be attached to a single volume==>we have
16 linux instance==>more close to A
3. "need a shared block device volume"==>EBS Multi-attach is Block Storage whereas EFS
is File Storage==> B out
4. EFS automatically replicates data within and across 3 AZ==>we use cluster placement so
all EC2 are within one AZ.
5. EBS Multi-attach volumes can be used for clients within a single AZ.
https://repost.aws/questions/QUK2RANw1QTKCwpDUwCCI72A/efs-vs-ebs-mult-attach
QUESTION NO: 719
회사 웹 사이트는 항목 카탈로그에 Amazon EC2 인스턴스 스토어를 사용합니다. 회사는
카탈로그의 가용성이 높고 카탈로그가 내구성 있는 위치에 저장되기를 원합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 카탈로그를 Redis용 Amazon ElastiCache로 이동합니다.
B. 더 큰 인스턴스 스토어로 더 큰 EC2 인스턴스를 배포합니다.
C. 인스턴스 스토어에서 Amazon S3 Glacier Deep Archive로 카탈로그를 이동합니다.
D. 카탈로그를 Amazon Elastic File System(Amazon EFS) 파일 시스템으로 이동합니다.
Answer: D
Explanation:
Moving the catalog to an Amazon Elastic File System (Amazon EFS) file system provides
both high availability and durability. Amazon EFS is a fully-managed, highly-available, and
durable file system that is built to scale on demand. With Amazon EFS, the catalog data can
be stored and accessed from multiple EC2 instances in different availability zones, ensuring
high availability. Also, Amazon EFS automatically stores files redundantly within and across
multiple availability zones, making it a durable storage option.
QUESTION NO: 720
병원은 최근 Amazon API Gateway 및 AWS Lambda와 함께 RESTful API를 배포했습니다.
병원은 API Gateway 및 Lambda를 사용하여 PDF 형식 및 JPEG 형식의 보고서를
업로드합니다. 병원은 Lambda 코드를 수정하여 보호된 건강 정보(PHI)를 식별해야 합니다.
보고서 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 기존 Python 라이브러리를 사용하여 보고서에서 텍스트를 추출하고 추출된 텍스트에서
PHI를 식별합니다.
B. Amazon Textract를 사용하여 보고서에서 텍스트 추출 Amazon SageMaker를 사용하여
추출된 텍스트에서 PHI를 식별합니다.
C. Amazon Textract를 사용하여 보고서에서 텍스트 추출 Amazon Comprehend Medical을
사용하여 추출된 텍스트에서 PHI 식별
D. Amazon Rekognition을 사용하여 보고서에서 텍스트 추출 Amazon Comprehend
Medical을 사용하여 추출된 텍스트에서 PHI 식별
Answer: C
Explanation:
To meet the requirements of the company to have access to both AWS and on-premises file
storage with minimum latency, a hybrid cloud architecture can be used. One solution is to
deploy and configure Amazon FSx for Windows File Server on AWS, which provides fully
493

IT Certification Guaranteed, The Easy Way!
managed Windows file servers. The on-premises file data can be moved to the FSx File
Gateway, which can act as a bridge between on-premises and AWS file storage. The cloud
workloads can be configured to use FSx for Windows File Server on AWS, while the on-
premises workloads can be configured to use the FSx File Gateway. This solution minimizes
operational overhead and requires no significant changes to the existing file access patterns.
The connectivity between on- premises and AWS can be established using an AWS Site-to
-Site VPN connection.
Reference:
AWS FSx for Windows File Server: https://aws.amazon.com/fsx/windows/
AWS FSx File Gateway: https://aws.amazon.com/fsx/file-gateway/
AWS Site-to-Site VPN: https://aws.amazon.com/vpn/site-to-site-vpn/
QUESTION NO: 721
회사에는 다음 구성 요소를 포함하는 데이터 수집 워크플로가 있습니다.
* 새로운 데이터 전송에 대한 알림을 받는 Amazon Simple Notation Service(Amazon SNS)
주제
* 데이터를 처리하고 저장하는 AWS Lambda 함수
네트워크 연결 문제로 인해 수집 워크플로가 실패하는 경우가 있습니다. 보유 기간이
발생하면 회사에서 작업을 수동으로 다시 실행하지 않는 한 해당 데이터가 수집되지
않습니다. 모든 알림이 최종적으로 처리되도록 하려면 솔루션 설계자는 무엇을 해야 합니까?
A. Lambda 함수 구성(또는 여러 가용 영역에 걸쳐 배포)
B. (기능에 대한 CPU 및 메모리 할당을 늘리도록 Lambda 함수 구성을 수정합니다.
C. 재시도 횟수와 재시도 간 대기 시간을 모두 늘리도록 SNS 주제의 재시도 전략을
구성합니다.
D. Amazon Simple Queue Service(Amazon SQS) 대기열을 실패 시 대상으로 구성합니다.
대기열의 메시지를 처리하도록 Lambda 함수를 수정합니다.
Answer: D
Explanation:
https://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html
QUESTION NO: 722
회사는 Docker 컨테이너로 애플리케이션을 구축했으며 AWS 클라우드에서 애플리케이션을
실행해야 합니다. 회사는 관리형 센/아이스를 사용하여 애플리케이션을 호스팅하려고 합니다.
솔루션은 개별 컨테이너 서비스의 수요에 따라 적절하게 확장 및 축소해야 합니다. 솔루션
또한 관리를 위한 추가 운영 오버헤드나 인프라가 발생해서는 안 됩니다. 어떤 솔루션이
이러한 요구 사항을 충족합니까? (2개 선택)
A. AWS Fargate와 함께 Amazon Elastic Container Service(Amazon ECS)를 사용합니다.
B. AWS Fargate와 함께 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용합니다.
C. Amazon API Gateway API 프로비저닝 API를 AWS Lambda에 연결하여 컨테이너를
실행합니다.
D. Amazon EC2 작업자 노드와 함께 Amazon Elastic Container Service(Amazon ECS)를
사용합니다.
E. Amazon EC2 작업자 노드와 함께 Amazon Elastic Kubernetes Service(Amazon EKS)를
사용합니다.
494

IT Certification Guaranteed, The Easy Way!
Answer: A B
Explanation:
These options are the best solutions because they allow the company to run the application
with Docker containers in the AWS Cloud using a managed service that scales automatically
and does not require any infrastructure to manage. By using AWS Fargate, the company can
launch and run containers without having to provision, configure, or scale clusters of EC2
instances. Fargate allocates the right amount of compute resources for each container and
scales them up or down as needed. By using Amazon ECS or Amazon EKS, the company
can choose the container orchestration platform that suits its needs. Amazon ECS is a fully
managed service that integrates with other AWS services and simplifies the deployment and
management of containers. Amazon EKS is a managed service that runs Kubernetes on
AWS and provides compatibility with existing Kubernetes tools and plugins.
C: Provision an Amazon API Gateway API Connect the API to AWS Lambda to run the
containers. This option is not feasible because AWS Lambda does not support running
Docker containers directly. Lambda functions are executed in a sandboxed environment that
is isolated from other functions and resources. To run Docker containers on Lambda, the
company would need to use a custom runtime or a wrapper library that emulates the Docker
API, which can introduce additional complexity and overhead.
D: Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes.
This option is not optimal because it requires the company to manage the EC2 instances that
host the containers. The company would need to provision, configure, scale, patch, and
monitor the EC2 instances, which can increase the operational overhead and infrastructure
costs.
E: Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes.
This option is not ideal because it requires the company to manage the EC2 instances that
host the containers. The company would need to provision, configure, scale, patch, and
monitor the EC2 instances, which can increase the operational overhead and infrastructure
costs.
References:
* 1 AWS Fargate - Amazon Web Services
* 2 Amazon Elastic Container Service - Amazon Web Services
* 3 Amazon Elastic Kubernetes Service - Amazon Web Services
* 4 AWS Lambda FAQs - Amazon Web Services
QUESTION NO: 723
회사는 Amazon EC2 인스턴스에서 Amazon S3 버킷으로 데이터를 이동해야 합니다. 회사는
공개 인터넷 경로를 통해 API 호출과 데이터가 라우팅되지 않도록 해야 합니다. EC2
인스턴스만 S3 버킷에 데이터를 업로드하는 데 액세스할 수 있습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. EC2 인스턴스가 있는 서브넷에 Amazon S3용 인터페이스 VPC 엔드포인트를 생성합니다.
EC2 인스턴스의 IAM 역할에만 액세스할 수 있도록 S3 버킷에 리소스 정책을 연결합니다.
B. EC2 인스턴스가 있는 가용 영역에 Amazon S3용 게이트웨이 VPC 엔드포인트를
생성합니다. 엔드포인트에 적절한 보안 그룹을 연결합니다. EC2 인스턴스의 IAM 역할에만
액세스할 수 있도록 S3 버킷에 리소스 정책을 연결합니다.
C. EC2 인스턴스 내부에서 nslookup 도구를 실행하여 S3 버킷 서비스 API 엔드포인트의
495

IT Certification Guaranteed, The Easy Way!
프라이빗 IP 주소를 얻습니다. EC2 인스턴스에 S3 버킷에 대한 액세스 권한을 제공하려면
VPC 라우팅 테이블에 경로를 생성하세요. EC2 인스턴스의 IAM 역할에만 액세스할 수 있도록
S3 버킷에 리소스 정책을 연결합니다.
D. AWS에서 제공하고 공개적으로 사용 가능한 ip-ranges.json 타일을 사용하여 S3 버킷
서비스 API 엔드포인트의 프라이빗 IP 주소를 얻습니다. EC2 인스턴스에 S3 버킷에 대한
액세스 권한을 제공하려면 VPC 라우팅 테이블에 경로를 생성하세요. EC2 인스턴스의 IAM
역할에만 액세스할 수 있도록 S3 버킷에 리소스 정책을 연결합니다.
Answer: A
Explanation:
(https://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-
specific-iam-role/)
QUESTION NO: 724
회사는 Amazon S3 버킷에 무단 구성 변경이 없는지 확인하기 위해 AWS 클라우드 배포를
검토해야 합니다.
이 목표를 달성하려면 솔루션 아키텍트가 무엇을 해야 합니까?
A. 적절한 규칙을 사용하여 AWS Config를 활성화합니다.
B. 적절한 점검을 통해 AWS Trusted Advisor를 활성화합니다.
C. 적절한 평가 템플릿을 사용하여 Amazon Inspector를 활성화합니다.
D. Amazon S3 서버 액세스 로깅을 켭니다. Amazon EventBridge(Amazon Cloud Watch
이벤트)를 구성합니다.
Answer: A
Explanation:
To ensure that Amazon S3 buckets do not have unauthorized configuration changes, a
solutions architect should turn on AWS Config with the appropriate rules. AWS Config is a
service that allows users to audit and assess their AWS resource configurations for
compliance with industry standards and internal policies. It provides a detailed view of the
resources and their configurations, including information on how the resources are related to
each other. By turning on AWS Config with the appropriate rules, users can identify and
remediate unauthorized configuration changes to their Amazon S3 buckets.
QUESTION NO: 725
회사는 AWS 클라우드에서 3티어 웹 애플리케이션을 호스팅합니다. MySQL용 다중 AZ
Amazon RDS 서버는 데이터베이스 계층을 형성합니다. Amazon ElastiCache는 캐시 계층을
형성합니다. 회사는 고객이 데이터베이스에 항목을 추가할 때 캐시에 데이터를 추가하거나
업데이트하는 캐싱 전략을 원합니다. 캐시의 데이터는 항상 데이터베이스의 데이터와
일치해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 지연 로딩 캐싱 전략 구현
B. 연속 쓰기 캐싱 전략을 구현합니다.
C. TTL 캐싱 전략 추가를 구현합니다.
D. AWS AppConfig 캐싱 전략을 구현합니다.
Answer: B
Explanation:
496

IT Certification Guaranteed, The Easy Way!
A write-through caching strategy adds or updates data in the cache whenever data is written
to the database.
This ensures that the data in the cache is always consistent with the data in the database. A
write-through caching strategy also reduces the cache miss penalty, as data is always
available in the cache when it is requested. However, a write-through caching strategy can
increase the write latency, as data has to be written to both the cache and the database. A
write-through caching strategy is suitable for applications that require high data consistency
and low read latency.
A lazy loading caching strategy only loads data into the cache when it is requested, and
updates the cache when there is a cache miss. This can result in stale data in the cache, as
data is not updated in the cache when it is changed in the database. A lazy loading caching
strategy is suitable for applications that can tolerate some data inconsistency and have a low
cache miss rate.
An adding TTL caching strategy assigns a time-to-live (TTL) value to each data item in the
cache, and removes the data from the cache when the TTL expires. This can help prevent
stale data in the cache, as data is periodically refreshed from the database. However, an
adding TTL caching strategy can also increase the cache miss rate, as data can be evicted
from the cache before it is requested. An adding TTL caching strategy is suitable for
applications that have a high cache hit rate and can tolerate some data inconsistency.
An AWS AppConfig caching strategy is not a valid option, as AWS AppConfig is a service
that enables customers to quickly deploy validated configurations to applications of any size
and scale. AWS AppConfig does not provide a caching layer for web applications.
References: Caching strategies - Amazon ElastiCache, Caching for high-volume workloads
with Amazon ElastiCache
QUESTION NO: 726
회사는 Amazon EC2 인스턴스를 시작하기 위해 AWS 계정에 여러 Amazon 머신
이미지(AMI)를 저장합니다. AMI에는 회사 운영에 필요한 중요한 데이터와 구성이 포함되어
있습니다. 회사는 실수로 삭제된 AMI를 빠르고 효율적으로 복구하는 솔루션을 구현하려고
합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AMI의 Amazon Elastic Block Store(Amazon EBS) 스냅샷을 생성합니다. 스냅샷을 별도의
AWS 계정에 저장합니다.
B. 모든 AMI를 정기적으로 다른 AWS 계정에 복사합니다.
C. 휴지통에 보관 규칙을 만듭니다.
D. 교차 리전 복제 기능이 있는 Amazon S3 버킷에 AMI를 업로드합니다.
Answer: C
Explanation:
Recycle Bin is a data recovery feature that enables you to restore accidentally deleted
Amazon EBS snapshots and EBS-backed AMIs. When using Recycle Bin, if your resources
are deleted, they are retained in the Recycle Bin for a time period that you specify before
being permanently deleted. You can restore a resource from the Recycle Bin at any time
before its retention period expires. This solution has the least operational overhead, as you
do not need to create, copy, or upload any additional resources. You can also manage tags
and permissions for AMIs in the Recycle Bin. AMIs in the Recycle Bin do not incur any
497

IT Certification Guaranteed, The Easy Way!
additional charges. References:
* Recover AMIs from the Recycle Bin
* Recover an accidentally deleted Linux AMI
QUESTION NO: 727
한 회사는 ELB(Elastic Load Balancing) 로드 밸런서 뒤의 Amazon EC2 인스턴스에서 실행될
새로운 웹 서비스를 설계하고 있습니다. 그러나 많은 웹 서비스 클라이언트는 방화벽에서
승인된 IP 주소에만 접근할 수 있습니다.
솔루션 아키텍트는 고객의 요구 사항을 충족하기 위해 무엇을 권장해야 합니까?
A. 탄력적 IP 주소가 연결된 Network Load Balancer입니다.
B. 탄력적 IP 주소가 연결된 Application Load Balancer.
C. 탄력적 IP 주소를 가리키는 Amazon Route 53 호스팅 영역의 A 레코드입니다.
D. 로드 밸런서 앞에서 프록시로 실행되는 퍼블릭 IP 주소가 있는 EC2 인스턴스입니다.
Answer: A
Explanation:
A Network Load Balancer can be assigned one Elastic IP address for each Availability Zone
it uses1. This allows the clients to reach the load balancer using a static IP address that can
be authorized on their firewalls. An Application Load Balancer cannot be assigned an Elastic
IP address2. An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP
address would not work because the load balancer would still use its own IP address as the
source of the forwarded requests to the web service. An EC2 instance with a public IP
address running as a proxy in front of the load balancer would add unnecessary complexity
and cost, and would not provide the same scalability and availability as a Network Load
Balancer. References: 1: Network Load Balancers - Elastic Load Balancing3, IP address type
section2: How to assign Elastic IP to Application Load Balancer in AWS?4, answer section.
QUESTION NO: 728
한 회사가 Amazon EC2 인스턴스 세트를 사용하여 웹사이트를 호스팅합니다. 웹사이트는
Amazon S3 버킷을 사용하여 이미지와 미디어 파일을 저장합니다.
이 회사는 웹사이트 인프라 생성을 자동화하여 여러 AWS 지역에 웹사이트를 배포하고자
합니다. 또한 이 회사는 EC2 인스턴스에 S3 버킷에 대한 액세스를 제공하여 인스턴스가 AWS
Identity and Access Management(1AM)를 사용하여 데이터를 저장하고 액세스할 수 있도록
하려고 합니다.
이러한 요구 사항을 가장 안전하게 충족할 수 있는 솔루션은 무엇입니까?
A. 웹 서버 EC2 인스턴스에 대한 AWS CloudFormation 템플릿을 만듭니다. CloudFormation
템플릿의 AWS::EC2::Instance 엔터티의 UserData 섹션에 1AM 액세스 키를 저장합니다.
B. 1AM 비밀 액세스 키와 액세스 키 ID를 포함하는 파일을 만듭니다. 파일을 새 S3 버킷에
저장합니다. AWS CloudFormation 템플릿을 만듭니다. 템플릿에서 액세스 키와 액세스 키
ID를 포함하는 S3 객체의 위치를 ​​지정하는 매개변수를 만듭니다.
C. 웹 서버 EC2 인스턴스가 S3 버킷에 액세스할 수 있도록 허용하는 1AM 역할과 1AM 액세스
정책을 만듭니다. 웹 서버 EC2 인스턴스에 대한 AWS CloudFormation 템플릿을 만듭니다.
1AM 역할과 1AM 액세스 정책을 참조하는 1AM 인스턴스 프로필 엔터티입니다.
D. 1AM에서 1AM 비밀 액세스 키와 액세스 키 ID를 검색하여 웹 서버 EC2 인스턴스에
저장하는 스크립트를 만듭니다. AWS CloudFormation 템플릿의 AWS::EC2::Instance
엔터티의 UserData 섹션에 스크립트를 포함합니다.
498

IT Certification Guaranteed, The Easy Way!
Answer: C
Explanation:
The most secure solution for allowing EC2 instances to access an S3 bucket is by using IAM
roles. An IAM role can be created with an access policy that grants the required permissions
(e.g., to read and write to the S3 bucket). The IAM role is then associated with the EC2
instances through an IAM instance profile.
By associating the role with the instances, the EC2 instances can securely assume the role
and receive temporary credentials via the instance metadata service. This avoids the need to
store credentials (such as access keys) on the instances or within the application, enhancing
security and reducing the risk of credentials being exposed.
AWS CloudFormation can be used to automate the creation of the entire infrastructure,
including EC2 instances, IAM roles, and associated policies.
AWS References:
* IAM Roles for EC2 Instances outlines the use of IAM roles for secure access to AWS
services.
* AWS CloudFormation User Guide details how to create and manage resources using
CloudFormation templates.
Why the other options are incorrect:
* A. Save IAM access key in UserData: This is insecure because it involves storing long-term
credentials in the instance user data, which can be exposed.
* B. Store access keys in S3: This is also insecure, as it involves managing and distributing
long-term credentials, which should be avoided.
* D. Retrieve access keys via a script: This approach is unnecessarily complex and less
secure than using IAM roles, which provide temporary credentials automatically.
QUESTION NO: 729
한 회사가 애플리케이션을 지원하기 위해 온프레미스에서 Microsoft Windows SMB 파일
공유를 실행합니다. 이 회사는 애플리케이션을 AWS로 마이그레이션하려고 합니다. 이
회사는 여러 Amazon EC2 인스턴스에서 스토리지를 공유하려고 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요? (2개를
선택하세요.)
A. 탄력적 처리량을 갖춘 Amazon Elastic File System(Amazon EFS) 파일 시스템을 만듭니다.
B. Amazon FSx for NetApp ONTAP 파일 시스템을 만듭니다.
C. Amazon Elastic Block Store(Amazon EBS)를 사용하여 인스턴스에서 자체 관리형
Windows 파일 공유를 만듭니다.
D. Amazon FSx for Windows File Server 파일 시스템을 만듭니다.
E. OpenZFS 파일 시스템용 Amazon FSx를 만듭니다.
Answer: A D
Explanation:
* A. Amazon EFS: Provides a scalable, shared file storage solution with minimal operational
overhead.
It's ideal for Linux-based workloads.
* B. Amazon FSx for NetApp ONTAP: More suited for workloads requiring NetApp-specific
features.
* C. Amazon EBS: Requires manual management of file shares, which increases operational
499

IT Certification Guaranteed, The Easy Way!
overhead.
* D. Amazon FSx for Windows File Server: Best suited for Windows SMB workloads with low
operational overhead.
* E. Amazon FSx for OpenZFS: Better for Linux and Unix-based workloads.
References: Amazon EFS, Amazon FSx
QUESTION NO: 730
한 회사가 회사의 온프레미스 데이터 센터에서 MySQL DB 인스턴스용 Amazon RDS로
MySQL 데이터베이스를 마이그레이션했습니다. 회사는 회사의 평균 일일 워크로드에 맞게
RDS DB 인스턴스의 크기를 조정했습니다. 한 달에 한 번 회사에서 보고서에 대한 쿼리를
실행할 때 데이터베이스 성능이 느려집니다. 회사는 보고서를 실행하고 일일 워크로드의
성능을 유지 관리할 수 있는 기능을 원합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 데이터베이스의 읽기 전용 복제본을 생성합니다. 쿼리를 읽기 전용 복제본으로 보냅니다.
B. 데이터베이스의 백업을 만듭니다. 백업을 다른 DB 인스턴스로 복원합니다. 쿼리를 새
데이터베이스로 보냅니다.
C. 데이터를 Amazon S3로 내보냅니다. Amazon Athena를 사용하여 S3 버킷을 쿼리합니다.
D. 추가 워크로드를 수용하도록 DB 인스턴스의 크기를 조정합니다.
Answer: C
Explanation:
Amazon Athena is a service that allows you to run SQL queries on data stored in Amazon
S3. It is serverless, meaning you do not need to provision or manage any infrastructure. You
only pay for the queries you run and the amount of data scanned1.
By using Amazon Athena to query your data in Amazon S3, you can achieve the following
benefits:
* You can run queries for your report without affecting the performance of your Amazon RDS
for MySQL DB instance. You can export your data from your DB instance to an S3 bucket
and use Athena to query the data in the bucket. This way, you can avoid the overhead and
contention of running queries on your DB instance.
* You can reduce the cost and complexity of running queries for your report. You do not need
to create a read replica or a backup of your DB instance, which would incur additional
charges and require maintenance. You also do not need to resize your DB instance to
accommodate the additional workload, which would increase your operational overhead.
* You can leverage the scalability and flexibility of Amazon S3 and Athena. You can store
large amounts of data in S3 and query them with Athena without worrying about capacity or
performance limitations. You can also use different formats, compression methods, and
partitioning schemes to optimize your data storage and query performance1.
QUESTION NO: 731
회사의 개발자는 최신 버전의 Amazon Linux를 실행하는 회사의 Amazon EC2 인스턴스에
대해 SSH 액세스를 얻을 수 있는 안전한 방법을 원합니다. 개발자는 원격으로 회사
사무실에서 작업합니다.
회사는 AWS 서비스를 솔루션의 일부로 사용하려고 합니다. EC2 인스턴스는 VPC 프라이빗
서브넷에서 호스팅되며 퍼블릭 서브넷에 배포된 NAT 게이트웨이를 통해 인터넷에
액세스합니다.
500

IT Certification Guaranteed, The Easy Way!
이러한 요구 사항을 가장 비용 효율적으로 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. EC2 인스턴스와 동일한 서브넷에 배스천 호스트를 생성합니다. 개발자에게 ec2:
CreateVpnConnection 1AM 권한을 부여합니다. 개발자가 EC2 인스턴스에 연결할 수 있도록
EC2 Instance Connect를 설치합니다.
B. 기업 네트워크와 VPC 간에 AWS Site-to-Site VPN 연결을 생성합니다. 개발자가 회사
네트워크에 있을 때 Site-to-Site VPN 연결을 사용하여 EC2 인스턴스에 액세스하도록
개발자에게 지시하십시오. 개발자에게 원격으로 작업할 때 액세스를 위해 다른 VPN 연결을
설정하도록 지시하십시오.
C. VPC의 퍼블릭 서브넷에 배스천 호스트를 생성합니다. 개발자의 회사 및 원격 네트워크의
연결 및 SSH 인증만 허용하도록 배스천 호스트의 보안 그룹 및 SSH 키를 구성합니다.
개발자에게 SSH를 사용하여 배스천 호스트를 통해 연결하여 EC2 인스턴스에 연결하도록
지시합니다.
D. AmazonSSMManagedlnstanceCore 1AM 정책을 EC2 인스턴스와 연결된 1AM 역할에
연결합니다. 개발자에게 AWS Systems Manager Session Manager를 사용하여 EC2
인스턴스에 액세스하도록 지시합니다.
Answer: D
Explanation:
AWS Systems Manager Session Manager is a service that enables you to securely connect
to your EC2 instances without using SSH keys or bastion hosts. You can use Session
Manager to access your instances through the AWS Management Console, the AWS CLI, or
the AWS SDKs. Session Manager uses IAM policies and roles to control who can access
which instances. By attaching the AmazonSSMManagedlnstanceCore IAM policy to an IAM
role that is associated with the EC2 instances, you grant the Session Manager service the
necessary permissions to perform actions on your instances. You also need to attach another
IAM policy to the developers' IAM users or roles that allows them to start sessions to the
instances. Session Manager uses the AWS Systems Manager Agent (SSM Agent) that is
installed by default on Amazon Linux 2 and other supported Linux distributions. Session
Manager also encrypts all session data between your client and your instances, and streams
session logs to Amazon S3, Amazon CloudWatch Logs, or both for auditing purposes. This
solution is the most cost-effective, as it does not require any additional resources or services,
such as bastion hosts, VPN connections, or NAT gateways. It also simplifies the security and
management of SSH access, as it eliminates the need for SSH keys, port opening, or firewall
rules. References:
* What is AWS Systems Manager?
* Setting up Session Manager
* Getting started with Session Manager
* Controlling access to Session Manager
* Logging Session Manager activity
QUESTION NO: 732
한 회사에는 자동차의 IoT 센서에서 데이터를 수집하는 애플리케이션이 있습니다. 데이터는
Amazon Kinesis Date Firehose를 통해 Amazon S3에 스트리밍 및 저장됩니다. 데이터는 매년
수조 개의 S3 객체를 생성합니다. 매일 아침 회사는 이전 30일 동안의 데이터를 사용하여
일련의 기계 학습(ML) 모델을 재교육합니다.
회사에서는 매년 4회 이전 12개월의 데이터를 사용하여 분석을 수행하고 다른 ML 모델을
501

IT Certification Guaranteed, The Easy Way!
교육합니다. 데이터는 최대 1년 동안 최소한의 지연으로 제공되어야 합니다. 1년이 지나면
데이터는 보관 목적으로 보관되어야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 스토리지 솔루션은 무엇입니까?
A. S3 Intelligent-Tiering 스토리지 클래스를 사용합니다. 1년 후 객체를 S3 Glacier Deep
Archive로 전환하는 S3 수명 주기 정책을 생성합니다.
B. S3 Intelligent-Tiering 스토리지 클래스를 사용합니다. 1년 후에 객체를 S3 Glacier Deep
Archive로 자동 이동하도록 S3 Intelligent-Tiering을 구성합니다.
C. S3 Standard-Infrequent Access(S3 Standard-IA) 스토리지 클래스를 사용합니다. 1년 후에
객체를 S3 Glacier Deep Archive로 전환하는 S3 수명 주기 정책을 생성합니다.
D. S3 Standard 스토리지 클래스를 사용합니다. 30일 후에 객체를 S3 Standard-Infrequent
Access(S3 Standard-IA)로 전환하고, 1년 후에 S3 Glacier Deep Archive로 전환하는 S3 수명
주기 정책을 생성합니다.
Answer: D
Explanation:
- First 30 days- data access every morning ( predictable and frequently) - S3 standard - After
30 days, accessed 4 times a year - S3 infrequently access - Data preserved- S3 Gllacier De
ep Archive
QUESTION NO: 733
회사에서 새로운 AWS 계정을 개설했습니다. 계정이 새로 프로비저닝되었으며 기본 설정이
변경되지 않았습니다. 회사는 AWS 계정 루트 사용자의 보안을 우려하고 있습니다.
루트 사용자를 보호하려면 어떻게 해야 합니까?
A. 일상적인 관리 작업을 위한 1AM 사용자를 만듭니다. 루트 사용자를 비활성화합니다.
B. 일상적인 관리 작업을 위한 오전 1시 사용자를 만듭니다. 루트 사용자에 대해 다단계
인증을 활성화합니다.
C. 루트 사용자를 위한 액세스 키 생성 AWS Management Console 대신 일일 관리 작업에
액세스 키를 사용합니다.
D. 최상위 솔루션 설계자에게 루트 사용자 자격 증명을 제공합니다. 솔루션 설계자가
일상적인 관리 작업에 루트 사용자를 사용하도록 하세요.
Answer: B
Explanation:
This answer is the most secure and recommended option for securing the root user of a new
AWS account.
The root user is the identity that has complete access to all AWS services and resources in
the account. It is accessed by signing in with the email address and password that were used
to create the account. To protect the root user credentials from unauthorized use, AWS
advises the following best practices:
* Create IAM users for daily administrative tasks. IAM users are identities that you create in
your account that have specific permissions to access AWS resources. You can create
individual IAM users for yourself and for others who need access to your account. You can
also assign IAM users to IAM groups that have a set of policies that grant permissions to
perform common tasks. By using IAM users instead of the root user, you can follow the
principle of least privilege and reduce the risk of compromising your account.
* Enable multi-factor authentication (MFA) on the root user. MFA is a security feature that
502

IT Certification Guaranteed, The Easy Way!
requires users to prove their identity by providing two pieces of information: their password
and a code from a device that only they have access to. By enabling MFA on the root user,
you can add an extra layer of protection to your account and prevent unauthorized access
even if your password is compromised.
* Limit the tasks you perform with the root user account. You should use the root user only for
tasks that require root user credentials, such as changing your account settings, closing your
account, or managing consolidated billing. For a complete list of tasks that require root user
credentials, see Tasks that require root user credentials. For all other tasks, you should use
IAM users or roles that have the appropriate permissions.
References:
* AWS account root user
* Root user best practices for your AWS account
* Tasks that require root user credentials
QUESTION NO: 734
이미지 호스팅 회사는 대규모 자산을 Amazon S3 Standard 버킷에 업로드합니다. 회사는 S3
API를 사용하여 멀티파트 업로드를 병렬로 사용하고 동일한 객체가 다시 업로드되면
덮어씁니다. 업로드 후 처음 30일 동안 객체에 자주 액세스됩니다. 30일 후에는 객체의 사용
빈도가 줄어들지만 각 객체에 대한 액세스 패턴은 일관되지 않습니다. 회사는 저장된 자산의
고가용성과 복원성을 유지하면서 S3 스토리지 비용을 최적화해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 작업 조합을 권장해야 합니까?
(2개를 선택하세요.)
A. 30일 후에 자산을 S3 Intelligent-Tiering으로 이동합니다.
B. 불완전한 멀티파트 업로드를 정리하도록 S3 수명 주기 정책을 구성합니다.
C. 만료된 객체 삭제 마커를 정리하도록 S3 수명 주기 정책을 구성합니다.
D. 30일 후에 자산을 S3 Standard-Infrequent Access(S3 Standard-IA)로 이동합니다.
E. 30일 후에 자산을 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 이동합니다.
Answer: A B
Explanation:
S3 Intelligent-Tiering is a storage class that automatically moves data to the most cost-
effective access tier based on access frequency, without performance impact, retrieval fees,
or operational overhead1. It is ideal for data with unknown or changing access patterns, such
as the company's assets. By moving assets to S3 Intelligent-Tiering after 30 days, the
company can optimize its storage costs while maintaining high availability and resilience of
stored assets.
S3 Lifecycle is a feature that enables you to manage your objects so that they are stored cost
effectively throughout their lifecycle2. You can create lifecycle rules to define actions that
Amazon S3 applies to a group of objects. One of the actions is to abort incomplete multipart
uploads that can occur when an upload is interrupted. By configuring an S3 Lifecycle policy
to clean up incomplete multipart uploads, the company can reduce its storage costs and
avoid paying for parts that are not used.
Option C is incorrect because expired object delete markers are automatically deleted by
Amazon S3 and do not incur any storage costs3. Therefore, configuring an S3 Lifecycle
policy to clean up expired object delete markers will not have any effect on the company's
storage costs.
503

IT Certification Guaranteed, The Easy Way!
Option D is incorrect because S3 Standard-IA is a storage class for data that is accessed
less frequently, but requires rapid access when needed1. It has a lower storage cost than S3
Standard, but it has a higher retrieval cost and a minimum storage duration charge of 30
days. Therefore, moving assets to S3 Standard-IA after 30 days may not optimize the
company's storage costs if the assets are still accessed occasionally.
Option E is incorrect because S3 One Zone-IA is a storage class for data that is accessed
less frequently, but requires rapid access when needed1. It has a lower storage cost than S3
Standard-IA, but it stores data in only one Availability Zone and has less resilience than other
storage classes. It also has a higher retrieval cost and a minimum storage duration charge of
30 days. Therefore, moving assets to S3 One Zone-IA after 30 days may not optimize the
company's storage costs if the assets are still accessed occasionally or require high
availability. Reference URL: 1:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.
html 2: https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html
3: https://docs.
aws.amazon.com/AmazonS3/latest/userguide/delete-or-empty-bucket.html#delete-bucket-
considerations :
https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html :
https://aws.amazon.com
/certification/certified-solutions-architect-associate/
QUESTION NO: 735
회사는 SaaS(Software as a Service) 애플리케이션 Salesforce 계정과 Amazon S3 간에
데이터를 안전하게 교환하려고 합니다. 회사는 AWS Key Management Service(AWS KMS)
고객 관리형 키(CMK)를 사용하여 저장 데이터를 암호화해야 합니다. 회사는 전송 중인
데이터도 암호화해야 합니다. 회사는 Salesforce 계정에 대한 API 액세스를 활성화했습니다.
최소한의 개발 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Salesforce에서 Amazon S3로 데이터를 안전하게 전송하는 AWS Lambda 함수를
생성합니다.
B. AWS Step Functions 워크플로 생성 Salesforce에서 Amazon S3로 데이터를 안전하게
전송하는 작업을 정의합니다.
C. Amazon AppFlow 흐름을 생성하여 Salesforce에서 Amazon S3로 데이터를 안전하게
전송합니다.
D. Salesforce용 사용자 정의 커넥터를 생성하여 Salesforce에서 Amazon S3로 데이터를
안전하게 전송합니다.
Answer: C
Explanation:
Amazon AppFlow is a fully managed integration service that enables users to transfer data
securely between SaaS applications and AWS services. It supports Salesforce as a source
and Amazon S3 as a destination. It also supports encryption of data at rest using AWS KMS
CMKs and encryption of data in transit using SSL
/TLS1. By using Amazon AppFlow, the solution can meet the requirements with the least
development effort.
A: Create AWS Lambda functions to transfer the data securely from Salesforce to Amazon
S3. This solution will not meet the requirement of the least development effort, as it involves
504

IT Certification Guaranteed, The Easy Way!
writing custom code to interact with Salesforce and Amazon S3 APIs, handle authentication,
encryption, error handling, and monitoring2.
B: Create an AWS Step Functions workflow Define the task to transfer the data securely from
Salesforce to Amazon S3. This solution will not meet the requirement of the least
development effort, as it involves creating a state machine definition to orchestrate the data
transfer task, and invoking Lambda functions or other services to perform the actual data
transfer3.
D: Create a custom connector for Salesforce to transfer the data securely from Salesforce to
Ama-zon S3. This solution will not meet the requirement of the least development effort, as it
involves using the Amazon AppFlow Custom Connector SDK to build and deploy a custom
connector for Salesforce, which requires additional configuration and management.
Reference URL: https://aws.amazon.com/appflow/
QUESTION NO: 736
회사에는 Auto Scaling 그룹의 Amazon EC2 인스턴스에서 실행되는 내부 애플리케이션이
있습니다. EC2 인스턴스는 컴퓨팅에 최적화되어 있으며 Amazon Elastic Block Store(Amazon
EBS) 볼륨을 사용합니다.
회사는 EC2 인스턴스, Auto Scaling 그룹 및 EBS 볼륨 전반에 걸쳐 비용 최적화를 식별하려고
합니다.
가장 효율적인 운영 효율성으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 새로운 AWS 비용 및 사용 보고서를 생성합니다. EC2 인스턴스, Auto Scaling 그룹 및 EBS
볼륨에 대한 비용 권장 사항에 대한 보고서를 검색합니다.
B. 새로운 Amazon CloudWatch 결제 알림을 생성합니다. EC2 인스턴스, Auto Scaling 그룹 및
EBS 볼륨에 대한 비용 권장 사항에 대한 경고 상태를 확인하세요.
C. EC2 인스턴스, Auto Scaling 그룹 및 EBS 볼륨에 대한 비용 권장 사항을 위해 AWS
Compute Optimizer를 구성합니다.
D. EC2 인스턴스에 대한 비용 권장 사항을 위해 AWS Compute Optimizer를 구성합니다.
새로운 AWS 비용 및 사용 보고서를 생성합니다. Auto Scaling 그룹 및 EBS 볼륨에 대한 비용
권장 사항에 대한 보고서를 검색합니다.
Answer: C
Explanation:
* Requirement Analysis: The company wants to identify cost optimizations for EC2 instances,
the Auto Scaling group, and EBS volumes with high operational efficiency.
* AWS Compute Optimizer: This service provides actionable recommendations to help
optimize your AWS resources, including EC2 instances, Auto Scaling groups, and EBS
volumes.
* Cost Recommendations: Compute Optimizer analyzes the utilization of resources and
provides specific recommendations for rightsizing or optimizing the configurations.
* Operational Efficiency: Using Compute Optimizer automates the process of identifying cost-
saving opportunities, reducing the need for manual analysis.
* Implementation:
* Enable AWS Compute Optimizer for your AWS account.
* Review the recommendations provided for EC2 instances, Auto Scaling groups, and EBS
volumes.
* Conclusion: This solution provides a comprehensive, automated approach to identifying
505

IT Certification Guaranteed, The Easy Way!
cost optimizations with minimal operational effort.
References
* AWS Compute Optimizer: AWS Compute Optimizer Documentation
QUESTION NO: 737
한 회사가 Application Load Balancer 뒤에 있는 Amazon EC2 인스턴스에서 전자상거래
애플리케이션을 실행합니다. 인스턴스는 여러 가용 영역에 걸쳐 Amazon EC2 Auto Scaling
그룹에서 실행됩니다. Auto Scaling 그룹은 CPU 사용률 지표를 기준으로 확장됩니다.
전자상거래 애플리케이션은 대규모 EC2 인스턴스에서 호스팅되는 MySQL 8.0
데이터베이스에 거래 데이터를 저장합니다.
애플리케이션 로드가 증가하면 데이터베이스 성능이 빠르게 저하됩니다. 애플리케이션은
쓰기 트랜잭션보다 더 많은 읽기 요청을 처리합니다. 회사는 고가용성을 유지하면서 예측할
수 없는 읽기 작업 부하의 수요를 충족하기 위해 데이터베이스를 자동으로 확장하는 솔루션을
원합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 리더 및 컴퓨팅 기능을 위해 단일 노드와 함께 Amazon Redshift를 사용합니다.
B. 단일 AZ 배포와 함께 Amazon RDS를 사용합니다. 다른 가용 영역에 리더 인스턴스를
추가하도록 Amazon RDS를 구성합니다.
C. 다중 AZ 배포와 함께 Amazon Aurora를 사용합니다. Aurora 복제본으로 Aurora Auto
Scaling을 구성합니다.
D. EC2 스팟 인스턴스와 함께 Memcached용 Amazon ElastiCache를 사용합니다.
Answer: C
Explanation:
AURORA is 5x performance improvement over MySQL on RDS and handles more read
requests than write,; maintaining high availability = Multi-AZ deployment
QUESTION NO: 738
한 회사에 Amazon S3 버킷에 수백만 개의 객체가 포함된 서버리스 웹 사이트가 있습니다.
회사는 S3 버킷을 Amazon CloudFront 배포의 오리진으로 사용합니다. 회사는 객체가
로드되기 전에 S3 버킷에 암호화를 설정하지 않았습니다. 솔루션 아키텍트는 모든 기존
객체와 향후 S3 버킷에 추가되는 모든 객체에 대해 암호화를 활성화해야 합니다.
최소한의 노력으로 이러한 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?
A. 새 S3 버킷을 생성합니다. 새 S3 버킷에 대한 기본 암호화 설정을 활성화합니다. 모든 기존
개체를 임시 로컬 저장소에 다운로드합니다. 새 S3 버킷에 객체를 업로드합니다.
B. S3 버킷에 대한 기본 암호화 설정을 켭니다. S3 인벤토리 기능을 사용하여 암호화되지 않은
객체를 나열하는 .csv 파일을 생성합니다. 복사 명령을 사용하여 해당 객체를 암호화하는 S3
배치 작업 작업을 실행합니다.
C. AWS Key Management Service(AWS KMS)를 사용하여 새 암호화 키를 생성합니다. AWS
KMS 관리형 암호화 키(SSE-KMS)를 통해 서버 측 암호화를 사용하도록 S3 버킷의 설정을
변경합니다. S3 버킷에 대한 버전 관리를 활성화합니다.
D. AWS Management Console에서 Amazon S3로 이동합니다. S3 버킷의 객체를 찾아보세요.
암호화 필드를 기준으로 정렬합니다. 암호화되지 않은 각 개체를 선택합니다. 수정 버튼을
사용하여 S3 버킷의 암호화되지 않은 모든 객체에 기본 암호화 설정을 적용합니다.
Answer: B
Explanation:
506

IT Certification Guaranteed, The Easy Way!
https://spin.atomicobject.com/2020/09/15/aws-s3-encrypt-existing-objects/
QUESTION NO: 739
회사에서 Amazon EC2 인스턴스에 새 애플리케이션을 배포하고 있습니다. 애플리케이션은
Amazon Elastic Block Store(Amazon EBS) 볼륨에 데이터를 씁니다. 회사는 EBS 볼륨에
기록된 모든 데이터가 유휴 상태에서 암호화되도록 해야 합니다.
이 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EBS 암호화를 지정하는 1AM 역할을 생성합니다. 역할을 EC2 인스턴스에 연결합니다.
B. EBS 볼륨을 암호화된 볼륨으로 생성합니다. EBS 볼륨을 EC2 인스턴스에 연결
C. 키가 Encrypt이고 값이 True인 EC2 인스턴스 태그를 생성합니다. EBS 수준에서 암호화가
필요한 모든 인스턴스에 태그를 지정합니다.
D. 계정에서 EBS 암호화를 시행하는 AWS Key Management Service(AWS KMS) 키 정책을
생성합니다. 키 정책이 활성 상태인지 확인
Answer: B
Explanation:
The solution that will meet the requirement of ensuring that all data that is written to the EBS
volumes is encrypted at rest is B. Create the EBS volumes as encrypted volumes and attach
the encrypted EBS volumes to the EC2 instances. When you create an EBS volume, you can
specify whether to encrypt the volume. If you choose to encrypt the volume, all data written to
the volume is automatically encrypted at rest using AWS- managed keys. You can also use
customer-managed keys (CMKs) stored in AWS KMS to encrypt and protect your EBS
volumes. You can create encrypted EBS volumes and attach them to EC2 instances to
ensure that all data written to the volumes is encrypted at rest.
QUESTION NO: 740
회사에서는 모바일 앱 사용을 추적하기 위해 보고서를 분석하고 생성하려고 합니다. 이 앱은
인기가 높으며 글로벌 사용자 기반을 보유하고 있습니다. 회사는 맞춤형 보고서 작성
프로그램을 사용하여 애플리케이션 사용을 분석합니다.
프로그램은 매월 마지막 주에 여러 보고서를 생성합니다. 프로그램은 각 보고서를 생성하는
데 10분 미만이 소요됩니다. 회사는 매월 마지막 주 외에는 보고서를 생성하기 위해 이
프로그램을 거의 사용하지 않습니다. 회사는 보고서를 요청할 때 최소한의 시간 내에
보고서를 생성하기를 원합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. Amazon EC2 온디맨드 인스턴스를 사용하여 프로그램을 실행합니다. 보고서가 요청되면
EC2 인스턴스를 시작하는 Amazon EventBridge 규칙을 생성합니다. 매월 마지막 주에 EC2
인스턴스를 지속적으로 실행합니다.
B. AWS Lambda에서 프로그램을 실행합니다. 보고서가 요청될 때 Lambda 함수를 실행하는
Amazon EventBridge 규칙을 생성합니다.
C. Amazon Elastic Container Service(Amazon ECS)에서 프로그램을 실행합니다. 보고서가
요청되면 프로그램을 실행하도록 Amazon ECS를 예약합니다.
D. Amazon EC2 스팟 인스턴스를 사용하여 프로그램을 실행합니다. 보고서가 요청되면 EC2
인스턴스를 시작하는 Amazon EventBridge 규칙을 생성합니다. 매월 마지막 주에 EC2
인스턴스를 지속적으로 실행합니다.
Answer: B
Explanation:
507

IT Certification Guaranteed, The Easy Way!
This solution meets the requirements most cost-effectively because it leverages the
serverless and event- driven capabilities of AWS Lambda and Amazon EventBridge. AWS
Lambda allows you to run code without provisioning or managing servers, and you pay only
for the compute time you consume. Amazon EventBridge is a serverless event bus service
that lets you connect your applications with data from various sources and routes that data to
targets such as AWS Lambda. By using Amazon EventBridge, you can create a rule that
triggers a Lambda function to run the program when reports are requested, and you can also
schedule the rule to run during the last week of each month. This way, you can generate
reports in the least amount of time and pay only for the resources you use.
References:
* AWS Lambda
* Amazon EventBridge
QUESTION NO: 741
한 회사가 최근 데이터 웨어하우스를 AWS로 마이그레이션했습니다. 이 회사는 AWS에 대한
AWS Direct Connect 연결을 가지고 있습니다. 회사 사용자는 시각화 도구를 사용하여 데이터
웨어하우스를 쿼리합니다. 데이터 웨어하우스가 반환하는 쿼리의 평균 크기는 50MB입니다.
시각화 도구가 생성하는 평균 시각화 크기는 500KB입니다. 데이터 웨어하우스가 반환하는
결과 세트는 캐시되지 않습니다.
회사는 데이터 웨어하우스와 회사 간 데이터 전송 비용을 최적화하려고 합니다.
어떤 솔루션이 이 요구 사항을 충족시킬까요?
A. 시각화 도구를 온프레미스에 호스팅합니다. 인터넷을 통해 데이터웨어하우스에 직접
연결합니다.
B. 데이터 웨어하우스와 동일한 AWS 지역에서 시각화 도구를 호스팅합니다. 인터넷을 통해
시각화 도구에 액세스합니다.
C. 온프레미스에서 시각화 도구를 호스팅합니다. Direct Connect 연결을 통해 데이터
웨어하우스에 연결합니다.
D. 데이터 웨어하우스와 동일한 AWS 지역에서 시각화 도구를 호스팅합니다. Direct Connect
연결을 통해 시각화 도구에 액세스합니다.
Answer: D
Explanation:
* A. On-premises tool via internet: Incurs high costs due to large data transfers over the
internet.
* B. AWS Region tool via internet: Does not utilize Direct Connect, leading to potential
latency and higher costs.
* C. On-premises tool via Direct Connect: Adds latency for querying and visualization.
* D. AWS Region tool via Direct Connect: Reduces latency and leverages Direct Connect for
optimized data transfer costs.
References: AWS Direct Connect
QUESTION NO: 742
한 회사는 데이터 센터에서 SMB 파일 서버를 운영하고 있습니다. 파일서버는 회사가 자주
접속하는 대용량 파일을 파일 생성일로부터 최대 7일까지 저장합니다. 7일이 지나면 회사는
최대 24시간의 검색 시간으로 파일에 액세스할 수 있어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
508

IT Certification Guaranteed, The Easy Way!
A. AWS DataSync를 사용하여 SMB 파일 서버에서 AWS로 7일보다 오래된 데이터를
복사합니다.
B. 회사의 저장 공간을 늘리기 위해 Amazon S3 파일 게이트웨이를 생성합니다. 7일 후에
데이터를 S3 Glacier Deep Archive로 전환하는 S3 수명 주기 정책을 생성합니다.
C. 회사의 저장 공간을 늘리기 위해 Amazon FSx 파일 게이트웨이를 생성합니다. 7일 후에
데이터를 전환하는 Amazon S3 수명 주기 정책을 생성합니다.
D. 각 사용자에 대해 Amazon S3에 대한 액세스를 구성합니다. 7일 후에 데이터를 S3 Glacier
유연한 검색으로 전환하는 S3 수명 주기 정책을 생성합니다.
Answer: B
Explanation:
Amazon S3 File Gateway is a service that provides a file-based interface to Amazon S3,
which appears as a network file share. It enables you to store and retrieve Amazon S3
objects through standard file storage protocols such as SMB. S3 File Gateway can also
cache frequently accessed data locally for low-latency access. S3 Lifecycle policy is a feature
that allows you to define rules that automate the management of your objects throughout
their lifecycle. You can use S3 Lifecycle policy to transition objects to different storage
classes based on their age and access patterns. S3 Glacier Deep Archive is a storage class
that offers the lowest cost for long-term data archiving, with a retrieval time of 12 hours or 48
hours. This solution will meet the requirements, as it allows the company to store large files in
S3 with SMB file access, and to move the files to S3 Glacier Deep Archive after 7 days for
cost savings and compliance.
References:
* 1 provides an overview of Amazon S3 File Gateway and its benefits.
* 2 explains how to use S3 Lifecycle policy to manage object storage lifecycle.
* 3 describes the features and use cases of S3 Glacier Deep Archive storage class.
QUESTION NO: 743
한 회사가 AWS 클라우드의 Auto Scaling 그룹에 속하는 Amazon EC2 인스턴스에서 게임
애플리케이션을 실행하려고 합니다. 애플리케이션은 UDP 패킷을 사용하여 데이터를
전송합니다. 회사는 트래픽이 증가하거나 감소함에 따라 애플리케이션을 확장 및 축소할 수
있기를 원합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. Auto Scaling 그룹에 Network Load Balancer 연결
B. Auto Scaling 그룹에 Application Load Balancer를 연결합니다.
C. 트래픽을 적절하게 라우팅하기 위한 가중치 정책이 포함된 Amazon Route 53 레코드
세트를 배포합니다.
D. Auto Scaling 그룹의 EC2 인스턴스에 대한 포트 전달로 구성된 NAT 인스턴스를
배포합니다.
Answer: A
Explanation:
This solution meets the requirements of running a gaming application that transmits data by
using UDP packets and scaling out and in as traffic increases and decreases. A Network
Load Balancer can handle millions of requests per second while maintaining high throughput
at ultra low latency, and it supports both TCP and UDP protocols. An Auto Scaling group can
509

IT Certification Guaranteed, The Easy Way!
automatically adjust the number of EC2 instances based on the demand and the scaling
policies.
Option B is incorrect because an Application Load Balancer does not support UDP protocol,
only HTTP and HTTPS. Option C is incorrect because Amazon Route 53 is a DNS service
that can route traffic based on different policies, but it does not provide load balancing or
scaling capabilities. Option D is incorrect because a NAT instance is used to enable
instances in a private subnet to connect to the internet or other AWS services, but it does not
provide load balancing or scaling capabilities.
References:
* https://aws.amazon.com/blogs/aws/new-udp-load-balancing-for-network-load-balancer/
* https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html
QUESTION NO: 744
솔루션 설계자는 회사의 온프레미스 인프라를 AWS로 확장하기 위해 새로운 하이브리드
아키텍처를 설계하고 있습니다. 회사는 AWS 리전에 대해 일관되게 짧은 지연 시간과 함께
고가용성 연결이 필요합니다.
회사는 비용을 최소화해야 하며 기본 연결이 실패할 경우 더 느린 트래픽을 기꺼이
받아들입니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. AWS Direct Connect 연결을 리전에 프로비저닝합니다. 기본 Direct Connect 연결이 실패할
경우 VPN 연결을 백업으로 프로비저닝합니다.
B. 개인 연결을 위해 지역에 VPN 터널 연결을 프로비저닝합니다. 기본 VPN 연결이 실패할
경우 개인 연결 및 백업으로 두 번째 VPN 터널을 프로비저닝합니다.
C. AWS Direct Connect 연결을 리전에 프로비저닝합니다. 기본 Direct Connect 연결이 실패할
경우 백업과 동일한 리전에 두 번째 Direct Connect 연결을 프로비저닝합니다.
D. AWS Direct Connect 연결을 리전에 프로비저닝합니다. AWS CLI의 Direct Connect 장애
조치 속성을 사용하여 기본 Direct Connect 연결이 실패할 경우 백업 연결을 자동으로
생성합니다.
Answer: A
Explanation:
"In some cases, this connection alone is not enough. It is always better to guarantee a
fallback connection as the backup of DX. There are several options, but implementing it with
an AWS Site-To-Site VPN is a real cost-effective solution that can be exploited to reduce
costs or, in the meantime, wait for the setup of a second DX."
https://www.proud2becloud.com/hybrid-cloud-networking-backup-aws-direct-connect-
network-connection- with-aws-site-to-site-vpn/
QUESTION NO: 745
회사는 여러 가용 영역에 걸쳐 VPC에서 3계층 웹 애플리케이션을 실행합니다. Amazon EC2
인스턴스는 애플리케이션 계층에 대한 Auto Scaling 그룹에서 실행됩니다.
회사는 각 리소스의 일일 및 주간 기록 워크로드 추세를 분석하는 자동화된 확장 계획을
수립해야 합니다. 구성은 활용도의 예측 및 실시간 변화에 따라 리소스를 적절하게 확장해야
합니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 확장 전략을 권장해야 합니까?
A. EC2 인스턴스의 평균 CPU 사용률을 기반으로 단계 조정을 통해 동적 조정을 구현합니다.
510

IT Certification Guaranteed, The Easy Way!
B. 예측 및 확장을 위해 예측 확장을 활성화합니다. 대상 추적을 사용하여 동적 조정을
구성합니다.
C. 웹 애플리케이션의 트래픽 패턴을 기반으로 자동화된 예약 조정 작업을 생성합니다.
D. 간단한 확장 정책을 설정합니다. EC2 인스턴스 시작 시간을 기준으로 휴지 기간을
늘립니다.
Answer: B
Explanation:
This solution meets the requirements because it allows the company to use both predictive
scaling and dynamic scaling to optimize the capacity of its Auto Scaling group. Predictive
scaling uses machine learning to analyze historical data and forecast future traffic patterns. It
then adjusts the desired capacity of the group in advance of the predicted changes. Dynamic
scaling uses target tracking to maintain a specified metric (such as CPU utilization) at a
target value. It scales the group in or out as needed to keep the metric close to the target. By
using both scaling methods, the company can benefit from faster, simpler, and more accurate
scaling that responds to both forecasted and live changes in utilization.
References:
* Predictive scaling for Amazon EC2 Auto Scaling
* [Target tracking scaling policies for Amazon EC2 Auto Scaling]
QUESTION NO: 746
한 회사가 이벤트 기반 주문 처리 시스템을 설계하고 있습니다. 각 주문은 주문이 생성된 후
여러 검증 단계가 필요합니다. 독립적인 AWS Lambda 함수가 각 검증 단계를 수행합니다. 각
검증 단계는 다른 검증 단계와 독립적입니다. 개별 검증 단계에는 주문 이벤트 정보의 하위
집합만 필요합니다.
회사에서는 각 검증 단계의 람다 함수가 함수에 필요한 주문 이벤트의 정보에만 액세스할 수
있도록 하려고 합니다. 주문 처리 시스템의 구성 요소는 향후 비즈니스 변경 사항을 수용할 수
있도록 느슨하게 결합되어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon EventBridge 이벤트 버스를 만듭니다. 각 검증 단계에 대한 이벤트 규칙을
만듭니다. 각 대상 검증 단계에 필요한 데이터만 전송하도록 입력 변환기를 구성합니다.
Lambda 함수.
B. Amazon Simple Queue Service {Amazon SQS} 대기열 만들기 SQS 대기열을 구독하고
주문 데이터를 각 검증 단계에 필요한 형식으로 변환하는 새 Lambda 함수를 만듭니다. 새
Lambda 함수를 사용하여 검증 단계 Lambda 함수의 동기 호출을 별도의 스레드에서 병렬로
수행합니다.
C. 각 검증 단계에 대한 Amazon Simple Queue Service(Amazon SQS) 대기열을 만듭니다. 각
검증 단계에 필요한 형식으로 주문 데이터를 변환하고 해당 SQS 대기열에 메시지를 게시하는
새 Lambda 함수를 만듭니다. 각 검증 단계 Lambda 함수를 해당 SQS 대기열에 구독합니다.
D. Amazon Simple Notification Service {Amazon SNS} 토픽을 만듭니다. 검증 단계 Lambda
함수를 SNS 토픽에 구독합니다. 메시지 본문 필터링을 사용하여 필요한 데이터만 구독된 각
Lambda 함수에 보냅니다.
Answer: A
QUESTION NO: 747
회사는 VPC에서 퍼블릭 3티어 웹 애플리케이션을 실행합니다. 애플리케이션은 여러 가용
511

IT Certification Guaranteed, The Easy Way!
영역에 걸쳐 Amazon EC2 인스턴스에서 실행됩니다. 프라이빗 서브넷에서 실행되는 EC2
인스턴스는 인터넷을 통해 라이선스 서버와 통신해야 합니다. 회사에는 운영 유지 관리를
최소화하는 관리형 솔루션이 필요합니다. 이러한 요구 사항을 충족하는 솔루션은
무엇입니까?
A. 퍼블릭 서브넷에 NAT 인스턴스를 프로비저닝합니다. NAT 인스턴스를 가리키는 기본
경로로 각 프라이빗 서브넷 라우팅 테이블을 수정합니다.
B. 프라이빗 서브넷에 NAT 인스턴스를 프로비저닝합니다. NAT 인스턴스를 가리키는 기본
경로로 각 프라이빗 서브넷의 라우팅 테이블을 수정합니다.
C. 퍼블릭 서브넷에서 NAT 게이트웨이를 프로비저닝합니다. NAT 게이트웨이를 가리키는
기본 경로로 각 프라이빗 서브넷의 라우팅 테이블을 수정합니다.
D. 프라이빗 서브넷에 NAT 게이트웨이를 프로비저닝합니다. NAT 게이트웨이를 가리키는
기본 경로로 각 프라이빗 서브넷의 라우팅 테이블을 수정합니다.
Answer: C
Explanation:
A NAT gateway is a type of network address translation (NAT) device that enables instances
in a private subnet to connect to the internet or other AWS services, but prevents the internet
from initiating connections with those instances1. A NAT gateway is a managed service that
requires minimal operational maintenance and can handle up to 45 Gbps of bursty traffic1. A
NAT gateway is suitable for scenarios where EC2 instances in private subnets need to
communicate with a license server over the internet, such as the three-tier web application in
the scenario1.
To meet the requirements of the scenario, the solutions architect should provision a NAT
gateway in a public subnet. The solutions architect should also modify each private subnet's
route table with a default route that points to the NAT gateway1. This way, the EC2 instances
that run in private subnets can access the license server over the internet through the NAT
gateway.
QUESTION NO: 748
회사의 규정 준수 팀은 파일 공유를 AWS로 이동해야 합니다. 공유는 Windows Server SMB
파일 공유에서 실행됩니다. 자체 관리형 온프레미스 Active Directory는 파일 및 폴더에 대한
액세스를 제어합니다.
회사는 솔루션의 일부로 Amazon FSx for Windows File Server를 사용하려고 합니다. 회사는
AWS로 이동한 후 온프레미스 Active Directory 그룹이 FSx for Windows File Server SMB
규정 준수 공유, 폴더 및 파일에 대한 액세스를 제한하는지 확인해야 합니다. 회사는 Windows
파일 서버 파일 시스템용 FSx를 만들었습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Active Directory에 연결하기 위한 Active Directory Connector를 생성합니다. Active
Directory 그룹을 IAM 그룹에 매핑하여 액세스를 제한합니다.
B. Restrict 태그 키와 규정 준수 태그 값을 사용하여 태그를 할당합니다. Active Directory
그룹을 IAM 그룹에 매핑하여 액세스를 제한합니다.
C. FSx for Windows File Server에 직접 연결되는 IAM 서비스 연결 역할을 생성하여 액세스를
제한합니다.
D. 파일 시스템을 Active Directory에 연결하여 액세스를 제한합니다.
Answer: D
512

IT Certification Guaranteed, The Easy Way!
Explanation:
Joining the FSx for Windows File Server file system to the on-premises Active Directory will
allow the company to use the existing Active Directory groups to restrict access to the file
shares, folders, and files after the move to AWS. This option allows the company to continue
using their existing access controls and management structure, making the transition to AWS
more seamless.
QUESTION NO: 749
회사는 Amazon S3에 데이터를 저장합니다. 규정에 따라 데이터에는 개인 식별 정보(Pll)가
포함되어서는 안 됩니다. 회사는 최근 S3 버킷에 Pll이 포함된 일부 객체가 있다는 사실을
발견했습니다. 회사는 S3 버킷에서 Pll을 자동으로 감지하고 회사 보안팀에 알려야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon Macie를 사용하세요. Macie 결과에서 SensitiveData 이벤트 유형을 필터링하고
Amazon SNS(Amazon SNS) 알림을 보안 팀에 보내는 Amazon EventBridge 규칙을
생성합니다.
B. Amazon GuardDuty를 사용하세요. GuardDuty 결과에서 CRITICAL 이벤트 유형을
필터링하고 Amazon SNS(Amazon SNS) 알림을 보안 팀에 보내는 Amazon EventBridge
규칙을 생성합니다.
C. Amazon Macie를 사용하세요. Macie 결과에서 SensitiveData:S30bject/Personal 이벤트
유형을 필터링하고 Amazon Simple Queue Service(Amazon SQS) 알림을 보안 팀에 보내는
Amazon EventBridge 규칙을 생성합니다.
D. Amazon GuardDuty를 사용합니다. GuardDuty 결과에서 CRITICAL 이벤트 유형을
필터링하고 Amazon Simple Queue Service(Amazon SQS) 알림을 보안 팀에 보내는 Amazon
EventBridge 규칙을 생성합니다.
Answer: A
Explanation:
Amazon Macie can also send its findings to Amazon EventBridge, which is a serverless
event bus that makes it easy to connect applications using data from a variety of sources.
You can create an EventBridge rule that filters the SensitiveData event type from Macie
findings and sends an Amazon SNS notification to the security team. Amazon SNS is a fully
managed messaging service that enables you to send messages to subscribers or other
applications. References: https://docs.aws.amazon.com/macie/latest/userguide/macie-
findings.html#macie-findings-eventbridge
QUESTION NO: 750
한 회사에는 매일 6시간씩 실행되는 대규모 데이터 워크로드가 있습니다. 회사는 프로세스가
실행되는 동안 데이터를 잃을 수 없습니다. 솔루션 아키텍트는 이 중요한 데이터 워크로드를
지원하기 위해 Amazon EMR 클러스터 구성을 설계하고 있습니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. 온디맨드 인스턴스에서 기본 노드와 핵심 노드를 실행하고 스팟 인스턴스에서 작업 노드를
실행하는 장기 실행 클러스터를 구성합니다.
B. 온디맨드 인스턴스의 기본 노드와 핵심 노드, 스팟 인스턴스의 작업 노드를 실행하는 임시
클러스터를 구성합니다.
C. 온디맨드 인스턴스에서 기본 노드를 실행하고 스팟 인스턴스에서 핵심 노드와 작업 노드를
실행하는 임시 클러스터를 구성합니다.
513

IT Certification Guaranteed, The Easy Way!
D. 온디맨드 인스턴스에서 기본 노드, 스팟 인스턴스에서 핵심 노드, 스팟 인스턴스에서 작업
노드를 실행하는 장기 실행 클러스터를 구성합니다.
Answer: B
Explanation:
For cost-effectiveness and high availability in Amazon EMR workloads, the best approach is
to configure a transient cluster (which runs for the duration of the job and then terminates)
with On-Demand Instances for the primary and core nodes, and Spot Instances for the task
nodes. Here's why:
* Primary and core nodes on On-Demand Instances: These nodes are critical because they
manage the cluster and store data on HDFS. Running them on On-Demand Instances
ensures stability and that no data is lost, as Spot Instances can be interrupted.
* Task nodes on Spot Instances: Task nodes handle additional processing and can be used
with Spot Instances to reduce costs. Spot Instances are much cheaper but can be
interrupted, which is fine for non- critical tasks as the framework can handle retries.
A transient cluster is more cost-effective than a long-running cluster for workloads that only
run for 6 hours a day. Transient clusters automatically terminate after the workload
completes, saving costs by not keeping the cluster running when it's not needed.
* Option A: A long-running cluster may result in unnecessary costs when the cluster isn't
being used.
* Option C: Running core nodes on Spot Instances risks data loss if the Spot Instances are
interrupted, violating the requirement for zero data loss.
* Option D: Running both core and task nodes on Spot Instances is highly risky for data-
critical workloads.
AWS References:
* Amazon EMR Cluster Management
* Using Spot Instances in EMR
QUESTION NO: 751
회사에서 AWS에서 웹 애플리케이션을 구축하려고 합니다. 웹 사이트에 대한 클라이언트
액세스 요청은 예측할 수 없으며 오랫동안 유휴 상태일 수 있습니다. 가입비를 지불한
고객만이 웹 애플리케이션에 로그인하고 사용할 수 있습니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 단계 조합은 무엇입니까? (3개를
선택하세요.)
A. Amazon DynamoDB에서 사용자 정보를 검색하는 AWS Lambda 함수를 생성합니다.
RESTful API를 수락할 Amazon API Gateway 엔드포인트를 생성합니다. API 호출을 Lambda
함수로 보냅니다.
B. Application Load Balancer 뒤에 Amazon Elastic Container Service(Amazon ECS)
서비스를 생성하여 Amazon RDS에서 사용자 정보를 검색합니다. RESTful API를 수락할
Amazon API Gateway 엔드포인트를 생성합니다. API 호출을 Lambda 함수로 보냅니다.
C. 사용자 인증을 위한 Amazon Cogmto 사용자 풀 생성
D. Amazon Cognito 자격 증명 풀을 생성하여 사용자를 인증합니다.
E. AWS Amplify를 사용하여 HTML로 프런트엔드 웹 콘텐츠를 제공합니다. CSS, JS. 통합
Amazon CloudFront 구성을 사용합니다.
F. PHP와 함께 Amazon S3 정적 웹 호스팅을 사용합니다. CSS. 그리고 JS. Amazon
CloudFront를 사용하여 프런트엔드 웹 콘텐츠를 제공합니다.
514

IT Certification Guaranteed, The Easy Way!
Answer: A C E
Explanation:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html
QUESTION NO: 752
회사는 애플리케이션에서 생성되는 대량의 스트리밍 데이터를 수집하고 처리해야 합니다.
애플리케이션은 Amazon EC2 인스턴스에서 실행되며 Amazon Kinesis Data Streams로
데이터를 보냅니다. 여기에는 야생 기본 설정이 포함되어 있습니다. 애플리케이션은 격일로
데이터를 소비하고 비즈니스 인텔리전스(BI) 처리를 위해 Amazon S3 버킷에 데이터를
기록합니다. 회사에서는 트리오 애플리케이션이 Kinesis Data Streams로 전송하는 모든
데이터를 Amazon S3가 수신하지 못하는 것을 발견했습니다.
이 문제를 해결하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 데이터 보존 기간을 수정하여 Kinesis Data Streams 기본 설정을 업데이트합니다.
B. Kinesis Producer Library(KPL)를 사용하도록 애플리케이션을 업데이트하고 데이터를
Kinesis Data Streams로 보냅니다.
C. Kinesis Data Streams로 전송되는 데이터 처리량을 처리하기 위해 Kinesis 샤드 수를
업데이트합니다.
D. S3 버킷 내에서 S3 버전 관리를 활성화하여 S3 버킷에 수집된 모든 객체의 모든 버전을
보존합니다.
Answer: A
Explanation:
The data retention period of a Kinesis data stream is the time period from when a record is
added to when it is no longer accessible1. The default retention period for a Kinesis data
stream is 24 hours, which can be extended up to 8760 hours (365 days)1. The data retention
period can be updated by using the AWS Management Console, the AWS CLI, or the Kinesis
Data Streams API1.
To meet the requirements of the scenario, the solutions architect should update the Kinesis
Data Streams default settings by modifying the data retention period. The solutions architect
should increase the retention period to a value that is greater than or equal to the frequency
of consuming the data and writing it to S32.
This way, the company can ensure that S3 receives all the data that the application sends to
Kinesis Data Streams.
QUESTION NO: 753
회사에서 회사의 AWS 계정에서 작업을 수행하기 위해 외부 공급업체를 고용했습니다.
공급업체는 공급업체가 소유한 AWS 계정에 호스팅된 자동화 도구를 사용합니다.
공급업체에는 회사의 AWS 계정에 대한 IAM 액세스 권한이 없습니다.
솔루션 설계자는 공급업체에 이러한 액세스 권한을 어떻게 부여해야 합니까?
A. 회사 계정에 IAM 역할을 생성하여 공급업체의 IAM 역할에 대한 액세스 권한을 위임합니다.
공급업체에 필요한 권한에 대한 역할에 적절한 IAM 정책을 연결합니다.
B. 회사 계정에서 암호 복잡성 요구 사항을 충족하는 암호를 사용하여 IAM 사용자를
생성합니다. 공급업체가 요구하는 권한에 대해 적절한 IAM 정책을 사용자에게 연결합니다.
C. 회사 계정에 IAM 그룹을 생성합니다. 공급업체 계정의 도구 IAM 사용자를 그룹에
추가합니다. 공급업체에 필요한 권한에 대해 적절한 IAM 정책을 그룹에 연결합니다.
D. IAM 콘솔에서 공급자 유형으로 "AWS 계정"을 선택하여 새 ID 공급자를 생성합니다.
515

IT Certification Guaranteed, The Easy Way!
공급업체의 AWS 계정 ID와 사용자 이름을 제공합니다. 공급업체에 필요한 권한을 위해
적절한 IAM 정책을 새 공급자에 연결합니다.
Answer: A
Explanation:
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-
party.html
QUESTION NO: 754
회사는 회사 로컬 데이터 센터의 Kubernetes 환경에서 컨테이너를 실행합니다. 회사는
Amazon Elastic Kubernetes Service(Amazon EKS) 및 기타 AWS 관리 서비스를 사용하려고
합니다. 데이터는 회사의 데이터 센터에 로컬로 유지되어야 하며 규정 준수를 유지하기 위해
원격 사이트나 클라우드에 저장할 수 없습니다. 이러한 요구 사항을 충족하는 솔루션은
무엇입니까?
A. 회사 데이터 센터에 AWS 로컬 영역 배포
B. 회사 데이터 센터에서 AWS Snowmobile을 사용합니다.
C. 회사 데이터 센터에 AWS Outposts 랙을 설치합니다.
D. 데이터 센터에 AWS Snowball Edge Storage Optimized 노드를 설치합니다.
Answer: C
Explanation:
AWS Outposts is a fully managed service that delivers AWS infrastructure and services to
virtually any on- premises or edge location for a consistent hybrid experience. AWS Outposts
supports Amazon EKS, which is a managed service that makes it easy to run Kubernetes on
AWS and on-premises. By installing an AWS Outposts rack in the company's data center, the
company can run containers in a Kubernetes environment using Amazon EKS and other
AWS managed services, while keeping the data locally in the company's data center and
meeting the compliance requirements. AWS Outposts also provides a seamless connection
to the local AWS Region for access to a broad range of AWS services.
Option A is not a valid solution because AWS Local Zones are not deployed in the company's
data center, but in large metropolitan areas closer to end users. AWS Local Zones are
owned, managed, and operated by AWS, and they provide low-latency access to the public
internet and the local AWS Region. Option B is not a valid solution because AWS
Snowmobile is a service that transports exabytes of data to AWS using a 45-foot long
ruggedized shipping container pulled by a semi-trailer truck. AWS Snowmobile is not
designed for running containers or AWS managed services on-premises, but for large-scale
data migration. Option D is not a valid solution because AWS Snowball Edge Storage
Optimized is a device that provides 80 TB of HDD or
210 TB of NVMe storage capacity for data transfer and edge computing. AWS Snowball
Edge Storage Optimized does not support Amazon EKS or other AWS managed services,
and it is not suitable for running containers in a Kubernetes environment.
References:
* AWS Outposts - Amazon Web Services
* Amazon EKS on AWS Outposts - Amazon EKS
* AWS Local Zones - Amazon Web Services
* AWS Snowmobile - Amazon Web Services
* [AWS Snowball Edge Storage Optimized - Amazon Web Services]
516

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 755
회사에는 AWS CloudFormation 스택이 성명서에 인라인 정책 또는 "*"가 포함된 AWS Identity
and Access Management(1AM) 리소스를 배포하지 못하도록 방지하는 솔루션이 필요합니다.
이 솔루션은 또한 퍼블릭 IP 주소가 있는 Amazon EC2 인스턴스의 배포를 금지해야 합니다.
AWS Organizations의 조직에서 AWS Control Tower가 활성화되었습니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Control Tower 사전 제어를 사용하여 퍼블릭 IP 주소 및 높은 액세스 권한 또는 "*"가
포함된 인라인 정책이 포함된 EC2 인스턴스 배포를 차단합니다.
B. AWS Control Tower 탐지 제어를 사용하여 퍼블릭 IP 주소 및 향상된 액세스 권한이 있는
인라인 정책이 포함된 EC2 인스턴스 배포를 차단하거나 ""
C. AWS Config를 사용하여 EC2 및 1AM 규정 준수에 대한 규칙 생성 규정을 준수하지 않을 때
리소스를 삭제하도록 AWS Systems Manager Session Manager 자동화를 실행하도록 규칙을
구성합니다.
D. 서비스 제어 정책(SCP)을 사용하여 작업이 규정을 준수하지 않는 경우 EC2 인스턴스 및
1AM 리소스에 대한 작업을 차단합니다.
Answer: D
Explanation:
A service control policy (SCP) is a type of policy that you can use to manage permissions in
your organization. SCPs offer central control over the maximum available permissions for all
accounts in your organization, allowing you to ensure your accounts stay within your
organization's access control guidelines.
SCPs are available only in an organization that has all features enabled. SCPs do not grant
permissions; instead, they specify the maximum permissions for an organization or
organizational unit (OU). SCPs limit permissions that identity-based policies or resource-
based policies grant to entities (users or roles) within the account, but do not grant
permissions to entities. You can use SCPs to restrict the actions that the root user in an
account can perform. You can also use SCPs to prevent users or roles in any account from
creating or modifying certain AWS resources, such as EC2 instances with public IP
addresses or IAM resources with inline policies or "". For example, you can create an SCP
that denies the ec2:RunInstances action if the request includes the AssociatePublicIpAddress
parameter set to true. You can also create an SCP that denies the iam:PutUserPolicy and
iam:PutRolePolicy actions if the request includes a policy document that contains
"". By attaching these SCPs to your organization or OUs, you can prevent the deployment of
AWS CloudFormation stacks that violate these rules.
AWS Control Tower proactive controls are guardrails that enforce preventive policies on your
accounts and resources. Proactive guardrails are implemented as AWS Organizations
service control policies (SCPs) and AWS Config rules. However, AWS Control Tower does
not provide a built-in proactive guardrail to block EC2 instances with public IP addresses or
IAM resources with inline policies or "*". You would have to create your own custom
guardrails using AWS CloudFormation templates and SCPs, which is essentially the same as
option D. Therefore, option A is not correct.
AWS Control Tower detective controls are guardrails that detect and alert on policy violations
in your accounts and resources. Detective guardrails are implemented as AWS Config rules
and Amazon CloudWatch alarms. Detective guardrails do not block or remediate
517

IT Certification Guaranteed, The Easy Way!
noncompliant resources; they only notify you of the issues. Therefore, option B is not correct.
AWS Config is a service that enables you to assess, audit, and evaluate the configurations of
your AWS resources. AWS Config continuously monitors and records your AWS resource
configurations and allows you to automate the evaluation of recorded configurations against
desired configurations. AWS Config rules are customizable, AWS Lambda functions that
AWS Config invokes to evaluate your AWS resource configurations. You can use AWS
Config rules to check for compliance with your policies, such as ensuring that EC2 instances
have public IP addresses disabled or IAM resources do not have inline policies or "*".
However, AWS Config rules alone cannot prevent the deployment of AWS CloudFormation
stacks that violate these policies; they can only report the compliance status. You would need
to use another service, such as AWS Systems Manager Session Manager, to run automation
scripts to delete or modify the noncompliant resources. This would require additional
configuration and permissions, and may not be the most efficient or secure way to enforce
your policies. Therefore, option C is not correct.
References:
* Service Control Policies
* AWS Control Tower Guardrails
* AWS Config
QUESTION NO: 756
한 회사는 최근 다른 AWS 리전에 재해 복구 사이트를 생성했습니다. 회사는 일정 기간 동안
두 리전의 NFS 파일 시스템 간에 대량의 데이터를 주고받아야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS DataSync를 사용하세요.
B. AWS Snowball 디바이스 사용
C. Amazon EC2에 SFTP 서버 설정
D. AWS Database Migration Service(AWS DMS) 사용
Answer: A
Explanation:
This option is the most efficient because it uses AWS DataSync, which is a secure, online
service that automates and accelerates moving data between on-premises and AWS Storage
services1. It also uses DataSync to transfer large amounts of data back and forth between
NFS file systems in the two Regions on a periodic basis, which simplifies and speeds up the
data transfer process with minimal operational overhead.
This solution meets the requirement of transferring large amounts of data back and forth
between NFS file systems in the two Regions on a periodic basis with the least operational
overhead. Option B is less efficient because it uses AWS Snowball devices, which are
physical devices that let you transfer large amounts of data into and out of AWS2. However,
this does not provide a periodic data transfer solution, as it requires manual handling and
shipping of the devices. Option C is less efficient because it sets up an SFTP server on
Amazon EC2, which is a way to provide secure file transfer protocol (SFTP) access to files
stored in Amazon S33.
However, this does not provide a periodic data transfer solution, as it requires manual
initiation and monitoring of the file transfers. Option D is less efficient because it uses AWS
Database Migration Service (AWS DMS), which is a service that helps you migrate
518

IT Certification Guaranteed, The Easy Way!
databases to AWS quickly and securely. However, this does not provide a data transfer
solution for NFS file systems, as it only supports relational databases and non-relational data
stores.
QUESTION NO: 757
한 회사에 직원 웹 포털이 있습니다. 직원은 포털에 로그인하여 급여 세부 정보를 확인합니다.
이 회사는 직원이 상환을 위해 스캔한 문서를 업로드할 수 있는 새로운 시스템을 개발하고
있습니다. 이 회사는 문서에서 텍스트 기반 데이터를 추출하고 추출된 정보를 각 직원의 상환
ID에 첨부하여 처리하는 프로그램을 실행합니다.
직원 웹 포털은 100% 가동 시간이 필요합니다. 문서 추출 프로그램은 주문형으로 하루 종일
드물게 실행됩니다. 회사는 기존 웹 포털에 최소한의 변경만 필요한 확장 가능하고 비용
효율적인 새 시스템을 구축하고자 합니다. 회사는 코드를 변경하고 싶어하지 않습니다.
어떤 솔루션이 가장 적은 구현 노력으로 이러한 요구 사항을 충족할 수 있을까요?
A. 웹 포털에 대한 자동 확장 그룹에서 Amazon EC2 온디맨드 인스턴스를 실행합니다. AWS
Lambda 함수를 사용하여 문서 추출 프로그램을 실행합니다. 직원이 새 상환 문서를 업로드할
때 Lambda 함수를 호출합니다.
B. 웹 포털에 대한 자동 확장 그룹에서 Amazon EC2 Spot 인스턴스를 실행합니다. EC2 Spot
인스턴스에서 문서 추출 프로그램을 실행합니다. 직원이 새 상환 문서를 업로드하면 문서
추출 프로그램 인스턴스를 시작합니다.
C. 웹 포털과 문서 추출 프로그램을 실행하기 위한 저축 플랜을 구매하세요. 자동 확장
그룹에서 웹 포털과 문서 추출 프로그램을 실행하세요.
D. 웹 포털을 호스팅할 Amazon S3 버킷을 만듭니다. 기존 기능에 Amazon API Gateway와
AWS Lambda 함수를 사용합니다. Lambda 함수를 사용하여 문서 추출 프로그램을
실행합니다. 새 문서 업로드와 관련된 API가 호출되면 Lambda 함수를 호출합니다.
Answer: A
Explanation:
This solution offers the most scalable and cost-effective approach with minimal changes to
the existing web portal and no code modifications.
* Amazon EC2 On-Demand Instances in an Auto Scaling Group: Running the web portal on
EC2 On- Demand instances ensures 100% uptime and scalability. The Auto Scaling group
will maintain the desired number of instances, automatically scaling up or down as needed,
ensuring high availability for the employee web portal.
* AWS Lambda for Document Extraction: Lambda is a serverless compute service that allows
you to run code in response to events without provisioning or managing servers. By using
Lambda to run the document extraction program, you can trigger the function whenever an
employee uploads a document.
This approach is cost-effective since you only pay for the compute time used by the Lambda
function.
* No Code Changes Required: This solution integrates with the existing infrastructure with
minimal implementation effort and does not require any modifications to the web portal's
code.
* Why Not Other Options?:
* Option B (Spot Instances): Spot Instances are not suitable for workloads requiring 100%
uptime, as they can be terminated by AWS with short notice.
* Option C (Savings Plan): A Savings Plan could reduce costs but does not address the
519

IT Certification Guaranteed, The Easy Way!
requirement for running the document extraction program efficiently or without code changes.
* Option D (S3 with API Gateway and Lambda): This would require significant changes to the
existing web portal setup, including moving the portal to S3 and reconfiguring its architecture,
which contradicts the requirement of minimal implementation effort and no code changes.
AWS References:
* Amazon EC2 Auto Scaling - Information on how to use Auto Scaling for EC2 instances.
* AWS Lambda - Overview of AWS Lambda and its use cases.
QUESTION NO: 758
회사의 실시간에 가까운 스트리밍 애플리케이션이 AWS에서 실행되고 있습니다. 데이터가
수집되면 해당 데이터에 대해 작업이 실행되고 완료하는 데 30분이 걸립니다. 워크로드는
대량의 수신 데이터로 인해 높은 대기 시간을 경험하는 경우가 많습니다. 솔루션 설계자는
성능을 향상시키기 위해 확장 가능한 서버리스 솔루션을 설계해야 합니다.
솔루션 설계자는 어떤 단계 조합을 수행해야 합니까? (2개를 선택하세요.)
A. Amazon Kinesis Data Firehose를 사용하여 데이터를 수집합니다.
B. AWS Step Functions와 함께 AWS Lambda를 사용하여 데이터를 처리합니다.
C. AWS Database Migration Service(AWS DMS)를 사용하여 데이터 수집
D. Auto Seating 그룹의 Amazon EC2 인스턴스를 사용하여 데이터를 처리합니다.
E. Amazon Elastic Container Service(Amazon ECS)와 함께 AWS Fargate를 사용하여
데이터를 처리합니다.
Answer: A
Explanation:
* Understanding the Requirement: The company needs to design a scalable and serverless
solution for a near-real-time streaming application that experiences high latency due to large
amounts of incoming data. The job processing takes about 30 minutes.
* Analysis of Options:
* Amazon Kinesis Data Firehose: Provides a fully managed service for real-time data
streaming and ingestion, allowing for seamless data delivery to destinations such as Amazon
S3, Redshift, and Elasticsearch.
* AWS Lambda with AWS Step Functions: While suitable for orchestration and lightweight
processing, Lambda might not handle long-running jobs (max 15 minutes execution limit)
efficiently.
* AWS DMS: Primarily used for database migration, not for real-time data ingestion in this
context.
* Amazon EC2 in Auto Scaling Group: Provides scalability but involves managing servers,
which is not serverless and adds operational overhead.
* AWS Fargate with ECS: Offers a serverless compute engine for containers, allowing easy
scaling and management without managing the underlying infrastructure.
* Best Solution:
* Amazon Kinesis Data Firehose: For ingesting the streaming data efficiently.
* AWS Fargate with ECS: For processing the data in a scalable and serverless manner.
References:
* Amazon Kinesis Data Firehose
* AWS Fargate
520

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 759
전자상거래 회사는 분석을 위해 판매 기록을 집계하고 채우기 위해 예약된 일일 작업을
실행해야 합니다. 회사는 판매 기록을 Amazon S3 버킷에 저장합니다. 각 개체의 크기는 최대
10G6일 수 있습니다. 판매 이벤트 수에 따라 작업을 완료하는 데 최대 1시간이 걸릴 수
있습니다. FOB의 CPU 및 메모리 사용량은 일정하며 미리 알려져 있습니다.
솔루션 설계자는 작업을 실행하는 데 필요한 운영 노력을 최소화해야 합니다. 이러한 요구
사항을 충족하는 솔루션은 무엇입니까?
A. Amazon EventBridge 알림이 있는 AWS Lambda 함수 생성 EventBridge 이벤트가 하루에
한 번 실행되도록 예약
B. AWS Lambda 함수 생성 Amazon API Gateway HTTP API를 생성하고 API를 함수와
통합합니다. API를 호출하고 함수를 호출하는 Amazon EventBridge 예약 광고를 생성합니다.
C. AWS Fargate 시작 유형을 사용하여 Amazon Elastic Container Service(Amazon ECS)
Duster를 생성합니다. 작업을 실행하기 위해 클러스터에서 ECS 작업을 시작하는 Amazon
EventBridge 예약 이벤트를 생성합니다.
D. Amazon EC2 시작 유형과 하나 이상의 EC2 인스턴스가 있는 Auto Scaling 그룹을
사용하여 Amazon Elastic Container Service(Amazon ECS) 더스터를 생성합니다. 작업을
실행하기 위해 더스터에서 ECS 작업을 시작하는 Amazon EventBridge 예약 이벤트를
생성합니다.
Answer: C
Explanation:
The solution that meets the requirements with the least operational overhead is to create a
**Regional AWS WAF web ACL with a rate-based rule** and associate the web ACL with the
API Gateway stage. This solution will protect the application from HTTP flood attacks by
monitoring incoming requests and blocking requests from IP addresses that exceed the
predefined rate. Amazon CloudFront distribution with Lambda@Edge in front of the API
Gateway Regional API endpoint is also a good solution but it requires more operational
overhead than the previous solution. Using Amazon CloudWatch metrics to monitor the
Count metric and alerting the security team when the predefined rate is reached is not a
solution that can protect against HTTP flood attacks. Creating an Amazon CloudFront
distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24
hours is not a solution that can protect against HTTP flood attacks.
QUESTION NO: 760
회사에는 원치 않는 콘텐츠가 포함된 사진이 회사의 웹 애플리케이션에 업로드되는 것을
방지하는 솔루션이 필요합니다. 솔루션에는 기계 학습(ML) 모델 교육이 포함되어서는 안
됩니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon SageMaker Autopilot을 사용하여 모델을 생성하고 배포합니다. 새 사진이
업로드될 때 웹 애플리케이션이 호출하는 실시간 엔드포인트를 만듭니다.
B. Amazon Rekognition을 사용하여 원치 않는 콘텐츠를 감지하는 AWS Lambda 함수를
생성합니다. 새 사진이 업로드될 때 웹 애플리케이션이 호출하는 Lambda 함수 URL을
생성합니다.
C. Amazon Comprehend를 사용하여 원치 않는 콘텐츠를 감지하는 Amazon CloudFront
함수를 생성합니다. 기능을 웹 애플리케이션과 연결합니다.
D. Amazon Rekognition Video를 사용하여 원치 않는 콘텐츠를 감지하는 AWS Lambda
521

IT Certification Guaranteed, The Easy Way!
함수를 생성합니다. 새 사진이 업로드될 때 웹 애플리케이션이 호출하는 Lambda 함수 URL을
생성합니다.
Answer: B
Explanation:
The solution that will meet the requirements is to create an AWS Lambda function that uses
Amazon Rekognition to detect unwanted content, and create a Lambda function URL that the
web application invokes when new photos are uploaded. This solution does not involve
training a machine learning model, as Amazon Rekognition is a fully managed service that
provides pre-trained computer vision models for image and video analysis. Amazon
Rekognition can detect unwanted content such as explicit or suggestive adult content,
violence, weapons, drugs, and more. By using AWS Lambda, the company can create a
serverless function that can be triggered by an HTTP request from the web application. The
Lambda function can use the Amazon Rekognition API to analyze the uploaded photos and
return a response indicating whether they contain unwanted content or not.
The other solutions are not as effective as the first one because they either involve training a
machine learning model, do not support image analysis, or do not work with photos. Creating
and deploying a model by using Amazon SageMaker Autopilot involves training a machine
learning model, which is not required for the scenario. Amazon SageMaker Autopilot is a
service that automatically creates, trains, and tunes the best machine learning models for
classification or regression based on the data provided by the user. Creating an Amazon
CloudFront function that uses Amazon Comprehend to detect unwanted content does not
support image analysis, as Amazon Comprehend is a natural language processing service
that analyzes text, not images. Amazon Comprehend can extract insights and relationships
from text such as language, sentiment, entities, topics, and more. Creating an AWS Lambda
function that uses Amazon Rekognition Video to detect unwanted content does not work with
photos, as Amazon Rekognition Video is designed for analyzing video streams, not static
images. Amazon Rekognition Video can detect activities, objects, faces, celebrities, text, and
more in video streams.
References:
* Amazon Rekognition
* AWS Lambda
* Detecting unsafe content - Amazon Rekognition
* Amazon SageMaker Autopilot
* Amazon Comprehend
QUESTION NO: 761
한 회사는 AWS 클라우드에서 긴밀하게 결합된 고성능 컴퓨팅(HPC) 환경을 설계하고
있습니다. 회사는 네트워킹 및 스토리지를 위해 HPC 환경을 최적화하는 기능을 포함해야
합니다.
이러한 요구 사항을 충족하는 솔루션 조합은 무엇입니까? (2개 선택)
A. AWS Global Accelerator에서 액셀러레이터를 생성합니다. 가속기에 대한 사용자 지정
라우팅을 구성합니다.
B. Lustre 파일 시스템용 Amazon FSx를 생성합니다. 스크래치 스토리지로 파일 시스템을
구성합니다.
C. Amazon CloudFront 배포를 생성합니다. 뷰어 프로토콜 정책을 HTTP 및 HTTPS로
522

IT Certification Guaranteed, The Easy Way!
구성합니다.
D. Amazon EC2 인스턴스를 시작합니다. EFA(Elastic Fabric Adapter)를 인스턴스에
연결합니다.
E. 환경을 관리하기 위한 AWS Elastic Beanstalk 배포를 생성합니다.
Answer: B D
Explanation:
These two solutions will optimize the HPC environment for networking and storage. Amazon
FSx for Lustre is a fully managed service that provides cost-effective, high-performance,
scalable storage for compute workloads. It is built on the world's most popular high-
performance file system, Lustre, which is designed for applications that require fast storage,
such as HPC and machine learning. By configuring the file system with scratch storage, you
can achieve sub-millisecond latencies, up to hundreds of GBs/s of throughput, and millions of
IOPS. Scratch file systems are ideal for temporary storage and shorter-term processing of
data.
Data is not replicated and does not persist if a file server fails. For more information, see
Amazon FSx for Lustre.
Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables
customers to run applications requiring high levels of inter-node communications at scale on
AWS. Its custom-built operating system (OS) bypass hardware interface enhances the
performance of inter-instance communications, which is critical to scaling HPC and machine
learning applications. EFA provides a low-latency, low-jitter channel for inter-instance
communications, enabling your tightly-coupled HPC or distributed machine learning
applications to scale to thousands of cores. EFA uses libfabric interface and libfabric APIs for
communications, which are supported by most HPC programming models. For more
information, see Elastic Fabric Adapter.
The other solutions are not suitable for optimizing the HPC environment for networking and
storage. AWS Global Accelerator is a networking service that helps you improve the
availability, performance, and security of your public applications by using the AWS global
network. It provides two global static public IPs, deterministic routing, fast failover, and TCP
termination at the edge for your application endpoints. However, it does not support OS-
bypass capabilities or high-performance file systems that are required for HPC and machine
learning applications. For more information, see AWS Global Accelerator.
Amazon CloudFront is a content delivery network (CDN) service that securely delivers data,
videos, applications, and APIs to customers globally with low latency, high transfer speeds,
all within a developer- friendly environment. CloudFront is integrated with AWS services such
as Amazon S3, Amazon EC2, AWS Elemental Media Services, AWS Shield, AWS WAF, and
AWS Lambda@Edge. However, CloudFront is not designed for HPC and machine learning
applications that require high levels of inter-node communications and fast storage. For more
information, see [Amazon CloudFront].
AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications
and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on
familiar servers such as Apache, Nginx, Passenger, and IIS. You can simply upload your
code and Elastic Beanstalk automatically handles the deployment, from capacity
provisioning, load balancing, auto-scaling to application health monitoring.
However, Elastic Beanstalk is not optimized for HPC and machine learning applications that
523

IT Certification Guaranteed, The Easy Way!
require OS- bypass capabilities and high-performance file systems. For more information, see
[AWS Elastic Beanstalk].
References: Amazon FSx for Lustre, Elastic Fabric Adapter, AWS Global Accelerator,
[Amazon CloudFront],
[AWS Elastic Beanstalk].
QUESTION NO: 762
한 회사가 온프레미스 워크로드를 AWS 클라우드로 마이그레이션하고 있습니다. 이 회사는
이미 여러 Amazon EC2 인스턴스와 Amazon RDS DB 인스턴스를 사용하고 있습니다. 회사는
업무 시간 외에 EC2 인스턴스와 D6 인스턴스를 자동으로 시작하고 중지하는 솔루션을
원합니다. 솔루션은 비용과 인프라 유지 관리를 최소화해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 탄력적 크기 조정을 사용하여 EC2 인스턴스를 확장합니다. 업무 시간 외에는 DB
인스턴스를 0으로 조정합니다.
B. 일정에 따라 EC2 인스턴스 및 OB 인스턴스를 자동으로 시작 및 중지하는 파트너 솔루션에
대한 AWS Marketplace를 살펴보세요.
C. 다른 EC2 인스턴스를 시작합니다. 일정에 따라 기존 EC2 인스턴스와 DB 인스턴스를
시작하고 중지하는 셸 스크립트를 실행하도록 crontab 일정을 구성합니다.
D. EC2 인스턴스 및 DB 인스턴스를 시작 및 중지하는 AWS Lambda 함수를 생성합니다.
일정에 따라 Lambda 함수를 호출하도록 Amazon EventBridge를 구성합니다.
Answer: D
Explanation:
The most efficient solution for automatically starting and stopping EC2 instances and DB
instances on a schedule while minimizing cost and infrastructure maintenance is to create an
AWS Lambda function and configure Amazon EventBridge to invoke the function on a
schedule.
Option A, scaling EC2 instances by using elastic resize and scaling DB instances to zero
outside of business hours, is not feasible as DB instances cannot be scaled to zero.
Option B, exploring AWS Marketplace for partner solutions, may be an option, but it may not
be the most efficient solution and could potentially add additional costs.
Option C, launching another EC2 instance and configuring a crontab schedule to run shell
scripts that will start and stop the existing EC2 instances and DB instances on a schedule,
adds unnecessary infrastructure and maintenance.
QUESTION NO: 763
회사에는 온프레미스 파일 시스템이 SFTP를 통해 매일 수신하는 보고서 파일을 분석하는
야간 일괄 처리 루틴이 있습니다. 회사는 솔루션을 AWS 클라우드로 이전하려고 합니다.
솔루션은 가용성과 복원력이 높아야 합니다. 또한 솔루션은 운영 노력을 최소화해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. SFTP용 AWS 전송 및 스토리지용 Amazon Elastic File System(Amazon EFS) 파일
시스템을 배포합니다. 예약된 조정 정책이 있는 Auto Scaling 그룹의 Amazon EC2 인스턴스를
사용하여 배치 작업을 실행합니다.
B. Linux 및 SFTP 서비스를 실행하는 Amazon EC2 인스턴스를 배포합니다. 저장에는
Amazon Elastic Block Store(Amazon EBS) 볼륨을 사용하세요. 최소 인스턴스 수와 원하는
인스턴스 수를 1로 설정한 Auto Scaling 그룹을 사용합니다.
524

IT Certification Guaranteed, The Easy Way!
C. Linux 및 SFTP 서비스를 실행하는 Amazon EC2 인스턴스를 배포합니다. 저장을 위해
Amazon Elastic File System(Amazon EFS) 파일 시스템을 사용합니다. 최소 인스턴스 수와
원하는 인스턴스 수를 1로 설정한 Auto Scaling 그룹을 사용합니다.
D. SFTP용 AWS 전송과 저장용 Amazon S3 버킷을 배포합니다. 처리를 위해 Amazon S3에서
Amazon EC2 인스턴스로 배치 파일을 가져오도록 애플리케이션을 수정합니다. 예약된 조정
정책이 있는 Auto Scaling 그룹의 EC2 인스턴스를 사용하여 일괄 작업을 실행합니다.
Answer: D
Explanation:
The solution that meets the requirements of high availability, performance, security, and
static IP addresses is to use Amazon CloudFront, Application Load Balancers (ALBs),
Amazon Route 53, and AWS WAF. This solution allows the company to distribute its HTTP-
based application globally using CloudFront, which is a content delivery network (CDN)
service that caches content at edge locations and provides static IP addresses for each edge
location. The company can also use Route 53 latency-based routing to route requests to the
closest ALB in each Region, which balances the load across the EC2 instances. The
company can also deploy AWS WAF on the CloudFront distribution to protect the application
against common web exploits by creating rules that allow, block, or count web requests
based on conditions that are defined. The other solutions do not meet all the requirements
because they either use Network Load Balancers (NLBs), which do not support HTTP-based
applications, or they do not use CloudFront, which provides better performance and security
than AWS Global Accelerator. References :=
* Amazon CloudFront
* Application Load Balancer
* Amazon Route 53
* AWS WAF
QUESTION NO: 764
솔루션 아키텍트는 AWS 클라우드에 배포되는 새로운 애플리케이션의 아키텍처를 설계하고
있습니다. 애플리케이션은 Amazon EC2 온디맨드 인스턴스에서 실행되며 여러 가용 영역에
걸쳐 자동으로 확장됩니다. EC2 인스턴스는 하루 종일 자주 확장 및 축소됩니다.
ALB(Application Load Balancer)가 로드 분산을 처리합니다. 아키텍처는 분산 세션 데이터
관리를 지원해야 합니다. 회사는 필요한 경우 코드를 기꺼이 변경할 것입니다.
아키텍처가 분산 세션 데이터 관리를 지원하는지 확인하려면 솔루션 설계자가 무엇을 해야
합니까?
A. Amazon ElastiCache를 사용하여 세션 데이터를 관리하고 저장합니다.
B. ALB의 세션 선호도(고정 세션)를 사용하여 세션 데이터를 관리합니다.
C. AWS Systems Manager의 Session Manager를 사용하여 세션을 관리합니다.
D. AWS Security Token Service(AWS STS)에서 GetSessionToken API 작업을 사용하여
세션을 관리합니다.
Answer: A
Explanation:
https://aws.amazon.com/vi/caching/session-management/
In order to address scalability and to provide a shared data storage for sessions that can be
accessible from any individual web server, you can abstract the HTTP sessions from the web
servers themselves. A common solution to for this is to leverage an In-Memory Key/Value
525

IT Certification Guaranteed, The Easy Way!
store such as Redis and Memcached. ElastiCache offerings for In-Memory key/value stores
include ElastiCache for Redis, which can support replication, and ElastiCache for
Memcached which does not support replication.
QUESTION NO: 765
회사에는 탄력적 IP 주소가 있는 퍼블릭 서브넷의 Amazon EC2 인스턴스에서 실행되는 웹
서버가 있습니다. 기본 보안 그룹은 EC2 인스턴스에 할당됩니다. 모든 트래픽을 차단하도록
기본 네트워크 ACL이 수정되었습니다. 솔루션 설계자는 포트 443을 통해 어디에서나 웹
서버에 액세스할 수 있도록 해야 합니다.
이 작업을 수행하려면 어떤 단계를 조합해야 합니까? (2개를 선택하세요.)
A. 소스 0.0.0.0/0에서 TCP 포트 443을 허용하는 규칙을 사용하여 보안 그룹을 생성합니다.
B. TCP 포트 443을 대상 0.0.0.0/0으로 허용하는 규칙을 사용하여 보안 그룹을 생성합니다.
C. 소스 0.0.0.0/0의 TCP 포트 443을 허용하도록 네트워크 ACL을 업데이트합니다.
D. 소스 0.0.0.0/0에서 대상 0.0.0.0/0으로의 인바운드/아웃바운드 TCP 포트 443을 허용하도록
네트워크 ACL을 업데이트합니다.
E. 소스 0.0.0.0/0에서 인바운드 TCP 포트 443을 허용하고 대상 0.0.0.0/0으로 아웃바운드
TCP 포트 32768-65535를 허용하도록 네트워크 ACL을 업데이트합니다.
Answer: A C
Explanation:
The combination of steps that will accomplish the task of making the web server accessible
from everywhere on port 443 is to create a security group with a rule to allow TCP port 443
from source 0.0.0.0/0 (A) and to update the network ACL to allow inbound TCP port 443 from
source 0.0.0.0/0 (C). This will ensure that traffic to port 443 is allowed both at the security
group level and at the network ACL level, which will make the web server accessible from
everywhere on port 443.
QUESTION NO: 766
연구 회사는 시뮬레이션 애플리케이션과 시각화 애플리케이션으로 구동되는 실험을
실행합니다. 시뮬레이션 애플리케이션은 Linux에서 실행되고 5분마다 NFS 공유에 중간
데이터를 출력합니다. 시각화 애플리케이션은 시뮬레이션 출력을 표시하고 SMB 파일
시스템이 필요한 Windows 데스크톱 애플리케이션입니다.
이 회사는 두 개의 동기화된 파일 시스템을 유지 관리합니다. 이 전략은 데이터 중복과
비효율적인 리소스 사용을 초래합니다. 이 회사는 두 애플리케이션 모두에 코드를 변경하지
않고도 애플리케이션을 AWS로 마이그레이션해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 두 애플리케이션을 AWS Lambda로 마이그레이션합니다. 애플리케이션 간에 데이터를
교환하기 위한 Amazon S3 버킷을 만듭니다.
B. 두 애플리케이션을 모두 Amazon Elastic Container Service(Amazon ECS)로
마이그레이션합니다. 스토리지를 위해 Amazon FSx File Gateway를 구성합니다.
C. 시뮬레이션 애플리케이션을 Linux Amazon EC2 인스턴스로 마이그레이션합니다. 시각화
애플리케이션을 Windows EC2 인스턴스로 마이그레이션합니다. 애플리케이션 간에 데이터를
교환하도록 Amazon Simple Queue Service(Amazon SQS)를 구성합니다.
D. 시뮬레이션 애플리케이션을 Linux Amazon EC2 인스턴스로 마이그레이션합니다. 시각화
애플리케이션을 Windows EC2 인스턴스로 마이그레이션합니다. 스토리지를 위해 Amazon
FSx for NetApp ONTAP을 구성합니다.
526

IT Certification Guaranteed, The Easy Way!
Answer: D
Explanation:
This solution will meet the requirements because Amazon FSx for NetApp ONTAP is a fully
managed service that provides highly reliable, scalable, and feature-rich file storage built on
NetApp's popular ONTAP file system. FSx for ONTAP supports both NFS and SMB
protocols, which means it can be accessed by both Linux and Windows applications without
code changes. FSx for ONTAP also eliminates data duplication and inefficient resource
usage by automatically tiering infrequently accessed data to a lower-cost storage tier and
providing storage efficiency features such as deduplication and compression. FSx for ONTAP
also integrates with other AWS services such as Amazon S3, AWS Backup, and AWS
CloudFormation. By migrating the applications to Amazon EC2 instances, the company can
leverage the scalability, security, and performance of AWS compute resources.
QUESTION NO: 767
회사의 애플리케이션에 성능 문제가 있습니다. 애플리케이션이 오래되어 Amazon EC2
인스턴스에서 m-메모리 작업을 완료해야 합니다. 회사는 AWS CloudFormation을 사용하여
인프라를 배포하고 M5 EC2 인스턴스 제품군을 사용했습니다. 트래픽이 증가함에 따라
애플리케이션 성능이 저하되었습니다. 사용자가 애플리케이션에 액세스하려고 할 때 지연이
발생한다고 보고하고 있습니다.
어떤 솔루션이 이러한 문제를 운영상 가장 효율적인 방식으로 해결할 수 있습니까?
A. EC2 인스턴스를 Auto Scaling 그룹에서 실행되는 T3 EC2 인스턴스로 교체합니다. AWS
Management Console을 사용하여 변경했습니다.
B. Auto Scaling 그룹에서 EC2 인스턴스를 실행하도록 CloudFormation 템플릿을 수정합니다.
증가가 필요한 경우 Auto Scaling 그룹의 원하는 용량과 최대 용량을 수동으로 늘립니다.
C. CloudFormation 템플릿을 수정합니다. EC2 인스턴스를 R5 EC2 인스턴스로 교체합니다.
Amazon CloudWatch에 내장된 EC2 메모리 지표를 사용하여 향후 용량 계획을 위한
애플리케이션 성능을 추적하세요.
D. CloudFormation 템플릿을 수정합니다. EC2 인스턴스를 R5 EC2 인스턴스로 교체합니다.
EC2 인스턴스에 Amazon CloudWatch 에이전트를 배포하여 향후 용량 계획을 위한 사용자
지정 애플리케이션 지연 시간 지표를 생성합니다.
Answer: D
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/cloudwatch-memory-metrics-
ec2/
QUESTION NO: 768
한 회사가 AWS 클라우드에서 애플리케이션을 개발하고 있습니다. 애플리케이션의 HTTP
API에는 Amazon API Gateway에 게시된 중요한 정보가 포함되어 있습니다. 중요한 정보는
회사 내부 네트워크에 속한
제한된 신뢰할 수 있는 IP 주소 집합에서만 액세스할 수 있어야 합니다 .
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. API Gateway 개인 통합을 설정하여 사전 정의된 IP 주소 집합에 대한 액세스를
제한합니다.
B. 특별히 허용되지 않은 IP 주소에 대한 액세스를 거부하는 API에 대한 리소스 정책을
만듭니다.
527

IT Certification Guaranteed, The Easy Way!
C. API를 프라이빗 서브넷에 직접 배포합니다. 네트워크 ACL을 만듭니다. 특정 IP 주소에서
트래픽을 허용하는 규칙을 설정합니다.
D. 신뢰할 수 있는 IP 주소에서만 인바운드 트래픽을 허용하도록 API Gateway에 연결된 보안
그룹을 수정합니다.
Answer: B
Explanation:
Amazon API Gateway supports resource policies, which allow you to control access to your
API by specifying the IP addresses or ranges that can access the API. By creating a resource
policy that explicitly denies access to any IP address outside the allowed set, you can ensure
that only trusted IP addresses (such as those from your internal network) can access the
critical information in your API. This approach provides fine- grained access control without
the need for additional infrastructure or complex configurations.
* Option A (Private integration): API Gateway private integrations are for creating private
APIs that are only accessible within a VPC, but this solution is about restricting access to
certain IP addresses.
* Option C (Private subnet and ACLs): Deploying the API in a private subnet and using
network ACLs adds unnecessary complexity and isn't the best fit for HTTP APIs.
* Option D (Security group): API Gateway doesn't have a security group because it isn't a
resource inside a VPC. Instead, resource policies are the correct mechanism for controlling
IP-based access.
AWS References:
* Controlling Access to API Gateway with Resource Policies
QUESTION NO: 769
로봇 회사는 의료 수술을 위한 솔루션을 설계하고 있습니다. 로봇은 고급 센서, 카메라 및 AI
알고리즘을 사용하여 환경을 인식하고 수술을 완료합니다.
회사에는 백엔드 서비스와의 원활한 통신을 보장할 AWS 클라우드의 공용 로드 밸런서가
필요합니다. 로드 밸런서는 쿼리 문자열을 기반으로 트래픽을 다른 대상 그룹으로 라우팅할
수 있어야 합니다. 트래픽도 암호화되어야 합니다. 어떤 솔루션이 이러한 요구 사항을
충족합니까?
A. AWS Certificate Manager(ACM)에서 첨부된 인증서와 함께 Network Load Balancer를
사용합니다. 쿼리 매개변수 기반 라우팅을 사용합니다.
B. 게이트웨이 로드 밸런서를 사용합니다. AWS Identity and Access Management(1AM)에서
생성된 인증서를 가져옵니다. 인증서를 로드 밸런서에 연결합니다. HTTP 경로 기반 라우팅을
사용합니다.
C. ACM(AWS Certificate Manager)에서 첨부된 인증서와 함께 Application Load Balancer를
사용합니다.
쿼리 매개변수 기반 라우팅을 사용합니다.
D. Network Load Balancer를 사용합니다. AWS Identity and Access Management(1AM)에서
생성된 인증서를 가져옵니다. 인증서를 로드 밸런서에 연결합니다. 쿼리 매개변수 기반
라우팅을 사용합니다.
Answer: C
Explanation:
* Understanding the Requirement: The robotics company needs a public load balancer to
ensure seamless communication with backend services, route traffic based on query strings,
528

IT Certification Guaranteed, The Easy Way!
and encrypt traffic.
* Analysis of Options:
* Network Load Balancer with ACM Certificate: NLBs primarily operate at the transport layer
(Layer 4) and do not natively support query parameter-based routing, which is a Layer 7
feature.
* Gateway Load Balancer with IAM Certificate: Gateway Load Balancers are designed for
deploying, scaling, and managing third-party virtual appliances and do not support HTTP
path- based or query parameter-based routing.
* Application Load Balancer with ACM Certificate: ALBs operate at the application layer
(Layer
7), supporting features like query parameter-based routing and SSL/TLS termination with
ACM certificates.
* Network Load Balancer with IAM Certificate: As with the first option, NLBs do not support
query parameter-based routing, making it unsuitable for this requirement.
* Best Solution:
* Application Load Balancer with ACM Certificate: This option provides the necessary Layer 7
routing capabilities and SSL/TLS termination to meet the requirements for query parameter-
based routing and encrypted communication.
References:
* Application Load Balancer
* AWS Certificate Manager
QUESTION NO: 770
한 회사가 문서 관리 애플리케이션을 AWS로 마이그레이션하고 있습니다. 애플리케이션은
Linux 서버에서 실행됩니다.
회사는 애플리케이션을 Auto Scaling 그룹의 Amazon EC2 인스턴스로 마이그레이션합니다.
회사는 공유 스토리지 파일 시스템에 7TiB의 문서를 저장합니다. 외부 관계형 데이터베이스가
문서를 추적합니다.
문서는 한 번 저장되며 언제든지 참조를 위해 여러 번 검색할 수 있습니다. 회사는
마이그레이션 도중 애플리케이션을 수정할 수 없습니다. 스토리지 솔루션은 가용성이 높아야
하며 시간에 따른 확장을 지원해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. 향상된 네트워킹을 갖춘 EC2 인스턴스를 공유 NFS 스토리지 시스템으로 배포합니다. NFS
공유를 내보냅니다. Auto Scaling 그룹의 EC2 인스턴스에 NFS 공유를 탑재합니다.
B. S3 Standard-Infrequent Access(S3 Standard-IA) 스토리지 클래스를 사용하는 Amazon S3
버킷을 생성합니다. Auto Scaling 그룹의 EC2 인스턴스에 S3 버킷을 탑재합니다.
C. SFTP용 AWS 전송 및 Amazon S3 버킷을 사용하여 SFTP 서버 엔드포인트를 배포합니다.
Auto Scaling 그룹의 EC2 인스턴스를 다음과 같이 구성합니다.
SFTP 서버에 연결하세요.
D. 여러 가용 영역에 탑재 지점이 있는 Amazon.. System(Amazon fcFS) 파일 시스템을
생성합니다. bFS Stondard-intrcqucnt Access(Standard-IA) 스토리지 클래스를 사용합니다.
Auto Scaling 그룹의 EC2 인스턴스에 NFS 공유를 탑재합니다.
Answer: D
Explanation:
* Requirement Analysis: The company needs highly available, scalable storage for a
529

IT Certification Guaranteed, The Easy Way!
document management application without modifying the application during migration.
* EFS Overview: Amazon EFS provides scalable file storage that can be mounted
concurrently on multiple EC2 instances across different Availability Zones.
* EFS Standard-IA: Using the Standard-IA storage class helps reduce costs for infrequently
accessed data while maintaining high availability and scalability.
* Implementation:
* Create an EFS file system.
* Configure mount targets in multiple Availability Zones to ensure high availability.
* Mount the EFS file system on EC2 instances in the Auto Scaling group.
* Conclusion: This solution meets the high availability, scalability, and cost-effectiveness
requirements without needing application modifications.
References
* Amazon EFS: Amazon EFS Documentation
* EFS Storage Classes: Amazon EFS Storage Classes
QUESTION NO: 771
회사의 AWS Organizations에 모든 기능이 활성화된 조직이 있습니다. 회사에서는 기존 또는
신규 AWS 계정의 모든 API 호출 및 로그인을 감사해야 합니다. 회사에서는 추가 작업을
방지하고 비용을 최소화하기 위해 관리형 솔루션이 필요합니다. 회사에도 필요합니다. AWS
계정이 AWS FSBP(Foundational Security Best Practices) 표준을 준수하지 않는 경우를 알 수
있습니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 조직 마스터 계정에 AWS Control Tower 환경을 배포합니다. 환경에서 AWS Security Hub
및 AWS Control Tower Account Factory를 활성화합니다.
B. 전용 조직 멤버 계정에 AWS Control Tower 환경을 배포합니다. 환경에서 AWS Security
Hub 및 AWS Control Tower Account Factory를 활성화합니다.
C. AWS Managed Services(AMS) Accelerate를 사용하여 다중 계정 랜딩 존(MALZ) 구축
MALZ의 셀프 서비스 프로비저닝 Amazon GuardDuty에 RFC를 제출합니다.
D. AWS Managed Services(AMS) Accelerate를 사용하여 다중 계정 랜딩 존(MALZ) 구축
MALZ의 셀프 서비스 프로비저닝 AWS Security Hub에 RFC를 제출합니다.
Answer: A
Explanation:
AWS Control Tower is a fully managed service that simplifies the setup and governance of a
secure, compliant, multi-account AWS environment. It establishes a landing zone that is
based on best-practices blueprints, and it enables governance using controls you can choose
from a pre-packaged list. The landing zone is a well-architected, multi-account baseline that
follows AWS best practices. Controls implement governance rules for security, compliance,
and operations. AWS Security Hub is a service that provides a comprehensive view of your
security posture across your AWS accounts. It aggregates, organizes, and prioritizes security
alerts and findings from multiple AWS services, such as Amazon GuardDuty, Amazon
Inspector, Amazon Macie, AWS Firewall Manager, and AWS IAM Access Analyzer, as well
as from AWS Partner solutions. AWS Security Hub continuously monitors your environment
using automated compliance checks based on the AWS best practices and industry
standards, such as the AWS Foundational Security Best Practices (FSBP) standard. AWS
Control Tower Account Factory is a feature that automates the provisioning of new AWS
530

IT Certification Guaranteed, The Easy Way!
accounts that are preconfigured to meet your business, security, and compliance
requirements.
By deploying an AWS Control Tower environment in the Organizations management
account, you can leverage the existing organization structure and policies, and enable AWS
Security Hub and AWS Control Tower Account Factory in the environment. This way, you
can audit all API calls and logins in any existing or new AWS account, monitor the
compliance status of each account with the FSBP standard, and provision new accounts with
ease and consistency. This solution meets the requirements with the least operational
overhead, as you do not need to manage any infrastructure, perform any data migration, or
submit any requests for changes.
References:
* AWS Control Tower
* [AWS Security Hub]
* [AWS Control Tower Account Factory]
QUESTION NO: 772
한 회사에서 마이크로서비스 기반 서버리스 웹 애플리케이션을 실행하고 있습니다.
애플리케이션은 여러 Amazon DynamoDB 테이블에서 데이터를 검색할 수 있어야 합니다.
솔루션 설계자는 애플리케이션의 기본 성능에 영향을 주지 않고 데이터를 검색할 수 있는
기능을 애플리케이션에 제공해야 합니다.
어떤 솔루션이 운영상 가장 효율적인 방식으로 이러한 요구 사항을 충족합니까?
A. AWSAppSync 파이프라인 해석기
B. Lambda@Edge 기능을 갖춘 Amazon CloudFront
C. AWS Lambda 기능을 갖춘 엣지 최적화 Amazon API Gateway
D. DynamoDB 커넥터를 사용한 Amazon Athena 통합 쿼리
Answer: C
Explanation:
An edge-optimized API Gateway is a way to create RESTful APIs that can access multiple
DynamoDB tables through AWS Lambda functions. The edge-optimized API Gateway
provides low latency and high performance by caching API responses at CloudFront edge
locations. The AWS Lambda functions can use the AWS SDK to query or scan the
DynamoDB tables and return the data to the API Gateway. This solution meets all the
requirements of the question, while the other options do not. References:
* https://aws.amazon.com/blogs/compute/understanding-database-options-for-your-
serverless-web- applications/
* https://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-
apigateway-s3- dynamodb-cognito/module-3/
* https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html
QUESTION NO: 773
한 회사가 AWS에서 전자상거래 애플리케이션을 실행합니다. 모든 새로운 주문은 단일 가용
영역의 Amazon EC2 인스턴스에서 실행되는 RabbitMQ 대기열에 메시지로 게시됩니다.
이러한 메시지는 별도의 EC2 인스턴스에서 실행되는 다른 애플리케이션에 의해 처리됩니다.
이 애플리케이션은 다른 EC2 인스턴스의 PostgreSQL 데이터베이스에 세부 정보를
저장합니다. 모든 EC2 인스턴스는 동일한 가용 영역에 있습니다.
531

IT Certification Guaranteed, The Easy Way!
회사는 최소한의 운영 오버헤드로 최고의 가용성을 제공하기 위해 아키텍처를 재설계해야
합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 대기열을 Amazon MQ에 있는 RabbitMQ 인스턴스의 중복 쌍(활성/대기)으로
마이그레이션합니다. 다중 AZ Auto Scaling 그룹(또는 애플리케이션을 호스팅하는 EC2
인스턴스)을 생성합니다. PostgreSQL 데이터베이스를 호스팅하는 EC2 인스턴스에 대한 또
다른 다중 AZ Auto Scaling 그룹을 생성합니다.
B. 대기열을 Amazon MQ에 있는 RabbitMQ 인스턴스의 중복 쌍(활성/대기)으로
마이그레이션합니다. 애플리케이션을 호스팅하는 EC2 인스턴스에 대한 다중 AZ Auto
Scaling 그룹을 생성합니다. PostgreSQL용 Amazon RDS의 다중 AZ 배포에서 실행되도록
데이터베이스를 마이그레이션합니다.
C. RabbitMQ 대기열을 호스팅하는 EC2 인스턴스에 대한 다중 AZ Auto Scaling 그룹을
생성합니다. 애플리케이션을 호스팅하는 EC2 인스턴스에 대한 또 다른 다중 AZ Auto Scaling
그룹을 생성합니다. Amazon RDS fqjPostgreSQL의 다중 AZ 배포에서 실행되도록
데이터베이스를 마이그레이션합니다.
D. RabbitMQ 대기열을 호스팅하는 EC2 인스턴스에 대한 다중 AZ Auto Scaling 그룹을
생성합니다. 애플리케이션을 호스팅하는 EC2 인스턴스에 대한 또 다른 다중 AZ Auto Scaling
그룹을 생성합니다. PostgreSQL 데이터베이스를 호스팅하는 EC2 인스턴스에 대한 세 번째
다중 AZ Auto Scaling 그룹을 생성합니다.
Answer: B
Explanation:
Migrating to Amazon MQ reduces the overhead on the queue management. C and D are
dismissed. Deciding between A and B means deciding to go for an AutoScaling group for
EC2 or an RDS for Postgress (both multi- AZ). The RDS option has less operational impact,
as provide as a service the tools and software required. Consider for instance, the effort to
add an additional node like a read replica, to the DB. https://docs.
aws.amazon.com/amazon-mq/latest/developer-guide/active-standby-broker-deployment.html
https://aws.
amazon.com/rds/postgresql/
QUESTION NO: 774
AWS 클라우드에 호스팅되는 한 회사의 웹 애플리케이션이 최근 인기가 높아졌습니다. 웹
애플리케이션은 현재 단일 퍼블릭 서브넷의 단일 Amazon EC2 인스턴스에 존재합니다. 웹
애플리케이션은 증가하는 웹 트래픽의 수요를 충족하지 못했습니다.
회사에는 웹 애플리케이션을 다시 작성하지 않고도 증가하는 사용자 요구를 충족할 수 있는
고가용성과 확장성을 제공하는 솔루션이 필요합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.)
A. EC2 인스턴스를 더 큰 컴퓨팅 최적화 인스턴스로 교체합니다.
B. 프라이빗 서브넷의 여러 가용 영역으로 Amazon EC2 Auto Scaling을 구성합니다.
C. 웹 요청을 처리하도록 퍼블릭 서브넷에 NAT 게이트웨이를 구성합니다.
D. EC2 인스턴스를 더 큰 메모리 최적화 인스턴스로 교체합니다.
E. 웹 트래픽을 분산하기 위해 퍼블릭 서브넷에서 Application Load Balancer를 구성합니다.
Answer: B E
Explanation:
532

IT Certification Guaranteed, The Easy Way!
These two steps will meet the requirements because they will provide high availability and
scalability for the web application without rewriting it. Amazon EC2 Auto Scaling allows you to
automatically adjust the number of EC2 instances in response to changes in demand. By
configuring Auto Scaling with multiple Availability Zones in private subnets, you can ensure
that your web application is distributed across isolated and fault-tolerant locations, and that
your instances are not directly exposed to the internet. An Application Load Balancer
operates at the application layer and distributes incoming web traffic across multiple targets,
such as EC2 instances, containers, or Lambda functions. By configuring an Application Load
Balancer in a public subnet, you can enable your web application to handle requests from the
internet and route them to the appropriate targets in the private subnets.
References:
* What is Amazon EC2 Auto Scaling?
* What is an Application Load Balancer?
QUESTION NO: 775
솔루션 아키텍트는 클라이언트 사례 파일을 저장하는 시스템을 설계해야 합니다. 파일은
회사의 핵심 자산이며 중요합니다. 시간이 지남에 따라 파일 수가 증가합니다.
파일은 Amazon EC2 인스턴스에서 실행되는 여러 애플리케이션 서버에서 동시에 액세스할
수 있어야 합니다. 솔루션에는 중복성이 내장되어 있어야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon Elastic File System(Amazon EFS)
B. Amazon Elastic Block Store(Amazon EBS)
C. Amazon S3 Glacier Deep 아카이브
D. AWS 백업
Answer: A
Explanation:
Amazon EFS provides a simple, scalable, fully managed file system that can be
simultaneously accessed from multiple EC2 instances and provides built-in redundancy. It is
optimized for multiple EC2 instances to access the same files, and it is designed to be highly
available, durable, and secure. It can scale up to petabytes of data and can handle
thousands of concurrent connections, and is a cost-effective solution for storing and
accessing large amounts of data.
QUESTION NO: 776
회사에는 프라이빗 서브넷의 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있습니다.
애플리케이션은 Amazon S3 버킷의 민감한 정보를 처리해야 합니다. 애플리케이션은 S3
버킷에 연결하기 위해 인터넷을 사용해서는 안 됩니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 인터넷 게이트웨이를 구성합니다. 인터넷 게이트웨이로부터의 액세스를 허용하도록 S3
버킷 정책을 업데이트합니다. 새 인터넷 게이트웨이를 사용하도록 애플리케이션을
업데이트합니다.
B. VPN 연결을 구성합니다. VPN 연결에서 액세스를 허용하도록 S3 버킷 정책을
업데이트합니다.
새 VPN 연결을 사용하도록 애플리케이션을 업데이트하세요.
C. NAT 게이트웨이를 구성합니다. NAT 게이트웨이에서의 액세스를 허용하도록 S3 버킷
533

IT Certification Guaranteed, The Easy Way!
정책을 업데이트합니다. 새 NAT 게이트웨이를 사용하도록 애플리케이션을 업데이트합니다.
D. VPC 엔드포인트를 구성합니다. VPC 엔드포인트에서의 액세스를 허용하도록 S3 버킷
정책을 업데이트합니다. 새 VPC 엔드포인트를 사용하도록 애플리케이션을 업데이트합니다.
Answer: D
Explanation:
* Understanding the Requirement: The application running on EC2 instances in a private
subnet needs to process sensitive information from an S3 bucket without using the internet.
* Analysis of Options:
* Internet Gateway: This would expose the application to the internet, which is not suitable for
accessing sensitive information securely.
* VPN Connection: VPN is primarily used for secure connections between on-premises
networks and AWS VPCs, not for direct S3 access within the same VPC.
* NAT Gateway: This allows instances in a private subnet to connect to the internet, but the
goal is to avoid internet access.
* VPC Endpoint: Provides a private connection between the VPC and S3 without using the
internet, ensuring secure access to the S3 bucket.
* Best Solution:
* VPC Endpoint: Configuring a VPC endpoint allows secure, private communication between
the EC2 instances and the S3 bucket without using the internet, ensuring data security and
compliance.
References:
* Amazon VPC Endpoints
* Amazon S3 VPC Endpoint
QUESTION NO: 777
전자상거래 회사가 AWS에서 자사 애플리케이션을 실행합니다. 이 애플리케이션은 기본
데이터베이스에 대해 Multi-AZ 모드에서 Amazon Aurora PostgreSQL 클러스터를
사용합니다. 최근 프로모션 캠페인 동안 애플리케이션은 많은 읽기 및 쓰기 부하를
겪었습니다. 사용자는 애플리케이션에 액세스하려고 할 때 시간 초과 문제를 겪었습니다.
솔루션 아키텍트는 애플리케이션 아키텍처의 확장성과 가용성을 높여야 합니다.
어떤 솔루션이 가장 적은 다운타임으로 이러한 요구 사항을 충족할 수 있을까요?
A. Aurora 클러스터를 소스로 하는 Amazon EventBndge 규칙을 만듭니다. Aurora 클러스터의
상태 변경 이벤트를 로깅하는 AWS Lambda 함수를 만듭니다. Lambda 함수를 EventBndge
규칙의 대상으로 추가합니다. 장애 조치할 추가 리더 노드를 추가합니다.
B. Aurora 클러스터를 수정하고 제로 다운타임 재시작(ZDR) 기능을 활성화합니다.
클러스터에서 Database Activity Streams를 사용하여 클러스터 상태를 추적합니다.
C. Aurora 클러스터에 추가 리더 인스턴스를 추가합니다. Aurora 클러스터에 대한 Amazon
RDS 프록시 대상 그룹을 생성합니다.
D. Redis 캐시용 Amazon ElastiCache를 만듭니다. 쓰기 방식과 함께 AWS Database
Migration Service(AWS DMS)를 사용하여 Aurora 클러스터에서 Redis로 데이터를
복제합니다.
Answer: C
Explanation:
This solution directly addresses the scalability and high availability requirements with minimal
downtime.
534

IT Certification Guaranteed, The Easy Way!
* Additional Reader Instances: Adding more reader instances to the Aurora cluster will
distribute the read load, improving the performance of the application under heavy read
traffic. Aurora reader instances automatically replicate the data from the writer instance,
enabling you to scale out read operations.
* Amazon RDS Proxy: RDS Proxy improves database availability by managing database
connections more efficiently and providing a connection pool. This reduces the overhead on
the Aurora cluster during peak loads, further enhancing performance and availability without
requiring changes to the application code.
* Why Not Other Options?:
* Option A (EventBridge and Lambda): This doesn't directly address the performance and
availability issues. Logging state changes and adding reader nodes on failure events doesn't
provide proactive scalability.
* Option B (Zero-Downtime Restart and Activity Streams): Zero-Downtime Restart (ZDR) is
useful for minimizing downtime during maintenance but doesn't directly improve scalability.
Database Activity Streams are more for security monitoring than for performance
enhancement.
* Option D (ElastiCache for Redis): While adding a caching layer can help with read
performance, it introduces complexity and may not be necessary if additional reader
instances can handle the load.
AWS References:
* Amazon Aurora Scaling - Information on scaling Aurora clusters with reader instances.
* Amazon RDS Proxy - Details on how RDS Proxy can improve database performance and
availability.
QUESTION NO: 778
한 회사에서 사용자가 Amazon S3에 작은 파일을 업로드하는 애플리케이션을 설계하고
있습니다. 사용자가 파일을 업로드한 후 데이터를 변환하고 나중에 분석할 수 있도록
데이터를 JSON 형식으로 저장하려면 파일에 일회성 단순 처리가 필요합니다.
각 파일은 업로드 후 최대한 빨리 처리해야 합니다. 수요는 다양할 것입니다. 어떤 날에는
사용자가 많은 수의 파일을 업로드합니다. 다른 날에는 사용자가 몇 개의 파일을
업로드하거나 파일을 업로드하지 않습니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon S3에서 텍스트 파일을 읽도록 Amazon EMR을 구성합니다. 처리 스크립트를
실행하여 데이터를 변환합니다. 결과 JSON 파일을 Amazon Aurora DB 클러스터에
저장합니다.
B. 이벤트 알림을 Amazon Simple Queue Service(Amazon SQS) 대기열로 보내도록 Amazon
S3를 구성합니다. Amazon EC2 인스턴스를 사용하여 대기열에서 읽고 데이터를 처리합니다.
결과 JSON 파일을 Amazon DynamoDB에 저장합니다.
C. 이벤트 알림을 Amazon Simple Queue Service(Amazon SQS) 대기열로 보내도록 Amazon
S3를 구성합니다. AWS Lambda 함수를 사용하여 대기열에서 읽고 데이터를 처리합니다.
결과 JSON 파일을 Amazon DynamoDB에 저장합니다. 최다 투표
D. 새 파일이 업로드될 때 Amazon Kinesis Data Streams에 이벤트를 보내도록 Amazon
EventBridge(Amazon CloudWatch Events)를 구성합니다. AWS Lambda 함수를 사용하여
스트림에서 이벤트를 소비하고 데이터를 처리합니다. 결과 JSON 파일을 Amazon Aurora DB
클러스터에 저장합니다.
535

IT Certification Guaranteed, The Easy Way!
Answer: C
Explanation:
Amazon S3 sends event notifications about S3 buckets (for example, object created, object
removed, or object restored) to an SNS topic in the same Region.
The SNS topic publishes the event to an SQS queue in the central Region.
The SQS queue is configured as the event source for your Lambda function and buffers the
event messages for the Lambda function.
The Lambda function polls the SQS queue for messages and processes the Amazon S3
event notifications according to your application's requirements.
https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/subscribe-a-lambda-
function-to-event- notifications-from-s3-buckets-in-different-aws-regions.html
QUESTION NO: 779
한 회사가 여러 가용 영역에 걸쳐 Amazon EC2 Linux 인스턴스에서 애플리케이션을
실행합니다. 애플리케이션에는 가용성이 높고 POSIX(Portable Operating System Interface)를
준수하는 스토리지 계층이 필요합니다. 스토리지 계층은 최대 데이터 내구성을 제공해야 하며
EC2 인스턴스 전체에서 공유 가능해야 합니다. 스토리지 계층의 데이터는 처음 30일 동안
자주 액세스되며 그 이후에는 자주 액세스되지 않습니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. Amazon S3 Standard 스토리지 클래스를 사용합니다. S3 수명 주기 정책을 생성하여 자주
액세스하지 않는 데이터를 S3 Glacier로 이동합니다.
B. Amazon S3 Standard 스토리지 클래스를 사용합니다. 자주 액세스하지 않는 데이터를 S3
Standard-Infrequent Access(EF3 Standard-IA)로 이동하는 S3 수명 주기 정책을 만듭니다.
C. Amazon Elastic File System(Amazon EFS) 표준 스토리지 클래스를 사용합니다. 자주
액세스하지 않는 데이터를 EFS Standard-Infrequent Access(EFS Standard-IA)로 이동하는
수명 주기 관리 정책을 만듭니다.
D. Amazon Elastic File System(Amazon EFS) One Zone 스토리지 클래스를 사용합니다.
자주 액세스하지 않는 데이터를 EFS One Zone-IA(EFS One Zone-Infrequent Access)로
이동하는 수명 주기 관리 정책을 만듭니다.
Answer: C
Explanation:
https://aws.amazon.com/efs/features/infrequent-access/
QUESTION NO: 780
한 회사가 AWS에서 온라인 마켓플레이스 웹 애플리케이션을 실행하고 있습니다. 이
애플리케이션은 피크 시간 동안 수십만 명의 사용자에게 서비스를 제공합니다. 회사에는
수백만 건의 금융 거래 세부 정보를 다른 여러 내부 애플리케이션과 공유하기 위해 확장
가능하고 실시간에 가까운 솔루션이 필요합니다. 또한 짧은 대기 시간의 검색을 위해 문서
데이터베이스에 저장하기 전에 중요한 데이터를 제거하기 위해 거래를 처리해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 권장해야 합니까?
A. 트랜잭션 데이터를 Amazon DynamoDB에 저장합니다. 쓰기 시 모든 트랜잭션에서 중요한
데이터를 제거하도록 DynamoDB에 규칙을 설정합니다. DynamoDB Streams를 사용하여
트랜잭션 데이터를 다른 애플리케이션과 공유합니다.
B. 트랜잭션 데이터를 Amazon Kinesis Data Firehose로 스트리밍하여 Amazon DynamoDB
및 Amazon S3에 데이터를 저장합니다. Kinesis Data Firehose와 AWS Lambda 통합을
536

IT Certification Guaranteed, The Easy Way!
사용하여 민감한 데이터를 제거합니다. 다른 애플리케이션은 Amazon S3에 저장된 데이터를
사용할 수 있습니다.
C. 트랜잭션 데이터를 Amazon Kinesis Data Streams로 스트리밍합니다. AWS Lambda
통합을 사용하여 모든 트랜잭션에서 중요한 데이터를 제거한 다음 Amazon DynamoDB에
트랜잭션 데이터를 저장합니다. 다른 애플리케이션은 Kinesis 데이터 스트림에서 트랜잭션
데이터를 사용할 수 있습니다.
D. 일괄 처리된 트랜잭션 데이터를 Amazon S3에 파일로 저장합니다. Amazon S3에서 파일을
업데이트하기 전에 AWS Lambda를 사용하여 모든 파일을 처리하고 중요한 데이터를
제거합니다. 그런 다음 Lambda 함수는 Amazon DynamoDB에 데이터를 저장합니다. 다른
애플리케이션은 Amazon S3에 저장된 트랜잭션 파일을 사용할 수 있습니다.
Answer: C
Explanation:
The destination of your Kinesis Data Firehose delivery stream. Kinesis Data Firehose can
send data records to various destinations, including Amazon Simple Storage Service
(Amazon S3), Amazon Redshift, Amazon OpenSearch Service, and any HTTP endpoint that
is owned by you or any of your third-party service providers. The following are the supported
destinations:
* Amazon OpenSearch Service
* Amazon S3
* Datadog
* Dynatrace
* Honeycomb
* HTTP Endpoint
* Logic Monitor
* MongoDB Cloud
* New Relic
* Splunk
* Sumo Logic
https://docs.aws.amazon.com/firehose/latest/dev/create-name.html
https://aws.amazon.com/kinesis/data-streams/
Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data
streaming service.
KDS can continuously capture gigabytes of data per second from hundreds of thousands of
sources such as website clickstreams, database event streams, financial transactions, social
media feeds, IT logs, and location- tracking events.
QUESTION NO: 781
이미지 처리 회사에는 사용자가 이미지를 업로드하는 데 사용하는 웹 응용 프로그램이
있습니다. 애플리케이션은 이미지를 Amazon S3 버킷에 업로드합니다. 회사는 객체 생성
이벤트를 Amazon Simple Queue Service(Amazon SQS) 표준 대기열에 게시하도록 S3
이벤트 알림을 설정했습니다. SQS 대기열은 이미지를 처리하고 결과를 이메일을 통해
사용자에게 보내는 AWS Lambda 함수의 이벤트 소스 역할을 합니다.
사용자는 업로드된 모든 이미지에 대해 여러 이메일 메시지를 수신하고 있다고 보고합니다.
솔루션 설계자는 SQS 메시지가 Lambda 함수를 두 번 이상 호출하여 여러 이메일 메시지를
생성한다고 판단합니다.
537

IT Certification Guaranteed, The Easy Way!
솔루션 설계자는 이 문제를 최소한의 운영 오버헤드로 해결하기 위해 무엇을 해야 합니까?
A. ReceiveMessage 대기 시간을 30초로 늘려 SQS 대기열에서 긴 폴링을 설정합니다.
B. SQS 표준 대기열을 SQS FIFO 대기열로 변경합니다. 메시지 중복 제거 ID를 사용하여
중복 메시지를 버리십시오.
C. SQS 대기열의 가시성 제한 시간을 함수 제한 시간과 일괄 처리 창 제한 시간의 합계보다 큰
값으로 늘립니다.
D. 처리 전에 메시지를 읽은 직후 SQS 대기열에서 각 메시지를 삭제하도록 Lambda 함수를
수정합니다.
Answer: C
QUESTION NO: 782
회사에는 가상 머신(VM)을 기반으로 하는 다중 계층 결제 처리 애플리케이션이 있습니다.
계층 간의 통신은 정확히 1회 전달을 보장하는 타사 미들웨어 솔루션을 통해 비동기적으로
발생합니다.
회사에는 최소한의 인프라 관리가 필요한 솔루션이 필요합니다. 솔루션은 애플리케이션
메시징에 대해 정확히 1회 전달을 보장해야 합니다. 이러한 요구 사항을 충족하는 작업 조합은
무엇입니까? (2개를 선택하세요.)
A. 아키텍처의 컴퓨팅 계층에 AWS Lambda를 사용합니다.
B. 아키텍처의 컴퓨팅 계층으로 Amazon EC2 인스턴스를 사용합니다.
C. 컴퓨팅 계층 사이의 메시징 구성 요소로 Amazon Simple 알림 서비스(Amazon SNS)를
사용합니다.
D. Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 컴퓨팅 계층 간의 메시징
구성 요소로 사용합니다.
E. 아키텍처의 컴퓨팅 계층에 Amazon Elastic Kubemetes Service(Amazon EKS)를 기반으로
하는 컨테이너를 사용합니다.
Answer: A D
Explanation:
This solution meets the requirements because it requires the least amount of infrastructure
management and guarantees exactly-once delivery for application messaging. AWS Lambda
is a serverless compute service that lets you run code without provisioning or managing
servers. You only pay for the compute time you consume. Lambda scales automatically with
the size of your workload. Amazon SQS FIFO queues are designed to ensure that messages
are processed exactly once, in the exact order that they are sent. FIFO queues have high
availability and deliver messages in a strict first-in, first-out order. You can use Amazon SQS
to decouple and scale microservices, distributed systems, and serverless applications.
References: AWS Lambda, Amazon SQS FIFO queues
QUESTION NO: 783
한 소셜 미디어 회사에서는 사용자가 자사 웹사이트에 이미지를 업로드할 수 있도록
허용합니다. 웹사이트는 Amazon EC2 인스턴스에서 실행됩니다. 업로드 요청 중에 웹
사이트는 이미지 크기를 표준 크기로 조정하고 크기 조정된 이미지를 Amazon S3에
저장합니다. 사용자의 웹사이트 업로드 요청 속도가 느려지고 있습니다.
회사는 애플리케이션 내 결합을 줄이고 웹사이트 성능을 개선해야 합니다. 솔루션 설계자는
이미지 업로드를 위해 운영상 가장 효율적인 프로세스를 설계해야 합니다.
538

IT Certification Guaranteed, The Easy Way!
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 조치 조합을 취해야 합니까? (2개를
선택하세요.)
A. S3 Glacier에 이미지를 업로드하도록 애플리케이션을 구성합니다.
B. 원본 이미지를 Amazon S3에 업로드하도록 웹 서버를 구성합니다.
C. 미리 서명된 URL을 사용하여 각 사용자의 브라우저에서 Amazon S3로 이미지를 직접
업로드하도록 애플리케이션을 구성합니다.
D. 이미지가 업로드될 때 AWS Lambda 함수를 호출하도록 S3 이벤트 알림을 구성합니다.
기능을 사용하여 이미지 크기를 조정하세요
E. 업로드된 이미지의 크기를 조정하기 위해 일정에 따라 AWS Lambda 함수를 호출하는
Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다.
Answer: C D
Explanation:
Amazon S3 is a highly scalable and durable object storage service that can store and retrieve
any amount of data from anywhere on the web1. Users can configure the application to
upload images directly from each user's browser to Amazon S3 through the use of a
presigned URL. A presigned URL is a URL that gives access to an object in an S3 bucket for
a limited time and with a specific action, such as uploading an object2.
Users can generate a presigned URL programmatically using the AWS SDKs or AWS CLI.
By using a presigned URL, users can reduce coupling within the application and improve
website performance, as they do not need to send the images to the web server first.
AWS Lambda is a serverless compute service that runs code in response to events and
automatically manages the underlying compute resources3. Users can configure S3 Event
Notifications to invoke an AWS Lambda function when an image is uploaded. S3 Event
Notifications is a feature that allows users to receive notifications when certain events
happen in an S3 bucket, such as object creation or deletion. Users can configure S3 Event
Notifications to invoke a Lambda function that resizes the image and stores it back in the
same or a different S3 bucket. This way, users can offload the image resizing task from the
web server to Lambda.
QUESTION NO: 784
한 금융회사의 고객이 문자 메시지를 보내 금융 자문가와의 약속을 요청합니다. Amazon EC2
인스턴스에서 실행되는 웹 애플리케이션은 약속 요청을 수락합니다. 문자 메시지는 웹
애플리케이션을 통해 Amazon Simple Queue Service(Amazon SQS) 대기열에 게시됩니다.
그러면 EC2 인스턴스에서 실행되는 또 다른 애플리케이션이 고객에게 회의 초대 및 회의 확인
이메일 메시지를 보냅니다. 예약이 성공적으로 완료되면 이 애플리케이션은 회의 정보를
Amazon DynamoDB 데이터베이스에 저장합니다.
회사가 확장됨에 따라 고객들은 회의 초대장이 도착하는 데 시간이 더 오래 걸린다고
보고합니다.
이 문제를 해결하기 위해 솔루션 설계자는 무엇을 권장해야 합니까?
A. DynamoDB 데이터베이스 앞에 DynamoDB Accelerator(DAX) 클러스터를 추가합니다.
B. 약속 요청을 수락하는 웹 애플리케이션 앞에 Amazon API Gateway API를 추가합니다.
C. Amazon CloudFront 배포를 추가합니다. 약속 요청을 수락하는 웹 애플리케이션으로
원본을 설정합니다.
D. 회의 초대를 보내는 애플리케이션에 대한 Auto Scaling 그룹을 추가합니다. SQS 대기열의
539

IT Certification Guaranteed, The Easy Way!
깊이에 따라 조정되도록 Auto Scaling 그룹을 구성합니다.
Answer: D
Explanation:
To resolve the issue of longer delivery times for meeting invitations, the solutions architect
can recommend adding an Auto Scaling group for the application that sends meeting
invitations and configuring the Auto Scaling group to scale based on the depth of the SQS
queue. This will allow the application to scale up as the number of appointment requests
increases, improving the performance and delivery times of the meeting invitations.
QUESTION NO: 785
솔루션 설계자는 공급업체가 Docker 컨테이너 이미지로 제공하는 애플리케이션에 대한
아키텍처를 설계해야 합니다. 컨테이너에는 임시 파일에 사용할 수 있는 50GB의 스토리지가
필요합니다. 인프라는 서버리스여야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 50GB가 넘는 공간이 있는 Amazon S3 탑재 볼륨과 함께 Docker 컨테이너 이미지를
사용하는 AWS Lambda 함수를 생성합니다.
B. 공간이 50GB가 넘는 Amazon Elastic Block Store(Amazon EBS) 볼륨과 함께 Docker
컨테이너 이미지를 사용하는 AWS Lambda 함수를 생성합니다.
C. AWS Fargate 시작 유형을 사용하는 Amazon Elastic Container Service(Amazon ECS)
클러스터를 생성합니다. Amazon Elastic File System(Amazon EFS) 볼륨이 있는 컨테이너
이미지에 대한 작업 정의를 생성합니다. 해당 작업 정의를 사용하여 서비스를 생성합니다.
D. 공간이 50GB가 넘는 Amazon Elastic Block Store(Amazon EBS) 볼륨과 함께 Amazon EC2
시작 유형을 사용하는 Amazon Elastic Container Service(Amazon ECS) 클러스터를
생성합니다. 컨테이너에 대한 작업 정의를 생성합니다. 영상. 해당 작업 정의를 사용하여
서비스를 생성합니다.
Answer: C
Explanation:
The AWS Fargate launch type is a serverless way to run containers on Amazon ECS, without
having to manage any underlying infrastructure. You only pay for the resources required to
run your containers, and AWS handles the provisioning, scaling, and security of the cluster.
Amazon EFS is a fully managed, elastic, and scalable file system that can be mounted to
multiple containers, and provides high availability and durability. By using AWS Fargate and
Amazon EFS, you can run your Docker container image with 50 GB of storage available for
temporary files, with the least operational overhead. This solution meets the requirements of
the question.
References:
* AWS Fargate
* Amazon Elastic File System
* Using Amazon EFS file systems with Amazon ECS
QUESTION NO: 786
미디어 회사가 비디오를 업로드하기 위해 AWS에 웹 애플리케이션을 호스팅합니다. 인증된
사용자만 인증 후 지정된 시간 내에 업로드해야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. 인증된 사용자에 대한 IAM 임시 보안 자격 증명을 생성하도록 애플리케이션을 구성합니다.
540

IT Certification Guaranteed, The Easy Way!
B. 사용자가 인증될 때 미리 서명된 URL을 생성하는 AWS Lambda 함수를 만듭니다.
C. 애플리케이션을 통한 S3 버킷 직접 액세스를 제어하고 기록하기 위해 Amazon Cognito와
통합되는 사용자 정의 인증 서비스를 개발합니다.
D. AWS 보안 토큰 서비스(AWS STS)를 사용해 인증된 사용자에게 비디오를 S3 버킷에 직접
업로드할 수 있는 임시 권한을 부여하는 사전 정의된 IAM 역할을 맡습니다.
Answer: B
* Option B: Pre-signed URLs provide temporary, authenticated access to S3, limiting uploads
to the time frame specified. This solution is lightweight, efficient, and easy to implement.
* Option A requires the management of IAM temporary credentials, adding complexity.
* Option C involves unnecessary development effort.
* Option D introduces more complexity with STS and roles than pre-signed URLs.
QUESTION NO: 787
회사는 Amazon S3를 사용하여 RTS 시알릭 웹사이트를 호스팅합니다. 회사는 웹페이지에
문의 양식을 추가하려고 합니다. 문의 양식에는 사용자가 이름, 이메일 주소, 전화번호 및
사용자 메시지를 입력할 수 있는 동적 서버-스케일 구성 요소가 있습니다. 회사는 다음을
예상합니다. 매달 사이트 방문 횟수가 100회 미만이 될 것입니다. 어떤 솔루션이 이러한 요구
사항을 가장 비용 효율적으로 충족합니까?
A. Amazon Elastic Container Service(Amazon ECS)에서 동적 문의 양식 페이지를
호스팅합니다. 타사 이메일 공급자에 연결하려면 Amazon Simple Email Service(Amazon
SES)를 설정하세요.
B. Amazon Simple Email Service(Amazon SES)를 호출하는 AWS Lambda 백엔드로 Amazon
API Gateway 엔드포인트를 생성합니다.
C. Amazon Ughtsail을 배포하여 정적 웹 페이지를 동적으로 변환합니다. 클라이언트 측
scnpting을 사용하여 문의 양식 작성 양식을 Amazon WorkMail과 통합합니다.
D. Q 마이크로 Amazon EC2 인스턴스 생성 LAMP(Linux Apache MySQL. PHP/Perl/Python)
스택을 배포하여 웹 페이지 호스팅 클라이언트 측 스크립팅을 사용하여 문의 양식 작성
양식을 Amazon WorkMail과 통합
Answer: D
Explanation:
Create a t2 micro Amazon EC2 instance. Deploy a LAMP (Linux Apache MySQL,
PHP/Perl/Python) stack to host the webpage. Use client-side scripting to build the contact
form. Integrate the form with Amazon WorkMail. This solution will provide the company with
the necessary components to host the contact form page and integrate it with Amazon
WorkMail at the lowest cost. Option A requires the use of Amazon ECS, which is more
expensive than EC2, and Option B requires the use of Amazon API Gateway, which is also
more expensive than EC2. Option C requires the use of Amazon Lightsail, which is more
expensive than EC2.
Using AWS Lambda with Amazon API Gateway - AWS Lambda
https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html
AWS Lambda FAQs
https://aws.amazon.com/lambda/faqs/
QUESTION NO: 788
한 회사가 AWS 클라우드에 애플리케이션을 구축하고 있습니다. 애플리케이션은 두 AWS
541

IT Certification Guaranteed, The Easy Way!
지역의 Amazon S3 버킷에 데이터를 저장합니다. 회사는 AWS Key Management
Service(AWS KMS) 고객 관리형 키를 사용하여 S3 버킷에 저장된 모든 데이터를 암호화해야
합니다. 두 S3 버킷의 데이터는 동일한 KMS 키를 사용하여 암호화 및 해독되어야 합니다.
데이터와 키는 두 리전에 각각 저장되어야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 각 리전에서 S3 버킷을 생성합니다. Amazon S3 관리형 암호화 키(SSE-S3)를 사용하여
서버 측 암호화를 사용하도록 S3 버킷을 구성합니다. S3 버킷 간 복제를 구성합니다.
B. 고객 관리형 다중 리전 KMS 키를 생성합니다. 각 리전에 S3 버킷을 생성합니다. S3 버킷
간의 복제를 구성합니다. 클라이언트 측 암호화와 함께 KMS 키를 사용하도록 애플리케이션을
구성합니다.
C. 각 리전에서 고객 관리형 KMS 키와 S3 버킷을 생성합니다. Amazon S3 관리형 암호화
키(SSE-S3)로 서버 측 암호화를 사용하도록 S3 버킷을 구성합니다. S3 버킷 간 복제를
구성합니다.
D. 각 리전에서 고객 관리형 KMS 키와 S3 버킷을 생성합니다. AWS KMS 키(SSE-KMS)로
서버 측 암호화를 사용하도록 S3 버킷을 구성합니다. S3 버킷 간 복제를 구성합니다.
Answer: B
Explanation:
From https://docs.aws.amazon.com/kms/latest/developerguide/custom-key-store-
overview.html For most users, the default AWS KMS key store, which is protected by FIPS
140-2 validated cryptographic modules, fulfills their security requirements. There is no need
to add an extra layer of maintenance responsibility or a dependency on an additional service.
However, you might consider creating a custom key store if your organization has any of the
following requirements: Key material cannot be stored in a shared environment. Key material
must be subject to a secondary, independent audit path. The HSMs that generate and store
key material must be certified at FIPS 140-2 Level 3.
https://docs.aws.amazon.com/kms/latest
/developerguide/custom-key-store-overview.html
https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html
QUESTION NO: 789
한 회사가 AWS 클라우드에서 3계층 전자상거래 애플리케이션을 호스팅하고 있습니다. 이
회사는 Amazon S3에서 웹사이트를 호스팅하고 판매 요청을 처리하는 API와 웹사이트를
통합합니다. 이 회사는 ALB(Application Load Balancer) 뒤에 있는 3개의 Amazon EC2
인스턴스에서 API를 호스팅합니다. API는 판매 요청을 비동기적으로 처리하는 백엔드
작업자와 함께 정적 및 동적 프런트엔드 콘텐츠로 구성됩니다.
회사에서는 신제품 출시 이벤트 기간 동안 판매 요청 수가 급격히 증가할 것으로 예상하고
있습니다. 모든 요청이 성공적으로 처리되도록 솔루션 설계자가 권장해야 하는 것은
무엇입니까?
A. 동적 콘텐츠를 위한 Amazon CloudFront 배포를 추가합니다. 트래픽 증가를 처리하기 위해
EC2 인스턴스 수를 늘립니다.
B. 정적 콘텐츠에 대한 Amazon CloudFront 배포를 추가합니다. EC2 인스턴스를 Auto Scaling
그룹에 배치하여 네트워크 트래픽을 기반으로 새 인스턴스를 시작합니다.
C. 동적 콘텐츠를 위한 Amazon CloudFront 배포를 추가합니다. API가 처리할 트래픽을
줄이려면 ALB 앞에 Amazon ElastiCache 인스턴스를 추가하세요.
D. 정적 콘텐츠에 대한 Amazon CloudFront 배포를 추가합니다. 나중에 EC2 인스턴스에서
542

IT Certification Guaranteed, The Easy Way!
처리할 웹 사이트의 요청을 수신하려면 Amazon Simple Queue Service(Amazon SOS)
대기열을 추가하세요.
Answer: B
Explanation:
This option is the most efficient because it uses Amazon CloudFront, which is a web service
that speeds up distribution of your static and dynamic web content, such as .html, .css, .js,
and image files, to your users1. It also uses a CloudFront distribution for the static content,
which reduces the load on the EC2 instances and improves the performance and availability
of the website. It also uses an Auto Scaling group to launch new instances based on network
traffic, which automatically adjusts the compute capacity of your EC2 instances based on
load or a schedule2. This solution meets the requirement of ensuring that all the requests are
processed successfully during events for the launch of new products. Option A is less
efficient because it uses a CloudFront distribution for the dynamic content, which is not
necessary as the dynamic content is already handled by the API on the EC2 instances. It
also increases the number of EC2 instances to handle the increase in traffic, which could
incur higher costs and complexity than using an Auto Scaling group. Option C is less efficient
because it uses an Amazon ElastiCache instance in front of the ALB to reduce traffic for the
API to handle, which is a way to provide a fully managed in-memory data store service that
provides sub-millisecond latency for caching and data processing3. However, this could
introduce additional complexity and latency, and does not scale automatically based on
network traffic. Option D is less efficient because it uses an Amazon Simple Queue Service
(Amazon SQS) queue to receive requests from the website for later processing by the EC2
instances, which is a way to send, store, and receive messages between software
components at any volume. However, this does not provide a faster response time to the
users as they have to wait for their requests to be processed by the EC2 instances.
QUESTION NO: 790
설립된 지 4년 된 한 미디어 회사는 AWS 계정을 구성하기 위해 AWS Organizations의 모든
기능 세트를 사용하고 있습니다. 회사 재무팀에 따르면, 회원 계정의 결제 정보는 회원 계정의
루트 사용자를 포함하여 누구에게도 접근할 수 없어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 모든 재무팀 사용자를 IAM 그룹에 추가합니다. Billing이라는 AWS 관리형 정책을 그룹에
연결합니다.
B. 루트 사용자를 포함한 모든 사용자의 청구 정보에 대한 액세스를 거부하는 ID 기반 정책을
연결합니다.
C. 청구 정보에 대한 액세스를 거부하는 서비스 제어 정책(SCP)을 만듭니다. SCP를 루트 조직
단위(OU)에 연결합니다.
D. 조직의 모든 기능 기능 세트를 조직 통합 결제 기능 세트로 변환합니다.
Answer: C
Explanation:
Service Control Policies (SCP): SCPs are an integral part of AWS Organizations and allow
you to set fine- grained permissions on the organizational units (OUs) within your AWS
Organization. SCPs provide central control over the maximum permissions that can be
granted to member accounts, including the root user.
Denying Access to Billing Information: By creating an SCP and attaching it to the root OU,
543

IT Certification Guaranteed, The Easy Way!
you can explicitly deny access to billing information for all accounts within the organization.
SCPs can be used to restrict access to various AWS services and actions, including billing-
related services. Granular Control: SCPs enable you to define specific permissions and
restrictions at the organizational unit level. By denying access to billing information at the root
OU, you can ensure that no member accounts, including root users, have access to the
billing information.
QUESTION NO: 791
회사는 의료 실험 결과를 Amazon S3 리포지토리에 저장해야 합니다. 저장소는 소수의
과학자가 새 파일을 추가할 수 있도록 허용해야 하며 다른 모든 사용자는 읽기 전용 액세스로
제한해야 합니다. 어떤 사용자도 저장소의 파일을 수정하거나 삭제할 수 없습니다. 회사는
모든 파일을 생성일로부터 최소 1년 동안 저장소에 보관해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 법적 보유 기간이 1년인 거버넌스 모드에서 S3 객체 잠금 사용
B. 보존 기간이 365일인 규정 준수 모드에서 S3 객체 잠금을 사용합니다.
C. IAM 역할을 사용하여 모든 사용자가 S3 버킷의 객체를 삭제하거나 변경하지 못하도록
제한합니다. S3 버킷 정책을 사용하여 IAM 역할만 허용합니다.
D. 객체가 추가될 때마다 AWS Lambda 함수를 호출하도록 S3 버킷을 구성합니다. 수정된
객체가 그에 따라 표시될 수 있도록 저장된 객체의 해시를 추적하는 함수를 구성합니다.
Answer: B
Explanation:
In compliance mode, a protected object version can't be overwritten or deleted by any user,
including the root user in your AWS account. When an object is locked in compliance mode,
its retention mode can't be changed, and its retention period can't be shortened. Compliance
mode helps ensure that an object version can't be overwritten or deleted for the duration of
the retention period. In governance mode, users can't overwrite or delete an object version or
alter its lock settings unless they have special permissions. With governance mode, you
protect objects against being deleted by most users, but you can still grant some users
permission to alter the retention settings or delete the object if necessary. In Governance
mode, Objects can be deleted by some users with special permissions, this is against the
requirement.
Compliance:
- Object versions can't be overwritten or deleted by any user, including the root user
- Objects retention modes can't be changed, and retention periods can't be shortened
Governance:
- Most users can't overwrite or delete an object version or alter its lock settings
- Some users have special permissions to change the retention or delete the object
QUESTION NO: 792
한 회사에서 모바일 앱 사용자를 대상으로 하는 마케팅 커뮤니케이션 서비스를 개발하고
있습니다. 회사는 SMS(Short Message Service)를 통해 사용자에게 확인 메시지를 보내야
합니다. 사용자는 SMS 메시지에 응답할 수 있어야 합니다. 회사는 분석을 위해 응답을 1년
동안 저장해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. Amazon Connect 고객 응대 흐름을 생성하여 SMS 메시지를 보냅니다. AWS Lambda를
544

IT Certification Guaranteed, The Easy Way!
사용하여 응답을 처리합니다.
B. Amazon Pinpoint 여정을 구축하세요. 분석 및 보관을 위해 Amazon Kinesis 데이터
스트림으로 이벤트를 보내도록 Amazon Pinpoint를 구성합니다.
C. Amazon Simple Queue Service(Amazon SQS)를 사용하여 SMS 메시지를 배포합니다.
AWS Lambda를 사용하여 응답을 처리합니다.
D. Amazon Simple 알림 서비스(Amazon SNS) FIFO 주제를 생성합니다. 분석 및 보관을 위해
SNS 주제에 Amazon Kinesis 데이터 스트림을 구독합니다.
Answer: B
Explanation:
https://aws.amazon.com/pinpoint/product-details/sms/ Two-Way Messaging: Receive SMS
messages from your customers and reply back to them in a chat-like interactive experience.
With Amazon Pinpoint, you can create automatic responses when customers send you
messages that contain certain keywords. You can even use Amazon Lex to create
conversational bots. A majority of mobile phone users read incoming SMS messages almost
immediately after receiving them. If you need to be able to provide your customers with
urgent or important information, SMS messaging may be the right solution for you. You can
use Amazon Pinpoint to create targeted groups of customers, and then send them campaign-
based messages. You can also use Amazon Pinpoint to send direct messages, such as
appointment confirmations, order updates, and one- time passwords.
QUESTION NO: 793
미디어 회사는 온프레미스에서 사용자 활동 데이터를 수집하고 분석합니다. 회사는 이 기능을
AWS로 마이그레이션하려고 합니다. 사용자 활동 데이터 저장소는 계속해서 성장하여 크기가
페타바이트에 이를 것입니다. 회사는 SQL을 통해 기존 데이터와 새 데이터에 대한 온디맨드
분석을 용이하게 하는 고가용성 데이터 수집 솔루션을 구축해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 활동 데이터를 Amazon Kinesis 데이터 스트림으로 보냅니다. Amazon S3 버킷에 데이터를
전송하도록 스트림을 구성합니다.
B. 활동 데이터를 Amazon Kinesis Data Firehose 전송 스트림으로 보냅니다. Amazon
Redshift 클러스터에 데이터를 전달하도록 스트림을 구성합니다.
C. Amazon S3 버킷에 활동 데이터를 저장합니다. 데이터가 S3 버킷에 도착하면 데이터에
대해 AWS Lambda 함수를 실행하도록 Amazon S3를 구성합니다.
D. 여러 가용 영역에 분산된 Amazon EC2 인스턴스에 수집 서비스를 생성합니다. Amazon
RDS 다중 AZ 데이터베이스로 데이터를 전달하도록 서비스를 구성합니다.
Answer: B
Explanation:
Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud.
You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This
allows you to use your data to gain new insights for your business and customers. The first
step to create a data warehouse is to launch a set of nodes, called an Amazon Redshift
cluster. After you provision your cluster, you can upload your data set and then perform data
analysis queries. Regardless of the size of the data set, Amazon Redshift offers fast query
performance using the same SQL-based tools and business intelligence applications that you
use today.
545

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 794
한 회사가 Amazon Elastic Kubernetes Service(Amazon EKS)에 배포될 마이크로서비스 기반
애플리케이션을 구축하고 있습니다. 마이크로서비스는 서로 상호 작용합니다. 회사는 향후
성능 문제를 식별하기 위해 애플리케이션을 관찰할 수 있는지 확인하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon ElastiCache를 사용하여 마이크로서비스로 전송되는 요청 수를 줄이도록
애플리케이션을 구성합니다.
B. EKS 클러스터에서 지표를 수집하도록 Amazon CloudWatch Container Insights를
구성합니다. 마이크로서비스 간 요청을 추적하도록 AWS X-Ray를 구성합니다.
C. API 호출을 검토하도록 AWS CloudTrail을 구성합니다. Amazon QuickSight 대시보드를
구축하여 마이크로서비스 상호 작용을 관찰하십시오.
D. AWS Trusted Advisor를 사용하여 애플리케이션 성능을 이해합니다.
Answer: B
Explanation:
This solution meets the requirements because it enables the company to observe the
performance and behavior of its microservices-based application on Amazon EKS. Amazon
CloudWatch Container Insights is a feature that collects, aggregates, and summarizes
metrics and logs from containerized applications and microservices. Container Insights
integrates with Amazon EKS and Kubernetes to provide metrics at the cluster, node, pod,
task, and service level. You can use Container Insights to monitor the CPU, memory, disk,
and network utilization of your EKS clusters and identify bottlenecks, latency spikes, and
other issues. AWS X-Ray is a service that collects data about requests that your application
serves, and provides tools that you can use to view, filter, and gain insights into that data. X-
Ray integrates with Amazon EKS and Kubernetes to trace the requests that your
microservices make to downstream AWS resources, microservices, databases, and web
APIs. You can use X-Ray to analyze the root cause of errors, faults, and performance issues,
and visualize the service map of your application.
References:
* Using Container Insights
* AWS X-Ray
QUESTION NO: 795
회사는 Amazon EC2 인스턴스에서 마이크로서비스 애플리케이션을 실행하고 있습니다. 이
회사는 확장성을 위해 애플리케이션을 Amazon Elastic Kubernetes Service(Amazon EKS)
클러스터로 마이그레이션하려고 합니다. 회사는 보안 규정 준수를 유지하기 위해 엔드포인트
프라이빗 액세스를 true로 설정하고 엔드포인트 퍼블릭 액세스를 false로 설정하여 Amazon
EKS 제어 플레인을 구성해야 합니다. 또한 데이터 플레인을 프라이빗 서브넷에 배치해야
합니다. 그러나 회사는 노드가 클러스터에 가입할 수 없기 때문에 오류 알림을 받았습니다.
노드가 클러스터에 가입하도록 허용하는 솔루션은 무엇입니까?
A. AWS Identity and Access Management(1AM)에서 필요한 권한을 AmazonEKSNodeRole
1AM 역할에 부여합니다.
B. 노드가 컨트롤 플레인에 액세스할 수 있도록 인터페이스 VPC 엔드포인트를 생성합니다.
C. 퍼블릭 서브넷에서 노드 재생성 EC2 노드에 대한 보안 그룹 제한
D. 노드의 보안 그룹에서 아웃바운드 트래픽을 허용합니다.
546

IT Certification Guaranteed, The Easy Way!
Answer: B
Explanation:
Kubernetes API requests within your cluster's VPC (such as node to control plane
communication) use the private VPC endpoint.
https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html
QUESTION NO: 796
솔루션 아키텍트가 새 VPC 설계를 생성 중입니다. 로드 밸런서용 퍼블릭 서브넷 2개, 웹
서버용 프라이빗 서브넷 2개, MySQL용 프라이빗 서브넷 2개가 있습니다. 웹 서버는
HTTPS만 사용합니다. 솔루션 아키텍트는 로드 밸런서를 위한 보안 그룹을 이미
생성했습니다. 0 0 0 0/0에서 포트 443을 허용합니다. 회사 정책에 따라 각 리소스에는 차가
있어야 합니다! 작업을 계속 수행하려면 액세스가 필요합니다. 솔루션 설계자가 이러한 요구
사항을 충족하기 위해 사용해야 하는 추가 구성 전략은 무엇입니까?
A. 웹 서버용 보안 그룹을 생성하고 0.0.0.0/0에서 포트 443을 허용합니다. MySQL 서버용
보안 그룹을 생성하고 웹 서버 보안 그룹에서 포트 3306을 허용합니다.
B. 웹 서버에 대한 네트워크 ACL을 생성하고 0.0.0.0/0에서 포트 443을 허용합니다. 네트워크
ACL(또는 MySQL 서버)을 생성하고 웹 서버 보안 그룹에서 포트 3306을 허용합니다.
C. 웹 서버용 보안 그룹을 생성하고 로드 밸런서에서 포트 443을 허용합니다. MySQL 서버용
보안 그룹을 생성하고 웹 서버 보안 그룹에서 포트 3306을 허용합니다.
D. 네트워크 ACL 또는 웹 서버를 생성하고 로드 밸런서에서 포트 443을 허용합니다. MySQL
서버용 네트워크 ACL을 생성하고 웹 서버 보안 그룹에서 포트 3306을 허용합니다.
Answer: C
Explanation:
This answer is correct because it provides a resilient and durable replacement for the on-
premises file share that is compatible with Windows IIS web servers. Amazon FSx for
Windows File Server is a fully managed service that provides shared file storage built on
Windows Server. It supports the SMB protocol and integrates with Microsoft Active Directory,
which enables seamless access and authentication for Windows-based applications. Amazon
FSx for Windows File Server also offers the following benefits:
* Resilience: Amazon FSx for Windows File Server can be deployed in multiple Availability
Zones, which provides high availability and failover protection. It also supports automatic
backups and restores, as well as self-healing features that detect and correct issues.
* Durability: Amazon FSx for Windows File Server replicates data within and across
Availability Zones, and stores data on highly durable storage devices. It also supports
encryption at rest and in transit, as well as file access auditing and data deduplication.
* Performance: Amazon FSx for Windows File Server delivers consistent sub-millisecond
latencies and high throughput for file operations. It also supports SSD storage, native
Windows features such as Distributed File System (DFS) Namespaces and Replication, and
user-driven performance scaling.
By configuring the Amazon FSx file share to use an AWS KMS CMK to encrypt the images in
the file share, the company can protect the images from unauthorized access and comply
with company policy. By using NTFS permission sets on the images, the company can
prevent accidental deletion of the images by restricting who can modify or delete them.
References:
* Amazon FSx for Windows File Server
547

IT Certification Guaranteed, The Easy Way!
* Using Microsoft Windows file shares
QUESTION NO: 797
회사에 여행 발권을 위한 웹 애플리케이션이 있습니다. 이 애플리케이션은 북미 지역의 단일
데이터 센터에서 실행되는 데이터베이스를 기반으로 합니다. 회사는 글로벌 사용자 기반에
서비스를 제공하기 위해 응용 프로그램을 확장하려고 합니다.
회사는 애플리케이션을 여러 AWS 리전에 배포해야 합니다. 예약 데이터베이스 업데이트 시
평균 대기 시간은 1초 미만이어야 합니다.
이 회사는 여러 지역에 걸쳐 웹 플랫폼을 별도로 배포하려고 합니다. 그러나 회사는 전
세계적으로 일관된 단일 기본 예약 데이터베이스를 유지해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 솔루션을 권장해야 합니까?
A. Amazon DynamoDB를 사용하도록 애플리케이션을 변환합니다. 중앙 예약 테이블에 전역
테이블을 사용합니다.
각 지역 배포에서 올바른 지역 엔드포인트를 사용합니다.
B. 데이터베이스를 Amazon Aurora MySQL 데이터베이스로 마이그레이션합니다. 각 지역에
Aurora 읽기 전용 복제본을 배포합니다. 데이터베이스에 액세스하려면 각 지역 배포에서
올바른 지역 엔드포인트를 사용하세요.
C. 데이터베이스를 MySQL 데이터베이스용 Amazon RDS로 마이그레이션합니다. 각 리전에
MySQL 읽기 전용 복제본을 배포합니다. 데이터베이스에 액세스하려면 각 지역 배포에서
올바른 지역 엔드포인트를 사용하세요.
D. 애플리케이션을 Amazon Aurora Serverless 데이터베이스로 마이그레이션합니다. 각
지역에 데이터베이스 인스턴스를 배포합니다. 각 지역 배포에서 올바른 지역 엔드포인트를
사용하여 데이터베이스에 액세스합니다. AWS Lambda 함수를 사용하여 각 지역에서 이벤트
스트림을 처리하여 데이터베이스를 동기화합니다.
Answer: B
Explanation:
https://aws.amazon.com/rds/aurora/global-database/
https://aws.amazon.com/blogs/architecture/using-amazon-aurora-global-database-for-low-
latency-without- application-changes/
QUESTION NO: 798
한 회사가 AWS에서 멀티플레이어 게임 애플리케이션을 호스팅하고 있습니다. 회사는
애플리케이션이 밀리초 미만의 대기 시간으로 데이터를 읽고 기록 데이터에 대해 일회성
쿼리를 실행하기를 원합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 자주 액세스하는 데이터에는 Amazon RDS를 사용하십시오. 정기적인 사용자 지정
스크립트를 실행하여 데이터를 Amazon S3 버킷으로 내보냅니다.
B. 데이터를 Amazon S3 버킷에 직접 저장합니다. 장기 저장을 위해 오래된 데이터를 S3
Glacier Deep Archive로 이동하는 S3 수명 주기 정책을 구현합니다. Amazon Athena를
사용하여 Amazon S3의 데이터에 대해 일회성 쿼리 실행
C. 자주 액세스하는 데이터에는 DynamoDB Accelerator(DAX)와 함께 Amazon DynamoDB를
사용합니다. DynamoDB 테이블 내보내기를 사용하여 데이터를 Amazon S3 버킷으로
내보냅니다. Amazon Athena를 사용하여 Amazon S3의 데이터에 대해 일회성 쿼리를
실행합니다.
D. 자주 액세스하는 데이터에 Amazon DynamoDB를 사용합니다. Amazon Kinesis Data
548

IT Certification Guaranteed, The Easy Way!
Streams로의 스트리밍을 켭니다. Amazon Kinesis Data Firehose를 사용하여 Kinesis Data
Streams에서 데이터를 읽습니다. Amazon S3 버킷에 레코드를 저장합니다.
Answer: C
Explanation:
As they would like to retrieve the data with sub-millisecond, DynamoDB with DAX is the
answer.
DynamoDB supports some of the world's largest scale applications by providing consistent,
single-digit millisecond response times at any scale. You can build applications with virtually
unlimited throughput and storage.
https://aws.amazon.com/dynamodb/dax/?nc1=h_ls
QUESTION NO: 799
회사는 다른 지역의 워크로드와 격리된 워크로드를 지원하고 실행하기 위해 AWS 지역
전체에 여러 VPC를 보유하고 있습니다. 최근 애플리케이션 시작 요구 사항으로 인해 회사의
VPC는 ​​모든 지역의 다른 모든 VPC와 통신해야 합니다.
최소한의 관리 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. VPC 피어링을 사용하여 단일 리전에서 VPC 통신을 관리합니다. 리전 간 VPC 피어링을
사용하여 VPC 통신을 관리합니다.
B. 모든 지역에서 AWS Direct Connect 게이트웨이를 사용하여 여러 지역에서 VPC를
연결하고 VPC 통신을 관리합니다.
C. AWS Transit Gateway를 사용하여 단일 지역의 VPC 통신을 관리하고 지역 간 Transit
Gateway 피어링을 사용하여 VPC 통신을 관리합니다.
D. 모든 지역에서 AWS PrivateLink를 사용하여 여러 지역에서 VPC를 연결하고 VPC 통신을
관리합니다.
Answer: C
Explanation:
* Understanding the Requirement: The company needs to enable communication between
VPCs across multiple AWS Regions with minimal administrative effort.
* Analysis of Options:
* VPC Peering: Managing multiple VPC peering connections across regions is complex and
difficult to scale, leading to significant administrative overhead.
* AWS Direct Connect Gateways: Primarily used for creating private connections between
AWS and on-premises environments, not for inter-VPC communication across regions.
* AWS Transit Gateway: Simplifies VPC interconnections within a region and supports
Transit Gateway peering for cross-region connectivity, reducing administrative complexity.
* AWS PrivateLink: Used for accessing AWS services and third-party services over a private
connection, not for inter-VPC communication.
* Best Solution:
* AWS Transit Gateway with Transit Gateway Peering: This option provides a scalable and
efficient solution for managing VPC communications both within a single region and across
multiple regions with minimal administrative overhead.
References:
* AWS Transit Gateway
* Transit Gateway Peering
549

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 800
한 회사가 AWS 클라우드에 데이터 레이크를 구현하려고 합니다. 회사는 특정 팀만 데이터
레이크의 민감한 데이터에 액세스할 수 있도록 해야 합니다. 회사는 데이터 레이크에 대한 행
수준 액세스 제어를 가져야 합니다.
옵션:
A. Amazon RDS를 사용하여 데이터를 저장합니다. 데이터 거버넌스와 액세스 제어를 위해
IAM 역할과 권한을 사용합니다.
B. Amazon Redshift를 사용하여 데이터를 저장합니다. 데이터 거버넌스와 액세스 제어를
위해 IAM 역할과 권한을 사용합니다.
C. Amazon S3를 사용하여 데이터를 저장합니다. AWS Lake Formation을 사용하여 데이터
거버넌스와 액세스 제어를 수행합니다.
D. AWS Glue Catalog를 사용하여 데이터를 저장합니다. AWS Glue DataBrew를 사용하여
데이터 거버넌스와 액세스 제어를 수행합니다.
Answer: C
Explanation:
Detailed Explanation:
* A. RDS: Suitable for relational databases but does not provide native support for data lakes
or row- level access.
* B. Redshift: Primarily for analytics, not designed for large-scale data lake governance.
* C. S3 + Lake Formation: Provides native support for data lakes with granular access
control, including row-level permissions.
* D. Glue Catalog + DataBrew: Focused on data preparation and metadata management, not
row-level access control.
References: AWS Lake Formation
QUESTION NO: 801
회사는 ALB(Application Load Balancer)를 사용하여 애플리케이션을 인터넷에 제공하고
있습니다. 회사는 애플리케이션 전체에서 비정상적인 트래픽 액세스 패턴을 발견합니다.
솔루션 설계자는 회사가 이러한 이상 현상을 더 잘 이해할 수 있도록 인프라에 대한 가시성을
향상해야 합니다.
이러한 요구 사항을 충족하는 가장 운영 효율적인 솔루션은 무엇입니까?
A. Amazon Athena에서 AWS CloudTrail 로그용 테이블을 생성합니다. 관련 정보에 대한
쿼리를 만듭니다.
B. Amazon S3에 대한 ALB 액세스 로깅을 활성화합니다. Amazon Athena에서 테이블을
생성하고 로그를 쿼리합니다.
C. Amazon S3에 대한 ALB 액세스 로깅 활성화 텍스트 편집기에서 각 파일을 열고 각 줄에서
관련 정보를 검색합니다.
D. 전용 Amazon EC2 인스턴스에서 Amazon EMR을 사용하여 ALB에 직접 쿼리하여 트래픽
액세스 로그 정보를 획득합니다.
Answer: B
Explanation:
This solution meets the requirements because it allows the company to improve visibility into
the infrastructure by using ALB access logging and Amazon Athena. ALB access logging is a
feature that captures detailed information about requests sent to the load balancer, such as
550

IT Certification Guaranteed, The Easy Way!
the client's IP address, request path, response code, and latency. By enabling ALB access
logging to Amazon S3, the company can store the access logs in an S3 bucket as
compressed files. Amazon Athena is an interactive query service that makes it easy to
analyze data in Amazon S3 using standard SQL. By creating a table in Amazon Athena for
the access logs, the company can query the logs and get results in seconds. This way, the
company can better understand the abnormal traffic access patterns across the application.
References:
* Access logs for your Application Load Balancer
* Querying Application Load Balancer Logs
QUESTION NO: 802
의료 회사가 여러 고객으로부터 온 방대한 양의 임상 시험 데이터에 대한 변환을 수행하려고
합니다. 이 회사는 고객 데이터가 포함된 관계형 데이터베이스에서 데이터를 추출해야
합니다. 그런 다음 이 회사는 일련의 복잡한 규칙을 사용하여 데이터를 변환합니다. 이 회사는
변환이 완료되면 Amazon S3에 데이터를 로드합니다.
모든 데이터는 회사가 Amazon S3에 데이터를 저장하기 전에 처리되는 곳에서 암호화되어야
합니다. 모든 데이터는 고객별 키를 사용하여 암호화되어야 합니다.
어떤 솔루션이 최소한의 운영 노력으로 이러한 요구 사항을 충족할 수 있을까요?
A. 고객별로 AWS Glue 작업을 하나씩 만듭니다. Amazon S3 관리 키(SSE-S3)를 사용하여
서버 측 암호화를 사용하고 데이터를 암호화하는 보안 구성을 각 작업에 연결합니다.
B. 고객별로 Amazon EMR 클러스터를 하나씩 만듭니다. 사용자 지정 클라이언트 측 루트
키(CSE-Custom)로 데이터를 암호화하여 클라이언트 측 암호화를 사용하는 보안 구성을 각
클러스터에 연결합니다.
C. 고객별로 AWS Glue 작업을 하나씩 만듭니다. AWS KMS 관리 키(CSE-KMS)를 사용하여
클라이언트 측 암호화를 사용하고 데이터를 암호화하는 보안 구성을 각 작업에 연결합니다.
D. 고객별로 Amazon EMR 클러스터를 하나씩 만듭니다. AWS KMS 키(SSE-KMS)를
사용하여 서버 측 암호화를 사용하는 보안 구성을 각 클러스터에 연결하여 데이터를
암호화합니다.
Answer: C
Explanation:
AWS Glue jobs are designed for extract, transform, and load (ETL) operations, which are
perfect for transforming clinical trial data. AWS Glue integrates with AWS Key Management
Service (KMS), allowing for customer-specific encryption keys, fulfilling the encryption
requirement with minimal operational effort.
Client-side encryption with AWS KMS ensures that the data is encrypted before it is sent to
S3, aligning with the security needs specified in the scenario.
Key aspects:
* AWS Glue: This managed ETL service simplifies data transformation, reduces operational
overhead, and integrates seamlessly with KMS.
* CSE-KMS: Client-side encryption with KMS ensures that the data is encrypted with
customer-specific keys before it is processed or stored in S3, offering robust security.
* Minimal Operational Overhead: Compared to managing an EMR cluster, AWS Glue
automates much of the process, making it a lower-effort solution.
* AWS Documentation: According to the AWS Well-Architected Framework, encryption with
AWS KMS offers strong security controls that meet the needs of industries requiring high
551

IT Certification Guaranteed, The Easy Way!
levels of confidentiality.
QUESTION NO: 803
한 회사가 다년간의 마이그레이션 프로젝트 중에 데이터와 애플리케이션을 AWS로 이전하고
있습니다. 회사는 회사의 AWS 리전과 회사의 온프레미스 위치에서 Amazon S3의 데이터에
안전하게 액세스하려고 합니다. 데이터가 인터넷을 통과해서는 안 됩니다. 회사는 해당
지역과 온프레미스 위치 간에 AWS Direct Connect 연결을 설정했습니다. 어떤 솔루션이
이러한 요구 사항을 충족합니까?
A. Amazon S3용 게이트웨이 엔드포인트를 생성합니다. 게이트웨이 엔드포인트를 사용하여
지역 및 온프레미스 위치의 데이터에 안전하게 액세스하세요.
B. AWS Transit Gateway에서 게이트웨이를 생성하여 리전 및 온프레미스 위치에서 Amazon
S3에 안전하게 액세스합니다.
C. Amazon S3용 인터페이스 엔드포인트 생성_ 인터페이스 엔드포인트를 사용하여 지역 및
온프레미스 위치의 데이터에 안전하게 액세스합니다.
D. AWS Key Management Service(AWS KMS) 키를 사용하여 리전 및 온프레미스 위치에서
데이터에 안전하게 액세스합니다.
Answer: B
Explanation:
A gateway endpoint is a gateway that is a target for a specified route in your route table, used
for traffic destined to a supported AWS service1. Amazon S3 does not support gateway
endpoints, only interface endpoints2. Therefore, option A is incorrect.
An interface endpoint is an elastic network interface with a private IP address that serves as
an entry point for traffic destined to a supported service1. An interface endpoint can provide
secure access to Amazon S3 from within the Region, but not from the on-premises location.
Therefore, option C is incorrect.
AWS Key Management Service (AWS KMS) is a service that allows you to create and
manage encryption keys to protect your data3. AWS KMS does not provide a way to access
data on Amazon S3 without traversing the internet. Therefore, option D is incorrect.
AWS Transit Gateway is a service that enables you to connect your Amazon Virtual Private
Clouds (VPCs) and your on-premises networks to a single gateway. You can create a
gateway in AWS Transit Gateway to access Amazon S3 securely from both the Region and
the on-premises location using AWS Direct Connect.
Therefore, option B is correct.
QUESTION NO: 804
솔루션 설계자가 다중 서브넷 VPC 아키텍처를 개발 중입니다. 솔루션은 2개의 가용 영역에
있는 6개의 서브넷으로 구성됩니다. 서브넷은 공용, 사설 및 데이터베이스 전용으로
정의됩니다. 프라이빗 서브넷에서 실행되는 Amazon EC2 인스턴스만 데이터베이스에
액세스할 수 있어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 퍼블릭 서브넷의 CIDR 블록에 대한 경로를 제외하는 이제 라우팅 테이블을 생성합니다.
라우팅 테이블을 데이터베이스 서브넷에 연결합니다.
B. 퍼블릭 서브넷의 인스턴스가 사용하는 보안 그룹으로부터의 인그레스를 거부하는 보안
그룹을 생성합니다. 보안 그룹을 Amazon RDS DB 인스턴스에 연결합니다.
C. 프라이빗 서브넷의 인스턴스가 사용하는 보안 그룹에서 유입을 허용하는 보안 그룹을
552

IT Certification Guaranteed, The Easy Way!
생성합니다. 보안 그룹을 Amazon RDS DB 인스턴스에 연결합니다.
D. 퍼블릭 서브넷과 프라이빗 서브넷 사이에 새로운 피어링 연결을 생성합니다. 프라이빗
서브넷과 데이터베이스 서브넷 간에 다른 피어링 연결을 만듭니다.
Answer: C
Explanation:
Security groups are stateful. All inbound traffic is blocked by default. If you create an inbound
rule allowing traffic in, that traffic is automatically allowed back out again. You cannot block
specific IP address using Security groups (instead use Network Access Control Lists).
"You can specify allow rules, but not deny rules." "When you first create a security group, it
has no inbound rules. Therefore, no inbound traffic originating from another host to your
instance is allowed until you add inbound rules to the security group." Source:
https://docs.aws.amazon.com/vpc/latest/userguide
/VPC_SecurityGroups.html#VPCSecurityGroups
QUESTION NO: 805
회사는 단일 공장에 있는 여러 기계에서 매일 10TB의 계측 데이터를 수신합니다.
데이터는 공장 내에 위치한 온프레미스 데이터 센터의 SAN(Storage Area Network)에 저장된
JSON 파일로 구성됩니다. 회사는 이 데이터를 Amazon S3로 전송하여 실시간에 가까운
중요한 분석을 제공하는 여러 추가 시스템에서 액세스할 수 있기를 원합니다. 데이터가
민감한 것으로 간주되기 때문에 안전한 전송이 중요합니다.
가장 안정적인 데이터 전송을 제공하는 솔루션은 무엇입니까?
A. 공용 인터넷을 통한 AWS DataSync
B. AWS Direct Connect를 통한 AWS DataSync
C. 공용 인터넷을 통한 AWS Database Migration Service(AWS DMS)
D. AWS Direct Connect를 통한 AWS Database Migration Service(AWS DMS)
Answer: B
Explanation:
These are some of the main use cases for AWS DataSync: * Data migration - Move active
datasets rapidly over the network into Amazon S3, Amazon EFS, or FSx for Windows File
Server. DataSync includes automatic encryption and data integrity validation to help make
sure that your data arrives securely, intact, and ready to use.
"DataSync includes encryption and integrity validation to help make sure your data arrives
securely, intact, and ready to use." https://aws.amazon.com/datasync/faqs/
QUESTION NO: 806
한 회사에서 온프레미스 Microsoft SQL Server Enterprise 에디션 데이터베이스를 AWS로
마이그레이션하려고 합니다. 회사의 온라인 애플리케이션은 데이터베이스를 사용하여 거래를
처리합니다. 데이터 분석 팀은 동일한 프로덕션 데이터베이스를 사용하여 분석 처리를 위한
보고서를 실행합니다. 회사는 가능한 한 관리형 서비스로 전환하여 운영 오버헤드를 줄이고
싶어합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Microsoft SQL Server용 Amazon RDS로 마이그레이션합니다. 보고 목적으로 읽기
복제본을 사용하세요.
B. Amazon EC2의 Microsoft SQL Server로 마이그레이션합니다. 보고 목적으로 Always On
읽기 복제본을 사용하세요.
553

IT Certification Guaranteed, The Easy Way!
C. Amazon DynamoDB로 마이그레이션합니다. 보고 목적으로 DynamoDB 온디맨드 복제본을
사용하세요.
D. Amazon Aurora MySQL로 마이그레이션합니다. 보고 목적으로 Aurora 읽기 전용 복제본을
사용하십시오.
Answer: A
Explanation:
Amazon RDS for Microsoft SQL Server is a fully managed service that offers SQL Server
2014, 2016, 2017, and 2019 editions while offloading database administration tasks such as
backups, patching, and scaling.
Amazon RDS supports read replicas, which are read-only copies of the primary database
that can be used for reporting purposes without affecting the performance of the online
application. This solution will meet the requirements with the least operational overhead, as it
does not require any code changes or manual intervention.
References:
* 1 provides an overview of Amazon RDS for Microsoft SQL Server and its benefits.
* 2 explains how to create and use read replicas with Amazon RDS.
QUESTION NO: 807
한 회사는 애플리케이션에서 데이터를 암호화해야 하는 개발자를 지원하기 위해 확장 가능한
키 관리 인프라를 구축하려고 합니다.
운영 부담을 줄이기 위해 솔루션 아키텍트는 무엇을 해야 합니까?
A. 다중 인증(MFA)을 사용하여 암호화 키를 보호합니다.
B. AWS Key Management Service(AWS KMS)를 사용하여 암호화 키를 보호합니다.
C. AWS Certificate Manager(ACM)를 사용하여 암호화 키를 생성, 저장 및 할당합니다.
D. IAM 정책을 사용하여 암호화 키를 보호하기 위한 액세스 권한이 있는 사용자의 범위를
제한합니다.
Answer: B
Explanation:
https://aws.amazon.com/kms/faqs/#:~:text=If%20you%20are%20a%20developer%20who%2
0needs%20to%
20digitally,a%20broad%20set%20of%20industry%20and%20regional%20compliance%20reg
imes.
QUESTION NO: 808
한 회사에서 역사적 사건의 이미지를 저장하는 웹사이트를 운영하고 있습니다. 웹사이트
사용자는 이미지 속 사건이 발생한 연도를 기준으로 이미지를 검색하고 볼 수 있는 기능이
필요합니다. 평균적으로 사용자는 각 이미지를 1년에 한두 번만 요청합니다. 회사는 이미지를
저장하고 사용자에게 전달할 수 있는 가용성이 뛰어난 솔루션을 원합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. Amazon Elastic Block Store(Amazon EBS)에 이미지를 저장합니다. Amazon EC2_에서
실행되는 웹 서버 사용
B. Amazon Elastic File System(Amazon EFS)에 이미지를 저장합니다. Amazon EC2에서
실행되는 웹 서버를 사용하십시오.
C. Amazon S3 Standard에 이미지를 저장합니다. S3 Standard를 사용하면 정적 웹사이트를
554

IT Certification Guaranteed, The Easy Way!
통해 이미지를 직접 전달할 수 있습니다.
D. Amazon S3 Standard-InfrequentAccess(S3 Standard-IA)에 이미지를 저장합니다. S3
Standard-IA를 사용하면 정적 웹 사이트를 통해 이미지를 직접 전달할 수 있습니다.
Answer: C
Explanation:
it allows the company to store and deliver images to users in a highly available and cost-
effective way. By storing images in Amazon S3 Standard, the company can use a durable,
scalable, and secure object storage service that offers high availability and performance. By
using S3 Standard to directly deliver images by using a static website, the company can
avoid running web servers and reduce operational overhead. S3 Standard also offers low
storage pricing and free data transfer within AWS Regions. References:
* Amazon S3 Storage Classes
* Hosting a Static Website on Amazon S3
QUESTION NO: 809
한 글로벌 기업이 AWS Organizations의 여러 AWS 계정에서 애플리케이션을 실행합니다.
회사의 애플리케이션은 멀티파트 업로드를 사용하여 AWS 리전의 여러 Amazon S3 버킷에
데이터를 업로드합니다. 회사는 비용 준수 목적으로 불완전한 멀티파트 업로드에 대해
보고하려고 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 불완전한 멀티파트 업로드 객체 수를 보고하는 규칙으로 AWS Config를 구성합니다.
B. 불완전한 멀티파트 업로드 개체 수를 보고하는 서비스 제어 정책(SCP)을 만듭니다.
C. 불완전한 멀티파트 업로드 객체 수를 보고하도록 S3 Storage Lens를 구성합니다.
D. S3 다중 지역 액세스 포인트를 생성하여 불완전한 멀티파트 업로드 객체 수를 보고합니다.
Answer: C
Explanation:
S3 Storage Lens is a cloud storage analytics feature that provides organization-wide visibility
into object storage usage and activity across multiple AWS accounts in AWS Organizations.
S3 Storage Lens can report the incomplete multipart upload object count as one of the
metrics that it collects and displays on an interactive dashboard in the S3 console. S3
Storage Lens can also export metrics in CSV or Parquet format to an S3 bucket for further
analysis. This solution will meet the requirements with the least operational overhead, as it
does not require any code development or policy changes.
References:
* 1 explains how to use S3 Storage Lens to gain insights into S3 storage usage and activity.
* 2 describes the concept and benefits of multipart uploads.
QUESTION NO: 810
여러 AWS 계정이 있는 한 회사가 온프레미스 Microsoft Active Directory를 유지 관리합니다.
이 회사는 직원을 위해 Single Sign-On을 구현할 솔루션이 필요합니다. 이 회사는 AWS IAM
Identity Center를 사용하려고 합니다.
솔루션은 다음 요구 사항을 충족해야 합니다.
* 기존 Active Directory 자격 증명을 사용하여 사용자가 AWS 계정 및 타사 애플리케이션에
액세스할 수 있도록 허용합니다.
* AWS 계정에 액세스하려면 다중 요소 인증(MFA)을 적용합니다.
555

IT Certification Guaranteed, The Easy Way!
* AWS 계정 및 애플리케이션에 대한 액세스 권한을 중앙에서 관리합니다.
옵션:
A. 각 AWS 계정에서 Active Directory에 대한 IAM ID 공급자를 만듭니다. Active Directory
사용자와 그룹이 IAM 역할을 통해 AWS 계정에 직접 액세스하도록 합니다. IAM Identity
Center를 사용하여 모든 사용자에 대해 각 계정에서 MFA를 적용합니다.
B. AWS Directory Service를 사용하여 새 AWS Managed Microsoft AD Active Directory를
만듭니다. 각 계정에서 IAM Identity Center를 구성하여 새 AWS Managed Microsoft AD Active
Directory를 ID 소스로 사용합니다. IAM Identity Center를 사용하여 모든 사용자에게 MFA를
적용합니다.
C. 기존 Active Directory를 ID 소스로 사용하여 IAM Identity Center를 사용합니다. 모든
사용자에게 MFA를 적용합니다. AWS Organizations 및 Active Directory 그룹을 사용하여
AWS 계정 및 애플리케이션 액세스에 대한 액세스 권한을 관리합니다.
D. AWS Lambda 함수를 사용하여 Active Directory 사용자 및 그룹을 각 AWS 계정의 IAM
사용자 및 그룹과 주기적으로 동기화합니다. IAM 역할 및 정책을 사용하여 애플리케이션
액세스를 관리합니다. MFA를 적용하기 위한 두 번째 Lambda 함수를 만듭니다.
Answer: C
Explanation:
Detailed Explanation:
* A. IAM identity provider: Does not support centralized management across multiple
accounts.
* B. AWS Managed AD: Unnecessary if an on-premises Active Directory already exists.
* C. IAM Identity Center + Existing AD: Best approach to integrate existing Active Directory
for SSO, with MFA and centralized permissions.
* D. Lambda for synchronization: Adds complexity and does not leverage IAM Identity Center
capabilities.
References: AWS IAM Identity Center
QUESTION NO: 811
회사는 AWS Organizations에 조직을 가지고 있습니다. 이 회사는 루트 조직 단위(OU)에 있는
4개의 AWS 계정에서 Amazon EC2 인스턴스를 실행합니다. 비프로덕션 계정 3개와 프로덕션
계정 1개가 있습니다. 회사는 사용자가 비프로덕션 계정에서 특정 크기의 EC2 인스턴스를
시작하는 것을 금지하려고 합니다. 회사는 금지된 유형을 사용하는 시작 인스턴스에 대한
액세스를 거부하기 위해 서비스 제어 정책(SCP)을 만들었습니다.
SCP를 배포하는 솔루션은 이러한 요구 사항을 충족합니까? (2개를 선택하세요.)
A. SCP를 조직의 루트 OU에 연결합니다.
B. 세 개의 비생산 조직 회원 계정에 SCP를 연결합니다.
C. SCP를 조직 관리 계정에 연결합니다.
D. 프로덕션 계정에 대한 OU를 생성합니다. SCP를 OU에 연결합니다. 프로덕션 구성원
계정을 새 OU로 이동합니다.
E. 필수 계정에 대한 OU를 생성합니다. SCP를 OU에 연결합니다. 비프로덕션 구성원 계정을
새 OU로 이동합니다.
Answer: B E
Explanation:
SCPs are a type of organization policy that you can use to manage permissions in your
556

IT Certification Guaranteed, The Easy Way!
organization. SCPs offer central control over the maximum available permissions for all
accounts in your organization. SCPs help you to ensure your accounts stay within your
organization's access control guidelines1.
To apply an SCP to a specific set of accounts, you need to create an OU for those accounts
and attach the SCP to the OU. This way, the SCP affects only the member accounts in that
OU and not the other accounts in the organization. If you attach the SCP to the root OU, it
will apply to all accounts in the organization, including the production account, which is not
the desired outcome. If you attach the SCP to the management account, it will have no effect,
as SCPs do not affect users or roles in the management account1.
Therefore, the best solutions to deploy the SCP are B and E. Option B attaches the SCP
directly to the three nonproduction accounts, while option E creates a separate OU for the
nonproduction accounts and attaches the SCP to the OU. Both options will achieve the same
result of restricting the EC2 instance types in the nonproduction accounts, but option E might
be more scalable and manageable if there are more accounts or policies to be applied in the
future2.
References:
* 1: Service control policies (SCPs) - AWS Organizations
* 2: Best Practices for AWS Organizations Service Control Policies in a Multi-Account
Environment
QUESTION NO: 812
회사는 일주일의 첫날이 되기 전에 매주 중요한 데이터 분석 작업을 실행합니다. 작업은
분석을 완료하는 데 최소 1시간이 필요합니다. 작업은 상태 저장이며 중단을 허용할 수
없습니다. 회사는 AWS에서 작업을 실행하기 위한 솔루션이 필요합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 작업에 대한 컨테이너를 생성합니다. Amazon EventBridge Scheduler를 사용하여 Amazon
Elastic Container Service(Amazon ECS) 클러스터에서 AWS Fargate 작업으로 실행되도록
작업을 예약합니다.
B. AWS Lambda 함수에서 실행되도록 작업을 구성합니다. Amazon EventBridge에서 예약
규칙을 생성하여 Lambda 함수를 호출합니다.
C. Amazon Linux를 실행하는 Amazon EC2 스팟 인스턴스의 Auto Scaling 그룹을 구성합니다.
분석을 실행할 인스턴스에 crontab 항목을 구성합니다.
D. 작업을 실행하도록 AWS DataSync 작업을 구성합니다. 일정에 따라 작업을 실행하도록
cron 표현식을 구성합니다.
Answer: A
Explanation:
* Understanding the Requirement: The job is stateful, cannot tolerate interruptions, and
needs to run reliably for at least one hour each week.
* Analysis of Options:
* AWS Fargate with Amazon ECS and EventBridge: This option provides a serverless
compute engine for containers that can run stateful tasks reliably. Using EventBridge
Scheduler, the job can be triggered automatically at the specified time without manual
intervention.
* AWS Lambda with EventBridge: Lambda functions are not suitable for long-running stateful
jobs since they have a maximum execution time of 15 minutes.
557

IT Certification Guaranteed, The Easy Way!
* EC2 Spot Instances: Spot Instances can be interrupted, making them unsuitable for a
stateful job that cannot tolerate interruptions.
* AWS DataSync: This service is primarily for moving large amounts of data and is not
designed to run stateful analysis jobs.
* Best Option for Reliable, Scheduled Execution:
* The Fargate task on ECS with EventBridge Scheduler meets all requirements, providing the
necessary reliability and scheduling capabilities without interruption risks.
References:
* Amazon ECS
* AWS Fargate
* Amazon EventBridge
QUESTION NO: 813
회사는 AWS에서 애플리케이션을 호스팅합니다. 회사는 Amazon Cognito를 사용하여
사용자를 관리합니다. 사용자가 애플리케이션에 로그인하면 애플리케이션은 Amazon API
Gateway에서 호스팅되는 REST API를 사용하여 Amazon DynamoDB에서 필요한 데이터를
가져옵니다. 회사는 개발 노력을 줄이기 위해 REST API에 대한 액세스를 제어하는 ​​AWS
관리형 솔루션을 원합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족할 솔루션은
무엇입니까?
A. AWS Lambda 함수를 인증하도록 구성하세요! 요청한 사용자를 확인하기 위해 API
게이트웨이에서
B. 각 사용자에 대해 각 요청과 함께 전송되어야 하는 API 키를 생성하고 할당합니다. AWS
Lambda 함수를 사용하여 키를 검증합니다.
C. 요청이 있을 때마다 헤더에 사용자의 이메일 주소를 보냅니다. AWS Lambda 함수를
호출하여 해당 이메일 주소를 가진 사용자에게 적절한 액세스 권한이 있는지 확인합니다.
D. Amazon Cognito가 각 요청을 검증할 수 있도록 API Gateway에서 Amazon Cognito 사용자
풀 권한 부여자를 구성합니다.
Answer: D
Explanation:
https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-
cognito.html To control access to the REST API and reduce development efforts, the
company can use an Amazon Cognito user pool authorizer in API Gateway. This will allow
Amazon Cognito to validate each request and ensure that only authenticated users can
access the API. This solution has the LEAST operational overhead, as it does not require the
company to develop and maintain any additional infrastructure or code.
QUESTION NO: 814
한 회사가 AWS 지역의 애플리케이션 로드 밸런서 뒤에 있는 Amazon Elastic Kubernetes
Service(Amazon EKS)에 애플리케이션을 배포합니다. 애플리케이션은 PostgreSQL
데이터베이스 엔진에 데이터를 저장해야 합니다. 이 회사는 데이터베이스의 데이터를
고가용성으로 유지하고 싶어합니다. 또한 이 회사는 읽기 워크로드에 대한 용량을 늘려야
합니다.
이러한 요구 사항을 충족하면서 가장 운영 효율적인 솔루션은 무엇일까요?
A. 글로벌 테이블로 구성된 Amazon DynamoDB 데이터베이스 테이블을 만듭니다.
B. 다중 AZ 배포를 사용하여 Amazon RDS 데이터베이스 생성
558

IT Certification Guaranteed, The Easy Way!
C. 다중 AZ DB 클러스터 배포를 통해 Amazon RDS 데이터베이스를 만듭니다.
D. 지역 간 읽기 복제본으로 구성된 Amazon RDS 데이터베이스를 만듭니다.
Answer: C
Explanation:
Amazon RDS Multi-AZ DB cluster deployment ensures high availability by automatically
replicating data across multiple Availability Zones (AZs), and it supports failover in case of a
failure in one AZ. This setup also provides increased capacity for read workloads by allowing
read scaling with reader instances in different AZs. This solution offers the most operational
efficiency with minimal manual intervention.
* Option A (DynamoDB): DynamoDB is not suitable for a relational database workload, which
requires a PostgreSQL engine.
* Option B (RDS with Multi-AZ): While this provides high availability, it doesn't offer read
scaling capabilities.
* Option D (Cross-Region Read Replicas): This adds complexity and is not necessary if the
requirement is high availability within a single region.
AWS References:
* Amazon RDS Multi-AZ DB Cluster
QUESTION NO: 815
회사는 사용자 트랜잭션 데이터를 Amazon DynamoDB 테이블에 보관해야 합니다.
회사는 데이터를 7년간 보관해야 합니다.
이러한 요구 사항을 충족하는 가장 운영 효율성이 높은 솔루션은 무엇입니까?
A. DynamoDB 지정 시간 복구를 사용하여 테이블을 지속적으로 백업합니다.
B. AWS Backup을 사용하여 테이블에 대한 백업 일정 및 보존 정책을 생성합니다.
C. DynamoDB 콘솔을 사용하여 테이블의 주문형 백업을 생성합니다. 백업을 Amazon S3
버킷에 저장합니다. S3 버킷에 대한 S3 수명 주기 구성을 설정합니다.
D. AWS Lambda 함수를 호출하는 Amazon EventBridge(Amazon CloudWatch Events)
규칙을 생성합니다. 테이블을 백업하고 Amazon S3 버킷에 백업을 저장하도록 Lambda
함수를 구성합니다. S3 버킷에 대한 S3 수명 주기 구성을 설정합니다.
Answer: C
QUESTION NO: 816
한 회사가 Amazon S3 Glacier Deep Archive 스토리지 클래스를 사용하여 Amazon S3 버킷에
여러 접두사에 걸쳐 수백만 개의 객체를 저장했습니다. 이 회사는 보관해야 하는 데이터 하위
집합을 제외하고 3년 이상 된 모든 데이터를 삭제해야 합니다. 이 회사는 보관해야 하는
데이터를 식별했고 서버리스 솔루션을 구현하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. S3 인벤토리를 사용하여 모든 객체를 나열합니다. AWS CLI를 사용하여 인벤토리
목록에서 객체를 삭제하는 Amazon EC2 인스턴스에서 실행되는 스크립트를 만듭니다.
B. AWS Batch를 사용하여 보존해야 하는 데이터를 제외한 3년 이상 된 객체를 삭제합니다.
C. 3년 이상 된 객체를 쿼리하기 위해 AWS Glue 크롤러를 프로비저닝합니다. 오래된 객체의
매니페스트 파일을 저장합니다. 매니페스트에서 객체를 삭제하는 스크립트를 만듭니다.
D. S3 인벤토리를 활성화합니다. 객체를 필터링하고 삭제하는 AWS Lambda 함수를
만듭니다. 인벤토리 보고서를 사용하여 객체를 삭제하려면 S3 배치 작업으로 Lambda 함수를
559

IT Certification Guaranteed, The Easy Way!
호출합니다.
Answer: D
Explanation:
To meet the requirement of deleting objects older than 3 years while retaining certain data,
this solution leverages serverless technologies to minimize operational overhead.
* S3 Inventory: S3 Inventory provides a flat file that lists all the objects in an S3 bucket and
their metadata, which can be configured to include data such as the last modified date. This
inventory can be generated daily or weekly.
* AWS Lambda Function: A Lambda function can be created to process the S3 Inventory
report, filtering out the objects that need to be retained and identifying those that should be
deleted.
* S3 Batch Operations: S3 Batch Operations can execute tasks such as object deletion at
scale. By invoking the Lambda function through S3 Batch Operations, you can automate the
process of deleting the identified objects, ensuring that the solution is serverless and requires
minimal operational management.
* Why Not Other Options?:
* Option A (AWS CLI script on EC2): Running a script on an EC2 instance adds unnecessary
operational overhead and is not serverless.
* Option B (AWS Batch): AWS Batch is designed for running large-scale batch computing
workloads, which is overkill for this scenario.
* Option C (AWS Glue + script): AWS Glue is more suited for ETL tasks, and this approach
would add unnecessary complexity compared to the serverless Lambda solution.
AWS References:
* Amazon S3 Inventory - Information on how to set up and use S3 Inventory.
* S3 Batch Operations - Documentation on how to perform bulk operations on S3 objects
using S3 Batch Operations.
QUESTION NO: 817
회사에는 IPv6 주소를 사용하여 Amazon EC2 인스턴스에 호스팅되는 애플리케이션이
있습니다. 애플리케이션은 인터넷을 사용하여 다른 외부 애플리케이션과의 통신을 시작해야
합니다.
그러나 회사의 보안 정책에는 외부 서비스가 EC2 인스턴스에 대한 연결을 시작할 수 없다고
명시되어 있습니다.
이 문제를 해결하기 위해 솔루션 설계자는 무엇을 권장해야 합니까?
A. NAT 게이트웨이를 생성하고 이를 서브넷 라우팅 테이블의 대상으로 만듭니다.
B. 인터넷 게이트웨이를 생성하고 이를 서브넷 라우팅 테이블의 대상으로 만듭니다.
C. 가상 프라이빗 게이트웨이를 생성하고 이를 서브넷 라우팅 테이블의 대상으로 만듭니다.
D. 외부 전용 인터넷 게이트웨이를 생성하고 이를 서브넷 라우팅 테이블의 대상으로
만듭니다.
Answer: D
Explanation:
An egress-only internet gateway is a VPC component that allows outbound communication
over IPv6 from instances in your VPC to the internet, and prevents the internet from initiating
an IPv6 connection with your instances. This meets the company's security policy and
requirements. To use an egress-only internet gateway, you need to add a route in the
560

IT Certification Guaranteed, The Easy Way!
subnet's route table that routes IPv6 internet traffic (::/0) to the egress- only internet gateway.
Reference URLs:
1 https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html
2 https://dev.to/aws-builders/what-is-an-egress-only-internet-gateways-in-aws-7gp
3 https://docs.aws.amazon.com/vpc/latest/userguide/route-table-options.html
QUESTION NO: 818
병원에서는 대규모 기록 기록 모음을 위한 디지털 사본을 생성하려고 합니다. 병원에서는
매일 수백 개의 새로운 문서를 계속해서 추가할 것입니다. 병원의 데이터 팀이 문서를
스캔하고 문서를 AWS 클라우드에 업로드합니다.
솔루션 설계자는 애플리케이션이 데이터에 대해 SQL 쿼리를 실행할 수 있도록 문서를
분석하고, 의료 정보를 추출하고, 문서를 저장하는 솔루션을 구현해야 합니다. 솔루션은
확장성과 운영 효율성을 극대화해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까?
(2개를 선택하세요.)
A. MySQL 데이터베이스를 실행하는 Amazon EC2 인스턴스에 문서 정보를 씁니다.
B. Amazon S3 버킷에 문서 정보를 씁니다. Amazon Athena를 사용하여 데이터를 쿼리합니다
.
C. Amazon EC2 인스턴스의 Auto Scaling 그룹을 생성하여 스캔한 파일을 처리하고 의료
정보를 추출하는 사용자 지정 애플리케이션을 실행합니다.
D. 새 문서가 업로드될 때 실행되는 AWS Lambda 함수를 생성합니다. Amazon Rekognition을
사용하여 문서를 원시 텍스트로 변환합니다. Amazon Transcribe Medical을 사용하여
텍스트에서 관련 의료 정보를 감지하고 추출합니다.
E. 새 문서가 업로드될 때 실행되는 AWS Lambda 함수를 생성합니다. Amazon Textract를
사용하여 문서를 원시 텍스트로 변환합니다. Amazon Comprehend Medical을 사용하여
텍스트에서 관련 의료 정보를 감지하고 추출합니다.
Answer: B E
Explanation:
This solution meets the requirements of creating digital copies for a large collection of
historical written records, analyzing the documents, extracting the medical information, and
storing the documents so that an application can run SQL queries on the data. Writing the
document information to an Amazon S3 bucket can provide scalable and durable storage for
the scanned files. Using Amazon Athena to query the data can provide serverless and
interactive SQL analysis on data stored in S3. Creating an AWS Lambda function that runs
when new documents are uploaded can provide event-driven and serverless processing of
the scanned files. Using Amazon Textract to convert the documents to raw text can provide
accurate optical character recognition (OCR) and extraction of structured data such as tables
and forms from documents using artificial intelligence (AI). Using Amazon Comprehend
Medical to detect and extract relevant medical information from the text can provide natural
language processing (NLP) service that uses machine learning that has been pre-trained to
understand and extract health data from medical text.
Option A is incorrect because writing the document information to an Amazon EC2 instance
that runs a MySQL database can increase the infrastructure overhead and complexity, and it
may not be able to handle large volumes of data. Option C is incorrect because creating an
Auto Scaling group of Amazon EC2 instances to run a custom application that processes the
561

IT Certification Guaranteed, The Easy Way!
scanned files and extracts the medical information can increase the infrastructure overhead
and complexity, and it may not be able to leverage existing AI and NLP services such as
Textract and Comprehend Medical. Option D is incorrect because using Amazon Rekognition
to convert the documents to raw text can provide image and video analysis, but it does not
support OCR or extraction of structured data from documents. Using Amazon Transcribe
Medical to detect and extract relevant medical information from the text can provide speech-
to-text transcription service for medical conversations, but it does not support text analysis or
extraction of health data from medical text.
References:
* https://aws.amazon.com/s3/
* https://aws.amazon.com/athena/
* https://aws.amazon.com/lambda/
* https://aws.amazon.com/textract/
* https://aws.amazon.com/comprehend/medical/
QUESTION NO: 819
한 회사가 워크로드를 위해 Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터를
구축하고 있습니다. Amazon EKS에 저장된 모든 비밀은 Kubernetes etcd 키-값 저장소에서
암호화되어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 새 AWS Key Management Service(AWS KMS) 키 생성 AWS Secrets Manager를 사용하여
교체를 관리하고 Amazon EKS에 모든 암호를 저장합니다.
B. 새 AWS Key Management Service(AWS KMS) 키를 생성합니다. Amazon EKS
클러스터에서 Amazon EKS KMS 암호 암호화를 활성화합니다.
C. 기본 옵션을 사용하여 Amazon EKS 클러스터 생성 Amazon Elastic Block Store(Amazon
EBS) CSI(컨테이너 스토리지 인터페이스) 드라이버를 추가 기능으로 사용합니다.
D. ahas/aws/ebs 별칭을 사용하여 새 AWS Key Management Service(AWS KMS) 키를
생성합니다. 계정에 대해 기본 Amazon Elastic Block Store(Amazon EBS) 볼륨 암호화를
활성화합니다.
Answer: B
Explanation:
This option is the most secure and simple way to encrypt the secrets that are stored in
Amazon EKS. AWS Key Management Service (AWS KMS) is a service that allows you to
create and manage encryption keys that can be used to encrypt your data. Amazon EKS
KMS secrets encryption is a feature that enables you to use a KMS key to encrypt the
secrets that are stored in the Kubernetes etcd key-value store. This provides an additional
layer of protection for your sensitive data, such as passwords, tokens, and keys. You can
create a new KMS key or use an existing one, and then enable the Amazon EKS KMS
secrets encryption on the Amazon EKS cluster. You can also use IAM policies to control who
can access or use the KMS key.
Option A is not correct because using AWS Secrets Manager to manage, rotate, and store all
secrets in Amazon EKS is not necessary or efficient. AWS Secrets Manager is a service that
helps you securely store, retrieve, and rotate your secrets, such as database credentials, API
keys, and passwords. You can use it to manage secrets that are used by your applications or
services outside of Amazon EKS, but it is not designed to encrypt the secrets that are stored
562

IT Certification Guaranteed, The Easy Way!
in the Kubernetes etcd key-value store. Moreover, using AWS Secrets Manager would incur
additional costs and complexity, and it would not leverage the native Kubernetes secrets
management capabilities.
Option C is not correct because using the Amazon EBS Container Storage Interface (CSI)
driver as an add-on does not encrypt the secrets that are stored in Amazon EKS. The
Amazon EBS CSI driver is a plugin that allows you to use Amazon EBS volumes as
persistent storage for your Kubernetes pods. It is useful for providing durable and scalable
storage for your applications, but it does not affect the encryption of the secrets that are
stored in the Kubernetes etcd key-value store. Moreover, using the Amazon EBS CSI driver
would require additional configuration and resources, and it would not provide the same level
of security as using a KMS key.
Option D is not correct because creating a new AWS KMS key with the alias aws/ebs and
enabling default Amazon EBS volume encryption for the account does not encrypt the
secrets that are stored in Amazon EKS.
The alias aws/ebs is a reserved alias that is used by AWS to create a default KMS key for
your account. This key is used to encrypt the Amazon EBS volumes that are created in your
account, unless you specify a different KMS key. Enabling default Amazon EBS volume
encryption for the account is a setting that ensures that all new Amazon EBS volumes are
encrypted by default. However, these features do not affect the encryption of the secrets that
are stored in the Kubernetes etcd key-value store. Moreover, using the default KMS key or
the default encryption setting would not provide the same level of control and security as
using a custom KMS key and enabling the Amazon EKS KMS secrets encryption feature.
References:
* Encrypting secrets used in Amazon EKS
* What Is AWS Key Management Service?
* What Is AWS Secrets Manager?
* Amazon EBS CSI driver
* Encryption at rest
QUESTION NO: 820
회사에서 응용 프로그램의 안정적인 아키텍처를 설계하기 위해 솔루션 설계자를
고용했습니다. 이 애플리케이션은 웹 서버를 실행하는 Amazon RDS DB 인스턴스 1개와
수동으로 프로비저닝된 Amazon EC2 인스턴스 2개로 구성됩니다. EC2 인스턴스는 단일 가용
영역에 있습니다.
직원이 최근 DB 인스턴스를 삭제했고 그 결과 애플리케이션을 24시간 동안 사용할 수
없었습니다. 회사는 환경의 전반적인 안정성에 관심이 있습니다.
애플리케이션 인프라의 안정성을 극대화하기 위해 솔루션 설계자는 무엇을 해야 합니까?
A. 하나의 EC2 인스턴스를 삭제하고 다른 EC2 인스턴스에서 종료 방지 기능을 활성화합니다.
다중 AZ가 되도록 DB 인스턴스를 업데이트하고 삭제 방지를 활성화합니다.
B. DB 인스턴스를 다중 AZ로 업데이트하고 삭제 방지를 활성화합니다. Application Load
Balancer 뒤에 EC2 인스턴스를 배치하고 여러 가용 영역에 걸쳐 EC2 Auto Scaling 그룹에서
실행합니다.
C. Amazon API Gateway 및 AWS Lambda 함수와 함께 추가 DB 인스턴스를 생성합니다. API
Gateway를 통해 Lambda 함수를 호출하도록 애플리케이션을 구성합니다. Lambda 함수가 두
DB 인스턴스에 데이터를 쓰도록 합니다.
563

IT Certification Guaranteed, The Easy Way!
D. 여러 가용 영역에 여러 서브넷이 있는 EC2 Auto Scaling 그룹에 EC2 인스턴스를
배치합니다. 온디맨드 인스턴스 대신 스팟 인스턴스를 사용하십시오. 인스턴스의 상태를
모니터링하도록 Amazon CloudWatch 경보를 설정합니다. 다중 AZ가 되도록 DB 인스턴스를
업데이트하고 삭제 방지를 활성화합니다.
Answer: B
Explanation:
This answer is correct because it meets the requirements of maximizing the reliability of the
application's infrastructure. You can update the DB instance to be Multi-AZ, which means that
Amazon RDS automatically provisions and maintains a synchronous standby replica in a
different Availability Zone. The primary DB instance is synchronously replicated across
Availability Zones to a standby replica to provide data redundancy and minimize latency
spikes during system backups. Running a DB instance with high availability can enhance
availability during planned system maintenance. It can also help protect your databases
against DB instance failure and Availability Zone disruption. You can also enable deletion
protection on the DB instance, which prevents the DB instance from being deleted by any
user. You can place the EC2 instances behind an Application Load Balancer, which
distributes incoming application traffic across multiple targets, such as EC2 instances, in
multiple Availability Zones. This increases the availability and fault tolerance of your
applications. You can run the EC2 instances in an EC2 Auto Scaling group across multiple
Availability Zones, which ensures that you have the correct number of EC2 instances
available to handle the load for your application. You can use scaling policies to adjust the
number of instances in your Auto Scaling group in response to changing demand.
References:
*
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZSingleStandb
y.html
* https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_DeleteInstance.
html#USER_DeleteInstance.DeletionProtection
* https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html
* https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html
QUESTION NO: 821
한 회사는 오래된 뉴스 영상의 비디오 아카이브를 AWS에 저장할 수 있는 솔루션을 찾고
있습니다. 회사는 비용을 최소화해야 하며 이러한 파일을 복원할 필요가 거의 없습니다.
h|es가 필요한 경우 최대 5분 내에 사용할 수 있어야 합니다.
가장 비용 효율적인 솔루션은 무엇입니까?
A. Amazon S3 Glacier에 비디오 아카이브를 저장하고 긴급 검색을 사용합니다.
B. Amazon S3 Glacier에 비디오 아카이브를 저장하고 표준 검색을 사용합니다.
C. Amazon S3 Standard-Infrequent Access(S3 Standard-IA)에 비디오 아카이브를
저장합니다.
D. Amazon S3 One Zone-Infrequent Access(S3 One Zone-IA)에 비디오 아카이브를
저장합니다.
Answer: A
Explanation:
Amazon S3 Glacier is a storage class that provides secure, durable, and extremely low-cost
564

IT Certification Guaranteed, The Easy Way!
storage for data archiving and long-term backup. It is designed for data that is rarely
accessed and for which retrieval times of several hours are suitable1. By storing the video
archives in Amazon S3 Glacier, the solution can minimize costs.
Amazon S3 Glacier offers three options for data retrieval: Expedited, Standard, and Bulk.
Expedited retrievals typically return data in 1-5 minutes and are suitable for Active Archive
use cases. Standard retrievals typically complete within 3-5 hours and are suitable for less
urgent needs. Bulk retrievals typically complete within 5-12 hours and are the lowest-cost
retrieval option2. By using Expedited retrievals, the solution can meet the requirement of
restoring the files in a maximum of five minutes.
B: Store the video archives in Amazon S3 Glacier and use Standard retrievals. This solution
will not meet the requirement of restoring the files in a maximum of five minutes, as Standard
retrievals typically complete within 3-5 hours.
C: Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).
This solution will not meet the requirement of minimizing costs, as S3 Standard-IA is a
storage class that provides low-cost storage for data that is accessed less frequently but
requires rapid access when needed. It has a higher storage cost than S3 Glacier.
D: Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA).
This solution will not meet the requirement of minimizing costs, as S3 One Zone-IA is a
storage class that provides low-cost storage for data that is accessed less frequently but
requires rapid access when needed. It has a higher storage cost than S3 Glacier.
Reference URL: https://aws.amazon.com/s3/glacier/
QUESTION NO: 822
기업의 백업 데이터는 총 700테라바이트(TB)에 달하며 데이터 센터의 NAS(Network Attached
Storage)에 보관됩니다. 이 백업 데이터는 가끔 규제 관련 문의가 있을 경우 사용할 수 있어야
하며 7년 동안 보존되어야 합니다. 조직은 백업 데이터를 온프레미스 데이터 센터에서
Amazon Web Services(AWS)로 재배치하기로 결정했습니다. 한 달 이내에 마이그레이션을
완료해야 합니다. 회사의 공용 인터넷 연결은 데이터 전송을 위한 500Mbps의 전용 용량을
제공합니다.
데이터를 가능한 최저 비용으로 마이그레이션하고 저장하려면 솔루션 설계자가 무엇을 해야
합니까?
A. AWS Snowball 디바이스를 주문하여 데이터를 전송합니다. 수명 주기 정책을 사용하여
파일을 Amazon S3 Glacier Deep Archive로 전환합니다.
B. 데이터 센터와 Amazon VPC 간에 VPN 연결을 배포합니다. AWS CLI를 사용하여
온프레미스에서 Amazon S3 Glacier로 데이터를 복사합니다.
C. 500Mbps AWS Direct Connect 연결을 프로비저닝하고 데이터를 Amazon S3로
전송합니다. 수명 주기 정책을 사용하여 파일을 Amazon S3 Glacier Deep Archive로
전환합니다.
D. AWS DataSync를 사용하여 데이터를 전송하고 온프레미스에 DataSync 에이전트를
배포합니다. DataSync 작업을 사용하여 온프레미스 NAS 스토리지에서 Amazon S3 Glacier로
파일을 복사합니다.
Answer: A
Explanation:
https://www.omnicalculator.com/other/data-transfer
565

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 823
AWS Organizations를 사용하는 회사는 30개의 서로 다른 AWS 계정에서 150개의
애플리케이션을 실행합니다. 회사는 AWS 비용 및 사용 보고서를 사용하여 마스터 계정에 새
보고서를 생성했습니다. 보고서는 데이터 수집의 버킷에 복제되는 Amazon S3 버킷으로
전달됩니다. 계정.
회사의 고위 경영진은 이번 달 초부터 매일 NAT 게이트웨이 비용을 제공하는 사용자 정의
대시보드를 확인하려고 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 요청된 테이블 시각적 개체가 포함된 Amazon QuickSight 대시보드를 공유합니다. AWS
DataSync를 사용하여 새 보고서를 쿼리하도록 QuickSight 구성
B. 요청된 테이블 시각적 개체가 포함된 Amazon QuickSight 대시보드를 공유합니다. Amazon
Athena를 사용하여 새 보고서를 쿼리하도록 QuickSight를 구성합니다.
C. 요청된 테이블 시각적 개체가 포함된 Amazon CloudWatch 대시보드를 공유합니다. AWS
DataSync를 사용하여 새 보고서를 쿼리하도록 CloudWatch를 구성합니다.
D. 요청된 테이블 시각적 개체가 포함된 Amazon CloudWatch 대시보드를 공유합니다.
Amazon Athena를 사용하여 새 보고서를 쿼리하도록 CloudWatch를 구성합니다.
Answer: B
Explanation:
* Understanding the Requirement: Senior leadership wants a custom dashboard displaying
NAT gateway costs daily, starting from the beginning of the current month.
* Analysis of Options:
* QuickSight with DataSync: While QuickSight is suitable for dashboards, DataSync is not
designed for querying and analyzing data reports.
* QuickSight with Athena: QuickSight can visualize data queried by Athena, which is
designed to analyze data directly from S3.
* CloudWatch with DataSync: CloudWatch is primarily for monitoring metrics, not for creating
detailed cost analysis dashboards.
* CloudWatch with Athena: Similarly, using CloudWatch with Athena does not align well with
the requirement for a visual dashboard.
* Best Solution for Visualization and Querying:
* Amazon QuickSight with Athena: This combination allows for powerful data visualization
and querying capabilities. QuickSight can create dynamic dashboards, while Athena
efficiently queries the cost and usage report data stored in S3.
References:
* Amazon QuickSight
* Amazon Athena
QUESTION NO: 824
애플리케이션은 여러 가용 영역에 걸쳐 Amazon EC2 인스턴스에서 실행됩니다. 인스턴스는
Application Load Balancer 뒤의 Amazon EC2 Auto Scaling 그룹에서 실행됩니다.
애플리케이션은 EC2 인스턴스의 CPU 사용률이 40% 또는 거의 40%에 가까울 때 가장 잘
수행됩니다.
그룹의 모든 인스턴스에서 원하는 성능을 유지하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 간단한 조정 정책을 사용하여 Auto Scaling 그룹을 동적으로 조정합니다.
B. 대상 추적 정책을 사용하여 Auto Scaling 그룹을 동적으로 조정합니다.
566

IT Certification Guaranteed, The Easy Way!
C. AWS Lambda 함수를 사용하여 원하는 Auto Scaling 그룹 용량을 업데이트합니다.
D. 예약된 조정 작업을 사용하여 Auto Scaling 그룹을 확장 및 축소합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-
target-tracking.html
QUESTION NO: 825
회사는 Amazon API Gateway 및 AWS Lambd를 사용하여 AWS에서 내부 서버리스
애플리케이션을 호스팅합니다. 회사 직원들은 매일 애플리케이션을 사용하기 시작할 때 대기
시간이 길어지는 문제를 보고합니다. 회사는 대기 시간을 줄이고 싶어합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. API Gateway 조절 한도를 늘립니다.
B. 직원이 매일 애플리케이션을 사용하기 전에 Lambda 프로비저닝 동시성을 높이기 위해
예약된 조정을 설정합니다.
C. Amazon CloudWatch 경보를 생성하여 매일 시작 시 경보 대상으로 Lambda 함수를
시작합니다.
D. Lambda 함수 메모리를 늘립니다.
Answer: B
Explanation:
AWS Lambda is a serverless compute service that lets you run code without provisioning or
managing servers. Lambda scales automatically based on the incoming requests, but it may
take some time to initialize new instances of your function if there is a sudden increase in
demand. This may result in high latency or cold starts for your application. To avoid this, you
can use provisioned concurrency, which ensures that your function is initialized and ready to
respond at any time. You can also set up a scheduled scaling policy that increases the
provisioned concurrency before employees begin to use the application each day, and
decreases it when the demand is low. References:
https://docs.aws.amazon.com/lambda/latest/dg/configuration- concurrency.html
QUESTION NO: 826
한 회사가 Amazon S3 버킷에 Apache Parquet 형식으로 10TB의 로그 파일을 저장했습니다.
회사는 때때로 SQL을 사용하여 로그 파일을 분석해야 합니다. 이러한 요구 사항을 가장 비용
효율적으로 충족하는 솔루션은 무엇입니까?
A. Amazon Aurora MySQL 데이터베이스 생성 AWS Database Migration Service(AWS
DMS)를 사용하여 S3 버킷의 데이터를 Aurora로 마이그레이션합니다. Aurora 데이터베이스에
SQL 문을 실행합니다.
B. Amazon Redshift 클러스터 생성 Redshift Spectrum을 사용하여 S3 버킷의 데이터에 대해
직접 SQL 문을 실행합니다.
C. AWS Glue 크롤러를 생성하여 S3 버킷에서 테이블 메타데이터를 저장하고 검색합니다.
Amazon Athena를 사용하여 S3 버킷의 데이터에 대해 직접 SQL 문을 실행합니다.
D. Amazon EMR 클러스터 생성 Apache Spark SQL을 사용하여 S3 버킷의 데이터에 대해
직접 SQL 문 실행
Answer: C
567

IT Certification Guaranteed, The Easy Way!
Explanation:
AWS Glue is a serverless data integration service that can crawl, catalog, and prepare data
for analysis. AWS Glue can automatically discover the schema and partitioning of the data
stored in Apache Parquet format in S3, and create a table in the AWS Glue Data Catalog.
Amazon Athena is a serverless interactive query service that can run SQL queries directly on
data in S3, without requiring any data loading or transformation. Athena can use the table
metadata from the AWS Glue Data Catalog to query the data in S3. By using AWS Glue and
Athena, you can analyze the log files in S3 most cost-effectively, as you only pay for the
resources consumed by the crawler and the queries, and you do not need to provision or
manage any servers or clusters.
References:
* AWS Glue
* Amazon Athena
* Analyzing Data in S3 using Amazon Athena
QUESTION NO: 827
한 글로벌 기업은 Amazon API Gateway를 사용하여 us-east-1 지역 및 ap-southeast-2 지역의
로열티 클럽 사용자를 위한 REST API를 설계하고 있습니다. 솔루션 설계자는 SQL 주입 및
교차 사이트 스크립팅 공격으로부터 여러 계정에 걸쳐 이러한 API Gateway 관리형 REST
API를 보호하기 위한 솔루션을 설계해야 합니다.
최소한의 관리 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 두 리전 모두에 AWS WAF를 설정합니다. 지역 웹 ACL을 API 단계와 연결합니다.
B. 두 지역 모두에 AWS Firewall Manager를 설정합니다. AWS WAF 규칙을 중앙에서
구성합니다.
C. 목욕 지역에 AWS Shield를 설정합니다. 지역 웹 ACL을 API 단계와 연결합니다.
D. 리전 중 하나에 AWS Shield를 설정합니다. 지역 웹 ACL을 API 단계와 연결합니다.
Answer: A
Explanation:
Using AWS WAF has several benefits. Additional protection against web attacks using
criteria that you specify. You can define criteria using characteristics of web requests such as
the following: Presence of SQL code that is likely to be malicious (known as SQL injection).
Presence of a script that is likely to be malicious (known as cross-site scripting). AWS
Firewall Manager simplifies your administration and maintenance tasks across multiple
accounts and resources for a variety of protections. https://docs.aws.amazon.com/waf/latest
/developerguide/what-is-aws-waf.html
QUESTION NO: 828
한 회사는 사용자에게 글로벌 속보, 지역 알림, 날씨 업데이트를 제공하는 웹 기반 포털을
운영하고 있습니다. 포털은 정적 콘텐츠와 동적 콘텐츠를 혼합하여 각 사용자에게 개인화된
보기를 제공합니다. 콘텐츠는 ALB(Application Load Balancer) 뒤의 Amazon EC2
인스턴스에서 실행되는 API 서버를 통해 HTTPS를 통해 제공됩니다. 회사는 포털이 이
콘텐츠를 전 세계 사용자에게 가능한 한 빨리 제공하기를 원합니다.
솔루션 설계자는 모든 사용자의 대기 시간을 최소화하기 위해 애플리케이션을 어떻게
설계해야 합니까?
A. 단일 AWS 리전에 애플리케이션 스택을 배포합니다. Amazon CloudFront를 사용하면
568

IT Certification Guaranteed, The Easy Way!
ALB를 오리진으로 지정하여 모든 정적 및 동적 콘텐츠를 제공할 수 있습니다.
B. 두 개의 AWS 지역에 애플리케이션 스택을 배포합니다. Amazon Route 53 지연 시간
라우팅 정책을 사용하여 가장 가까운 지역의 ALB에서 모든 콘텐츠를 제공합니다.
C. 단일 AWS 리전에 애플리케이션 스택을 배포합니다. Amazon CloudFront를 사용하여 정적
콘텐츠를 제공합니다. ALB에서 직접 동적 콘텐츠를 제공합니다.
D. 두 AWS 지역에 애플리케이션 스택을 배포합니다. Amazon Route 53 지리적 위치 라우팅
정책을 사용하여 가장 가까운 지역의 ALB에서 모든 콘텐츠를 제공합니다.
Answer: A
Explanation:
https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-
content-using- amazon-cloudfront-getting-started-template/
QUESTION NO: 829
회사에는 온프레미스 환경과 AWS 간의 보안 연결이 필요합니다. 이 연결에는 높은 대역폭이
필요하지 않으며 소량의 트래픽을 처리합니다. 연결이 빨리 설정되어야 합니다.
이러한 유형의 연결을 설정하는 가장 비용 효율적인 방법은 무엇입니까?
A. 클라이언트 VPN 구현
B. AWS Direct Connect를 구현합니다.
C. Amazon EC2에서 배스천 호스트를 구현합니다.
D. AWS Site-to-Site VPN 연결을 구현합니다.
Answer: D
Explanation:
* AWS Site-to-Site VPN: This provides a secure and encrypted connection between an on-
premises environment and AWS. It is a cost-effective solution suitable for low bandwidth and
small traffic needs.
* Quick Setup:
* Site-to-Site VPN can be quickly set up by configuring a virtual private gateway on the AWS
side and a customer gateway on the on-premises side.
* It uses standard IPsec protocol to establish the VPN tunnel.
* Cost-Effectiveness: Compared to AWS Direct Connect, which requires dedicated physical
connections and higher setup costs, a Site-to-Site VPN is less expensive and easier to
implement for smaller traffic requirements.
References:
* AWS Site-to-Site VPN
QUESTION NO: 830
회사는 Amazon Elastic Kubernetes Service(Amazon EKS) 및 Kubernetes Horizontal Pod
Autoscaler를 사용하여 컨테이너 애플리케이션을 실행합니다. 작업량은 하루 종일 일정하지
않습니다. 솔루션 설계자는 기존 노드가 클러스터의 최대 용량에 도달했을 때 노드 수가
자동으로 확장되지 않아 성능 문제가 발생함을 알게 됩니다. 어떤 솔루션이 최소한의 관리
오버헤드로 이 문제를 해결할 것입니까?
A. 메모리 사용량을 추적하여 노드를 확장합니다.
B. Kubernetes Cluster Autoscaler를 사용하여 클러스터의 노드 수를 관리합니다.
C. AWS Lambda 함수를 사용하여 EKS 클러스터의 크기를 자동으로 조정합니다.
569

IT Certification Guaranteed, The Easy Way!
D. Amazon EC2 Auto Scaling 그룹을 사용하여 워크로드를 분산합니다.
Answer: B
Explanation:
The Kubernetes Cluster Autoscaler is a component that automatically adjusts the number of
nodes in your cluster when pods fail or are rescheduled onto other nodes. It uses Auto
Scaling groups to scale up or down the nodes according to the demand and capacity of your
cluster1.
By using the Kubernetes Cluster Autoscaler in your Amazon EKS cluster, you can achieve
the following benefits:
* You can improve the performance and availability of your container applications by ensuring
that there are enough nodes to run your pods and that there are no idle nodes wasting
resources.
* You can reduce the administrative overhead of managing your cluster size manually or
using custom scripts. The Cluster Autoscaler handles the scaling decisions and actions for
you based on the metrics and events from your cluster.
* You can leverage the integration of Amazon EKS and AWS Auto Scaling to optimize the
cost and efficiency of your cluster. You can use features such as launch templates, mixed
instances policies, and spot instances to customize your node configuration and save up to
90% on compute costs2
QUESTION NO: 831
한 회사는 GPS 추적기를 사용하여 수천 마리의 바다거북의 이동 패턴을 기록합니다.
추적기는 거북이가 100야드(91.4미터) 이상 이동했는지 확인하기 위해 5분마다 확인합니다.
거북이가 움직인 경우 추적기는 하나의 AWS 리전의 여러 가용 영역에 있는 3개의 Amazon
EC2 인스턴스에서 실행되는 웹 애플리케이션에 새 좌표를 보냅니다.
Jgpently. 예상치 못한 양의 추적기 데이터를 처리하는 동안 웹 애플리케이션이
과부하되었습니다. 이벤트를 재생할 방법이 없어 데이터가 손실되었습니다. 솔루션은 이러한
문제가 다시 발생하지 않도록 방지해야 하며 운영 오버헤드가 가장 적은 솔루션이
필요합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 데이터를 저장할 Amazon S3 버킷을 생성합니다. 처리를 위해 버킷에서 새 데이터를
검색하도록 애플리케이션을 구성합니다.
B. 전송된 위치 좌표를 처리하기 위해 Amazon API Gateway 엔드포인트를 생성합니다. AWS
Lambda 함수를 사용하여 각 항목을 동시에 처리합니다.
C. 수신 데이터를 저장할 Amazon Simple Queue Service(Amazon SOS) 대기열을
생성합니다. 처리할 새 메시지를 폴링하도록 애플리케이션을 구성합니다.
D. 전송된 위치 좌표를 저장할 Amazon DynamoDB 테이블을 생성합니다. 처리할 새 데이터에
대한 테이블을 쿼리하도록 애플리케이션을 구성합니다. TTL을 사용하여 처리된 데이터를
제거합니다.
Answer: C
Explanation:
* Requirement Analysis: The application was overwhelmed with unexpected data volume,
leading to data loss and the need for a replay mechanism.
* Amazon SQS Overview: SQS is a fully managed message queuing service that decouples
and scales microservices, distributed systems, and serverless applications.
570

IT Certification Guaranteed, The Easy Way!
* Data Decoupling: By using an SQS queue, the application can store incoming tracker data
reliably and process it asynchronously, preventing data loss.
* Implementation:
* Create an SQS queue.
* Modify the web application to send incoming data to the SQS queue.
* Configure the application instances to poll the SQS queue and process the messages.
* Conclusion: This solution meets the requirements with minimal operational overhead,
ensuring data is not lost and can be processed at the application's own pace.
References
* Amazon SQS: Amazon SQS Documentation
QUESTION NO: 832
한 회사는 Amazon EC2 인스턴스 및 Amazon Elastic Block Store(Amazon EBS)에서 자체
관리형 Microsoft SOL Server를 실행합니다. EBS 볼륨의 일일 스냅샷이 생성됩니다.
최근 만료된 모든 EBS 스냅샷을 삭제하는 스냅샷 정리 스크립트를 실행하는 동안 회사의
모든 EBS 스냅샷이 실수로 삭제되었습니다. 솔루션 아키텍트는 EBS 스냅샷을 무기한
보관하지 않고 데이터 손실을 방지하기 위해 아키텍처를 업데이트해야 합니다.
최소한의 개발 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EBS 스냅샷 삭제를 거부하도록 사용자의 오전 1시 정책을 변경합니다.
B. 매일 스냅샷을 완료한 후 EBS 스냅샷을 다른 AWS 리전에 복사합니다.
C. 휴지통에 7일 EBS 스냅샷 보관 규칙을 생성하고 모든 스냅샷에 규칙을 적용합니다.
D. EBS 스냅샷을 Amazon S3 Standard-Infrequent Access(S3 Standard-IA)에 복사합니다.
Answer: C
Explanation:
* Requirement Analysis: The goal is to prevent accidental deletion of EBS snapshots while
avoiding indefinite retention.
* Recycle Bin for EBS Snapshots: AWS Recycle Bin allows for retention rules that prevent
immediate deletion of snapshots, providing a safety net against accidental deletions.
* Retention Rule: A 7-day retention rule ensures snapshots are not permanently deleted
immediately, giving time to recover from accidental deletions.
* Implementation:
* Enable Recycle Bin in your AWS account.
* Create a retention rule that specifies a 7-day period for EBS snapshots.
* Apply this rule to all EBS snapshots.
* Conclusion: This solution provides an automated way to prevent data loss from accidental
deletions with minimal development effort.
References
* AWS Recycle Bin: AWS Recycle Bin Documentation
QUESTION NO: 833
한 회사에는 여러 AWS 지역의 Amazon EC2 인스턴스에 배포된 HTTP 기반 애플리케이션에
액세스하는 전 세계 사용자가 있습니다. 회사는 애플리케이션의 가용성과 성능을 개선하려고
합니다. 또한 회사는 가용성에 영향을 미치거나, 보안을 손상시키거나, 과도한 리소스를
소비할 수 있는 일반적인 웹 공격으로부터 애플리케이션을 보호하려고 합니다. 고정 IP 주소가
필요합니다.
571

IT Certification Guaranteed, The Easy Way!
이를 달성하기 위해 솔루션 설계자는 무엇을 권장해야 합니까?
A. 각 리전의 NLB(Network Load Balancer) 뒤에 EC2 인스턴스를 배치합니다. NLB에 AWS
WAF를 배포합니다. AWS Global Accelerator를 사용하여 액셀러레이터를 생성하고 NLB를
엔드포인트로 등록합니다.
B. 각 지역의 ALB(Application Load Balancer) 뒤에 EC2 인스턴스를 배치합니다. ALB에 AWS
WAF를 배포합니다. AWS Global Accelerator를 사용하여 액셀러레이터를 생성하고 ALB를
엔드포인트로 등록합니다.
C. 각 지역의 NLB(Network Load Balancer) 뒤에 EC2 인스턴스를 배치합니다. NLB에 AWS
WAF를 배포합니다. Amazon Route 53 지연 시간 기반 라우팅을 사용하여 요청을 NLB로
라우팅하는 오리진으로 Amazon CloudFront 배포를 생성합니다.
D. 각 지역의 ALB(Application Load Balancer) 뒤에 EC2 인스턴스를 배치합니다. Amazon
Route 53 지연 시간 기반 라우팅을 사용하여 요청을 ALB로 라우팅하는 오리진으로 Amazon
CloudFront 배포를 생성합니다. CloudFront 배포에 AWS WAF를 배포합니다.
Answer: A
Explanation:
The company wants to improve the availability and performance of the application, as well as
protect it against common web exploits. The company also needs static IP addresses for the
application. To meet these requirements, a solutions architect should recommend the
following solution:
* Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. NLBs are
designed to handle millions of requests per second while maintaining high throughput at
ultra-low latency. NLBs also support static IP addresses for each Availability Zone, which can
be useful for whitelisting or firewalling purposes.
* Deploy AWS WAF on the NLBs. AWS WAF is a web application firewall that helps protect
web applications from common web exploits that could affect availability, security, or
performance. AWS WAF lets you define customizable web security rules that control which
traffic to allow or block to your web applications.
* Create an accelerator using AWS Global Accelerator and register the NLBs as endpoints.
AWS Global Accelerator is a service that improves the availability and performance of your
applications with local or global users. It provides static IP addresses that act as a fixed entry
point to your application endpoints in any AWS Region. It uses the AWS global network to
optimize the path from your users to your applications, improving the performance of your
TCP and UDP traffic.
This solution will provide high availability across Availability Zones and Regions, improve
performance by routing traffic over the AWS global network, protect the application from
common web attacks, and provide static IP addresses for the application.
References:
* Network Load Balancer
* AWS WAF
* AWS Global Accelerator
QUESTION NO: 834
한 회사에서 클라이언트와 서버 간 통신에 UDP를 사용하는 실시간 멀티플레이어 게임을
개발하고 있습니다. Auto Scaling 그룹에서는 하루 동안 수요가 급증할 것으로 예상되므로
게임 서버 플랫폼도 그에 맞게 조정해야 합니다. 개발자는 게이머 점수 및 기타 정보를
572

IT Certification Guaranteed, The Easy Way!
저장하려고 합니다. 개입 없이 확장되는 데이터베이스 솔루션의 비관계형 데이터 솔루션
설계자는 어떤 솔루션을 권장해야 합니까?
A. 트래픽 분산에는 Amazon Route 53을 사용하고 데이터 저장에는 Amazon Aurora
Serverless를 사용합니다.
B. 트래픽 분산을 위해 Network Load Balancer를 사용하고 데이터 저장을 위해 온디맨드
Amazon DynamoDB를 사용합니다.
C. 트래픽 분산을 위해 Network Load Balancer를 사용하고 데이터 저장을 위해 Amazon
Aurora 글로벌 데이터베이스를 사용합니다.
D. 트래픽 분산을 위해 Application Load Balancer를 사용하고 데이터 저장을 위해 Amazon
DynamoDB 글로벌 테이블을 사용합니다.
Answer: B
Explanation:
A Network Load Balancer is a type of load balancer that operates at the connection level
(Layer 4) and can load balance both TCP and UDP traffic1. A Network Load Balancer is
suitable for scenarios where high performance and low latency are required, such as real-
time multiplayer games1. A Network Load Balancer can also handle sudden and volatile
traffic patterns while using a single static IP address per Availability Zone
1.
To meet the requirements of the scenario, the solutions architect should use a Network Load
Balancer for traffic distribution between the EC2 instances in the Auto Scaling group. The
Network Load Balancer can route UDP traffic from the client to the servers on the appropriate
port2. The Network Load Balancer can also support TLS offloading for secure
communications between the client and servers1.
Amazon DynamoDB is a fully managed NoSQL database service that can store and retrieve
any amount of data with consistent performance and low latency3. Amazon DynamoDB on-
demand is a flexible billing option that requires no capacity planning and charges only for the
read and write requests that are performed on the tables3. Amazon DynamoDB on-demand
is ideal for scenarios where the application traffic is unpredictable or sporadic, such as
gaming applications3.
To meet the requirements of the scenario, the solutions architect should use Amazon
DynamoDB on-demand for data storage. Amazon DynamoDB on-demand can store gamer
scores and other non-relational data without intervention from the developers. Amazon
DynamoDB on-demand can also scale automatically to handle any level of request traffic
without affecting performance or availability3.
QUESTION NO: 835
한 회사가 전자상거래 웹 애플리케이션에 대한 평가 시스템을 개발하고 있습니다. 이 회사는
사용자가 Amazon DynamoDB 테이블에 제출한 평가를 저장하는 솔루션이 필요합니다.
이 회사는 개발자가 DynamoDB 테이블과 직접 상호 작용할 필요가 없도록 하려고 합니다.
솔루션은 확장 가능하고 재사용 가능해야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. 애플리케이션 로드 밸런서(ALB)를 만듭니다. AWS Lambda 함수를 만들고 ALB에서
함수를 대상 그룹으로 설정합니다. ALB를 통해 put_item 메서드를 사용하여 Lambda 함수를
호출합니다.
B. AWS Lambda 함수를 만듭니다. Boto3의 put-item 메서드를 사용하여 DynamoDB 테이블과
573

IT Certification Guaranteed, The Easy Way!
상호 작용하도록 Lambda 함수를 구성합니다. 웹 애플리케이션에서 Lambda 함수를
호출합니다.
C. Amazon Simple Queue Service(Amazon SQS) 대기열과 SQS 트리거 유형이 있는 AWS
Lambda 함수를 만듭니다. 개발자에게 고객 평가를 JSON 메시지로 SQS 대기열에
추가하도록 지시합니다. 대기열에서 평가를 가져와 DynamoDB에 저장하도록 Lambda 함수를
구성합니다.
D. Amazon API Gateway REST API 만들기 리소스 정의 및 새 POST 메서드 만들기 통합
유형으로 AWS를 선택하고 서비스로 DynamoDB를 선택합니다. 작업을 PutItem으로
설정합니다.
Answer: D
Explanation:
Amazon API Gateway provides a scalable and reusable solution for interacting with
DynamoDB without requiring direct access by developers. By setting up a REST API with a
POST method that integrates with DynamoDB's PutItem action, developers can submit data
(such as user ratings) to the DynamoDB table through API Gateway, without having to
directly interact with the database. This solution is serverless and minimizes operational
overhead.
* Option A: Using ALB with Lambda adds complexity and is less efficient for this use case.
* Option B: While using Lambda is possible, API Gateway provides a more scalable, reusable
interface.
* Option C: SQS with Lambda introduces unnecessary components for a simple put
operation.
AWS References:
* Amazon API Gateway with DynamoDB
QUESTION NO: 836
한 회사에 모바일 앱을 사용하는 사용자가 백만 명 있습니다. 회사는 거의 실시간으로 데이터
사용량을 분석해야 합니다. 또한 회사는 거의 실시간으로 데이터를 암호화해야 하며 추가
처리를 위해 중앙 위치에 Apache Parquet 형식으로 데이터를 저장해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon Kinesis 데이터 스트림을 생성하여 Amazon S3에 데이터를 저장합니다. 데이터를
분석하기 위해 Amazon Kinesis Data Analytics 애플리케이션을 생성합니다. AWS Lambda
함수를 호출하여 Kinesis Data Analytics 애플리케이션으로 데이터를 보냅니다.
B. Amazon Kinesis 데이터 스트림을 생성하여 Amazon S3에 데이터를 저장합니다. 데이터를
분석하기 위해 Amazon EMR 클러스터를 생성합니다. AWS Lambda 함수를 호출하여
데이터를 EMR 클러스터로 보냅니다.
C. Amazon Kinesis Data Firehose 전송 스트림을 생성하여 Amazon S3에 데이터를
저장합니다. 데이터를 분석하기 위해 Amazon EMR 클러스터를 생성합니다.
D. Amazon Kinesis Data Firehose 전송 스트림을 생성하여 Amazon S3에 데이터를
저장합니다. 데이터를 분석하기 위한 Amazon Kinesis Data Analytics 애플리케이션 생성
Answer: D
Explanation:
This solution will meet the requirements with the least operational overhead as it uses
Amazon Kinesis Data Firehose, which is a fully managed service that can automatically
handle the data collection, data transformation, encryption, and data storage in near-real
574

IT Certification Guaranteed, The Easy Way!
time. Kinesis Data Firehose can automatically store the data in Amazon S3 in Apache
Parquet format for further processing. Additionally, it allows you to create an Amazon Kinesis
Data Analytics application to analyze the data in near real-time, with no need to manage any
infrastructure or invoke any Lambda function. This way you can process a large amount of
data with the least operational overhead.
QUESTION NO: 837
회사에서는 기계 학습과 보고를 위해 대규모로 차량의 원격 측정 데이터를 수집하고 분석해야
합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. Amazon Timestream for LiveAnalytics를 사용하여 데이터 포인트를 저장합니다. Amazon
SageMaker에 데이터 액세스 권한을 부여합니다. Amazon QuickSight를 사용하여 데이터를
시각화합니다.
B. Amazon DynamoDB를 사용하여 데이터 포인트를 저장합니다. DynamoDB Connector를
사용하여 Amazon EMR에 데이터를 수집하여 처리합니다. Amazon QuickSight를 사용하여
데이터를 시각화합니다.
C. Amazon Neptune을 사용하여 데이터 포인트를 저장합니다. Amazon Kinesis Data
Streams를 사용하여 Lambda 함수로 데이터를 수집하여 처리합니다. Amazon QuickSight를
사용하여 데이터를 시각화합니다.
D. Amazon Timestream for LiveAnalytics를 사용하여 데이터 포인트를 저장합니다. Amazon
SageMaker에 데이터 액세스 권한을 부여합니다. Amazon Athena를 사용하여 데이터를
시각화합니다.
Answer: A
* Amazon Timestream is purpose-built for storing and analyzing time-series data like
telemetry.
* Option A leverages Timestream, SageMaker for ML, and QuickSight for visualization,
meeting all requirements with minimal complexity.
* Option B involves more complex DynamoDB-EMR integration.
* Option C uses Neptune, which is designed for graph databases, not telemetry data.
* Option D incorrectly uses Athena for visualization instead of QuickSight.
QUESTION NO: 838
회사는 AWS에서 워크로드를 실행합니다. 회사는 외부 공급자의 서비스에 연결해야 합니다.
서비스는 공급자의 VPC에서 호스팅됩니다. 회사 보안팀에 따르면 연결은 비공개여야 하며
대상 서비스로 제한되어야 합니다. 연결은 회사의 VPC에서만 시작되어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 회사의 VPC와 공급자의 VPC 간에 VPC 피어링 연결을 생성합니다. 대상 서비스에
연결하려면 라우팅 테이블을 업데이트하세요.
B. 공급자에게 해당 VPC에 가상 프라이빗 게이트웨이를 생성해 달라고 요청하세요. AWS
PrivateLink를 사용하여 대상 서비스에 연결합니다.
C. 회사 VPC의 퍼블릭 서브넷에 NAT 게이트웨이를 생성합니다. 대상 서비스에 연결하려면
라우팅 테이블을 업데이트하세요.
D. 공급자에게 대상 서비스에 대한 VPC 엔드포인트를 생성하도록 요청합니다. AWS
PrivateLink를 사용하여 대상 서비스에 연결합니다.
Answer: D
575

IT Certification Guaranteed, The Easy Way!
Explanation:
**AWS PrivateLink provides private connectivity between VPCs, AWS services, and your on-
premises networks, without exposing your traffic to the public internet**. AWS PrivateLink
makes it easy to connect services across different accounts and VPCs to significantly simplify
your network architecture. Interface
**VPC endpoints**, powered by AWS PrivateLink, connect you to services hosted by AWS
Partners and supported solutions available in AWS Marketplace.
https://aws.amazon.com/privatelink/
QUESTION NO: 839
한 회사는 필요한 인프라를 수동으로 프로비저닝하여 새 웹사이트에 대한 인프라
프로토타입을 만들고 있습니다. 이 인프라에는 Auto Scaling 그룹, Application Load Balancer
및 Amazon RDS 데이터베이스가 포함됩니다. 구성이 철저하게 검증된 후 회사는 자동화된
방식으로 두 개의 가용 영역에서 개발 및 프로덕션 사용을 위한 인프라를 즉시 배포할 수 있는
기능을 원합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 권장해야 합니까?
A. AWS Systems Manager를 사용하여 두 개의 가용 영역에서 프로토타입 인프라를 복제하고
프로비저닝합니다.
B. 프로토타입 인프라를 가이드로 사용하여 인프라를 템플릿으로 정의합니다. AWS
CloudFormation을 사용하여 인프라 배포
C. AWS Config를 사용하여 프로토타입 인프라에 사용되는 리소스의 인벤토리를 기록합니다.
AWS Config를 사용하여 프로토타입 인프라를 두 개의 가용 영역에 배포합니다.
D. AWS Elastic Beanstalk를 사용하고 프로토타입 인프라에 대한 자동화된 참조를 사용하여
두 개의 가용 영역에 새 환경을 자동으로 배포하도록 구성합니다.
Answer: B
Explanation:
AWS CloudFormation is a service that helps you model and set up your AWS resources by
using templates that describe all the resources that you want, such as Auto Scaling groups,
load balancers, and databases. You can use AWS CloudFormation to deploy your
infrastructure in an automated and consistent way across multiple environments and regions.
You can also use AWS CloudFormation to update or delete your infrastructure as a single
unit.
Reference URLs:
1 https://aws.amazon.com/cloudformation/
2 https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html
3 https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-
concepts.html
QUESTION NO: 840
솔루션 설계자는 공용 서브넷과 데이터베이스 서브넷을 포함하는 2계층 아키텍처를 설계하고
있습니다. 퍼블릭 서브넷의 웹 서버는 포트 443에서 인터넷에 열려 있어야 합니다.
데이터베이스 서브넷의 MySQL D6용 Amazon RDS 인스턴스는 포트 3306의 웹 서버에만
액세스할 수 있어야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까?
(2개를 선택하세요.)
576

IT Certification Guaranteed, The Easy Way!
A. 퍼블릭 서브넷에 대한 네트워크 ACL을 생성합니다. 포트 3306에서 0 0 0 0/0으로의
아웃바운드 트래픽을 거부하는 규칙을 추가합니다.
B. DB 인스턴스에 대한 보안 그룹을 생성합니다. 포트 3306에서 퍼블릭 서브넷 CIDR 블록의
트래픽을 허용하는 규칙을 추가합니다.
C. 퍼블릭 서브넷의 웹 서버에 대한 보안 그룹을 생성합니다. 포트 443에서 0 0 0 O'O의
트래픽을 허용하는 규칙을 추가합니다.
D. DB 인스턴스에 대한 보안 그룹을 생성합니다. 포트 3306에서 웹 서버 보안 그룹의
트래픽을 허용하는 규칙을 추가합니다.
E. DB 인스턴스에 대한 보안 그룹 생성 포트 3306에서 웹 서버 보안 그룹의 트래픽을 제외한
모든 트래픽을 거부하는 규칙을 추가합니다.
Answer: B C
Explanation:
Security groups are virtual firewalls that protect AWS instances and can be applied to EC2,
ELB and RDS1
. Security groups have rules for inbound and outbound traffic and are stateful, meaning that
responses to allowed inbound traffic are allowed to flow out of the instance2. Network ACLs
are different from security groups in several ways. They cover entire subnets, not individual
instances, and are stateless, meaning that they require rules for both inbound and outbound
traffic2. Network ACLs also support deny rules, while security groups only support allow
rules2.
To meet the requirements of the scenario, the solutions architect should create two security
groups: one for the DB instance and one for the web servers in the public subnet. The
security group for the DB instance should allow traffic from the public subnet CIDR block on
port 3306, which is the default port for MySQL3. This way, only the web servers in the public
subnet can access the DB instance on that port. The security group for the web servers
should allow traffic from 0 0 0 O'O on port 443, which is the default port for HTTPS4. This
way, the web servers can accept secure connections from the internet on that port.
QUESTION NO: 841
한 회사에서 전자상거래 웹사이트를 위한 다중 계층 애플리케이션을 만들었습니다. 웹
사이트는 퍼블릭 서브넷에 있는 Application Load Balancer, 퍼블릭 서브넷의 웹 계층,
프라이빗 서브넷의 Amazon EC2 인스턴스에서 호스팅되는 MySQL 클러스터를 사용합니다.
MySQL 데이터베이스는 타사 제공업체가 인터넷에서 호스팅하는 제품 카탈로그 및 가격
정보를 검색해야 합니다. 솔루션 설계자는 운영 오버헤드를 늘리지 않고 보안을 극대화하는
전략을 고안해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. VPC에 NAT 인스턴스를 배포합니다. NAT 인스턴스를 통해 모든 인터넷 기반 트래픽을
라우팅합니다.
B. 퍼블릭 서브넷에 NAT 게이트웨이를 배포합니다. 모든 인터넷 바인딩 트래픽이 NAT
게이트웨이로 향하도록 프라이빗 서브넷 라우팅 테이블을 수정합니다.
C. 인터넷 게이트웨이를 구성하고 이를 VPC에 연결합니다. 인터넷 바인딩 트래픽을 인터넷
게이트웨이로 전달하도록 프라이빗 서브넷 라우팅 테이블을 수정합니다.
D. 가상 프라이빗 게이트웨이를 구성하고 이를 VPC에 연결합니다. 인터넷 바인딩 트래픽을
가상 프라이빗 게이트웨이로 전달하도록 프라이빗 서브넷 라우팅 테이블을 수정합니다.
577

IT Certification Guaranteed, The Easy Way!
Answer: B
Explanation:
To allow the MySQL database in the private subnets to access the internet without exposing
it to the public, a NAT gateway is a suitable solution. A NAT gateway enables instances in a
private subnet to connect to the internet or other AWS services, but prevents the internet
from initiating a connection with those instances. A NAT gateway resides in the public
subnets and can handle high throughput of traffic with low latency. A NAT gateway is also a
managed service that does not require any operational overhead.
References:
* NAT Gateways
* NAT Gateway Pricing
QUESTION NO: 842
회사는 Application Load Balancer 뒤의 Amazon EC2 인스턴스에서 비즈니스 크리티컬 웹
애플리케이션을 실행하고 있습니다. EC2 인스턴스는 Auto Scaling 그룹에 있습니다.
애플리케이션은 단일 가용 영역에 배포된 Amazon Aurora PostgreSQL 데이터베이스를
사용합니다. 회사는 다운타임과 데이터 손실을 최소화하면서 애플리케이션의 고가용성을
원합니다.
최소한의 운영 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. EC2 인스턴스를 다른 AWS 리전에 배치합니다. Amazon Route 53 상태 확인을 사용하여
트래픽을 리디렉션합니다. Aurora PostgreSQL 교차 리전 복제를 사용합니다.
B. 여러 가용 영역을 사용하도록 Auto Scaling 그룹을 구성합니다. 데이터베이스를 다중 AZ로
구성합니다. 데이터베이스에 대한 Amazon RDS 프록시 인스턴스를 구성합니다.
C. 하나의 가용 영역을 사용하도록 Auto Scaling 그룹을 구성합니다. 데이터베이스의 시간별
스냅샷을 생성합니다. 장애가 발생한 경우 스냅샷에서 데이터베이스를 복구합니다.
D. 여러 AWS 리전을 사용하도록 Auto Scaling 그룹을 구성합니다. 애플리케이션의 데이터를
Amazon S3에 씁니다. S3 이벤트 알림을 사용하여 AWS Lambda 함수를 시작하여
데이터베이스에 데이터를 씁니다.
Answer: B
Explanation:
To achieve high availability with minimum downtime and minimum loss of data, the Auto
Scaling group should be configured to use multiple Availability Zones to ensure that there is
no single point of failure. The database should be configured as Multi-AZ to enable automatic
failover in case of an outage in the primary Availability Zone. Additionally, an Amazon RDS
Proxy instance can be used to improve the scalability and availability of the database by
reducing connection failures and improving failover times.
QUESTION NO: 843
솔루션 설계자는 회사의 스토리지 비용을 줄이기 위한 솔루션을 구현해야 합니다. 회사의
모든 데이터는 Amazon S3 Standard 스토리지 클래스에 있습니다. 회사는 모든 데이터를 최소
25년 동안 보관해야 합니다. 최근 2년간의 데이터는 가용성이 높고 즉시 검색 가능해야
합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 객체를 S3 Glacier Deep Archive로 즉시 전환하도록 S3 수명 주기 정책을 설정합니다.
B. 2년 후에 객체를 S3 Glacier Deep Archive로 전환하도록 S3 수명 주기 정책을 설정합니다.
578

IT Certification Guaranteed, The Easy Way!
C. S3 지능형 계층화를 사용합니다. 데이터가 S3 Glacier Deep Archive에 보관되도록 보관
옵션을 활성화합니다.
D. 객체를 S3 One Zone-Infrequent Access(S3 One Zone-IA)로 즉시 전환하고 2년 후에 S3
Glacier Deep Archive로 전환하도록 S3 수명 주기 정책을 설정합니다.
Answer: B
Explanation:
https://aws.amazon.com/about-aws/whats-new/2018/04/announcing-s3-one-zone-infrequent-
access-a-new- amazon-s3-storage-class/?nc1=h_ls
QUESTION NO: 844
솔루션 아키텍트는 개인 식별 정보(Pll)를 검색하기 위해 회사의 Amazon S3 버킷을 검토해야
합니다. 회사는 us-east-I 지역과 us-west-2 지역에 Pll 데이터를 저장합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 각 리전에서 Amazon Macie를 구성합니다. Amazon S3_에 있는 데이터를 분석하는 작업
생성
B. 모든 지역에 대해 AWS Security Hub를 구성합니다. Amazon S3_에 있는 데이터를
분석하기 위한 AWS Config 규칙을 생성합니다.
C. Amazon S3에 있는 데이터를 분석하도록 Amazon Inspector를 구성합니다.
D. Amazon S3에 있는 데이터를 분석하도록 Amazon GuardDuty를 구성합니다.
Answer: A
Explanation:
it allows the solutions architect to review the S3 buckets to discover personally identifiable
information (Pll) with the least operational overhead. Amazon Macie is a fully managed data
security and data privacy service that uses machine learning and pattern matching to
discover and protect sensitive data in AWS. Amazon Macie can analyze data in S3 buckets
across multiple regions and provide insights into the type, location, and level of sensitivity of
the data. References:
* Amazon Macie
* Analyzing data with Amazon Macie
QUESTION NO: 845
온라인 교육 플랫폼은 수천 명의 학생이 동시에 비디오 수업에 접속하는 피크 사용 시간 동안
지연과 버퍼링을 경험합니다. 솔루션 아키텍트는 교육 플랫폼의 성능을 개선해야 합니다.
플랫폼은 반응성을 잃지 않으면서 예측할 수 없는 트래픽 급증을 처리해야 합니다. 플랫폼은
항상 원활한 비디오 재생 성능을 제공해야 합니다. 플랫폼은 각 비디오 레슨의 사본을 여러 개
만들고 다양한 비트레이트로 사본을 저장하여 인터넷 속도가 다른 사용자에게 제공해야
합니다. 가장 작은 비디오 크기는 7GB입니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. Amazon ElastiCache를 사용하여 모든 필수 비트레이트로 비디오를 캐시합니다. AWS
Lambda 함수를 사용하여 비디오를 처리하고 비디오를 필수 비트레이트로 변환합니다.
B. 최대 부하에 맞춰 크기가 조정된 Amazon EC2 인스턴스를 포함하는 자동 크기 조정 그룹을
만듭니다.
Auto Scaling 그룹을 사용하여 비디오를 제공합니다. Auto Scaling 그룹을 사용하여 비디오를
필요한 비트레이트로 변환합니다.
579

IT Certification Guaranteed, The Easy Way!
C. 모든 비디오의 사본을 Amazon S3 버킷에 필요한 모든 비트레이트로 저장합니다. 단일
Amazon EC2 인스턴스를 사용하여 비디오를 제공합니다.
D. Amazon Kinesis Video Streams를 사용하여 비디오를 저장하고 제공합니다. AWS Lambda
함수를 사용하여 비디오를 처리하고 비디오를 필요한 비트레이트로 변환합니다.
Answer: C
Explanation:
The most cost-effective solution for serving video content with different bitrates is to store
multiple versions of each video in Amazon S3. S3 provides scalable and cost-effective
storage for large media files. Serving the videos from a single Amazon EC2 instance ensures
low-latency delivery, and S3 storage helps minimize costs.
* Option A (ElastiCache): Caching large video files in memory would be prohibitively
expensive and unnecessary.
* Option B (Auto Scaling group): Using Auto Scaling groups to serve video is less cost-
effective compared to leveraging S3 for static storage.
* Option D (Kinesis Video Streams): Kinesis Video Streams is designed for real-time video
streaming and is not suitable for storing and serving pre-recorded videos.
AWS References:
* Amazon S3 for Media Storage
QUESTION NO: 846
미디어 회사가 AWS에서 비디오 처리 워크로드를 호스팅합니다. 워크로드는 Auto Scaling
그룹의 Amazon EC2 인스턴스를 사용하여 다양한 수준의 수요를 처리합니다. 워크로드는
원본 비디오와 처리된 비디오를 Amazon S3 버킷에 저장합니다.
이 회사는 비디오 처리 작업 부하가 확장 가능한지 확인하고자 합니다. 이 회사는 리소스
제약으로 인해 처리 시도가 실패하는 것을 방지하고자 합니다. 아키텍처는 처리 기능에
영향을 주지 않고 비디오 업로드의 갑작스러운 급증을 처리할 수 있어야 합니다.
어떤 솔루션이 최소한의 오버헤드로 이러한 요구 사항을 충족할 수 있을까요?
A. Amazon EC2 인스턴스에서 AWS Lambda 함수로 워크로드를 마이그레이션합니다. 새
비디오가 업로드되면 Lambda 함수를 호출하도록 Amazon S3 이벤트 알림을 구성합니다.
Lambda 함수를 구성하여 비디오를 직접 처리하고 처리된 비디오를 S3 버킷에 다시
저장합니다.
B. Amazon EC2 인스턴스에서 AWS Lambda 함수로 워크로드를 마이그레이션합니다. 새
비디오가 업로드되면 Amazon S3를 사용하여 Amazon Simple Notification Service(Amazon
SNS) 토픽을 호출합니다.
Lambda 함수를 SNS 토픽에 구독합니다. Lambda 함수를 구성하여 비디오를 비동기적으로
처리하고 처리된 비디오를 S3 버킷에 다시 저장합니다.
C. 새 비디오가 업로드되면 Amazon Simple Queue Service(Amazon SQS) 대기열에 메시지를
보내도록 Amazon S3 이벤트 알림을 구성합니다. 기존 Auto Scaling 그룹을 구성하여 SQS
대기열을 폴링하고 비디오를 처리하고 처리된 비디오를 S3 버킷에 다시 저장합니다.
D. 새 비디오가 업로드될 때 AWS Step Functions 상태 머신을 호출하도록 Amazon S3 업로드
트리거를 구성합니다. 작업 메시지를 Amazon SQS 대기열에 배치하여 비디오 처리
워크플로를 조정하도록 상태 머신을 구성합니다. 작업 메시지를 구성하여 비디오를 처리하기
위해 EC2 인스턴스를 호출합니다. 처리된 비디오를 S3 버킷에 다시 저장합니다.
Answer: C
Explanation:
580

IT Certification Guaranteed, The Easy Way!
This solution addresses the scalability needs of the workload while preventing failed
processing attempts due to resource constraints.
* Amazon S3 event notifications can be used to trigger a message to an SQS queue
whenever a new video is uploaded.
* The existing Auto Scaling group of EC2 instances can poll the SQS queue, ensuring that
the EC2 instances only process videos when there is a job in the queue.
* SQS decouples the video upload and processing steps, allowing the system to handle
sudden spikes in video uploads without overloading EC2 instances.
The use of Auto Scaling ensures that the EC2 instances can scale in or out based on the
demand, maintaining cost efficiency while avoiding processing failures due to insufficient
resources.
AWS References:
* S3 Event Notifications details how to configure notifications for S3 events.
* Amazon SQS is a fully managed message queuing service that decouples components of
the system.
* Auto Scaling EC2 explains how to manage automatic scaling of EC2 instances based on
demand.
Why the other options are incorrect:
* A. AWS Lambda functions: While Lambda can handle some workloads, video processing is
often resource-intensive and long-running, making EC2 a more suitable solution.
* B. Using SNS with Lambda: Similar to A, Lambda is not ideal for large-scale video
processing due to its time and memory limitations.
* D. AWS Step Functions: While a valid orchestration solution, this introduces more
complexity and overhead compared to the simpler SQS-based solution.
QUESTION NO: 847
온라인 게임 회사가 회사의 성장하는 사용자 기반을 지원하기 위해 사용자 데이터 스토리지를
Amazon DynamoDB로 전환하고 있습니다. 현재 아키텍처에는 사용자 프로필, 업적 및 게임
내 거래가 포함된 DynamoDB 테이블이 포함됩니다.
회사에서는 사용자에게 원활한 게임 경험을 제공하기 위해 강력하고 지속적으로 사용
가능하며 회복성이 뛰어난 DynamoDB 아키텍처를 설계해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. 단일 AWS 리전에서 DynamoDB 테이블을 만듭니다. 주문형 용량 모드를 사용합니다.
글로벌 테이블을 사용하여 여러 리전에 데이터를 복제합니다.
B. DynamoDB Accelerator(DAX)를 사용하여 자주 액세스하는 데이터를 캐시합니다. 단일
AWS 리전에 테이블을 배포하고 자동 확장을 활성화합니다. 추가 리전에 대한 크로스 리전
복제를 수동으로 구성합니다.
C. 여러 AWS 리전에 DynamoDB 테이블을 만듭니다. 주문형 용량 모드를 사용합니다. 리전 간
크로스 리전 복제를 위해 DynamoDB Streams를 사용합니다.
D. Use DynamoDB global tables for automatic multi-Region replication. Deploy tables in
multiple AWS Regions. Use provisioned capacity mode. Enable auto scaling.
Answer: D
Explanation:
DynamoDB Global Tables provide a fully managed, multi-region, and multi-master database
solution that allows you to deploy DynamoDB tables in multiple AWS Regions. This ensures
581

IT Certification Guaranteed, The Easy Way!
high availability and resiliency across different geographical locations, providing a seamless
gaming experience for users. Using provisioned capacity mode with auto-scaling ensures
cost-efficiency by scaling up or down based on actual demand.
* Option A: While on-demand capacity mode is flexible, provisioned capacity with auto-
scaling is more cost-effective for predictable workloads.
* Option B (DAX): DAX improves read performance, but it doesn't provide the multi-region
replication needed for high availability and resiliency.
* Option C: DynamoDB Streams with manual cross-region replication adds more complexity
and operational overhead compared to Global Tables.
AWS References:
* DynamoDB Global Tables
QUESTION NO: 848
VPC-A의 Amazon EC2 인스턴스에서 실행되는 애플리케이션은 VPC-B의 다른 EC2
인스턴스에 있는 파일에 액세스해야 합니다. 두 VPC 모두 별도의 AWS 계정에 있습니다.
네트워크 관리자는 VPC-A에서 VPC-B의 EC2 인스턴스에 대한 보안 액세스를 구성하기 위한
솔루션을 설계해야 합니다.
연결에는 단일 장애 지점이나 대역폭 문제가 있어서는 안 됩니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. VPC-A와 VPC-B 사이에 VPC 피어링 연결을 설정합니다.
B. VPC-B에서 실행되는 EC2 인스턴스에 대한 VPC 게이트웨이 엔드포인트를 설정합니다.
C. 가상 프라이빗 게이트웨이를 VPC-B에 연결하고 VPC-A에서 라우팅을 설정합니다.
D. VPC-B에서 실행되는 EC2 인스턴스에 대한 프라이빗 가상 인터페이스(VIF)를 생성하고
VPC-A에서 적절한 경로를 추가합니다.
Answer: A
Explanation:
AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is
neither a gateway nor a VPN connection, and does not rely on a separate piece of physical
hardware. There is no single point of failure for communication or a bandwidth bottleneck.
https://docs.aws.amazon.com/vpc/latest/peering/what-is- vpc-peering.html
QUESTION NO: 849
한 회사가 지난 3개월 동안 여러 애플리케이션을 AWS로 마이그레이션했습니다. 이 회사는 각
애플리케이션의 비용 내역을 알고 싶어합니다. 이 회사는 이 정보가 포함된 정기 보고서를
받고 싶어합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족할 수 있는 솔루션은 무엇일까요?
A. AWS Budgets를 사용하여 지난 3개월 동안의 데이터를 csv 파일로 다운로드합니다.
원하는 정보를 찾습니다.
B. AWS 비용 및 사용 보고서를 Amazon RDS DB 인스턴스에 로드합니다. SQL 쿼리를
실행하여 원하는 정보를 젤화합니다.
C. 모든 AWS 리소스에 비용 키와 애플리케이션 이름 값을 태그합니다. 비용 할당 태그 활성화
Cost Explorer를 사용하여 원하는 정보를 얻습니다.
D. 모든 AWS 리소스에 비용 키와 애플리케이션 이름 값을 태그합니다. AWS Billing and Cost
Management 콘솔을 사용하여 지난 3개월 동안의 청구서를 다운로드합니다. 원하는 정보를
찾습니다.
582

IT Certification Guaranteed, The Easy Way!
Answer: C
Explanation:
This solution is the most cost-effective and efficient way to break down costs per application.
* Tagging Resources: By tagging all AWS resources with a specific key (e.g., "cost") and a
value representing the application's name, you can easily identify and categorize costs
associated with each application. This tagging strategy allows for granular tracking of costs
within AWS.
* Activating Cost Allocation Tags: Once tags are applied to resources, you need to activate
cost allocation tags in the AWS Billing and Cost Management console. This ensures that the
costs associated with each tag are included in your billing reports and can be used for cost
analysis.
* AWS Cost Explorer: Cost Explorer is a powerful tool that allows you to visualize,
understand, and manage your AWS costs and usage over time. You can filter and group your
cost data by the tags you' ve applied to resources, enabling you to easily see the cost
breakdown for each application. Cost Explorer also supports generating regular reports,
which can be scheduled and emailed to stakeholders.
* Why Not Other Options?:
* Option A (AWS Budgets): AWS Budgets is more focused on setting cost and usage
thresholds and monitoring them, rather than providing detailed cost breakdowns by
application.
* Option B (Load Cost and Usage Reports into RDS): This approach is less cost-effective and
involves more operational overhead, as it requires setting up and maintaining an RDS
instance and running SQL queries.
* Option D (AWS Billing and Cost Management Console): While you can download bills, this
method is more manual and less dynamic compared to using Cost Explorer with activated
tags.
AWS References:
* AWS Tagging Strategies - Overview of how to use tagging to organize and track AWS
resources.
* AWS Cost Explorer - Details on how to use Cost Explorer to analyze costs.
QUESTION NO: 850
한 회사는 Amazon API Gateway와 AWS Lambd를 사용하여 AWS에서 RESTful 서버리스 웹
애플리케이션을 구축하고 있습니다. 이 웹 애플리케이션의 사용자는 지리적으로 분산되어
있으며 회사는 이러한 사용자에 대한 API 요청의 대기 시간을 줄이고 싶어합니다. 솔루션
설계자는 이러한 요구 사항을 충족하기 위해 어떤 유형의 엔드포인트를 사용해야 합니까?
A. 프라이빗 엔드포인트
B. 지역 종점
C. 인터페이스 VPC 엔드포인트
D. 엣지 최적화 엔드포인트
Answer: D
Explanation:
An edge-optimized API endpoint is best for geographically distributed clients, as it routes the
API requests to the nearest CloudFront Point of Presence (POP). This reduces the latency
and improves the performance of the API. Edge-optimized endpoints are the default type for
583

IT Certification Guaranteed, The Easy Way!
API Gateway REST APIs1.
A regional API endpoint is intended for clients in the same region as the API, and it does not
use CloudFront to route the requests. A private API endpoint is an API endpoint that can only
be accessed from a VPC using an interface VPC endpoint. A regional or private endpoint
would not meet the requirement of reducing the latency for geographically distributed users1.
QUESTION NO: 851
한 회사가 Amazon EC2 인스턴스를 사용하고 Amazon Elastic Block Store(Amazon EBS)
볼륨에 데이터를 저장합니다. 회사는 AWS Key Management Service(AWS KMS)를 사용하여
모든 데이터가 저장 중에 암호화되도록 해야 합니다. 회사는 암호화 키의 순환을 제어할 수
있어야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. 고객 관리 키를 생성합니다. 이 키를 사용하여 EBS 볼륨을 암호화합니다.
B. AWS 관리 키를 사용하여 EBS 볼륨을 암호화합니다. 키를 사용하여 자동 키 로테이션을
구성합니다.
C. 가져온 키 자료로 외부 KMS 키를 만듭니다. 키를 사용하여 EBS 볼륨을 암호화합니다.
D. AWS 소유 키를 사용하여 EBS 볼륨을 암호화합니다.
Answer: A
Explanation:
To meet the requirement of controlling key rotation with minimal operational overhead,
creating a customer managed key (CMK) in AWS KMS is the optimal solution. With CMKs,
you can define custom key rotation policies, ensuring that you retain control over the key
lifecycle, including enabling automatic key rotation every year.
Key AWS features:
* Custom Key Management: A customer managed key allows you to control the key policies,
lifecycle, and enable key rotation for compliance.
* Least Operational Overhead: Using a customer managed key simplifies encryption
management while offering more flexibility than AWS managed or owned keys.
* AWS Documentation: The AWS Well-Architected Framework recommends customer
managed keys for environments where key control and flexibility are required.
QUESTION NO: 852
회사는 여러 AWS 계정에 수 페타바이트의 데이터를 저장합니다. 회사는 AWS Lake
Formation을 사용하여 데이터 레이크를 관리합니다. 회사의 데이터 과학 팀은 분석 목적으로
회사 엔지니어링 팀과 계정의 선택적 데이터를 안전하게 공유하려고 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 필수 데이터를 일반 계정에 복사합니다. 해당 계정에서 오전 1시 액세스 역할을
생성합니다. 엔지니어링 팀 계정의 사용자를 신뢰할 수 있는 엔터티로 포함하는 권한 정책을
지정하여 액세스 권한을 부여합니다.
B. 필요한 엔지니어링 팀 사용자가 데이터에 액세스할 수 있도록 데이터가 저장되는 각
계정에서 Lake Formation 권한 부여 명령을 사용합니다.
C. AWS Data Exchange를 사용하여 필요한 엔지니어링 팀 계정에 필요한 데이터를 비공개로
게시합니다.
D. Lake Formation 태그 기반 액세스 제어를 사용하여 엔지니어링 팀 계정에 필요한 데이터에
대한 교차 계정 권한을 승인하고 부여합니다.
584

IT Certification Guaranteed, The Easy Way!
Answer: D
Explanation:
* Understanding the Requirement: The data science team needs to securely share selective
data with the engineering team across multiple AWS accounts with minimal operational
overhead.
* Analysis of Options:
* Copy data to a common account: Involves data duplication and increased storage costs,
and requires managing additional permissions.
* Lake Formation permissions Grant command: This method can be effective but may involve
significant operational overhead if managing permissions across multiple accounts and
datasets manually.
* AWS Data Exchange: Designed for sharing data externally or between organizations, which
adds unnecessary complexity for internal sharing within the same organization.
* Lake Formation tag-based access control: Provides a scalable and efficient way to manage
access permissions based on tags, allowing fine-grained control and simplified management
across accounts.
* Best Solution:
* Lake Formation tag-based access control: This solution meets the requirements with the
least operational overhead, allowing efficient management of cross-account permissions and
secure data sharing.
References:
* AWS Lake Formation
* Tag-based access control
QUESTION NO: 853
개발자는 AWS Lambda 함수를 사용하여 Amazon S3에 파일을 업로드하는 애플리케이션을
보유하고 있으며 작업을 수행하는 데 필요한 권한이 필요합니다. 개발자는 이미 Amazon S3에
필요한 유효한 IAM 자격 증명을 가진 IAM 사용자를 보유하고 있습니다. 허가?
A. Lambda 함수의 리소스 정책에 필수 IAM 권한을 추가합니다.
B. Lambda 함수의 기존 IAM 자격 증명을 사용하여 서명된 요청을 생성합니다.
C. 새 IAM 사용자를 생성하고 Lambda 함수에서 기존 IAM 자격 증명을 사용합니다.
D. 필요한 권한이 있는 IAM 실행 역할을 생성하고 IAM 로트를 Lambda 함수에 연결합니다.
Answer: D
Explanation:
To grant the necessary permissions to an AWS Lambda function to upload files to Amazon
S3, a solutions architect should create an IAM execution role with the required permissions
and attach the IAM role to the Lambda function. This approach follows the principle of least
privilege and ensures that the Lambda function can only access the resources it needs to
perform its specific task.
QUESTION NO: 854
회사는 회사 AWS 계정의 Amazon DynamoDB 테이블에 중요한 데이터를 저장합니다. IT
관리자가 실수로 DynamoDB 테이블을 삭제했습니다. 삭제로 인해 상당한 데이터 손실이
발생하고 회사 운영이 중단되었습니다. 회사는 앞으로 이러한 유형의 중단을 방지하기를
원합니다.
585

IT Certification Guaranteed, The Easy Way!
최소한의 운영 오버헤드로 이 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS CloudTrail에서 추적을 구성합니다. 삭제 작업에 대한 Amazon EventBridge 규칙을
생성합니다. 삭제된 DynamoDB 테이블을 자동으로 복원하는 AWS Lambda 함수를
생성합니다.
B. DynamoDB 테이블에 대한 백업 및 복원 계획을 생성합니다. DynamoDB 테이블을
수동으로 복구합니다.
C. DynamoDB 테이블에 대한 삭제 방지를 구성합니다.
D. DynamoDB 테이블에서 특정 시점 복구를 활성화합니다.
Answer: C
Explanation:
Deletion protection is a feature of DynamoDB that prevents accidental deletion of tables.
When deletion protection is enabled, you cannot delete a table unless you explicitly disable it
first. This adds an extra layer of security and reduces the risk of data loss and operational
disruption. Deletion protection is easy to enable and disable using the AWS Management
Console, the AWS CLI, or the DynamoDB API. This solution has the least operational
overhead, as you do not need to create, manage, or invoke any additional resources or
services. References:
* Using deletion protection to protect your table
* Preventing Accidental Table Deletion in DynamoDB
* Amazon DynamoDB now supports table deletion protection
QUESTION NO: 855
회사에서 기존 애플리케이션을 AWS로 마이그레이션하고 있습니다. 애플리케이션은 매시간
배치 작업을 실행하고 CPU를 많이 사용합니다. 온프레미스 서버에서 배치 작업은 평균
15분이 걸립니다. 서버에는 64개의 가상 CPU(vCPU)와 512GiB의 메모리가 있습니다.
솔루션은 최소한의 운영 오버헤드로 15분 이내에 배치 작업을 실행합니까?
A. 기능적 확장과 함께 AWS Lambda 사용
B. AWS Fargate와 함께 Amazon Elastic Container Service(Amazon ECS) 사용
C. AWS Auto Scaling과 함께 Amazon Lightsail 사용
D. Amazon EC2에서 AWS Batch 사용
Answer: D
Explanation:
Use AWS Batch on Amazon EC2. AWS Batch is a fully managed batch processing service
that can be used to easily run batch jobs on Amazon EC2 instances. It can scale the number
of instances to match the workload, allowing the batch job to be completed in the desired
time frame with minimal operational overhead.
Using AWS Lambda with Amazon API Gateway - AWS Lambda
https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html
AWS Lambda FAQs
https://aws.amazon.com/lambda/faqs/
QUESTION NO: 856
한 회사가 Amazon S3 버킷을 저장용으로 사용할 파일 공유 애플리케이션을 개발 중입니다.
회사는 Amazon CloudFront 배포를 통해 모든 파일을 제공하려고 합니다. 회사는 S3 URL에
대한 직접 탐색을 통해 파일에 액세스하는 것을 원하지 않습니다.
586

IT Certification Guaranteed, The Easy Way!
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. 각 S3 버킷에 대해 개별 정책을 작성하여 CloudFront 액세스에 대해서만 읽기 권한을
부여합니다.
B. IAM 사용자를 생성합니다. 사용자에게 S3 버킷의 객체에 대한 읽기 권한을 부여합니다.
사용자를 CloudFront에 할당합니다.
C. CloudFront 배포 ID를 주체로 할당하고 대상 S3 버킷을 Amazon 리소스 이름(ARN)으로
할당하는 S3 버킷 정책을 작성합니다.
D. OAI(원본 액세스 ID)를 생성합니다. CloudFront 배포에 OAI를 할당합니다. OAI에만 읽기
권한이 있도록 S3 버킷 권한을 구성합니다.
Answer: D
Explanation:
https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3
/
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-
restricting-access-to- s3.html#private-content-restricting-access-to-s3-overview
QUESTION NO: 857
게임 회사가 사용자 데이터를 저장하기 위해 데이터베이스를 사용하는 애플리케이션을
구축하고 있습니다. 이 회사는 보조 AWS 지역에 데이터를 쓸 수 있는 액티브-액티브 구성을
데이터베이스에 적용하고자 합니다. 데이터베이스는 1초 미만의 복구 지점 목표(RPO)를
달성해야 합니다.
옵션:
A. Amazon ElastiCache(Redis OSS) 클러스터를 배포합니다. 재해 복구를 위한 글로벌
데이터 스토어를 구성합니다. 기본 지역에 배포된 Amazon RDS 데이터베이스에서 데이터를
캐시하도록 ElastiCache 클러스터를 구성합니다.
B. 기본 지역과 보조 지역에 Amazon DynamoDB 테이블을 배포합니다. Amazon DynamoDB
Streams를 구성하여 AWS Lambda 함수를 호출하여 기본 지역의 테이블에서 보조 지역의
테이블에 변경 사항을 씁니다.
C. 기본 지역에 Amazon Aurora MySQL 데이터베이스를 배포합니다. 보조 지역에 글로벌
데이터베이스를 구성합니다.
D. 기본 지역에 Amazon DynamoDB 테이블을 배포합니다. 보조 지역에 대한 글로벌 테이블을
구성합니다.
Answer: D
Explanation:
* A. ElastiCache: Provides in-memory caching, not suitable for persistent, scalable
databases.
* B. DynamoDB Streams + Lambda: Manages replication manually, increasing latency and
operational complexity.
* C. Aurora Global Database: Provides high availability but does not support active-active
configuration.
* D. DynamoDB Global Tables: Provides active-active configuration and sub-second RPO.
References: Amazon DynamoDB Global Tables
QUESTION NO: 858
회사에는 온프레미스 Windows Server Trie 애플리케이션에서 실행되는 Microsoft NET
587

IT Certification Guaranteed, The Easy Way!
애플리케이션이 있으며 Oracle Database Standard Edition 서버를 사용하여 데이터를
저장합니다. 회사는 AWS로의 마이그레이션을 계획하고 있으며 애플리케이션을 이동하는
동안 개발 변경을 최소화하려고 합니다. AWS 애플리케이션 환경 가용성이 높아야 합니다.
회사는 이러한 요구 사항을 충족하기 위해 어떤 조치 조합을 취해야 합니까? (2개 선택)
A. NET Cote를 실행하는 AWS Lambda 함수를 사용하여 애플리케이션을 서버리스로
리팩터링합니다.
B. 다중 AZ 배포에서 NET 플랫폼을 사용하여 AWS Elastic Beanstalk에서 애플리케이션을
다시 호스팅합니다.
C. Amazon Linux Amazon 머신 이미지(AMI)를 사용하여 Amazon EC2에서 실행되도록
애플리케이션을 다시 플랫폼화합니다.
D. AWS Database Migration Service(AWS DMS)를 사용하여 다중 AZ 배포에서 Oracle
데이터베이스에서 Amazon DynamoDB로 마이그레이션
E. AWS Database Migration Service(AWS DMS)를 사용하여 다중 AZ 배포에서 Oracle
데이터베이스에서 Amazon RDS의 Oracle로 마이그레이션합니다.
Answer: B E
Explanation:
To minimize development changes while moving the application to AWS and to ensure a high
level of availability, the company can rehost the application in AWS Elastic Beanstalk with the
.NET platform in a Multi-AZ deployment. This will allow the application to run in a highly
available environment without requiring any changes to the application code.
The company can also use AWS Database Migration Service (AWS DMS) to migrate the
Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment. This will allow the
company to maintain the existing database platform while still achieving a high level of
availability.
QUESTION NO: 859
한 회사는 AWS 클라우드를 사용하여 기존 애플리케이션의 가용성과 복원력을 높이고
싶어합니다. 현재 버전의 애플리케이션은 회사의 데이터 센터에 있습니다. 최근 예상치 못한
정전으로 인해 데이터베이스 서버가 중단된 후 애플리케이션에서 데이터 손실이
발생했습니다.
회사에는 단일 실패 지점을 방지하는 솔루션이 필요합니다. 솔루션은 사용자 요구에 맞게
확장할 수 있는 기능을 애플리케이션에 제공해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 여러 가용 영역에 걸쳐 Auto Scaling 그룹의 Amazon EC2 인스턴스를 사용하여
애플리케이션 서버를 배포합니다. 다중 AZ 구성에서 Amazon RDS DB 인스턴스를
사용합니다.
B. 단일 가용 영역의 Auto Scaling 그룹에 있는 Amazon EC2 인스턴스를 사용하여
애플리케이션 서버를 배포합니다. EC2 인스턴스에 데이터베이스를 배포합니다. EC2 자동
복구를 활성화합니다.
C. 여러 가용 영역에 걸쳐 Auto Scaling 그룹의 Amazon EC2 인스턴스를 사용하여
애플리케이션 서버를 배포합니다. 단일 가용 영역에서 읽기 전용 복제본과 함께 Amazon RDS
DB 인스턴스를 사용합니다. 기본 DB 인스턴스가 실패하는 경우 읽기 전용 복제본을 승격하여
기본 DB 인스턴스를 교체합니다.
D. 여러 가용 영역에 걸쳐 Auto Scaling 그룹의 Amazon EC2 인스턴스를 사용하여
588

IT Certification Guaranteed, The Easy Way!
애플리케이션 서버를 배포합니다. 여러 가용 영역에 걸쳐 EC2 인스턴스에 기본 및 보조
데이터베이스 서버를 배포합니다. Amazon Elastic Block Store(Amazon EBS) 다중 연결을
사용하여 생성합니다. 인스턴스 간 공유 스토리지.
Answer: A
Explanation:
Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group
across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ
configuration. To make an existing application highly available and resilient while avoiding
any single points of failure and giving the application the ability to scale to meet user
demand, the best solution would be to deploy the application servers using Amazon EC2
instances in an Auto Scaling group across multiple Availability Zones and use an Amazon
RDS DB instance in a Multi-AZ configuration. By using an Amazon RDS DB instance in a
Multi-AZ configuration, the database is automatically replicated across multiple Availability
Zones, ensuring that the database is highly available and can withstand the failure of a single
Availability Zone. This provides fault tolerance and avoids any single points of failure.
QUESTION NO: 860
한 회사의 보고 시스템은 매일 수백 개의 .csv 파일을 Amazon S3 버킷으로 전송합니다.
회사는 이러한 파일을 Apache Parquet 형식으로 변환하고 변환된 데이터 버킷에 파일을
저장해야 합니다.
최소한의 개발 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Apache Spark가 설치된 Amazon EMR 클러스터를 생성합니다. 데이터를 변환하는 Spark
애플리케이션을 작성합니다. EMRFS(EMR 파일 시스템)를 사용하여 변환된 데이터 버킷에
파일을 씁니다.
B. AWS Glue 크롤러를 생성하여 데이터를 검색합니다. AWS Glue ETL(추출, 변환 및 로드)
작업을 생성하여 데이터를 변환합니다. 출력 단계에서 변환된 데이터 버킷을 지정합니다.
C. AWS Batch를 사용하여 Bash 구문으로 작업 정의를 생성하여 데이터를 변환하고 변환된
데이터 버킷에 데이터를 출력합니다. 작업 정의를 사용하여 작업을 제출합니다. 작업
유형으로 어레이 작업을 지정합니다.
D. 데이터를 변환하고 변환된 데이터 버킷으로 데이터를 출력하는 AWS Lambda 함수를
생성합니다. S3 버킷에 대한 이벤트 알림을 구성합니다. 이벤트 알림의 대상으로 Lambda
함수를 지정합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/three-aws-glue-etl-job-
types-for- converting-data-to-apache-parquet.html
QUESTION NO: 861
회사는 Amazon S3를 사용하여 기밀 감사 문서를 저장합니다. S3 버킷은 버킷 정책을
사용하여 최소 권한 원칙에 따라 감사 팀 IAM 사용자 자격 증명에 대한 액세스를 제한합니다.
회사 관리자는 S3 버킷에서 실수로 문서가 삭제되는 것을 걱정하고 더 안전한 솔루션을
원합니다.
솔루션 설계자는 감사 문서를 보호하기 위해 무엇을 해야 합니까?
A. S3 버킷에서 버전 관리 및 MFA 삭제 기능을 활성화합니다.
B. 각 감사 팀 IAM 사용자 계정의 IAM 사용자 자격 증명에 대해 다단계 인증(MFA)을
589

IT Certification Guaranteed, The Easy Way!
활성화합니다.
C. 감사 날짜 동안 s3:DeleteObject 작업을 거부하도록 감사 팀의 IAM 사용자 계정에 S3 수명
주기 정책을 추가합니다.
D. AWS Key Management Service(AWS KMS)를 사용하여 S3 버킷을 암호화하고 감사 팀
IAM 사용자 계정이 KMS 키에 액세스하지 못하도록 제한합니다.
Answer: A
QUESTION NO: 862
회사는 수집된 원시 데이터를 Amazon S3 버킷에 저장합니다. 데이터는 회사 고객을 대신하여
여러 유형의 분석에 사용됩니다. S3 객체에 대한 액세스 패턴을 결정하기 위해 요청된 분석
유형입니다.
회사는 접속 패턴을 예측하거나 통제할 수 없습니다. 회사는 S3 비용을 줄이고 싶어합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. S3 복제를 사용하여 자주 액세스하지 않는 객체를 S3 Standard-Infrequent Access(S3
Standard-1A)로 전환합니다.
B. S3 수명 주기 규칙을 사용하여 객체를 S3 Standard에서 Standard-Infrequent Access(S3
Standard-1A)로 전환합니다.
C. S3 Standard에서 S3 Intelligent-Tiering으로 객체를 전환하는 데 S3 수명 주기 규칙을
사용합니다.
D. S3 Inventory를 사용하여 S3 Standard에서 S3 Intelligent-Tiering으로 액세스되지 않은
객체를 식별하고 전환합니다.
Answer: C
Explanation:
S3 Intelligent-Tiering is a storage class that automatically reduces storage costs by moving
data to the most cost-effective access tier based on access frequency. It has two access
tiers: frequent access and infrequent access. Data is stored in the frequent access tier by
default, and moved to the infrequent access tier after 30 consecutive days of no access. If
the data is accessed again, it is moved back to the frequent access tier1. By using S3
Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering, the solution
can reduce S3 costs for data with unknown or changing access patterns.
A: Use S3 replication to transition infrequently accessed objects to S3 Standard-Infrequent
Access (S3 Standard-IA). This solution will not meet the requirement of reducing S3 costs for
data with unknown or changing access patterns, as S3 replication is a feature that copies
objects across buckets or Regions for redundancy or compliance purposes. It does not
automatically move objects to a different storage class based on access frequency2.
B: Use S3 Lifecycle rules to transition objects from S3 Standard to Standard-Infrequent
Access (S3 Standard- IA). This solution will not meet the requirement of reducing S3 costs for
data with unknown or changing access patterns, as S3 Standard-IA is a storage class that
offers lower storage costs than S3 Standard, but charges a retrieval fee for accessing the
data. It is suitable for long-lived and infrequently accessed data, not for data with changing
access patterns1.
D: Use S3 Inventory to identify and transition objects that have not been accessed from S3
Stand-ard to S3 Intelligent-Tiering. This solution will not meet the requirement of reducing S3
costs for data with unknown or changing access patterns, as S3 Inventory is a feature that
provides a report of the objects in a bucket and their metadata on a daily or weekly basis. It
590

IT Certification Guaranteed, The Easy Way!
does not automatically move objects to a different storage class based on access
frequency3.
Reference URL: https://aws.amazon.com/s3/storage-classes/intelligent-tiering/ S3 Intelligent-
Tiering is the best solution for reducing S3 costs when the access pattern is unpredictable or
changing. S3 Intelligent-Tiering automatically moves objects between two access tiers
(frequent and infrequent) based on the access frequency, without any performance impact or
retrieval fees. S3 Intelligent- Tiering also has an optional archive tier for objects that are rarely
accessed. S3 Lifecycle rules can be used to transition objects from S3 Standard to S3
Intelligent-Tiering.
Reference URLs:
1 https://aws.amazon.com/s3/storage-classes/intelligent-tiering/
2 https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-intelligent-tiering.html
3 https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html
QUESTION NO: 863
회사에서 온프레미스 애플리케이션을 AWS로 마이그레이션하려고 합니다. 애플리케이션은
크기가 수십 기가바이트에서 수백 테라바이트에 이르는 다양한 출력 파일을 생성합니다.
애플리케이션 데이터는 표준 파일 시스템 구조에 저장해야 합니다. 회사는 자동으로 확장되고
가용성이 높으며 운영 오버헤드가 최소화되는 솔루션을 원합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. Amazon Elastic Container Service(Amazon ECS)에서 컨테이너로 실행하도록
애플리케이션 마이그레이션 스토리지에 Amazon S3 사용
B. Amazon Elastic Kubernetes Service(Amazon EKS)에서 컨테이너로 실행하도록
애플리케이션 마이그레이션 스토리지에 Amazon Elastic Block Store(Amazon EBS) 사용
C. 다중 AZ Auto Scaling 그룹의 Amazon EC2 인스턴스로 애플리케이션을
마이그레이션합니다. 스토리지에 Amazon Elastic File System(Amazon EFS)을 사용합니다.
D. 다중 AZ Auto Scaling 그룹의 Amazon EC2 인스턴스로 애플리케이션을
마이그레이션합니다. 스토리지에 Amazon Elastic Block Store(Amazon EBS)를 사용합니다.
Answer: C
Explanation:
EFS is a standard file system, it scales automatically and is highly available.
QUESTION NO: 864
회사는 Auto Scaling 그룹의 Application Load Balancer(ALB) 뒤에 있는 Amazon EC2
인스턴스에서 전자상거래 웹 사이트를 운영합니다. 사이트에서 IP 주소가 변경되는 불법 외부
시스템의 높은 요청 비율과 관련된 성능 문제가 발생하고 있습니다. 보안 팀은 웹 사이트에
대한 잠재적인 DDoS 공격에 대해 걱정하고 있습니다. 회사는 합법적인 사용자에게 최소한의
영향을 미치는 방식으로 불법적으로 들어오는 요청을 차단해야 합니다.
솔루션 설계자는 무엇을 추천해야 합니까?
A. Amazon Inspector를 배포하고 ALB와 연결합니다.
B. AWS WAF를 배포하고 ALB와 연결하고 속도 제한 규칙을 구성합니다.
C. 들어오는 트래픽을 차단하기 위해 ALB와 연결된 네트워크 ACL에 규칙을 배포합니다.
D. Amazon GuardDuty를 배포하고 GuardDuty를 구성할 때 속도 제한 보호를 활성화합니다.
Answer: B
Explanation:
591

IT Certification Guaranteed, The Easy Way!
This answer is correct because it meets the requirements of blocking the illegitimate
incoming requests in a way that has a minimal impact on legitimate users. AWS WAF is a
web application firewall that helps protect your web applications or APIs against common
web exploits that may affect availability, compromise security, or consume excessive
resources. AWS WAF gives you control over how traffic reaches your applications by
enabling you to create security rules that block common attack patterns, such as SQL
injection or cross-site scripting, and rules that filter out specific traffic patterns you define. You
can associate AWS WAF with an ALB to protect the web application from malicious requests.
You can configure a rate-limiting rule in AWS WAF to track the rate of requests for each
originating IP address and block requests from an IP address that exceeds a certain limit
within a five-minute period. This way, you can mitigate potential DDoS attacks and improve
the performance of your website.
References:
* https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html
* https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-
based.html
QUESTION NO: 865
한 회사에서 모바일 장치용 멀티플레이어 게임을 배포했습니다. 이 게임에는 위도와 경도를
기반으로 플레이어의 실시간 위치 추적이 필요합니다. 게임의 데이터 저장소는 신속한
업데이트와 위치 검색을 지원해야 합니다.
이 게임은 읽기 전용 복제본이 있는 PostgreSQL DB 인스턴스용 Amazon RDS를 사용하여
위치 데이터를 저장합니다. 사용량이 가장 많은 기간에는 데이터베이스가 업데이트를 읽고
쓰는 데 필요한 성능을 유지할 수 없습니다. 게임의 사용자 기반이 빠르게 증가하고 있습니다.
데이터 계층의 성능을 향상하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 기존 DB 인스턴스의 스냅샷을 찍습니다. 다중 AZ가 활성화된 스냅샷을 복원합니다.
B. OpenSearch 대시보드를 사용하여 Amazon RDS에서 Amazon OpenSearch Service로
마이그레이션합니다.
C. 기존 DB 인스턴스 앞에 Amazon DynamoDB Accelerator(DAX)를 배포합니다. DAX를
사용하도록 게임을 수정합니다.
D. 기존 DB 인스턴스 앞에 Redis용 Amazon ElastiCache 클러스터를 배포합니다. Redis를
사용하도록 게임을 수정합니다.
Answer: D
Explanation:
The solution that will improve the performance of the data tier is to deploy an Amazon
ElastiCache for Redis cluster in front of the existing DB instance and modify the game to use
Redis. This solution will enable the game to store and retrieve the location data of the players
in a fast and scalable way, as Redis is an in-memory data store that supports geospatial data
types and commands. By using ElastiCache for Redis, the game can reduce the load on the
RDS for PostgreSQL DB instance, which is not optimized for high-frequency updates and
queries of location data. ElastiCache for Redis also supports replication, sharding, and auto
scaling to handle the increasing user base of the game.
The other solutions are not as effective as the first one because they either do not improve
the performance, do not support geospatial data, or do not leverage caching. Taking a
snapshot of the existing DB instance and restoring it with Multi-AZ enabled will not improve
592

IT Certification Guaranteed, The Easy Way!
the performance of the data tier, as it only provides high availability and durability, but not
scalability or low latency. Migrating from Amazon RDS to Amazon OpenSearch Service with
OpenSearch Dashboards will not improve the performance of the data tier, as OpenSearch
Service is mainly designed for full-text search and analytics, not for real-time location
tracking.
OpenSearch Service also does not support geospatial data types and commands natively,
unlike Redis.
Deploying Amazon DynamoDB Accelerator (DAX) in front of the existing DB instance and
modifying the game to use DAX will not improve the performance of the data tier, as DAX is
only compatible with DynamoDB, not with RDS for PostgreSQL. DAX also does not support
geospatial data types and commands.
References:
* Amazon ElastiCache for Redis
* Geospatial Data Support - Amazon ElastiCache for Redis
* Amazon RDS for PostgreSQL
* Amazon OpenSearch Service
* Amazon DynamoDB Accelerator (DAX)
QUESTION NO: 866
게임 회사는 여러 AWS 리전에서 새로운 인터넷 연결 애플리케이션을 시작하려고 합니다.
애플리케이션은 통신에 TCP 및 UDP 프로토콜을 사용합니다. 회사는 글로벌 사용자에게
고가용성과 최소 대기 시간을 제공해야 합니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 조치 조합을 취해야 합니까? (2개를
선택하세요.)
A. 각 리전의 애플리케이션 앞에 내부 Network Load Balancer를 생성합니다.
B. 각 리전의 애플리케이션 앞에 외부 Application Load Balancer를 생성합니다.
C. AWS Global Accelerator 액셀러레이터를 생성하여 각 리전의 로드 밸런서로 트래픽을
라우팅합니다.
D. 지리적 위치 라우팅 정책을 사용하여 트래픽을 분산하도록 Amazon Route 53을
구성합니다.
E. 트래픽을 처리하고 각 지역의 애플리케이션에 대한 요청을 라우팅하도록 Amazon
CloudFront를 구성합니다.
Answer: B C
Explanation:
This combination of actions will provide high availability and minimum latency for global users
by using AWS Global Accelerator and Application Load Balancers. AWS Global Accelerator
is a networking service that helps you improve the availability, performance, and security of
your internet-facing applications by using the AWS global network. It provides two global
static public IPs that act as a fixed entry point to your application endpoints, such as
Application Load Balancers, in multiple Regions1. Global Accelerator uses the AWS
backbone network to route traffic to the optimal regional endpoint based on health, client
location, and policies that you configure. It also offers TCP and UDP support, traffic
encryption, and DDoS protection2.
Application Load Balancers are external load balancers that distribute incoming application
traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. They
593

IT Certification Guaranteed, The Easy Way!
support both HTTP and HTTPS (SSL/TLS) protocols, and offer advanced features such as
content-based routing, health checks, and integration with other AWS services3. By creating
external Application Load Balancers in front of the application in each Region, you can
ensure that the application can handle varying load patterns and scale on demand. By
creating an AWS Global Accelerator accelerator to route traffic to the load balancers in each
Region, you can leverage the performance, security, and availability of the AWS global
network to deliver the best possible user experience.
References: 1: What is AWS Global Accelerator? - AWS Global Accelerator4, Overview
section2: Network Acceleration Service - AWS Global Accelerator - AWS5, Why AWS Globa
l Accelerator? section. 3: What is an Application Load Balancer? - Elastic Load Balancing6,
Overview section.
QUESTION NO: 867
회사에는 매주 금요일 저녁에 실행되는 대규모 워크로드가 있습니다. 워크로드는 us-east-1
리전의 2개 가용 영역에 있는 Amazon EC2 인스턴스에서 실행됩니다. 일반적으로 회사는
항상 2개 이하의 인스턴스를 실행해야 합니다. 그러나 회사는 정기적으로 반복적으로
증가하는 워크로드를 처리하기 위해 매주 금요일 최대 6개의 인스턴스까지 확장하려고
합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. Amazon EventBridge에서 인스턴스 규모 조정 알림을 생성합니다.
B. 예약된 작업이 있는 Auto Scaling 그룹을 생성합니다.
C. 수동 조정을 사용하는 Auto Scaling 그룹을 생성합니다.
D. 자동 스케일링을 사용하는 Auto Scaling 그룹을 생성합니다.
Answer: B
Explanation:
An Auto Scaling group is a collection of EC2 instances that share similar characteristics and
can be scaled in or out automatically based on demand. An Auto Scaling group can have a
scheduled action, which is a configuration that tells the group to scale to a specific size at a
specific time. This way, the company can scale up to six instances each Friday evening to
handle the increased workload, and scale down to two instances at other times to save costs.
This solution meets the requirements with the least operational overhead, as it does not
require manual intervention or custom scripts.
References:
* 1 explains how to create a scheduled action for an Auto Scaling group.
* 2 describes the concept and benefits of an Auto Scaling group.
QUESTION NO: 868
한 회사에 Amazon DynamoDB 테이블을 저장용으로 사용하는 애플리케이션이 있습니다.
솔루션 설계자는 테이블에 대한 많은 요청이 최신 데이터를 반환하지 않는다는 것을
발견했습니다. 회사 사용자는 데이터베이스 성능과 관련된 다른 문제를 보고하지 않았습니다.
지연 시간이 허용 가능한 범위 내에 있습니다.
솔루션 설계자는 어떤 디자인 변경을 권장해야 합니까?
A. 테이블에 읽기 전용 복제본을 추가합니다.
B. 글로벌 보조 인덱스(GSI)를 사용합니다.
C. 테이블에 대해 강력한 일관된 읽기를 요청합니다.
594

IT Certification Guaranteed, The Easy Way!
D. 테이블에 대한 최종적 일관된 읽기를 요청합니다.
Answer: C
Explanation:
The most suitable design change for the company's application is to request strongly
consistent reads for the table. This change will ensure that the requests to the table return
the latest data, reflecting the updates from all prior write operations.
Amazon DynamoDB is a fully managed NoSQL database service that provides fast and
predictable performance with seamless scalability. DynamoDB supports two types of read
consistency: eventually consistent reads and strongly consistent reads. By default,
DynamoDB uses eventually consistent reads, unless users specify otherwise1.
Eventually consistent reads are reads that may not reflect the results of a recently completed
write operation.
The response might not include the changes because of the latency of propagating the data
to all replicas. If users repeat their read request after a short time, the response should return
the updated data. Eventually consistent reads are suitable for applications that do not require
up-to-date data or can tolerate eventual consistency1.
Strongly consistent reads are reads that return a result that reflects all writes that received a
successful response prior to the read. Users can request a strongly consistent read by setting
the ConsistentRead parameter to true in their read operations, such as GetItem, Query, or
Scan. Strongly consistent reads are suitable for applications that require up-to-date data or
cannot tolerate eventual consistency1.
The other options are not correct because they do not address the issue of read consistency
or are not relevant for the use case. Adding read replicas to the table is not correct because
this option is not supported by DynamoDB. Read replicas are copies of a primary database
instance that can serve read-only traffic and improve availability and performance. Read
replicas are available for some relational database services, such as Amazon RDS or
Amazon Aurora, but not for DynamoDB2. Using a global secondary index (GSI) is not correct
because this option is not related to read consistency. A GSI is an index that has a partition
key and an optional sort key that are different from those on the base table. A GSI allows
users to query the data in different ways, with eventual consistency3. Requesting eventually
consistent reads for the table is not correct because this option is already the default
behavior of DynamoDB and does not solve the problem of requests not returning the latest
data.
References:
* Read consistency - Amazon DynamoDB
* Working with read replicas - Amazon Relational Database Service
* Working with global secondary indexes - Amazon DynamoDB
QUESTION NO: 869
한 회사가 최근 3티어 애플리케이션을 VPC로 마이그레이션하는 것을 검토하고 있습니다.
보안 팀은 애플리케이션 계층 간의 Amazon EC2 보안 그룹 수신 및 송신 규칙에 최소 권한
원칙이 적용되지 않는다는 사실을 발견했습니다.
이 문제를 해결하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 인스턴스 ID를 소스 또는 대상으로 사용하여 보안 그룹 규칙을 생성합니다.
B. 보안 그룹 ID를 소스 또는 대상으로 사용하여 보안 그룹 규칙을 생성합니다.
595

IT Certification Guaranteed, The Easy Way!
C. VPC CIDR 블록을 소스 또는 대상으로 사용하여 보안 그룹 규칙을 생성합니다.
D. 서브넷 CIDR 블록을 소스 또는 대상으로 사용하여 보안 그룹 규칙을 생성합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules.html
QUESTION NO: 870
회사는 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용하여 컨테이너
애플리케이션을 실행합니다. 이 애플리케이션에는 고객을 관리하고 주문하는
마이크로서비스가 포함되어 있습니다. 회사는 들어오는 요청을 적절한 마이크로 서비스로
라우팅해야 합니다.
이 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?
A. AWS 로드 밸런서 컨트롤러를 사용하여 Network Load Balancer를 프로비저닝합니다.
B. AWS 로드 밸런서 컨트롤러를 사용하여 Application Load Balancer를 프로비저닝합니다.
C. AWS Lambda 함수를 사용하여 요청을 Amazon EKS에 연결합니다.
D. Amazon API Gateway를 사용하여 요청을 Amazon EKS에 연결합니다.
Answer: B
Explanation:
An Application Load Balancer is a type of Elastic Load Balancer that operates at the
application layer (layer
7) of the OSI model. It can distribute incoming traffic across multiple targets, such as Amazon
EC2 instances, containers, IP addresses, and Lambda functions. It can also route requests
based on the content of the request, such as the host name, path, or query parameters1.
The AWS Load Balancer Controller is a controller that helps you manage Elastic Load
Balancers for your Kubernetes cluster. It can provision Application Load Balancers or
Network Load Balancers when you create Kubernetes Ingress or Service resources2.
By using the AWS Load Balancer Controller to provision an Application Load Balancer for
your Amazon EKS cluster, you can achieve the following benefits:
* You can route incoming requests to the appropriate microservices based on the rules you
define in your Ingress resource. For example, you can route requests with different host
names or paths to different microservices that handle customers and orders2.
* You can improve the performance and availability of your container applications by
distributing the load across multiple targets and enabling health checks and automatic
scaling1.
* You can reduce the cost and complexity of managing your load balancers by using a single
controller that integrates with Amazon EKS and Kubernetes. You do not need to manually
create or configure load balancers or update them when your cluster changes2.
QUESTION NO: 871
회사는 AWS에서 인프라를 실행하고 res 문서 관리 애플리케이션에 대해 700.000명의 등록
기반을 가지고 있습니다. 회사는 큰 pdf 파일을 jpg Imago 파일로 변환하는 제품을 만들려고
합니다. .pdf 파일의 크기는 평균 5MB입니다. 회사는 원본 파일과 변환 파일을 보관해야
합니다.
솔루션 설계자는 시간이 지나면서 빠르게 증가할 수요를 수용할 수 있는 확장 가능한
솔루션을 설계해야 합니다.
596

IT Certification Guaranteed, The Easy Way!
어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?
A. pdf 파일을 Amazon S3에 저장 AWS Lambda 함수를 호출하도록 S3 PUT 이벤트를
구성하여 파일을 jpg 형식으로 변환하고 Amazon S3에 다시 저장합니다.
B. pdf 파일을 Amazon DynamoDB에 저장합니다. DynamoDB 스트림 기능을 사용하여 AWS
Lambda 함수를 호출하여 파일을 jpg 형식으로 변환하고 DynamoDB에 해킹 저장
C. Amazon EC2 인스턴스가 포함된 AWS Elastic Beanstalk 애플리케이션에 PDF 파일을
업로드합니다.
Amazon Elastic Block Store(Amazon EBS) 스토리지 및 Auto Scaling 그룹. 프로그램을
사용하여 EC2 인스턴스에서 파일을 jpg 형식으로 변환합니다. .pdf 파일과 .jpg 파일을 EBS
저장소에 저장합니다.
D. Amazon EC2 인스턴스, Amazon Elastic File System(Amazon EPS) 스토리지 및 Auto
Scaling 그룹이 포함된 AWS Elastic Beanstalk 애플리케이션에 .pdf 파일을 업로드합니다.
EC2 인스턴스의 프로그램을 사용하여 파일을 jpg 형식으로 변환 EBS 스토어에 pdf 파일과
jpg 파일을 저장합니다.
Answer: A
Explanation:
Elastic BeanStalk is expensive, and DocumentDB has a 400KB max to upload files. So
Lambda and S3 should be the one.
QUESTION NO: 872
서버리스 애플리케이션은 Amazon API Gateway를 사용합니다. AWS Lambda 및 Amazon
DynamoDB. Lambda 함수에는 DynamoDB 테이블을 읽고 쓸 수 있는 권한이 필요합니다.
DynamoDB 테이블에 대한 Lambda 함수 액세스를 가장 안전하게 제공하는 솔루션은
무엇입니까?
A. 프로그래밍 방식으로 Lambda 함수에 액세스할 수 있는 오전 1시 사용자를 생성합니다.
DynamoDB 테이블에 대한 읽기 및 쓰기 액세스를 허용하는 정책을 사용자에게 연결합니다.
access_key_id 및 secret_access_key 파라미터를 Lambda 환경 변수의 일부로 저장합니다.
다른 AWS 사용자에게 Lambda 함수 구성에 대한 읽기 및 쓰기 액세스 권한이 없는지
확인합니다.
B. Lambda를 신뢰할 수 있는 서비스로 포함하는 1AM 역할을 생성합니다. DynamoDB
테이블에 대한 읽기 및 쓰기 액세스를 허용하는 역할에 정책을 연결합니다. 새 역할을 실행
역할로 사용하도록 Lambda 함수의 구성을 업데이트합니다.
C. Lambda 함수에 프로그래밍 방식으로 액세스할 수 있는 오전 1시 사용자를 생성합니다.
DynamoDB 테이블에 대한 읽기 및 쓰기 액세스를 허용하는 정책을 사용자에게 연결합니다.
AWS Systems Manager Parameter Store에 access_key_id 및 secret_access_key 파라미터를
보안 문자열 파라미터로 저장합니다. DynamoDB 테이블에 연결하기 전에 보안 문자열
파라미터를 검색하도록 Lambda 함수 코드를 업데이트합니다.
D. DynamoDB를 신뢰할 수 있는 서비스로 포함하는 1AM 역할을 생성합니다. Lambda
함수에서 읽기 및 쓰기 액세스를 허용하는 역할에 정책을 연결합니다. 새 역할에 실행 역할로
연결되도록 Lambda 함수의 코드를 업데이트합니다.
Answer: B
Explanation:
Option B suggests creating an IAM role that includes Lambda as a trusted service, meaning
the role is specifically designed for Lambda functions. The role should have a policy attached
597

IT Certification Guaranteed, The Easy Way!
to it that grants the required read and write access to the DynamoDB table.
QUESTION NO: 873
한 병원에서는 환자의 증상을 수집하는 새로운 애플리케이션을 설계하고 있습니다. 병원은
아키텍처에 Amazon Simple Queue Service(Amazon SOS)와 Amazon Simple 알림
서비스(Amazon SNS)를 사용하기로 결정했습니다.
솔루션 설계자가 인프라 설계를 검토하고 있습니다. 데이터는 테스트 및 전송 중에
암호화되어야 합니다. 병원의 승인된 직원만이 데이터에 접근할 수 있어야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까?
(2개를 선택하세요.)
A. SQS 구성 요소에서 서버 측 암호화를 활성화합니다. 연결 기본 키 정책을 업데이트하여
인증된 주체 집합으로 키 사용을 제한합니다.
B. AWS Key Management Service(AWS KMS) 고객 관리형 키를 사용하여 SNS 구성
요소에서 서버 측 암호화를 활성화합니다. 키 정책을 적용하여 인증된 보안 주체 집합으로 키
사용을 제한합니다.
C. SNS 구성 요소에 암호화를 설정합니다. 기본 키 정책을 업데이트하여 인증된 주체
집합으로 키 사용을 제한합니다. TLS를 통한 암호화된 연결만 허용하도록 주제 pokey에
조건을 설정합니다.
D. AWS Key Management Service(AWS KMS) 고객 관리형 키를 사용하여 SOS 구성
요소에서 서버 측 암호화를 활성화합니다. 키 Pokey를 적용하여 인증된 보안 주체 집합으로
키 사용을 제한합니다. TLS를 통한 암호화된 연결만 허용하도록 대기열 포키에 조건을
설정합니다.
E. AWS Key Management Service(AWS KMS) 고객 관리형 키를 사용하여 SOS 구성
요소에서 서버 측 암호화를 활성화합니다. IAM Pokey를 적용하여 키 사용을 승인된 주체
집합으로 제한합니다. TLS를 통한 암호화된 연결만 허용하도록 대기열 포키에 조건 설정
Answer: B D
Explanation:
https://docs.aws.amazon.com/sns/latest/dg/sns-server-side-encryption.html
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-
server-side-encryption.html
QUESTION NO: 874
회사는 여러 프로덕션 애플리케이션을 호스팅합니다. 애플리케이션 중 하나는 여러 AWS
리전에 걸쳐 있는 Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification
Service(Amazon SNS) 및 Amazon Simple Queue Service(Amazon SQS)의 리소스로
구성됩니다. 모든 회사 리소스에는 "application"이라는 태그 이름과 각 애플리케이션에
해당하는 값이 태그로 지정됩니다. 솔루션 설계자는 태그가 지정된 모든 구성 요소를
식별하기 위한 가장 빠른 솔루션을 제공해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS CloudTrail을 사용하여 애플리케이션 태그가 있는 리소스 목록을 생성합니다.
B. AWS CLI를 사용하여 모든 리전의 각 서비스를 쿼리하여 태그가 지정된 구성 요소를
보고합니다.
C. Amazon CloudWatch Logs Insights에서 쿼리를 실행하여 애플리케이션 태그가 있는 구성
요소에 대해 보고합니다.
D. AWS Resource Groups Tag Editor로 쿼리를 실행하여 애플리케이션 태그를 사용하여
598

IT Certification Guaranteed, The Easy Way!
전역적으로 리소스에 대해 보고합니다.
Answer: D
Explanation:
https://docs.aws.amazon.com/tag-editor/latest/userguide/tagging.html
QUESTION NO: 875
한 회사는 최근 웹 공격으로 인해 공개 웹 애플리케이션의 보안이 우려되고 있습니다.
애플리케이션은 ALB(Application Load Balancer)를 사용합니다. 솔루션 설계자는
애플리케이션에 대한 DDoS 공격 위험을 줄여야 합니다.
이 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. ALB에 Amazon Inspector 에이전트를 추가합니다.
B. 공격을 방지하도록 Amazon Macie를 구성합니다.
C. 공격을 방지하려면 AWS Shield Advanced를 활성화합니다.
D. ALB를 모니터링하도록 Amazon GuardDuty를 구성합니다.
Answer: C
Explanation:
AWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2
instances, Elastic Load Balancing load balancers, CloudFront distributions, Route 53 hosted
zones, and AWS Global Accelerator standard accelerators.
https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.
html
QUESTION NO: 876
회사는 Amazon EC2 인스턴스와 AWS Lambda 함수를 사용하여 애플리케이션을 실행합니다.
회사의 AWS 계정에는 퍼블릭 서브넷과 프라이빗 서브넷이 있는 VPC가 있습니다. EC2
인스턴스는 VPC 중 하나의 프라이빗 서브넷에서 실행됩니다. 애플리케이션이 작동하려면
Lambda 함수가 EC2 인스턴스에 대한 직접 네트워크 액세스가 필요합니다.
신청서는 최소 1년 동안 실행됩니다. 회사는 해당 기간 동안 애플리케이션이 사용하는
Lambda 함수 수가 증가할 것으로 예상합니다. 회사는 모든 애플리케이션 리소스에 대한 절감
효과를 극대화하고 서비스 간의 네트워크 대기 시간을 낮게 유지하기를 원합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. EC2 인스턴스 Savings Plan을 구매합니다. Lambda 함수 기간, 메모리 사용량, 호출 수를
최적화합니다. EC2 인스턴스가 포함된 프라이빗 서브넷에 Lambda 함수를 연결합니다.
B. EC2 인스턴스 Savings Plan을 구매합니다. Lambda 함수 기간, 메모리 사용량, 호출 횟수,
전송되는 데이터 양을 최적화합니다. EC2 인스턴스가 실행되는 동일한 VPC의 퍼블릭
서브넷에 Lambda 함수를 연결합니다.
C. Compute Savings Plan을 구매하세요. Lambda 함수 기간 및 메모리 사용량, 호출 수,
전송되는 데이터 양을 최적화합니다. EC2 인스턴스가 포함된 프라이빗 서브넷에 Lambda
함수를 연결합니다.
D. Compute Savings Plan을 구매하세요. Lambda 함수의 기간과 메모리 사용량, 호출 수,
전송되는 데이터 양을 최적화합니다. Lambda 함수를 Lambda 서비스 VPC에 유지합니다.
Answer: C
Explanation:
By purchasing a Compute Savings Plan, the company can save on the costs of running both
599

IT Certification Guaranteed, The Easy Way!
EC2 instances and Lambda functions. The Lambda functions can be connected to the private
subnet that contains the EC2 instances through a VPC endpoint for AWS services or a VPC
peering connection. This provides direct network access to the EC2 instances while keeping
the traffic within the private network, which helps to minimize network latency. Optimizing the
Lambda functions' duration, memory usage, number of invocations, and amount of data
transferred can help to further minimize costs and improve performance.
Additionally, using a private subnet helps to ensure that the EC2 instances are not directly
accessible from the public internet, which is a security best practice.
QUESTION NO: 877
us-east-1 지역에서 사진 호스팅 서비스를 운영하는 회사가 있습니다. 이 서비스를 통해 여러
국가의 사용자가 사진을 업로드하고 볼 수 있습니다. 일부 사진은 몇 달 동안 많이 조회되지만
다른 사진은 일주일 미만 동안 조회됩니다. 이 애플리케이션에서는 각 사진당 최대 20MB까지
업로드할 수 있습니다. 이 서비스는 사진 메타데이터를 사용하여 각 사용자에게 표시할
사진을 결정합니다.
가장 비용 효율적으로 적절한 사용자 액세스를 제공하는 솔루션은 무엇입니까?
A. Amazon DynamoDB에 사진을 저장합니다. DynamoDB Accelerator(DAX)를 켜서 자주
보는 항목을 캐시합니다.
B. Amazon S3 Intelligent-Tiering 스토리지 클래스에 사진을 저장합니다. 사진 메타데이터와
해당 S3 위치를 DynamoDB에 저장합니다.
C. Amazon S3 Standard 스토리지 클래스에 사진을 저장합니다. 30일이 지난 사진을 S3
Standard-Infrequent Access(S3 Standard-IA) 스토리지 클래스로 이동하도록 S3 수명 주기
정책을 설정합니다. 메타데이터를 추적하려면 객체 태그를 사용하세요.
D. Amazon S3 Glacier 스토리지 클래스에 사진을 저장합니다. 30일이 지난 사진을 S3 Glacier
Deep Archive 스토리지 클래스로 이동하도록 S3 수명 주기 정책을 설정합니다. 사진
메타데이터와 해당 S3 위치를 Amazon OpenSearch Service에 저장합니다.
Answer: B
Explanation:
This solution provides the appropriate user access most cost-effectively because it uses the
Amazon S3 Intelligent-Tiering storage class, which automatically optimizes storage costs by
moving data to the most cost- effective access tier when access patterns change, without
performance impact or operational overhead1. This storage class is ideal for data with
unknown, changing, or unpredictable access patterns, such as photos that are heavily
viewed for months or less than a week. By storing the photo metadata and its S3 location in
DynamoDB, the application can quickly query and retrieve the relevant photos for each user.
DynamoDB is a fast, scalable, and fully managed NoSQL database service that supports
key-value and document data models2.
References: 1: Amazon S3 Intelligent-Tiering Storage Class | AWS3, Overview section2:
Amazon DynamoDB - NoSQL Cloud Database Service4, Overview section.
QUESTION NO: 878
솔루션 아키텍트는 애플리케이션이 Amazon RDS DB 인스턴스에 액세스하는 데 사용하는
데이터베이스 사용자 이름과 암호를 안전하게 저장해야 합니다. 데이터베이스에 액세스하는
애플리케이션은 Amazon EC2 인스턴스에서 실행됩니다. 솔루션 아키텍트는 AWS Systems
Manager Parameter Store에서 보안 매개변수를 생성하려고 합니다.
600

IT Certification Guaranteed, The Easy Way!
이 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. Parameter Store 파라미터에 대한 읽기 액세스 권한이 있는 IAM 역할을 생성합니다.
파라미터를 암호화하는 데 사용되는 AWS Key Management Service(AWS KMS) 키에 대한
암호 해독 액세스를 허용합니다. 이 IAM 역할을 EC2 인스턴스에 할당합니다.
B. Parameter Store 파라미터에 대한 읽기 액세스를 허용하는 IAM 정책을 생성합니다.
파라미터를 암호화하는 데 사용되는 AWS Key Management Service(AWS KMS) 키에 대한
암호 해독 액세스를 허용합니다. 이 IAM 정책을 EC2 인스턴스에 할당합니다.
C. Parameter Store 파라미터와 EC2 인스턴스 사이에 IAM 신뢰 관계를 생성합니다. 신뢰
정책에서 Amazon RDS를 보안 주체로 지정합니다.
D. DB 인스턴스와 EC2 인스턴스 간의 IAM 신뢰 관계를 생성합니다. 신뢰 정책에서 Systems
Manager를 보안 주체로 지정합니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-
iam.html
QUESTION NO: 879
기업은 고객에게 데이터에 대한 안전한 액세스를 제공해야 합니다.
ㅏ. 회사는 고객 데이터를 처리하고 결과를 Amazon S3 버킷에 저장합니다.
모든 데이터에는 강력한 규정과 보안 요구 사항이 적용됩니다. 저장된 데이터는 암호화되어야
합니다. 각 고객은 AWS 계정의 데이터에만 액세스할 수 있어야 합니다. 회사 직원은 데이터에
접근할 수 없어야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 각 고객에 대해 AWS Certificate Manager(ACM) 인증서를 프로비저닝합니다. 클라이언트
측 데이터를 암호화합니다. 개인 인증서 정책에서 고객이 제공하는 오전 1시 역할을 제외한
모든 주체의 인증서에 대한 액세스를 거부합니다.
B. 각 고객에 대해 별도의 AWS Key Management Service(AWS KMS) 키를
프로비저닝합니다. 서버측 데이터를 암호화합니다. S3 버킷 정책에서 고객이 제공하는 오전
1시 역할을 제외한 모든 보안 주체에 대한 데이터 암호 해독을 거부합니다.
C. 각 고객에 대해 별도의 AWS Key Management Service(AWS KMS) 키를
프로비저닝합니다. 서버측 데이터를 암호화합니다. 각 KMS 키 정책에서 고객이 제공하는
오전 1시 역할을 제외한 모든 보안 주체에 대한 데이터 암호 해독을 거부합니다.
D. 각 고객에 대해 AWS Certificate Manager(ACM) 인증서를 프로비저닝합니다. 클라이언트
측 데이터를 암호화합니다. 공용 인증서 정책에서 고객이 제공하는 오전 1시 역할을 제외한
모든 주체의 인증서에 대한 액세스를 거부합니다.
Answer: C
Explanation:
The correct solution is to provision a separate AWS KMS key for each customer and encrypt
the data server- side. This way, the company can use the S3 encryption feature to protect the
data at rest and delegate the control of the encryption keys to the customers. The customers
can then use their own IAM roles to access and decrypt their data. The company employees
will not be able to access the data because they are not authorized by the KMS key policies.
The other options are incorrect because:
* Option A and D are using ACM certificates to encrypt the data client-side. This is not a
601

IT Certification Guaranteed, The Easy Way!
recommended practice for S3 encryption because it adds complexity and overhead to the
encryption process.
Moreover, the company will have to manage the certificates and their policies for each
customer, which is not scalable and secure.
* Option B is using a separate KMS key for each customer, but it is using the S3 bucket
policy to control the decryption access. This is not a secure solution because the bucket
policy applies to the entire bucket, not to individual objects. Therefore, the customers will be
able to access and decrypt each other' s data if they have the permission to list the bucket
contents. The bucket policy also overrides the KMS key policy, which means the company
employees can access the data if they have the permission to use the KMS key.
References:
* S3 encryption
* KMS key policies
* ACM certificates
QUESTION NO: 880
솔루션 아키텍트는 Amazon S3 오리진과 함께 Amazon CloudFront를 사용하여 정적 웹
사이트를 저장하는 솔루션을 설계해야 합니다. 회사의 보안 정책에 따라 모든 웹 사이트
트래픽은 AWS WAR에서 검사되어야 합니다. 솔루션 아키텍트는 이러한 요구 사항을 어떻게
준수해야 합니까?
A. S3 버킷 정책을 구성하여 AWS WAF Amazon 리소스 이름(ARN)에서 오는 요청만
수락합니다.
B. S3 오리진에서 콘텐츠를 요청하기 전에 모든 수신 요청을 AWS WAF로 전달하도록
Amazon CloudFront를 구성합니다.
C. Amazon CloudFront IP 주소가 Amazon S3에만 액세스하도록 허용하는 보안 그룹을
구성합니다. AWS WAF를 CloudFront에 연결합니다.
D. 원본 액세스 ID(OAI)를 사용하여 S3 버킷에 대한 액세스를 제한하도록 Amazon CloudFront
및 Amazon S3를 구성합니다. 배포에서 AWS WAF를 활성화합니다.
Answer: D
Explanation:
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-
restricting-access-to- s3.html
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-
awswaf.html
QUESTION NO: 881
회사에는 액세스 패턴이 자주 발생하지 않는 글로벌 테일즈 팀에서 사용하는 온프레미스
MySQL 데이터베이스가 있습니다. 영업팀에서는 데이터베이스의 가동 중지 시간을
최소화해야 합니다. 데이터베이스 관리자는 향후 더 많은 사용자를 예상하여 특정 인스턴스
유형을 선택하지 않고 이 데이터베이스를 AWS로 마이그레이션하려고 합니다.
솔루션 설계자는 어떤 서비스를 추천해야 합니까?
A. Amazon Aurora MySQL
B. MySQL을 위한 Amazon Aurora 서버리스
C. Amazon Redshift 스펙트럼
D. MySQL용 Amazon RDS
602

IT Certification Guaranteed, The Easy Way!
Answer: B
Explanation:
Amazon Aurora Serverless for MySQL is a fully managed, auto-scaling relational database
service that scales up or down automatically based on the application demand. This service
provides all the capabilities of Amazon Aurora, such as high availability, durability, and
security, without requiring the customer to provision any database instances. With Amazon
Aurora Serverless for MySQL, the sales team can enjoy minimal downtime since the
database is designed to automatically scale to accommodate the increased traffic.
Additionally, the service allows the customer to pay only for the capacity used, making it cost-
effective for infrequent access patterns. Amazon RDS for MySQL could also be an option,
but it requires the customer to select an instance type, and the database administrator would
need to monitor and adjust the instance size manually to accommodate the increasing traffic.
QUESTION NO: 882
회사는 하드웨어 용량 제약으로 인해 온프레미스 데이터 센터에서 AWS 클라우드로 레거시
애플리케이션을 마이그레이션해야 합니다. 애플리케이션은 하루 24시간 실행됩니다. &
일주일에. 애플리케이션 데이터베이스 스토리지는 시간이 지남에 따라 계속해서 증가합니다.
비용 효율성 측면에서 이러한 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야 합니까?
A. 애플리케이션 계층을 Amazon FC2 스팟 인스턴스로 마이그레이션합니다. 데이터
스토리지 계층을 Amazon S3으로 마이그레이션합니다.
B. 애플리케이션 계층을 Amazon EC2 예약 인스턴스로 마이그레이션 데이터 스토리지
계층을 Amazon RDS 온디맨드 인스턴스로 마이그레이션합니다.
C. 애플리케이션 계층을 Amazon EC2 예약 인스턴스로 마이그레이션 데이터 스토리지 계층을
Amazon Aurora 예약 인스턴스로 마이그레이션합니다.
D. 애플리케이션 계층을 Amazon EC2 온디맨드 Amazon으로 마이그레이션합니다. 데이터
스토리지 계층을 Amazon RDS 예약 인스턴스로 마이그레이션합니다.
Answer: C
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.ht
ml
QUESTION NO: 883
미디어 회사는 us-east-1 리전에 다중 계정 AWS 환경을 보유하고 있습니다. 이 회사는 성능
지표를 게시하는 프로덕션 계정에 Amazon Simple 알림 서비스(Amazon SNS) 주제를 가지고
있습니다. 회사는 로그 데이터를 처리하고 분석하기 위해 관리자 계정에 AWS Lambda 기능을
가지고 있습니다.
관리자 계정에 있는 Lambda 함수는 중요한 지표 tM*이 보고될 때 프로덕션 계정에 있는 SNS
주제의 메시지에 의해 호출되어야 합니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.)
A. Amazon SNS가 함수를 호출하도록 허용하는 Lambda 함수에 대한 IAM 리소스 정책을
생성합니다. 프로덕션 계정에 있는 SNS 주제의 메시지를 버퍼링하기 위해 관리자 계정에
Amazon Simple Queue Service(Amazon SQS) 대기열을 구현합니다. Lambda 함수를
호출하도록 SOS 대기열을 구성합니다.
B. Lambda 함수가 주제를 구독하도록 허용하는 SNS 주제에 대한 IAM 정책을 생성합니다.
C. 프로덕션 계정에서 Amazon EventBridge 규칙을 사용하여 SNS 주제 알림을 캡처합니다.
603

IT Certification Guaranteed, The Easy Way!
관리자 계정에 있는 Lambda 함수에 알림을 전달하도록 EventBridge 규칙을 구성합니다.
D. 프로덕션 계정의 Amazon S3 버킷에 성능 지표를 저장합니다. Amazon Athena를 사용하여
관리자 계정의 지표를 분석합니다.
Answer: A B
Explanation:
* Requirement Analysis: The Lambda function in the administrator account needs to process
messages from an SNS topic in the production account.
* IAM Policy for SNS Topic: Allows the Lambda function to subscribe and be invoked by the
SNS topic.
* SQS Queue for Buffering: Using an SQS queue provides reliable message delivery and
buffering between SNS and Lambda, ensuring all messages are processed.
* Implementation:
* Create an SQS queue in the administrator account.
* Set an IAM policy to allow the Lambda function to subscribe to and be invoked by the SNS
topic.
* Configure the SNS topic to send messages to the SQS queue.
* Set up the SQS queue to trigger the Lambda function.
* Conclusion: This solution ensures reliable message delivery and processing with
appropriate permissions.
References
* Amazon SNS: Amazon SNS Documentation
* Amazon SQS: Amazon SQS Documentation
* AWS Lambda: AWS Lambda Documentation
QUESTION NO: 884
회사는 사용자 요청을 수집하고 요청 유형에 따라 처리를 위해 적절한 마이크로서비스로
요청을 전달하는 데 사용되는 비동기 API를 소유하고 있습니다. 이 회사는 Amazon API
Gateway를 사용하여 API 프런트 엔드를 배포하고 Amazon DynamoDB를 호출하여 사용자
요청을 처리 마이크로서비스에 전달하기 전에 저장하는 AWS Lambda 함수를 사용하고
있습니다.
회사는 예산이 허용하는 한 많은 DynamoDB 처리량을 프로비저닝했지만 여전히 가용성
문제를 겪고 있으며 사용자 요청이 손실되고 있습니다.
기존 사용자에게 영향을 주지 않고 이 문제를 해결하려면 솔루션 설계자가 무엇을 해야
합니까?
A. 서버 측 조절 한도를 사용하여 API 게이트웨이에 조절을 추가합니다.
B. DynamoDB Accelerator(DAX)와 Lambda를 사용하여 DynamoDB에 대한 쓰기를
버퍼링합니다.
C. 사용자 요청이 있는 테이블에 대해 DynamoDB에 보조 인덱스를 생성합니다.
D. Amazon Simple Queue Service(Amazon SQS) 대기열과 Lambda를 사용하여
DynamoDB에 대한 쓰기를 버퍼링합니다.
Answer: D
Explanation:
By using an SQS queue and Lambda, the solutions architect can decouple the API front end
from the processing microservices and improve the overall scalability and availability of the
system. The SQS queue acts as a buffer, allowing the API front end to continue accepting
604

IT Certification Guaranteed, The Easy Way!
user requests even if the processing microservices are experiencing high workloads or are
temporarily unavailable. The Lambda function can then retrieve requests from the SQS
queue and write them to DynamoDB, ensuring that all user requests are stored and
processed. This approach allows the company to scale the processing microservices
independently from the API front end, ensuring that the API remains available to users even
during periods of high demand.
QUESTION NO: 885
한 회사는 us-east-1 리전에서 Microsoft SQL Server 단일 AZ DB 인스턴스용 100GB Amazon
RDS를 사용하여 고객 트랜잭션을 저장합니다. 회사에는 DB 인스턴스에 대한 고가용성과
자동 복구가 필요합니다.
또한 회사는 1년에 여러 번 RDS 데이터베이스에 대한 보고서를 실행해야 합니다. 보고
프로세스로 인해 거래가 고객 계정에 게시되는 데 평소보다 오랜 시간이 걸립니다.
이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.)
A. DB 인스턴스를 단일 AZ DB 인스턴스에서 다중 AZ 배포로 수정합니다.
B. 현재 DB 인스턴스의 스냅샷을 찍습니다. 스냅샷을 다른 가용 영역의 새 RDS 배포로
복원합니다.
C. 다른 가용 영역에 DB 인스턴스의 읽기 전용 복제본을 생성합니다. 보고서에 대한 모든
요청을 읽기 전용 복제본으로 지정합니다.
D. 데이터베이스를 RDS Custom으로 마이그레이션합니다.
E. RDS Proxy를 사용하여 보고 요청을 유지 관리 기간으로 제한합니다.
Answer: A C
Explanation:
https://medium.com/awesome-cloud/aws-difference-between-multi-az-and-read-replicas-in-
amazon-rds-60fe848ef53a
QUESTION NO: 886
회사가 운영 데이터를 생성하고 회사의 연간 감사를 위해 Amazon S3 버킷에 데이터를
저장하고, 외부 컨설턴트는 S3 버킷에 저장된 연간 보고서에 액세스해야 합니다. 외부
컨설턴트는 7일 동안 보고서에 액세스해야 합니다.
회사에서는 외부 컨설턴트가 보고서에만 접근할 수 있도록 솔루션을 구현해야 합니다.
이러한 요구 사항을 충족하면서 가장 운영 효율적인 솔루션은 무엇일까요?
A. Create a new S3 bucket that is configured to host a public static website. Migrate the
operations data to the new S3 bucket. Share the S3 website URL with the external
consultant.
B. Enable public access to the S3 bucket for 7 days. Remove access to the S3 bucket when
the external consultant completes the audit.
C. Create a new 1AM user that has access to the report in the S3 bucket. Provide the access
keys to the external consultant. Revoke the access keys after 7 days.
D. Generate a presigned URL that has the required access to the location of the report on the
S3 bucket.Share the presigned URL with the external consultant.
Answer: D
Explanation:
A presigned URL allows temporary access to a specific object in an S3 bucket without
605

IT Certification Guaranteed, The Easy Way!
needing to make the bucket public or creating and managing additional IAM users. The URL
is time-limited, and permissions are granted only to the specific object (in this case, the
annual report), making it a highly secure and operationally efficient solution.
With a presigned URL, the consultant can access the report for the specified duration (7
days), after which the URL will expire automatically, removing the need for manual
intervention to revoke access.
AWS References:
* Amazon S3 Presigned URLs explain how to generate a presigned URL to grant temporary
access to S3 objects.
* Best Practices for S3 Security emphasize using presigned URLs for sharing temporary
access to S3 objects securely.
Why the other options are incorrect:
* A. Public static website: This approach involves making the S3 bucket publicly accessible,
which is unnecessary and insecure for sensitive data.
* B. Enable public access: Granting public access to the entire bucket, even temporarily, is a
security risk and violates best practices.
* C. Create an IAM user: Creating an IAM user and managing credentials is unnecessary
overhead and less secure compared to a presigned URL for this short-term need.
QUESTION NO: 887
회사에 동일한 AWS 계정에 있는 Amazon S3 버킷에 대한 읽기 액세스가 필요한 AWS
Lambda 함수가 있습니다. 가장 안전한 방식으로 이러한 요구 사항을 충족하는 솔루션은
무엇입니까?
A. S3 버킷에 대한 도로 접근 권한을 부여하는 S3 버킷 포키 적용
B. Lambda 함수에 IAM 역할을 적용합니다. 역할에 IAM 정책을 적용하여 S3 버킷에 대한 읽기
액세스 권한을 부여합니다.
C. S3 버킷에 대한 읽기 액세스에 필요한 IAM 권한을 부여하기 위해 Lambda 함수의 코다에
액세스 키와 비밀 키를 삽입합니다.
D. Lambda 함수에 IAM 역할을 적용합니다. 역할에 IAM 정책을 적용하여 모든 S3 버킷에 대한
읽기 액세스 권한을 부여합니다.
Answer: B
Explanation:
This option is the most secure because it follows the principle of least privilege and grants
only the necessary permissions to the Lambda function without exposing any credentials in
the code. The IAM role can be configured as the Lambda function's execution role and the
IAM policy can specify the S3 bucket ARN and the s3:GetObject action12. Option A is less
secure because it grants read access to any principal that has access to the S3 bucket,
which could be more than the Lambda function. Option C is less secure because it embeds
credentials in the code, which could be compromised or exposed. Option D is less secure
because it grants read access to all S3 buckets in the account, which could be more than
what the Lambda function needs.
QUESTION NO: 888
한 회사는 사용자가 사진을 업로드하고 이미지에 액자를 추가할 수 있는 이미지 분석
애플리케이션을 만들었습니다. 사용자는 이미지와 메타데이터를 업로드하여 이미지에 추가할
606

IT Certification Guaranteed, The Easy Way!
사진 프레임을 나타냅니다. 애플리케이션은 단일 Amazon EC2 인스턴스와 Amazon
DynamoDB를 사용하여 메타데이터를 저장합니다.
애플리케이션이 점점 인기를 얻고 있으며 사용자 수가 증가하고 있습니다. 회사에서는
시간대와 요일에 따라 동시접속자 수가 크게 달라질 것으로 예상하고 있다. 회사는 늘어나는
사용자 기반의 요구 사항을 충족하도록 애플리케이션을 확장할 수 있는지 확인해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. AWS Lambda를 사용하여 사진을 처리합니다. DynamoDB에 사진과 메타데이터를
저장합니다.
B. Amazon Kinesis Data Firehose를 사용하여 사진을 처리하고 사진과 메타데이터를
저장합니다.
C. AWS Lambda를 사용하여 사진을 처리합니다. Amazon S3에 사진을 저장합니다.
메타데이터를 저장하기 위해 DynamoDB를 보관합니다.
D. EC2 인스턴스 수를 3개로 늘립니다. 프로비저닝된 IOPS SSD(io2) Amazon Elastic Block
Store(Amazon EBS) 볼륨을 사용하여 사진과 메타데이터를 저장합니다.
Answer: C
Explanation:
https://www.quora.com/How-can-I-use-DynamoDB-for-storing-metadata-for-Amazon-S3-
objects This solution meets the requirements of scalability, performance, and availability.
AWS Lambda can process the photos in parallel and scale up or down automatically
depending on the demand. Amazon S3 can store the photos and metadata reliably and
durably, and provide high availability and low latency. DynamoDB can store the metadata
efficiently and provide consistent performance. This solution also reduces the cost and
complexity of managing EC2 instances and EBS volumes.
Option A is incorrect because storing the photos in DynamoDB is not a good practice, as it
can increase the storage cost and limit the throughput. Option B is incorrect because Kinesis
Data Firehose is not designed for processing photos, but for streaming data to destinations
such as S3 or Redshift. Option D is incorrect because increasing the number of EC2
instances and using Provisioned IOPS SSD volumes does not guarantee scalability, as it
depends on the load balancer and the application code. It also increases the cost and
complexity of managing the infrastructure.
References:
* https://aws.amazon.com/certification/certified-solutions-architect-professional/
* https://www.examtopics.com/discussions/amazon/view/7193-exam-aws-certified-solutions-
architect- professional-topic-1/
* https://aws.amazon.com/architecture/
QUESTION NO: 889
회사에는 Amazon RDS MySQL DB 인스턴스에서 정보를 검색하는 자격 증명이 내장된
사용자 지정 애플리케이션이 있습니다. 경영진은 최소한의 프로그래밍 노력으로
애플리케이션을 더욱 안전하게 만들어야 한다고 말합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?
A. AWS Key Management Service(AWS KMS) 고객 마스터 키(CMK)를 사용하여 키를
생성합니다. AWS KMS에서 데이터베이스 자격 증명을 로드하도록 애플리케이션을
구성합니다. 자동 키 순환을 활성화합니다.
B. 애플리케이션 사용자를 위한 RDS for MySQL 데이터베이스에 자격 증명을 생성하고 AWS
607

IT Certification Guaranteed, The Easy Way!
Secrets Manager에 자격 증명을 저장합니다. Secrets Manager에서 데이터베이스 자격
증명을 로드하도록 애플리케이션을 구성합니다. Secret Manager에서 자격 증명을 교체하는
AWS Lambda 함수를 생성합니다.
C. 애플리케이션 사용자를 위한 RDS for MySQL 데이터베이스에 자격 증명을 생성하고 AWS
Secrets Manager에 자격 증명을 저장합니다. Secrets Manager에서 데이터베이스 자격
증명을 로드하도록 애플리케이션을 구성합니다. Secrets Manager를 사용하여 MySQL용 RDS
데이터베이스에서 애플리케이션 사용자에 대한 자격 증명 교체 일정을 설정합니다.
D. 애플리케이션 사용자를 위한 RDS for MySQL 데이터베이스에 대한 자격 증명을 생성하고
AWS Systems Manager Parameter Store에 자격 증명을 저장합니다. Parameter Store에서
데이터베이스 자격 증명을 로드하도록 애플리케이션을 구성합니다. Parameter Store를
사용하여 MySQL용 RDS 데이터베이스에서 애플리케이션 사용자에 대한 자격 증명 교체
일정을 설정합니다.
Answer: C
Explanation:
https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-
automatically-with-aws- secrets-manager/
QUESTION NO: 890
솔루션 아키텍트는 회사의 기업 네트워크를 VPC에 연결하여 온프레미스에서 AWS 리소스에
액세스할 수 있도록 해야 합니다. 솔루션은 네트워크 계층과 세션 계층에서 기업 네트워크와
VPC 간의 모든 트래픽을 암호화해야 합니다. 또한 솔루션은 AWS와 온프레미스 시스템 간의
무제한 액세스를 방지하기 위한 보안 제어를 제공해야 합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. AWS Direct Connect를 구성하여 VPC에 연결합니다. 필요에 따라 AWS와 온프레미스 간
트래픽을 허용 및 거부하도록 VPC 경로 테이블을 구성합니다.
B. 정의된 회사 IP 주소 집합에서만 AWS Management Console에 액세스할 수 있도록 하는
1AM 정책을 만듭니다. 1AM 정책 및 역할을 사용하여 작업 책임에 따라 사용자 액세스를
제한합니다.
C. AWS Site-to-Site VPN을 구성하여 VPC에 연결합니다. 온프레미스에서 VPC로 트래픽을
전달하도록 경로 테이블 항목을 구성합니다. 인스턴스 보안 그룹과 네트워크 ACL을 구성하여
온프레미스에서 필요한 트래픽만 허용합니다.
D. AWS Transit Gateway를 구성하여 VPC에 연결합니다. 온프레미스에서 VPC로 트래픽을
전달하도록 경로 테이블 항목을 구성합니다. 인스턴스 보안 그룹과 네트워크 ACL을 구성하여
온프레미스에서 필요한 트래픽만 허용합니다.
Answer: C
Explanation:
This solution meets the requirements of providing encryption at both the network and session
layers while also allowing for controlled access between on-premises systems and AWS
resources.
* AWS Site-to-Site VPN: This service allows you to establish a secure and encrypted
connection between your on-premises network and AWS VPC over the internet or via AWS
Direct Connect. The VPN encrypts data at the network layer (IPsec) as it travels between the
corporate network and AWS.
* Routing and Security Controls: By configuring route table entries, you can ensure that only
the traffic intended for AWS resources is directed to the VPC. Additionally, by setting up
608

IT Certification Guaranteed, The Easy Way!
security groups and network ACLs, you can further restrict and control which traffic is allowed
to communicate with the instances within your VPC. This approach provides the necessary
security to prevent unrestricted access, aligning with the company's security policies.
* Why Not Other Options?:
* Option A (AWS Direct Connect): While Direct Connect provides a private connection, it
does not inherently provide encryption. Additional steps would be required to encrypt traffic,
and it doesn't address the session layer encryption.
* Option B (IAM policies for Console access): This option does not meet the requirement for
network-level encryption and security between the corporate network and the VPC.
* Option D (AWS Transit Gateway): Although Transit Gateway can help in managing multiple
connections, it doesn't directly provide encryption at the network layer. You would still need
to configure a VPN or use other methods for encryption.
AWS References:
* AWS Site-to-Site VPN - Overview of AWS Site-to-Site VPN capabilities, including
encryption.
* Security Groups and Network ACLs - Information on configuring security groups and
network ACLs to control traffic.
QUESTION NO: 891
회사는 기본 TTL이 0초인 Amazon CloudFront 배포를 사용하여 트래픽이 많은 정적 웹
사이트를 Amazon S3에서 호스팅하고 있습니다. 회사는 웹 사이트의 성능을 향상시키기 위해
캐싱을 구현하려고 합니다. 그러나 회사는 또한 오래된 콘텐츠가 배포 후 몇 분 이상 서비스가
제공되지 않음 이러한 요구 사항을 충족하기 위해 솔루션 설계자가 구현해야 하는 캐싱 방법
조합은 무엇입니까?
(2개를 선택하세요.)
A. CloudFront 기본 TTL을 2분으로 설정합니다.
B. S3 버킷에 기본 TTL을 2분으로 설정합니다.
C. Amazon S3의 객체에 Cache-Control 개인 지시문을 추가합니다.
D. AWS Lambda@Edge 함수를 생성하여 HTTP 응답에 Expires 헤더를 추가합니다. 최종
사용자 응답에서 실행되도록 함수를 구성합니다.
E. Amazon S3의 객체에 24시간의 Cache-Control max-age 지시문을 추가합니다. 배포 시
CloudFront 무효화를 생성하여 엣지 캐시에서 변경된 파일을 모두 지웁니다.
Answer: A E
Explanation:
* Understanding the Requirement: The company wants to improve caching to enhance
website performance while ensuring that stale content is not served for more than a few
minutes after a deployment.
* Analysis of Options:
* Set CloudFront TTL: Setting a short TTL (e.g., 2 minutes) ensures that cached content is
refreshed frequently, reducing the risk of serving stale content.
* S3 Bucket TTL: This would not control the cache duration for the CloudFront distribution.
* Cache-Control Private: This directive is for controlling caching by private caches (e.g.,
browsers) and is not applicable for CloudFront.
* Lambda@Edge: While this can add headers dynamically, it adds complexity and
operational overhead.
609

IT Certification Guaranteed, The Easy Way!
* Cache-Control max-age and CloudFront Invalidation: Setting a longer max-age for objects
ensures they are cached longer, reducing load on the origin. Invalidation ensures that
updated content is refreshed immediately after deployment.
* Best Combination of Caching Methods:
* Set the CloudFront default TTL to 2 minutes: This balances caching and freshness of
content.
* Add a Cache-Control max-age directive of 24 hours and use CloudFront invalidation: This
ensures efficient caching while providing a mechanism to clear outdated content immediately
after a deployment.
References:
* Amazon CloudFront Caching
* Invalidating Files in CloudFront
QUESTION NO: 892
솔루션 설계자는 웹사이트를 위한 고가용성 인프라를 설계해야 합니다. 이 웹 사이트는
Amazon EC2 인스턴스에서 실행되는 Windows 웹 서버로 구동됩니다. 솔루션 설계자는 수천
개의 IP 주소에서 발생하는 대규모 DDoS 공격을 완화할 수 있는 솔루션을 구현해야 합니다.
웹사이트의 다운타임은 허용되지 않습니다.
그러한 공격으로부터 웹사이트를 보호하기 위해 솔루션 설계자는 어떤 조치를 취해야
합니까? (2개를 선택하세요.)
A. AWS Shield Advanced를 사용하여 DDoS 공격을 중지합니다.
B. 공격자를 자동으로 차단하도록 Amazon GuardDuty를 구성합니다.
C. 정적 콘텐츠와 동적 콘텐츠 모두에 Amazon CloudFront를 사용하도록 웹 사이트를
구성합니다.
D. AWS Lambda 함수를 사용하여 공격자 IP 주소를 VPC 네트워크 ACL에 자동으로
추가합니다.
E. CPU 사용률이 80%로 설정된 대상 추적 조정 정책을 사용하여 Auto Scaling 그룹에서 EC2
스팟 인스턴스를 사용합니다.
Answer: A C
Explanation:
(https://aws.amazon.com/cloudfront
QUESTION NO: 893
한 도시에서는 ALB(Application Load Balancer) 뒤에 Amazon EC2 인스턴스에서 실행되는 웹
애플리케이션을 배포했습니다. 애플리케이션 사용자는 산발적인 성능을 보고했는데, 이는
무작위 IP 주소에서 발생하는 DDoS 공격과 관련된 것으로 보입니다. 도시에는 구성 변경을
최소화하고 DDoS 소스에 대한 감사 추적을 제공하는 솔루션이 필요합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. ALB에서 AWS WAF 웹 ACL을 활성화하고 알 수 없는 소스의 트래픽을 차단하는 규칙을
구성합니다.
B. Amazon Inspector를 구독하세요. AWS DDoS 대응 팀(DRT)을 참여시켜 완화 제어 기능을
서비스에 통합하십시오.
C. AWS Shield Advanced를 구독합니다. AWS DDoS 대응 팀(DRT)을 참여시켜 완화 제어
기능을 서비스에 통합하십시오.
D. 애플리케이션에 대한 Amazon CloudFront 배포를 생성하고 ALB를 오리진으로 설정합니다.
610

IT Certification Guaranteed, The Easy Way!
배포에서 AWS WAF 웹 ACL을 활성화하고 알 수 없는 소스의 트래픽을 차단하는 규칙을
구성합니다.
Answer: C
Explanation:
To protect the web application from DDoS attacks originating from random IP addresses, a
solutions architect should subscribe to AWS Shield Advanced and engage the AWS DDoS
Response Team (DRT) to integrate mitigating controls into the service. AWS Shield
Advanced is a managed service that provides protection against large and sophisticated
DDoS attacks, with access to 24/7 support and response from the DRT. The DRT can help
the city configure proactive and reactive safeguards, such as AWS WAF rules, rate-based
rules, and network ACLs, to block malicious traffic and improve the application's resilience.
The service also provides an audit trail for the DDoS sources through detailed attack reports
and Amazon CloudWatch metrics.
QUESTION NO: 894
회사에는 소프트웨어 엔지니어링에 사용되는 AWS 계정이 있습니다. AWS 계정은 한 쌍의
AWS Direct Connect 연결을 통해 회사의 온프레미스 데이터 센터에 액세스할 수 있습니다.
VPC가 아닌 모든 트래픽은 가상 프라이빗 게이트웨이로 라우팅됩니다.
개발 팀은 최근 콘솔을 통해 AWS Lambda 함수를 생성했습니다. 개발팀은 해당 기능이 회사
데이터 센터의 프라이빗 서브넷에서 실행되는 데이터베이스에 액세스할 수 있도록 허용해야
합니다.
어떤 솔루션이 이러한 요구 사항을 충족합니까?
A. 적절한 보안 그룹을 사용하여 VPC에서 실행되도록 Lambda 함수를 구성합니다.
B. AWS에서 데이터 센터로의 VPN 연결을 설정합니다. VPN을 통해 Lambda 함수의 트래픽을
라우팅합니다.
C. Lambda 함수가 Direct Connect를 통해 온프레미스 데이터 센터에 액세스할 수 있도록
VPC의 라우팅 테이블을 업데이트합니다.
D. 탄력적 IP 주소를 생성합니다. 탄력적 네트워크 인터페이스 없이 탄력적 IP 주소를 통해
트래픽을 전송하도록 Lambda 함수를 구성합니다.
Answer: A
Explanation:
https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html#vpc-managing-eni
QUESTION NO: 895
온라인 게임 회사가 여러 AWS 리전의 네트워크 로드 밸런서(NLB) 뒤에 있는 Amazon EC2
인스턴스에서 플랫폼을 호스팅합니다. NLB는 인터넷을 통해 대상에 요청을 라우팅할 수
있습니다. 이 회사는 글로벌 고객 기반의 엔드투엔드 로드 시간을 줄여 고객 플레이 경험을
개선하고자 합니다.
어떤 솔루션이 이러한 요구 사항을 충족시킬까요?
A. 각 지역에 기존 NLB를 대체하기 위해 애플리케이션 로드 밸런서(ALB)를 생성합니다. 각
지역의 ALB에 대한 대상으로 기존 EC2 인스턴스를 등록합니다.
B. 각 지역의 NLB에 동일한 가중치의 트래픽을 라우팅하도록 Amazon Route 53을
구성합니다.
C. 회사가 대규모 고객 기반을 보유하고 있는 다른 지역에 추가 NLB 및 EC2 인스턴스를
만듭니다.
611

IT Certification Guaranteed, The Easy Way!
D. AWS Global Accelerator에서 표준 가속기를 만듭니다. 기존 NLB를 대상 엔드포인트로
구성합니다.
Answer: D
Explanation:
The company wants to reduce end-to-end load time for its global customer base. AWS
Global Accelerator provides a network optimization service that reduces latency by routing
traffic to the nearest AWS edge locations, improving the user experience for globally
distributed customers.
* AWS Global Accelerator:
* Global Accelerator improves the performance of your applications by routing traffic through
AWS's global network infrastructure. This reduces the number of hops and latency compared
to using the public internet.
* By creating a standard accelerator and configuring the existing NLBs as target endpoints,
Global Accelerator ensures that traffic from users around the world is routed to the nearest
AWS edge location and then through optimized paths to the NLBs in each region. This
significantly improves end-to-end load time for global customers.
* Why Not the Other Options?:
* Option A (ALBs instead of NLBs): ALBs are designed for HTTP/HTTPS traffic and provide
layer 7 features, but they wouldn't solve the latency issue for a global customer base. The
key problem here is latency, and Global Accelerator is specifically designed to address that.
* Option B (Route 53 weighted routing): Route 53 can route traffic to different regions, but it
doesn't optimize network performance. It simply balances traffic between endpoints without
improving latency.
* Option C (Additional NLBs in more regions): This could potentially improve latency but
would require setting up infrastructure in multiple regions. Global Accelerator is a simpler and
more efficient solution that leverages AWS's existing global network.
AWS References:
* AWS Global Accelerator
By using AWS Global Accelerator with the existing NLBs, the company can optimize global
traffic routing and improve the customer experience by minimizing latency. Therefore, Option
D is the correct answer.
QUESTION NO: 896
한 회사에서 빠르게 성장하고 있는 음식 배달 서비스를 제공하고 있습니다. 성장으로 인해
회사의 주문 처리 시스템은 트래픽이 가장 많은 시간대에 확장 문제를 겪고 있습니다. 현재
아키텍처에는 다음이 포함됩니다.
* 애플리케이션에서 주문을 수집하기 위해 Amazon EC2 Auto Scaling 그룹에서 실행되는
Amazon EC2 인스턴스 그룹입니다.
* 주문을 이행하기 위해 Amazon EC2 Auto Scaling 그룹에서 실행되는 또 다른 EC2 인스턴스
그룹 주문 수집 프로세스는 빠르게 발생하지만 주문 이행 프로세스는 더 오래 걸릴 수
있습니다. 확장 이벤트로 인해 데이터가 손실되어서는 안 됩니다.
솔루션 설계자는 주문 수집 프로세스와 주문 이행 프로세스가 트래픽이 가장 많은 시간 동안
적절하게 확장될 수 있는지 확인해야 합니다. 솔루션은 회사의 AWS 리소스 활용도를
최적화해야 합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
612

IT Certification Guaranteed, The Easy Way!
A. Amazon CloudWatch 지표를 사용하여 Auto Scaling 그룹에 있는 각 인스턴스의 CPU를
모니터링합니다. 최대 워크로드 값에 따라 각 Auto Scaling 그룹의 최소 용량을 구성합니다.
B. Amazon CloudWatch 지표를 사용하여 Auto Scaling 그룹에 있는 각 인스턴스의 CPU를
모니터링합니다. 필요에 따라 추가 Auto Scaling 그룹을 생성하는 Amazon Simple Notification
Service(Amazon SNS) 주제를 호출하도록 CloudWatch 경보를 구성합니다.
C. 두 개의 Amazon Simple Queue Service(Amazon SQS) 대기열을 프로비저닝합니다.
하나는 주문 수집용이고 다른 하나는 주문 이행용입니다. 해당 대기열을 폴링하도록 EC2
인스턴스를 구성합니다. 대기열이 보내는 알림을 기반으로 Auto Scaling 그룹을 조정합니다.
D. 두 개의 Amazon Simple Queue Service(Amazon SQS) 대기열을 프로비저닝합니다.
하나는 주문 수집용이고 다른 하나는 주문 이행용입니다. 해당 대기열을 폴링하도록 EC2
인스턴스를 구성합니다. 인스턴스당 백로그 계산을 기반으로 지표를 생성합니다. 이 지표를
기반으로 Auto Scaling 그룹을 조정합니다.
Answer: D
Explanation:
The number of instances in your Auto Scaling group can be driven by how long it takes to
process a message and the acceptable amount of latency (queue delay). The solution is to
use a backlog per instance metric with the target value being the acceptable backlog per
instance to maintain.
QUESTION NO: 897
한 회사가 Amazon EC2 인스턴스에서 실행될 새로운 웹 애플리케이션을 설계하고 있습니다.
애플리케이션은 백엔드 데이터 저장을 위해 Amazon DynamoDB를 사용합니다. 애플리케이션
트래픽은 예측할 수 없습니다. T 회사에서는 데이터베이스에 대한 애플리케이션 읽기 및 쓰기
처리량이 보통에서 높을 것으로 예상합니다. 회사는 애플리케이션 트래픽에 대응하여
확장해야 합니다.
이러한 요구 사항을 가장 비용 효율적으로 충족하는 DynamoDB 테이블 구성은 무엇입니까?
A. DynamoDB 표준 테이블 클래스를 사용하여 프로비저닝된 읽기 및 쓰기로 DynamoDB를
구성합니다. DynamoDB Auto Scaling을 정의된 최대 용량으로 설정합니다.
B. DynamoDB 표준 테이블 클래스를 사용하여 온디맨드 모드로 DynamoDB를 구성합니다.
C. DynamoDB Standard Infrequent Access(DynamoDB Standard-IA) 테이블 클래스를
사용하여 프로비저닝된 읽기 및 쓰기로 DynamoDB를 구성합니다. DynamoDB Auto
Scaling을 정의된 최대 용량으로 설정합니다.
D. DynamoDB Standard Infrequent Access(DynamoDB Standard-IA) 테이블 클래스를
사용하여 온디맨드 모드로 DynamoDB를 구성합니다.
Answer: B
Explanation:
The most cost-effective DynamoDB table configuration for the web application is to configure
DynamoDB in on-demand mode by using the DynamoDB Standard table class. This
configuration will allow the company to scale in response to application traffic and pay only
for the read and write requests that the application performs on the table.
On-demand mode is a flexible billing option that can handle thousands of requests per
second without capacity planning. On-demand mode automatically adjusts the table's
capacity based on the incoming traffic, and charges only for the read and write requests that
are actually performed. On-demand mode is suitable for applications with unpredictable or
613

IT Certification Guaranteed, The Easy Way!
variable workloads, or applications that prefer the ease of paying for only what they use1.
The DynamoDB Standard table class is the default and recommended table class for most
workloads. The DynamoDB Standard table class offers lower throughput costs than the
DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA) table class, and is more
cost-effective for tables where throughput is the dominant cost. The DynamoDB Standard
table class also offers the same performance, durability, and availability as the DynamoDB
Standard-IA table class2.
The other options are not correct because they are either not cost-effective or not suitable for
the use case.
Configuring DynamoDB with provisioned read and write by using the DynamoDB Standard
table class, and setting DynamoDB auto scaling to a maximum defined capacity is not correct
because this configuration requires manual estimation and management of the table's
capacity, which adds complexity and cost to the solution. Provisioned mode is a billing option
that requires users to specify the amount of read and write capacity units for their tables, and
charges for the reserved capacity regardless of usage. Provisioned mode is suitable for
applications with predictable or stable workloads, or applications that require finer-grained
control over their capacity settings1. Configuring DynamoDB with provisioned read and write
by using the DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA) table class,
and setting DynamoDB auto scaling to a maximum defined capacity is not correct because
this configuration is not cost-effective for tables with moderate to high throughput. The
DynamoDB Standard-IA table class offers lower storage costs than the DynamoDB Standard
table class, but higher throughput costs. The DynamoDB Standard-IA table class is optimized
for tables where storage is the dominant cost, such as tables that store infrequently accessed
data2.
Configuring DynamoDB in on-demand mode by using the DynamoDB Standard-Infrequent
Access (DynamoDB Standard-IA) table class is not correct because this configuration is not
cost-effective for tables with moderate to high throughput. As mentioned above, the
DynamoDB Standard-IA table class has higher throughput costs than the DynamoDB
Standard table class, which can offset the savings from lower storage costs.
References:
* Table classes - Amazon DynamoDB
* Read/write capacity mode - Amazon DynamoDB
QUESTION NO: 898
한 회사가 AWS에서 새로운 기계 학습(ML) 모델 솔루션을 개발하고 있습니다. 모델은 시작 시
Amazon $3에서 약 1GB의 모델 데이터를 가져와 메모리에 로드하는 독립적인
마이크로서비스로 개발됩니다. 사용자는 비동기 API를 통해 모델에 액세스합니다. 사용자는
요청 또는 요청 배치를 보내고 결과를 보낼 위치를 지정할 수 있습니다.
회사는 수백 명의 사용자에게 모델을 제공합니다. 모델의 사용 패턴이 불규칙합니다. 일부
모델은 며칠 또는 몇 주 동안 사용하지 않을 수 있습니다. 다른 모델은 한 번에 수천 개의 요청
배치를 수신할 수 있습니다.
이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 디자인을 권장해야 합니까?
A. API의 요청을 Network Load Balancer(NLB)로 보냅니다. NLB에서 호출하는 AWS Lambda
함수로 모델을 배포합니다.
B. API의 요청을 Application Load Balancer(ALB)로 보냅니다. Amazon Simple Queue
614

IT Certification Guaranteed, The Easy Way!
Service(Amazon SQS) 대기열에서 읽는 Amazon Elastic Container Service(Amazon ECS)
서비스로 모델을 배포합니다. AWS App Mesh를 사용하여 SQS 대기열 크기에 따라 ECS
클러스터의 인스턴스를 확장합니다.
C. API의 요청을 Amazon Simple Queue Service(Amazon SQS) 대기열로 보냅니다. SQS
이벤트에 의해 호출되는 AWS Lambda 함수로 모델을 배포합니다. AWS Auto Scaling을
사용하여 SQS 대기열 크기에 따라 Lambda 함수의 vCPU 수를 늘립니다.
D. API의 요청을 Amazon Simple Queue Service(Amazon SQS) 대기열로 보냅니다.
대기열에서 읽는 Amazon Elastic Container Service(Amazon ECS) 서비스로 모델을
배포합니다. 대기열 크기에 따라 서비스의 클러스터와 복사본 모두에 대해 Amazon ECS에서
AWS Auto Scaling을 활성화합니다.
Answer: D
Explanation:
This answer is correct because it meets the requirements of running the ML models as
independent microservices that can handle irregular and unpredictable usage patterns. By
directing the requests from the API into an Amazon SQS queue, the company can decouple
the request processing from the model execution, and ensure that no requests are lost due to
spikes in demand. By deploying the models as Amazon ECS services that read from the
queue, the company can leverage containers to isolate and package each model as a
microservice, and fetch the model data from S3 at startup. By enabling AWS Auto Scaling on
Amazon ECS for both the cluster and copies of the service based on the queue size, the
company can automatically scale up or down the number of EC2 instances in the cluster and
the number of tasks in each service to match the demand and optimize performance.
References:
*
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcom
e.html
* https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html
* https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-ecs.html
QUESTION NO: 899
한 회사는 Amazon EC2 인스턴스에서 실행되는 RESTful 웹 서비스 애플리케이션을 사용하여
수천 개의 원격 장치에서 데이터를 수집합니다. EC2 인스턴스는 원시 데이터를 수신하고,
원시 데이터를 변환하고, 모든 데이터를 Amazon S3 버킷에 저장합니다. 원격 장치의 수는 곧
수백만 개로 늘어날 것입니다. 회사에는 운영 오버헤드를 최소화하는 확장성이 뛰어난
솔루션이 필요합니다.
이러한 요구 사항을 충족하기 위해 솔루션 아키텍트가 수행해야 하는 단계 조합9(2개 선택)
A. AWS Glue를 사용하여 Amazon S3의 원시 데이터를 처리합니다.
B. Amazon Route 53을 사용하여 트래픽을 다른 EC2 인스턴스로 라우팅합니다.
C. 증가하는 수신 데이터 양을 수용하기 위해 더 많은 EC2 인스턴스를 추가합니다.
D. 원시 데이터를 Amazon Simple Queue Service(Amazon SOS)로 보냅니다. EC2
인스턴스를 사용하여 데이터를 처리합니다.
E. Amazon API Gateway를 사용하여 원시 데이터를 Amazon Kinesis 데이터 스트림으로
보냅니다. 데이터 스트림을 소스로 사용하여 Amazon S3에 데이터를 전송하도록 Amazon
Kinesis Data Firehose를 구성합니다.
615

IT Certification Guaranteed, The Easy Way!
Answer: A E
Explanation:
"RESTful web services" => API Gateway.
"EC2 instance receives the raw data, transforms the raw data, and stores all the data in an
Amazon S3 bucket"
=> GLUE with (Extract - Transform - Load)
QUESTION NO: 900
의학 연구소에서는 새로운 연구와 관련된 데이터를 생성합니다. 연구소에서는 온프레미스
파일 기반 애플리케이션을 위해 전국의 진료소에서 대기 시간을 최소화하면서 데이터를
사용할 수 있도록 하려고 합니다. 데이터 파일은 각 클리닉에 대해 읽기 전용 권한이 있는
Amazon S3 버킷에 저장됩니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 권장해야 합니까?
A. AWS Storage Gateway 파일 게이트웨이를 각 병원의 온프레미스에 가상 머신(VM)으로
배포합니다.
B. 처리를 위해 AWS DataSync를 사용하여 파일을 각 병원의 온프레미스 애플리케이션으로
마이그레이션합니다.
C. AWS Storage Gateway 볼륨 게이트웨이를 각 병원의 온프레미스에 가상 머신(VM)으로
배포합니다.
D. Amazon Elastic File System(Amazon EFS) 파일 시스템을 각 병원의 온프레미스 서버에
연결합니다.
Answer: A
Explanation:
AWS Storage Gateway is a service that connects an on-premises software appliance with
cloud-based storage to provide seamless and secure integration between an organization's
on-premises IT environment and AWS's storage infrastructure. By deploying a file gateway
as a virtual machine on each clinic's premises, the medical research lab can provide low-
latency access to the data stored in the S3 bucket while maintaining read-only permissions
for each clinic. This solution allows the clinics to access the data files directly from their on-
premises file-based applications without the need for data transfer or migration.
QUESTION NO: 901
한 회사는 AWS 서비스와 비 AWS 서비스에서 실행되는 워크로드에 대한 AWS에 대한 API
호출을 감사하기 위한 중앙 집중형 솔루션을 설정해야 합니다. 회사는 7년 동안 감사 로그를
저장해야 합니다.
어떤 솔루션이 운영 비용을 최소화하면서 이러한 요구 사항을 충족할 수 있을까요?
A. Amazon S3에 데이터 레이크를 설정합니다. AWS CloudTrail 로그와 AWS가 아닌
서비스의 로그를 데이터 레이크에 통합합니다. CloudTrail을 사용하여 7년 동안 로그를
저장합니다.
B. AWS CloudTrail Lake에 대한 사용자 정의 통합을 구성하여 AWS 서비스 및 비 AWS
서비스에서 CloudTrail 이벤트를 수집하고 저장합니다. CloudTrail을 사용하여 7년 동안
로그를 저장합니다.
C. AWS 서비스에 대해 AWS CloudTrail을 활성화합니다. 7년 동안 로그를 저장하기 위해
CloudTrail에 AWS가 아닌 서비스를 수집합니다.
D. 새로운 Amazon CloudWatch Logs 그룹을 만듭니다. AWS가 아닌 서비스에서 CloudWatch
616

IT Certification Guaranteed, The Easy Way!
Logs 그룹으로 감사 데이터를 보냅니다. AWS에서 실행되는 워크로드에 대해 AWS
CloudTrail을 활성화합니다. CloudTrail을 사용하여 7년 동안 로그를 저장합니다.
Answer: B
Explanation:
AWS CloudTrail Lake is a fully managed service that allows the collection, storage, and
querying of CloudTrail events for both AWS and non-AWS services. CloudTrail Lake can be
customized to collect logs from various sources, ensuring a centralized audit solution. It also
supports long-term storage, so logs can be retained for 7 years, meeting the compliance
requirement.
* Option A (Data Lake): Setting up a data lake in S3 introduces unnecessary operational
complexity compared to CloudTrail Lake.
* Option C (Ingest non-AWS services into CloudTrail): CloudTrail Lake is better suited for this
task with less operational overhead.
* Option D (CloudWatch Logs): While CloudWatch can store logs, CloudTrail Lake is
specifically designed for API auditing and storage.
AWS References:
* AWS CloudTrail Lake
QUESTION NO: 902
한 회사는 회사 웹사이트에 널리 사용되는 콘텐츠 관리 시스템(CMS)을 사용합니다. 그러나
필요한 패치와 유지 관리가 부담스럽습니다. 회사는 웹사이트를 재설계하고 있으며 새로운
솔루션을 원합니다. 웹사이트는 1년에 4번 업데이트되며 동적 콘텐츠를 사용할 필요가
없습니다. 솔루션은 높은 확장성과 향상된 보안을 제공해야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 변경 사항 조합은 무엇입니까?
(2개를 선택하세요.)
A. 웹 사이트 앞에 AWS WAF 웹 ACL을 배포하여 HTTPS 기능을 제공합니다.
B. 웹 사이트 콘텐츠를 관리하고 제공하기 위한 AWS Lambda 함수를 생성 및 배포합니다.
C. 새 웹 사이트 및 Amazon S3 버킷 생성 정적 웹 사이트 호스팅이 활성화된 S3 버킷에 웹
사이트 배포
D. 새 웹사이트를 만듭니다. Application Load Balancer 뒤에 있는 Amazon EC2 인스턴스의
Auto Scaling 그룹을 사용하여 웹 사이트를 배포합니다.
Answer: A D
Explanation:
A -> We can configure CloudFront to require HTTPS from clients (enhanced security)
https://docs.aws.
amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-
cloudfront.html D -> storing static website on S3 provides scalability and less operational
overhead, then configuration of Application LB and EC2 instances (hence E is out)
QUESTION NO: 903
한 회사에는 많은 Amazon EC2 인스턴스를 사용하여 완료하는 매우 동적인 일괄 처리 작업이
있습니다. 작업은 본질적으로 상태 비저장이며 부정적인 영향 없이 언제든지 시작 및 중지될
수 있으며 일반적으로 완료하는 데 총 60분 이상이 소요됩니다. 회사는 솔루션 설계자에게
작업 요구 사항을 충족하는 확장 가능하고 비용 효율적인 솔루션을 설계하도록 요청했습니다.
솔루션 설계자는 무엇을 추천해야 합니까?
617

IT Certification Guaranteed, The Easy Way!
A. EC2 스팟 인스턴스 구현
B. EC2 예약 인스턴스 구매
C. EC2 온디맨드 인스턴스 구현
D. AWS Lambda에서 처리 구현
Answer: A
Explanation:
EC2 Spot Instances allow users to bid on spare Amazon EC2 computing capacity and can be
a cost-effective solution for stateless, interruptible workloads that can be started and stopped
at any time. Since the batch processing job is stateless, can be started and stopped at any
time, and typically takes upwards of 60 minutes to complete, EC2 Spot Instances would be a
good fit for this workload.
QUESTION NO: 904
한 회사가 AWS에서 웹 애플리케이션을 설계하고 있습니다. 애플리케이션은 회사의 기존
데이터 센터와 회사의 VPC 간에 VPN 연결을 사용합니다. 이 회사는 DNS 서비스로 Amazon
Route 53을 사용합니다. 애플리케이션은 프라이빗 DNS 레코드를 사용하여 VPC에서
온프레미스 서비스와 통신해야 합니다. 가장 안전한 방식으로 이러한 요구 사항을 충족하는
솔루션은 무엇입니까?
A. Route 53 Resolver 아웃바운드 엔드포인트를 생성합니다. 해석기 규칙을 만듭니다. 해석기
규칙을 VPC와 연결
B. Route 53 Resolver 인바운드 엔드포인트를 생성합니다. 해석기 규칙을 만듭니다. 해석기
규칙을 VPC와 연결합니다.
C. Route 53 프라이빗 호스팅 영역을 생성합니다. 프라이빗 호스팅 영역을 VPC와
연결합니다.
D. Route 53 퍼블릭 호스팅 영역을 생성합니다. 서비스 통신을 허용하려면 각 서비스에 대한
레코드를 만듭니다.
Answer: A
Explanation:
To meet the requirements of the web application in the most secure manner, the company
should create a Route 53 Resolver outbound endpoint, create a resolver rule, and associate
the resolver rule with the VPC.
This solution will allow the application to use private DNS records to communicate with the
on-premises services from a VPC. Route 53 Resolver is a service that enables DNS
resolution between on-premises networks and AWS VPCs. An outbound endpoint is a set of
IP addresses that Resolver uses to forward DNS queries from a VPC to resolvers on an on-
premises network. A resolver rule is a rule that specifies the domain names for which
Resolver forwards DNS queries to the IP addresses that you specify in the rule. By creating
an outbound endpoint and a resolver rule, and associating them with the VPC, the company
can securely resolve DNS queries for the on-premises services using private DNS records12.
The other options are not correct because they do not meet the requirements or are not
secure. Creating a Route 53 Resolver inbound endpoint, creating a resolver rule, and
associating the resolver rule with the VPC is not correct because this solution will allow DNS
queries from on-premises networks to access resources in a VPC, not vice versa. An
inbound endpoint is a set of IP addresses that Resolver uses to receive DNS queries from
618

IT Certification Guaranteed, The Easy Way!
resolvers on an on-premises network1. Creating a Route 53 private hosted zone and
associating it with the VPC is not correct because this solution will only allow DNS resolution
for resources within the VPC or other VPCs that are associated with the same hosted zone.
A private hosted zone is a container for DNS records that are only accessible from one or
more VPCs3. Creating a Route 53 public hosted zone and creating a record for each service
to allow service communication is not correct because this solution will expose the on-
premises services to the public internet, which is not secure. A public hosted zone is a
container for DNS records that are accessible from anywhere on the internet3.
References:
* Resolving DNS queries between VPCs and your network - Amazon Route 53
* Working with rules - Amazon Route 53
* Working with private hosted zones - Amazon Route 53
QUESTION NO: 905
회사의 소프트웨어 개발 팀에는 Amazon RDS 다중 AZ 클러스터가 필요합니다. RDS
클러스터는 온프레미스에 배포된 데스크톱 클라이언트의 백엔드 역할을 합니다. 데스크톱
클라이언트에는 RDS 클러스터에 대한 직접 연결이 필요합니다.
회사는 개발팀이 사무실에 있을 때 클라이언트를 사용하여 클러스터에 연결할 수 있는 기능을
제공해야 합니다.
필요한 연결을 가장 안전하게 제공하는 솔루션은 무엇입니까?
A. VPC 1개와 퍼블릭 서브넷 2개를 생성합니다. 퍼블릭 서브넷에 RDS 클러스터를
생성합니다. 회사 사무실에서 고객 게이트웨이와 함께 AWS Site-to-Site VPN을
사용하십시오.
B. VPC와 프라이빗 서브넷 2개를 생성합니다. 프라이빗 서브넷에 RDS 클러스터를
생성합니다. 회사 사무실에서 고객 게이트웨이와 함께 AWS Site-to-Site VPN을
사용하십시오.
C. VPC와 프라이빗 서브넷 2개를 생성합니다. 프라이빗 서브넷에 RDS 클러스터를
생성합니다. RDS 보안 그룹을 사용하여 회사의 사무실 IP 범위가 클러스터에 액세스하도록
허용합니다.
D. VPC 1개와 퍼블릭 서브넷 2개를 생성합니다. 퍼블릭 서브넷에 RDS 클러스터를
생성합니다. 각 개발자에 대해 클러스터 사용자를 만듭니다. RDS 보안 그룹을 사용하여
사용자가 클러스터에 액세스하도록 허용합니다.
Answer: B
Explanation:
* Requirement Analysis: Need secure, direct connectivity from an on-premises client to an
RDS cluster, accessible only when in the office.
* VPC with Private Subnets: Ensures the RDS cluster is not publicly accessible, enhancing
security.
* Site-to-Site VPN: Provides secure, encrypted connection between on-premises office and
AWS VPC.
* Implementation:
* Create a VPC with two private subnets.
* Launch the RDS cluster in the private subnets.
* Set up a Site-to-Site VPN connection with a customer gateway in the office.
* Conclusion: This setup ensures secure and direct connectivity with minimal exposure,
619

IT Certification Guaranteed, The Easy Way!
meeting the requirement for secure access from the office.
References
* AWS Site-to-Site VPN: AWS Site-to-Site VPN Documentation
* Amazon RDS: Amazon RDS Documentation
QUESTION NO: 906
솔루션 설계자는 보안 그룹이 0.0.0.0/0의 SSH를 허용하는 규칙을 포함할 수 없다고 명시하는
회사의 규정 준수 정책에 대한 자동화된 솔루션을 제공해야 합니다. 정책을 위반한 경우
회사에 통보해야 합니다. 가능한 한 빨리 해결책이 필요합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하려면 솔루션 설계자가 무엇을 해야
합니까?
A. 0.0.0.0/0 주소에 열려 있는 SSH에 대한 보안 그룹을 모니터링하고 이를 발견할 때마다
알림을 생성하는 AWS Lambda 스크립트를 작성합니다.
B. 제한적 SSH AWS Config 관리형 규칙을 활성화하고 비준수 규칙이 생성되면 Amazon
Simple Notification Service(Amazon SNS) 알림을 생성합니다.
C. 보안 그룹 및 네트워크 ACL을 전역적으로 공개할 수 있는 권한이 있는 1AM 역할을
생성합니다. 사용자가 역할을 맡을 때마다 알림을 생성하려면 Amazon Simple 알림
서비스(Amazon SNS) 주제를 생성합니다.
D. 관리자가 아닌 사용자가 보안 그룹을 생성하거나 편집하지 못하도록 방지하는 서비스 제어
정책(SCP)을 구성합니다. 사용자가 관리자 권한이 필요한 규칙을 요청할 때 티켓팅 시스템에
알림을 만듭니다.
Answer: B
Explanation:
The most suitable solution for the company's compliance policy is to enable the restricted-ssh
AWS Config managed rule and generate an Amazon Simple Notification Service (Amazon
SNS) notification when a noncompliant rule is created. This solution has the least operational
overhead because it uses a predefined rule that is already available in AWS Config, which is
a service that enables users to assess, audit, and evaluate the configurations of their AWS
resources. The restricted-ssh rule checks whether security groups that are in use have
inbound rules that allow SSH from 0.0.0.0/0 addresses, and reports them as noncompliant1.
Users can configure the rule to send notifications to an Amazon SNS topic when a
noncompliant change occurs, and subscribe to the topic to receive alerts via email, SMS, or
other methods2.
The other options are not correct because they either have more operational overhead or do
not meet the requirements. Writing an AWS Lambda script that monitors security groups for
SSH being open to 0.0.0.0/0 addresses and creates a notification every time it finds one is
not correct because it requires custom code development and maintenance, which adds
complexity and cost to the solution. Creating an IAM role with permissions to globally open
security groups and network ACLs, and creating an Amazon SNS topic to generate a
notification every time the role is assumed by a user is not correct because it does not
prevent or detect the creation of noncompliant rules by other users or roles, and it does not
address the existing rules that may violate the policy. Configuring a service control policy
(SCP) that prevents non-administrative users from creating or editing security groups, and
creating a notification in the ticketing system when a user requests a rule that needs
administrator permissions is not correct because it does not provide an automated solution
620

IT Certification Guaranteed, The Easy Way!
for the policy enforcement and notification, and it may limit the flexibility and productivity of
the users.
References:
* restricted-ssh - AWS Config
* Getting Notifications When Your Resources Change - AWS Config
QUESTION NO: 907
회사에서 산발적인 사용 패턴을 가진 웹 애플리케이션을 가지고 있습니다. 매달 초에
사용량이 많습니다. 매주 초에 사용량이 보통이고 주중에는 사용량이 예측할 수 없습니다.
애플리케이션은 데이터 내부에서 실행되는 웹 서버와 MySQL 데이터베이스 서버로
구성됩니다. 센터 회사는 애플리케이션을 AWS 클라우드로 이동하려고 하며 데이터베이스
수정이 필요하지 않은 비용 효율적인 데이터베이스 플랫폼을 선택해야 합니다. 어떤 솔루션이
이러한 요구 사항을 충족합니까?
A. Amazon DynamoDB
B. MySQL용 Amazon RDS
C. MySQL 호환 Amazon Aurora 서버리스
D. Auto Scaling 그룹의 Amazon EC2에 배포된 MySQL
Answer: C
Explanation:
Amazon RDS for MySQL is a fully-managed relational database service that makes it easy to
set up, operate, and scale MySQL deployments in the cloud. Amazon Aurora Serverless is
an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible edition),
where the database will automatically start up, shut down, and scale capacity up or down
based on your application's needs. It is a simple, cost-effective option for infrequent,
intermittent, or unpredictable workloads.
QUESTION NO: 908
회사는 기존 AWS 사용 비용을 운영 비용 대시보드에 추가하려고 합니다. 솔루션 설계자는
회사가 프로그래밍 방식으로 사용 비용에 액세스할 수 있도록 하는 솔루션을 추천해야
합니다. 회사는 올해의 비용 데이터에 액세스하고 향후 12개월 동안의 비용을 예측할 수
있어야 합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. 페이지 매김 기능이 있는 AWS Cost Explorer API를 사용하여 사용량 비용 관련 데이터에
액세스합니다.
B. 다운로드 가능한 AWS Cost Explorer 보고서 csv 파일을 사용하여 사용 비용 관련 데이터에
액세스합니다.
C. FTP를 통해 회사에 사용 비용 데이터를 전송하도록 AWS Budgets 작업을 구성합니다.
D. 사용 비용 데이터에 대한 AWS Budgets 보고서를 생성합니다. SMTP를 통해 회사에
데이터를 보냅니다.
Answer: A
Explanation:
* Understanding the Requirement: The company needs programmatic access to its AWS
usage costs for the current year and cost forecasts for the next 12 months, with minimal
operational overhead.
* Analysis of Options:
621

IT Certification Guaranteed, The Easy Way!
* AWS Cost Explorer API: Provides programmatic access to detailed usage and cost data,
including forecast costs. It supports pagination for handling large datasets, making it an
efficient solution.
* Downloadable AWS Cost Explorer report csv files: While useful, this method requires
manual handling of files and does not provide real-time access.
* AWS Budgets actions via FTP: This is less suitable as it involves setting up FTP transfers
and does not provide the same level of detail and real-time access as the API.
* AWS Budgets reports via SMTP: Similar to FTP, this method involves additional setup and
lacks the real-time access and detail provided by the API.
* Best Option for Minimal Operational Overhead:
* AWS Cost Explorer API provides direct, programmatic access to cost data, including
detailed usage and forecasting, with minimal setup and operational effort. It is the most
efficient solution for integrating cost data into an operational cost dashboard.
References:
* AWS Cost Explorer API
* AWS Cost and Usage Reports
QUESTION NO: 909
한 회사에서 수요가 갑자기 증가하고 있습니다. 회사는 Amazon 머신 이미지(AMI)에서 대규모
Amazon EC2 인스턴스를 프로비저닝해야 합니다. 인스턴스는 Auto Scaling 그룹에서
실행됩니다. 회사는 요구 사항을 충족하기 위해 최소 초기화 대기 시간을 제공하는 솔루션이
필요합니다.
이러한 요구 사항을 충족하는 솔루션은 무엇입니까?
A. aws ec2 Register-image 명령을 사용하여 스냅샷에서 AMI를 생성합니다. AWS Step
Functions를 사용하여 Auto Scaling 그룹의 AMI를 교체합니다.
B. 스냅샷에서 Amazon Elastic Block Store(Amazon EBS) 빠른 스냅샷 복원 활성화 스냅샷을
사용하여 AMI 프로비저닝 Auto Scaling 그룹의 AMI를 새 AMI로 교체
C. Amazon Data Lifecycle Manager(Amazon DLM)에서 AMI 생성을 활성화하고 수명 주기
규칙을 정의합니다. Auto Scaling 그룹에서 AMI를 수정하는 AWS Lambda 함수를
생성합니다.
D. Amazon EventBridge(Amazon CloudWatch Events)를 사용하여 AMI를 프로비저닝하는
AWS 백업 수명 주기 정책을 호출합니다. EventBridge에서 Auto Scaling 그룹 용량 제한을
이벤트 소스로 구성합니다.
Answer: B
Explanation:
Enabling Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot
allows you to quickly create a new Amazon Machine Image (AMI) from a snapshot, which
can help reduce the initialization latency when provisioning new instances. Once the AMI is
provisioned, you can replace the AMI in the Auto Scaling group with the new AMI. This will
ensure that new instances are launched from the updated AMI and are able to meet the
increased demand quickly.
QUESTION NO: 910
회사에 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있습니다. 솔루션 설계자는
회사의 현재 요구 사항에 따라 특정 인스턴스 제품군과 다양한 인스턴스 크기로 회사를
622

IT Certification Guaranteed, The Easy Way!
표준화했습니다.
회사는 향후 3년 동안 해당 애플리케이션에 대한 비용 절감을 극대화하고자 합니다. 회사는
애플리케이션 인기도와 사용량에 따라 향후 6개월 내에 인스턴스 제품군과 크기를 변경할 수
있어야 합니다. 어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까?
A. 컴퓨팅 절감 계획
B. EC2 인스턴스 절감 계획
C. 영역 예약 인스턴스
D. 표준 예약 인스턴스
Answer: A
Explanation:
* Understanding the Requirement: The company wants to maximize cost savings for their
application over the next three years, with the flexibility to change the instance family and
sizes within the next six months based on application popularity and usage.
* Analysis of Options:
* Compute Savings Plan: This plan offers the most flexibility, allowing the company to change
instance families, sizes, and regions. It applies to EC2, AWS Fargate, and AWS Lambda,
offering significant cost savings with this flexibility.
* EC2 Instance Savings Plan: This plan is less flexible than the Compute Savings Plan, as it
only applies to EC2 instances and allows changes within a specific instance family.
* Zonal Reserved Instances: These provide a discount on EC2 instances but are tied to a
specific availability zone and instance type, offering the least flexibility.
* Standard Reserved Instances: These offer discounts on EC2 instances but with more
restrictions compared to Savings Plans, particularly when changing instance types and
families.
* Best Option for Flexibility and Savings:
* The Compute Savings Plan is the most cost-effective solution because it allows the
company to maintain flexibility while still achieving significant cost savings. This is critical for
adapting to changing application demands without being locked into specific instance types
or families.
References:
* AWS Savings Plans
* EC2 Instance Types
QUESTION NO: 911
회사에서는 다음과 관련된 액세스 거부 오류 및 인증되지 않은 오류를 분석하고 문제를
해결하려고 합니다.
오전 1시 권한 회사에서 AWS CloudTrail을 활성화했습니다. 어떤 솔루션이 최소한의
노력으로 이러한 요구 사항을 충족합니까?
A. AWS Glue를 사용하고 사용자 지정 스크립트를 작성하여 CloudTrail 로그에서 오류를
쿼리합니다.
B. AWS Batch를 사용하고 사용자 지정 스크립트를 작성하여 CloudTrail 로그에서 오류를
쿼리합니다.
C. Amazon Athena 쿼리로 CloudTrail 로그를 검색하여 오류를 식별합니다.
D. Amazon QuickSight를 사용하여 CloudTrail 로그를 검색합니다. 오류를 식별하기 위해
대시보드를 만듭니다.
623

IT Certification Guaranteed, The Easy Way!
Answer: C
Explanation:
This solution meets the following requirements:
* It is the least effort, as it does not require any additional AWS services, custom scripts, or
data processing steps. Amazon Athena is a serverless interactive query service that allows
you to analyze data in Amazon S3 using standard SQL. You can use Athena to query
CloudTrail logs directly from the S3 bucket where they are stored, without any data loading or
transformation. You can also use the AWS Management Console, the AWS CLI, or the
Athena API to run and manage your queries.
* It is effective, as it allows you to filter, aggregate, and join CloudTrail log data using SQL
syntax. You can use various SQL functions and operators to specify the criteria for identifying
Access Denied and Unauthorized errors, such as the error code, the user identity, the event
source, the event name, the event time, and the resource ARN. You can also use
subqueries, views, and common table expressions to simplify and optimize your queries.
* It is flexible, as it allows you to customize and save your queries for future use. You can
also export the query results to other formats, such as CSV or JSON, or integrate them with
other AWS services, such as Amazon QuickSight, for further analysis and visualization.
References:
* Querying AWS CloudTrail Logs - Amazon Athena
* Analyzing Data in S3 using Amazon Athena | AWS Big Data Blog
* Troubleshoot IAM permisson access denied or unauthorized errors | AWS re:Post
QUESTION NO: 912
한 회사는 최근 글로벌 전자 상거래 애플리케이션을 위한 데이터 스토어로 Amazon Aurora를
사용하기 시작했습니다. 대규모 보고서가 실행될 때 개발자는 전자 상거래 애플리케이션의
성능이 좋지 않다고 보고합니다. Amazon CloudWatch에서 지표를 검토한 후 솔루션 설계자는
ReadlOPS 및 CPUUtilization 지표가 다음과 같은 경우 급증하고 있음을 발견했습니다. 월별
보고서가 실행됩니다.
가장 비용 효율적인 솔루션은 무엇입니까?
A. 월별 보고를 Amazon Redshift로 마이그레이션합니다.
B. 월별 보고를 Aurora 복제본으로 마이그레이션
C. Aurora 데이터베이스를 더 큰 인스턴스 클래스로 마이그레이션
D. Aurora 인스턴스에서 프로비저닝된 IOPS를 늘립니다.
Answer: B
Explanation:
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html
#Aurora.Replication.Replicas Aurora Replicas have two main purposes. You can issue
queries to them to scale the read operations for your application. You typically do so by
connecting to the reader endpoint of the cluster. That way, Aurora can spread the load for
read-only connections across as many Aurora Replicas as you have in the cluster. Aurora
Replicas also help to increase availability. If the writer instance in a cluster becomes
unavailable, Aurora automatically promotes one of the reader instances to take its place as
the new writer.
https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html
624

IT Certification Guaranteed, The Easy Way!
QUESTION NO: 913
한 회사는 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용하여 컨테이너
애플리케이션을 실행합니다. 회사의 워크로드가 하루 종일 일관되지 않습니다. 회사는
Amazon EKS가 워크로드에 따라 확장 및 축소하기를 원합니다.
최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? {2개를
선택하세요.)
A. AWS Lambda 함수를 사용하여 EKS 클러스터 크기 조정
B. Kubernetes Metrics Server를 사용하여 수평형 Pod 자동 확장을 활성화합니다.
C. Kubernetes Cluster Autoscaler를 사용하여 클러스터의 노드 수를 관리합니다.
D. Amazon API Gateway를 사용하여 Amazon EKS에 연결합니다.
E. AWS App Mesh를 사용하여 네트워크 활동을 관찰합니다.
Answer: B C
Explanation:
https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html
https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html
Horizontal pod autoscaling is a feature of Kubernetes that automatically scales the number of
pods in a deployment, replication controller, or replica set based on that resource's CPU
utilization. It requires a metrics source such as the Kubernetes Metrics Server to provide
CPU usage data1. Cluster autoscaling is a feature of Kubernetes that automatically adjusts
the number of nodes in a cluster when pods fail or are rescheduled onto other nodes. It
requires an integration with AWS Auto Scaling groups to manage the EC2 instances that join
the cluster2. By using both horizontal pod autoscaling and cluster autoscaling, the solution
can ensure that Amazon EKS scales in and out according to the workload.
QUESTION NO: 914
보안 팀은 팀의 모든 AWS 계정에서 특정 서비스 또는 작업에 대한 액세스를 제한하려고
합니다. 모든 계정은 AWS Organizations의 대규모 조직에 속합니다. 솔루션은 확장 가능해야
하며 권한을 유지 관리할 수 있는 단일 지점이 있어야 합니다.
이를 달성하려면 솔루션 아키텍트가 무엇을 해야 합니까?
A. 서비스 또는 작업에 대한 액세스를 제공하는 ACL을 만듭니다.
B. 계정을 허용하는 보안 그룹을 생성하고 이를 사용자 그룹에 연결합니다.
C. 서비스 또는 작업에 대한 액세스를 거부하려면 각 계정에 교차 계정 역할을 만듭니다.
D. 서비스 또는 작업에 대한 액세스를 거부하려면 루트 조직 단위에 서비스 제어 정책을
만듭니다.
Answer: D
Explanation:
Service control policies (SCPs) are one type of policy that you can use to manage your
organization. SCPs offer central control over the maximum available permissions for all
accounts in your organization, allowing you to ensure your accounts stay within your
organization's access control guidelines. See https://docs.aws.
amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html.
QUESTION NO: 915
한 회사가 Amazon RDS DB 인스턴스에서 실행되는 모든 데이터베이스에 대해 새로운 데이터
625

IT Certification Guaranteed, The Easy Way!
보존 정책을 구현하고 있습니다. 회사는 최소 2년 동안 일일 백업을 보관해야 합니다. 백업은
일관되고 복원 가능해야 합니다.
솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 솔루션을 권장해야 합니까?
A. RDS 백업을 보관하려면 AWS Backup에 백업 볼트를 생성하세요. 일일 일정과 생성 후
만료 기간이 2년인 새 백업 계획을 생성합니다. RDS DB 인스턴스를 백업 계획에 할당합니다.
B. 일일 스냅샷을 위해 RDS DB 인스턴스의 백업 기간을 구성합니다. 각 RDS DB 인스턴스에
2년의 스냅샷 보존 정책을 할당합니다. Amazon Data Lifecycle Manager(Amazon DLM)를
사용하여 스냅샷 삭제를 예약합니다.
C. 만료 기간이 2년인 Amazon CloudWatch Logs에 자동으로 백업되도록 데이터베이스
트랜잭션 로그를 구성합니다.
D. AWS Database Migration Service(AWS DMS) 복제 작업을 구성합니다. 복제 인스턴스를
배포하고 데이터베이스 변경 사항을 Amazon S3 대상으로 스트리밍하도록 변경 데이터
캡처(CDC) 작업을 구성합니다. 2년 후에 스냅샷을 삭제하도록 S3 수명 주기 정책을
구성합니다.
Answer: A
Explanation:
AWS Backup is a fully managed service that enables users to centralize and automate the
backup of data across AWS services. It can create and manage backup plans that specify
the frequency and retention period of backups. It can also assign backup resources to
backup vaults, which are containers that store backup data1.
By using AWS Backup, the solution can ensure that the RDS backups are consistent,
restorable, and retained for a minimum period of 2 years.
B: Configure a backup window for the RDS DB instances for daily snapshots. Assign a
snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle
Manager (Amazon DLM) to schedule snapshot deletions. This solution will not meet the
requirement of ensuring that the backups are consistent and restorable, as Amazon DLM is
not compatible with RDS snapshots and cannot be used to schedule snapshot deletions2.
C: Configure database transaction logs to be automatically backed up to Amazon
CloudWatch Logs with an expiration period of 2 years. This solution will not meet the
requirement of ensuring that the backups are consistent and restorable, as database
transaction logs are not sufficient to restore a database to a point in time. They only capture
the changes made to the database, not the full state of the database3.
D: Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a
replication instance, and configure a change data capture (CDC) task to stream database
changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots
after 2 years. This solution will not meet the requirement of ensuring that the backups are
consistent and restorable, as AWS DMS is a service that helps users migrate databases to
AWS, not back up databases. It also requires additional resources and configuration, such as
replication instances and CDC tasks.
Reference URL: https://docs.aws.amazon.com/aws-
backup/latest/devguide/whatisbackup.html
626